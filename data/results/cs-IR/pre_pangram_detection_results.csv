arxiv_id,paper_type,period,year,month,pangram_prediction
2001.02344,regular,pre_llm,2020,1,"{'ai_likelihood': 4.006756676567926e-06, 'text': ""Citation Recommendations Considering Content and Structural Context\n  Embedding\n\n  The number of academic papers being published is increasing exponentially in\nrecent years, and recommending adequate citations to assist researchers in\nwriting papers is a non-trivial task. Conventional approaches may not be\noptimal, as the recommended papers may already be known to the users, or be\nsolely relevant to the surrounding context but not other ideas discussed in the\nmanuscript. In this work, we propose a novel embedding algorithm DocCit2Vec,\nalong with the new concept of ``structural context'', to tackle the\naforementioned issues. The proposed approach demonstrates superior performances\nto baseline models in extensive experiments designed to simulate practical\nusage scenarios.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.00186,regular,pre_llm,2020,1,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'NeuroBoun: An inquiry-based approach for exploring scientific literature\n  -- a use case in neuroscience\n\n  Online scientific publications provide vast opportunities for researchers.\nAlas, the quantity and the rate of increase in the articles make the\nutilization of these resources very challenging. This work presents as\ninquiry-based approach to support the articulation of complex inter-related\nqueries to gain insights regarding how these subjects have been studied in\nconjunction with one another as reported in the scientific literature. For this\npurpose we introduce inquiries that represent inter-related subqueries that are\nof interest to a researcher. The inquiries are expanded to better capture the\nintent of the inquirer, from which several queries are generated that represent\nvarious juxtapositions of the subjects in consideration. The sets of queries\nare used to search repositories to yield results that reveal quantitative and\ntemporal relations among the subjects of the inquiry. A web-based tool,\nNeuroBoun, is developed as a proof of concept for medical publications found in\nPubMed. A use case related to the asymmetry of amygdala is presented to\nillustrate the potentials of the proposed approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.11021,review,pre_llm,2020,1,"{'ai_likelihood': 1.6225708855523005e-06, 'text': 'Infodemiological Study Using Google Trends on Coronavirus Epidemic in\n  Wuhan, China\n\n  The recent emergence of a new coronavirus (COVID-19) has gained a high cover\nin public media and worldwide news. The virus has caused a viral pneumonia in\ntens of thousands of people in Wuhan, a central city of China. This short paper\ngives a brief introduction on how the demand for information on this new\nepidemic is reported through Google Trends. The reported period is 31 December\n2020 to 20 March 2020. The authors draw conclusions on current infodemiological\ndata on COVID-19 using three main search keywords: coronavirus, SARS and MERS.\nTwo approaches are set. First is the worldwide perspective, second - the\nChinese one, which reveals that in China this disease in the first days was\nmore often referred to SARS then to general coronaviruses, whereas worldwide,\nsince the beginning, it is more often referred to coronaviruses.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.00802,regular,pre_llm,2020,1,"{'ai_likelihood': 1.2914339701334636e-06, 'text': ""Characterizing Reading Time on Enterprise Emails\n\n  Email is an integral part of people's work and life, enabling them to perform\nactivities such as communicating, searching, managing tasks and storing\ninformation. Modern email clients take a step forward and help improve users'\nproductivity by automatically creating reminders, tasks or responses. The act\nof reading is arguably the only activity that is in common in most -- if not\nall -- of the interactions that users have with their emails.\n  In this paper, we characterize how users read their enterprise emails, and\nreveal the various contextual factors that impact reading time. Our approach\nstarts with a reading time analysis based on the reading events from a major\nemail platform, followed by a user study to provide explanations for some\ndiscoveries. We identify multiple temporal and user contextual factors that are\ncorrelated with reading time. For instance, email reading time is correlated\nwith user devices: on desktop reading time increases through the morning and\npeaks at noon but on mobile it increases through the evening till midnight. The\nreading time is also negatively correlated with the screen size.\n  We have established the connection between user status and reading time:\nusers spend more time reading emails when they have fewer meetings and busy\nhours during the day. In addition, we find that users also reread emails across\ndevices. Among the cross-device reading events, 76% of reread emails are first\nvisited on mobile and then on desktop. Overall, our study is the first to\ncharacterize enterprise email reading time on a very large scale. The findings\nprovide insights to develop better metrics and user models for understanding\nand improving email interactions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.11369,regular,pre_llm,2020,1,"{'ai_likelihood': 9.271833631727431e-07, 'text': ""Learning to Structure Long-term Dependence for Sequential Recommendation\n\n  Sequential recommendation recommends items based on sequences of users'\nhistorical actions. The key challenge in it is how to effectively model the\ninfluence from distant actions to the action to be predicted, i.e., recognizing\nthe long-term dependence structure; and it remains an underexplored problem. To\nbetter model the long-term dependence structure, we propose a GatedLongRec\nsolution in this work. To account for the long-term dependence, GatedLongRec\nextracts distant actions of top-$k$ related categories to the user's ongoing\nintent with a top-$k$ gating network, and utilizes a long-term encoder to\nencode the transition patterns among these identified actions. As user intent\nis not directly observable, we take advantage of available side-information\nabout the actions, i.e., the category of their associated items, to infer the\nintents. End-to-end training is performed to estimate the intent representation\nand predict the next action for sequential recommendation. Extensive\nexperiments on two large datasets show that the proposed solution can recognize\nthe structure of long-term dependence, thus greatly improving the sequential\nrecommendation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.05357,regular,pre_llm,2020,1,"{'ai_likelihood': 4.0398703681098095e-06, 'text': 'DSR: A Collection for the Evaluation of Graded Disease-Symptom Relations\n\n  The effective extraction of ranked disease-symptom relationships is a\ncritical component in various medical tasks, including computer-assisted\nmedical diagnosis or the discovery of unexpected associations between diseases.\nWhile existing disease-symptom relationship extraction methods are used as the\nfoundation in the various medical tasks, no collection is available to\nsystematically evaluate the performance of such methods. In this paper, we\nintroduce the Disease-Symptom Relation collection (DSR-collection), created by\nfive fully trained physicians as expert annotators. We provide graded symptom\njudgments for diseases by differentiating between ""symptoms"" and ""primary\nsymptoms"". Further, we provide several strong baselines, based on the methods\nused in previous studies. The first method is based on word embeddings, and the\nsecond on co-occurrences of keywords in medical articles. For the co-occurrence\nmethod, we propose an adaption in which not only keywords are considered, but\nalso the full text of medical articles. The evaluation on the DSR-collection\nshows the effectiveness of the proposed adaption in terms of nDCG, precision,\nand recall.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.09897,regular,pre_llm,2020,1,"{'ai_likelihood': 1.357661353217231e-06, 'text': 'CAHPHF: Context-Aware Hierarchical QoS Prediction with Hybrid Filtering\n\n  With the proliferation of Internet-of-Things and continuous growth in the\nnumber of web services at the Internet-scale, the service recommendation is\nbecoming a challenge nowadays. One of the prime aspects influencing the service\nrecommendation is the Quality-of-Service (QoS) parameter, which depicts the\nperformance of a web service. In general, the service provider furnishes the\nvalue of the QoS parameters during service deployment. However, in reality, the\nQoS values of service vary across different users, time, locations, etc.\nTherefore, estimating the QoS value of service before its execution is an\nimportant task, and thus the QoS prediction has gained significant research\nattention. Multiple approaches are available in the literature for predicting\nservice QoS. However, these approaches are yet to reach the desired accuracy\nlevel. In this paper, we study the QoS prediction problem across different\nusers, and propose a novel solution by taking into account the contextual\ninformation of both services and users. Our proposal includes two key steps:\n(a) hybrid filtering and (b) hierarchical prediction mechanism. On the one\nhand, the hybrid filtering method aims to obtain a set of similar users and\nservices, given a target user and a service. On the other hand, the goal of the\nhierarchical prediction mechanism is to estimate the QoS value accurately by\nleveraging hierarchical neural-regression. We evaluate our framework on the\npublicly available WS-DREAM datasets. The experimental results show the\noutperformance of our framework over the major state-of-the-art approaches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.04719,regular,pre_llm,2020,1,"{'ai_likelihood': 5.894237094455296e-06, 'text': 'Semi-automatic methods for adding words to the dictionary of VepKar\n  corpus based on inflectional rules extracted from Wiktionary\n\n  The article describes a technique for using English Wiktionary inflection\ntables for generating word forms for Veps verbs and nominals in the Open corpus\nof Veps and Karelian languages. The information concerning Karelian and Veps\nWiktionary entries with inflection tables is given. The operating principle of\nthe Wiktionary static and dynamic templates is explained with the use of the\njogi (river) dictionary entry as an example. The method of constructing the\ninflection table in the dictionary of the VepKar corpus according to the data\nof the dynamic template of the English Wiktionary is presented.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.11358,regular,pre_llm,2020,1,"{'ai_likelihood': 2.053048875596788e-06, 'text': ""Correcting for Selection Bias in Learning-to-rank Systems\n\n  Click data collected by modern recommendation systems are an important source\nof observational data that can be utilized to train learning-to-rank (LTR)\nsystems. However, these data suffer from a number of biases that can result in\npoor performance for LTR systems. Recent methods for bias correction in such\nsystems mostly focus on position bias, the fact that higher ranked results\n(e.g., top search engine results) are more likely to be clicked even if they\nare not the most relevant results given a user's query. Less attention has been\npaid to correcting for selection bias, which occurs because clicked documents\nare reflective of what documents have been shown to the user in the first\nplace. Here, we propose new counterfactual approaches which adapt Heckman's\ntwo-stage method and accounts for selection and position bias in LTR systems.\nOur empirical evaluation shows that our proposed methods are much more robust\nto noise and have better accuracy compared to existing unbiased LTR algorithms,\nespecially when there is moderate to no position bias.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.07075,regular,pre_llm,2020,1,"{'ai_likelihood': 8.609559800889757e-07, 'text': ""Quantum-like Structure in Multidimensional Relevance Judgements\n\n  A large number of studies in cognitive science have revealed that\nprobabilistic outcomes of certain human decisions do not agree with the axioms\nof classical probability theory. The field of Quantum Cognition provides an\nalternative probabilistic model to explain such paradoxical findings. It posits\nthat cognitive systems have an underlying quantum-like structure, especially in\ndecision-making under uncertainty. In this paper, we hypothesise that relevance\njudgement, being a multidimensional, cognitive concept, can be used to probe\nthe quantum-like structure for modelling users' cognitive states in information\nseeking. Extending from an experiment protocol inspired by the Stern-Gerlach\nexperiment in Quantum Physics, we design a crowd-sourced user study to show\nviolation of the Kolmogorovian probability axioms as a proof of the\nquantum-like structure, and provide a comparison between a quantum\nprobabilistic model and a Bayesian model for predictions of relevance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.0744,regular,pre_llm,2020,1,"{'ai_likelihood': 6.059805552164714e-06, 'text': 'Hybrid Semantic Recommender System for Chemical Compounds\n\n  Recommending Chemical Compounds of interest to a particular researcher is a\npoorly explored field. The few existent datasets with information about the\npreferences of the researchers use implicit feedback. The lack of Recommender\nSystems in this particular field presents a challenge for the development of\nnew recommendations models. In this work, we propose a Hybrid recommender model\nfor recommending Chemical Compounds. The model integrates\ncollaborative-filtering algorithms for implicit feedback (Alternating Least\nSquares (ALS) and Bayesian Personalized Ranking(BPR)) and semantic similarity\nbetween the Chemical Compounds in the ChEBI ontology (ONTO). We evaluated the\nmodel in an implicit dataset of Chemical Compounds, CheRM. The Hybrid model was\nable to improve the results of state-of-the-art collaborative-filtering\nalgorithms, especially for Mean Reciprocal Rank, with an increase of 6.7% when\ncomparing the collaborative-filtering ALS and the Hybrid ALS_ONTO.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.10382,regular,pre_llm,2020,1,"{'ai_likelihood': 1.0596381293402778e-06, 'text': ""Selective Weak Supervision for Neural Information Retrieval\n\n  This paper democratizes neural information retrieval to scenarios where large\nscale relevance training signals are not available. We revisit the classic IR\nintuition that anchor-document relations approximate query-document relevance\nand propose a reinforcement weak supervision selection method, ReInfoSelect,\nwhich learns to select anchor-document pairs that best weakly supervise the\nneural ranker (action), using the ranking performance on a handful of relevance\nlabels as the reward. Iteratively, for a batch of anchor-document pairs,\nReInfoSelect back propagates the gradients through the neural ranker, gathers\nits NDCG reward, and optimizes the data selection network using policy\ngradients, until the neural ranker's performance peaks on target relevance\nmetrics (convergence). In our experiments on three TREC benchmarks, neural\nrankers trained by ReInfoSelect, with only publicly available anchor data,\nsignificantly outperform feature-based learning to rank methods and match the\neffectiveness of neural rankers trained with private commercial search logs.\nOur analyses show that ReInfoSelect effectively selects weak supervision\nsignals based on the stage of the neural ranker training, and intuitively picks\nanchor-document pairs similar to query-document pairs.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.10781,regular,pre_llm,2020,1,"{'ai_likelihood': 4.569689432779948e-06, 'text': 'Aspect-based Academic Search using Domain-specific KB\n\n  Academic search engines allow scientists to explore related work relevant to\na given query. Often, the user is also aware of the ""aspect"" to retrieve a\nrelevant document. In such cases, existing search engines can be used by\nexpanding the query with terms describing that aspect. However, this approach\ndoes not guarantee good results since plain keyword matches do not always imply\nrelevance. To address this issue, we define and solve a novel academic search\ntask, called ""aspect-based retrieval"", which allows the user to specify the\naspect along with the query to retrieve a ranked list of relevant documents.\nThe primary idea is to estimate a language model for the aspect as well as the\nquery using a domain-specific knowledge base and use a mixture of the two to\ndetermine the relevance of the article. Our evaluation of the results over the\nOpen Research Corpus dataset shows that our method outperforms keyword-based\nexpansion of query with aspect with and without relevance feedback.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.0691,review,pre_llm,2020,1,"{'ai_likelihood': 2.615981631808811e-06, 'text': 'Common Conversational Community Prototype: Scholarly Conversational\n  Assistant\n\n  This paper discusses the potential for creating academic resources (tools,\ndata, and evaluation approaches) to support research in conversational search,\nby focusing on realistic information needs and conversational interactions.\nSpecifically, we propose to develop and operate a prototype conversational\nsearch system for scholarly activities. This Scholarly Conversational Assistant\nwould serve as a useful tool, a means to create datasets, and a platform for\nrunning evaluation challenges by groups across the community. This article\nresults from discussions of a working group at Dagstuhl Seminar 19461 on\nConversational Search.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.00267,regular,pre_llm,2020,1,"{'ai_likelihood': 9.569856855604385e-06, 'text': ""Multi-Graph Convolution Collaborative Filtering\n\n  Personalized recommendation is ubiquitous, playing an important role in many\nonline services. Substantial research has been dedicated to learning vector\nrepresentations of users and items with the goal of predicting a user's\npreference for an item based on the similarity of the representations.\nTechniques range from classic matrix factorization to more recent deep learning\nbased methods. However, we argue that existing methods do not make full use of\nthe information that is available from user-item interaction data and the\nsimilarities between user pairs and item pairs. In this work, we develop a\ngraph convolution-based recommendation framework, named Multi-Graph Convolution\nCollaborative Filtering (Multi-GCCF), which explicitly incorporates multiple\ngraphs in the embedding learning process. Multi-GCCF not only expressively\nmodels the high-order information via a partite user-item interaction graph,\nbut also integrates the proximal information by building and processing\nuser-user and item-item graphs. Furthermore, we consider the intrinsic\ndifference between user nodes and item nodes when performing graph convolution\non the bipartite graph. We conduct extensive experiments on four publicly\naccessible benchmarks, showing significant improvements relative to several\nstate-of-the-art collaborative filtering and graph neural network-based\nrecommendation models. Further experiments quantitatively verify the\neffectiveness of each component of our proposed model and demonstrate that the\nlearned embeddings capture the important relationship structure.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.05917,regular,pre_llm,2020,1,"{'ai_likelihood': 1.8212530348036025e-06, 'text': ""Assigning credit to scientific datasets using article citation networks\n\n  A citation is a well-established mechanism for connecting scientific\nartifacts. Citation networks are used by citation analysis for a variety of\nreasons, prominently to give credit to scientists' work. However, because of\ncurrent citation practices, scientists tend to cite only publications, leaving\nout other types of artifacts such as datasets. Datasets then do not get\nappropriate credit even though they are increasingly reused and experimented\nwith. We develop a network flow measure, called DataRank, aimed at solving this\ngap. DataRank assigns a relative value to each node in the network based on how\ncitations flow through the graph, differentiating publication and dataset flow\nrates. We evaluate the quality of DataRank by estimating its accuracy at\npredicting the usage of real datasets: web visits to GenBank and downloads of\nFigshare datasets. We show that DataRank is better at predicting this usage\ncompared to alternatives while offering additional interpretable outcomes. We\ndiscuss improvements to citation behavior and algorithms to properly track and\nassign credit to datasets.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.07139,regular,pre_llm,2020,1,"{'ai_likelihood': 3.642506069607205e-06, 'text': 'BiOnt: Deep Learning using Multiple Biomedical Ontologies for Relation\n  Extraction\n\n  Successful biomedical relation extraction can provide evidence to researchers\nand clinicians about possible unknown associations between biomedical entities,\nadvancing the current knowledge we have about those entities and their inherent\nmechanisms. Most biomedical relation extraction systems do not resort to\nexternal sources of knowledge, such as domain-specific ontologies. However,\nusing deep learning methods, along with biomedical ontologies, has been\nrecently shown to effectively advance the biomedical relation extraction field.\nTo perform relation extraction, our deep learning system, BiOnt, employs four\ntypes of biomedical ontologies, namely, the Gene Ontology, the Human Phenotype\nOntology, the Human Disease Ontology, and the Chemical Entities of Biological\nInterest, regarding gene-products, phenotypes, diseases, and chemical\ncompounds, respectively. We tested our system with three data sets that\nrepresent three different types of relations of biomedical entities. BiOnt\nachieved, in F-score, an improvement of 4.93 percentage points for drug-drug\ninteractions (DDI corpus), 4.99 percentage points for phenotype-gene relations\n(PGR corpus), and 2.21 percentage points for chemical-induced disease relations\n(BC5CDR corpus), relatively to the state-of-the-art. The code supporting this\nsystem is available at https://github.com/lasigeBioTM/BiOnt.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.08085,regular,pre_llm,2020,1,"{'ai_likelihood': 3.1789143880208335e-06, 'text': 'Experiments on Manual Thesaurus based Query Expansion for Ad-hoc\n  Monolingual Gujarati Information Retrieval Tasks\n\n  In this paper, we present the experimental work done on Query Expansion (QE)\nfor retrieval tasks of Gujarati text documents. In information retrieval, it is\nvery difficult to estimate the exact user need, query expansion adds terms to\nthe original query, which provides more information about the user need. There\nare various approaches to query expansion. In our work, manual thesaurus based\nquery expansion was performed to evaluate the performance of widely used\ninformation retrieval models for Gujarati text documents. Results show that\nquery expansion improves the recall of text documents.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.12021,review,pre_llm,2020,2,"{'ai_likelihood': 7.020102606879341e-06, 'text': 'Dataset Search In Biodiversity Research: Do Metadata In Data\n  Repositories Reflect Scholarly Information Needs?\n\n  The increasing amount of research data provides the opportunity to link and\nintegrate data to create novel hypotheses, to repeat experiments or to compare\nrecent data to data collected at a different time or place. However, recent\nstudies have shown that retrieving relevant data for data reuse is a\ntime-consuming task in daily research practice. In this study, we explore what\nhampers dataset retrieval in biodiversity research, a field that produces a\nlarge amount of heterogeneous data. We analyze the primary source in dataset\nsearch - metadata - and determine if they reflect scholarly search interests.\nWe examine if metadata standards provide elements corresponding to search\ninterests, we inspect if selected data repositories use metadata standards\nrepresenting scholarly interests, and we determine how many fields of the\nmetadata standards used are filled. To determine search interests in\nbiodiversity research, we gathered 169 questions that researchers aimed to\nanswer with the help of retrieved data, identified biological entities and\ngrouped them into 13 categories. Our findings indicate that environments,\nmaterials and chemicals, species, biological and chemical processes, locations,\ndata parameters and data types are important search interests in biodiversity\nresearch. The comparison with existing metadata standards shows that\ndomain-specific standards cover search interests quite well, whereas general\nstandards do not explicitly contain elements that reflect search interests. We\ninspect metadata from five large data repositories. Our results confirm that\nmetadata currently poorly reflect search interests in biodiversity research.\nFrom these findings, we derive recommendations for researchers and data\nrepositories how to bridge the gap between search interest and metadata\nprovided.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.085,regular,pre_llm,2020,2,"{'ai_likelihood': 2.4504131740993925e-06, 'text': 'Processing topical queries on images of historical newspaper pages\n\n  Historical newspapers are a source of research for the human and social\nsciences. However, these image collections are difficult to read by machine due\nto the low quality of the print, the lack of standardization of the pages in\naddition to the low quality photograph of some files. This paper presents the\nprocessing model of a topic navigation system in historical newspaper page\nimages. The general procedure consists of four modules which are: segmentation\nof text sub-images and text extraction, preprocessing and representation,\ninduced topic extraction and representation, and document viewing and retrieval\ninterface. The algorithmic and technological approaches of each module are\ndescribed and the initial test results about a collection covering a range of\n28 years are presented.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.00207,review,pre_llm,2020,2,"{'ai_likelihood': 4.470348358154297e-06, 'text': 'Web Table Extraction, Retrieval and Augmentation: A Survey\n\n  Tables are a powerful and popular tool for organizing and manipulating data.\nA vast number of tables can be found on the Web, which represents a valuable\nknowledge resource. The objective of this survey is to synthesize and present\ntwo decades of research on web tables. In particular, we organize existing\nliterature into six main categories of information access tasks: table\nextraction, table interpretation, table search, question answering, knowledge\nbase augmentation, and table augmentation. For each of these tasks, we identify\nand describe seminal approaches, present relevant resources, and point out\ninterdependencies among the different tasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.0853,regular,pre_llm,2020,2,"{'ai_likelihood': 3.543164994981554e-06, 'text': 'Learning Multi-granular Quantized Embeddings for Large-Vocab Categorical\n  Features in Recommender Systems\n\n  Recommender system models often represent various sparse features like users,\nitems, and categorical features via embeddings. A standard approach is to map\neach unique feature value to an embedding vector. The size of the produced\nembedding table grows linearly with the size of the vocabulary. Therefore, a\nlarge vocabulary inevitably leads to a gigantic embedding table, creating two\nsevere problems: (i) making model serving intractable in resource-constrained\nenvironments; (ii) causing overfitting problems. In this paper, we seek to\nlearn highly compact embeddings for large-vocab sparse features in recommender\nsystems (recsys). First, we show that the novel Differentiable Product\nQuantization (DPQ) approach can generalize to recsys problems. In addition, to\nbetter handle the power-law data distribution commonly seen in recsys, we\npropose a Multi-Granular Quantized Embeddings (MGQE) technique which learns\nmore compact embeddings for infrequent items. We seek to provide a new angle to\nimprove recommendation performance with compact model sizes. Extensive\nexperiments on three recommendation tasks and two datasets show that we can\nachieve on par or better performance, with only ~20% of the original model\nsize.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.00807,regular,pre_llm,2020,2,"{'ai_likelihood': 4.635916815863716e-07, 'text': 'Fake Review Detection Using Behavioral and Contextual Features\n\n  User reviews reflect significant value of product in the world of e-market.\nMany firms or product providers hire spammers for misleading new customers by\nposting spam reviews. There are three types of fake reviews, untruthful\nreviews, brand reviews and non-reviews. All three types mislead the new\ncustomers. A multinomial organization ""Yelp"" is separating fake reviews from\nnon-fake reviews since last decade. However, there are many e-commerce sites\nwhich do not filter fake and non-fake reviews separately. Automatic fake review\ndetection is focused by researcher for last ten years. Many approaches and\nfeature set are proposed for improving classification model of fake review\ndetection. There are two types of dataset commonly used in this research area:\npsuedo fake and real life reviews. Literature reports low performance of\nclassification model real life dataset if compared with pseudo fake reviews.\nAfter investigation behavioral and contextual features are proved important for\nfake review detection Our research has exploited important behavioral feature\nof reviewer named as ""reviewer deviation"". Our study comprises of investigating\nreviewer deviation with other contextual and behavioral features. We\nempirically proved importance of selected feature set for classification model\nto identify fake reviews. We ranked features in selected feature set where\nreviewer deviation achieved ninth rank. To assess the viability of selected\nfeature set we scaled dataset and concluded that scaling dataset can improve\nrecall as well as accuracy. Our selected feature set contains a contextual\nfeature which capture text similarity between reviews of a reviewer. We\nexperimented on NNC, LTC and BM25 term weighting schemes for calculating text\nsimilarity of reviews. We report that BM25 outperformed other term weighting\nscheme.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.02375,regular,pre_llm,2020,2,"{'ai_likelihood': 5.099508497450087e-06, 'text': 'Generating Biomedical Question Answering Corpora from Q&A forums\n\n  Question Answering (QA) is a natural language processing task that aims at\nobtaining relevant answers to user questions. While some progress has been made\nin this area, biomedical questions are still a challenge to most QA approaches,\ndue to the complexity of the domain and limited availability of training sets.\nWe present a method to automatically extract question-article pairs from Q\\&A\nweb forums, which can be used for document retrieval, a crucial step of most QA\nsystems. The proposed framework extracts from selected forums the questions and\nthe respective answers that contain citations. This way, QA systems based on\ndocument retrieval can be developed and evaluated using the question-article\npairs annotated by users of these forums. We generated the BiQA corpus by\napplying our framework to three forums, obtaining 7,453 questions and 14,239\nquestion-article pairs. We evaluated how the number of articles associated with\neach question and the number of votes on each answer affects the performance of\nbaseline document retrieval approaches. Also, we demonstrated that the articles\ngiven as answers are significantly similar to the questions and trained a\nstate-of-the-art deep learning model that obtained similar performance to using\na dataset manually annotated by experts. The proposed framework can be used to\nupdate the BiQA corpus from the same forums as new posts are made, and from\nother forums that support their answers with documents. The BiQA corpus and the\nframework used to generate it are available at\n\\url{https://github.com/lasigeBioTM/BiQA}.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.05653,regular,pre_llm,2020,2,"{'ai_likelihood': 4.635916815863716e-06, 'text': 'An Ontology-driven Treatment Article Retrieval System for Precision\n  Oncology\n\n  This paper presents an ontology-driven treatment article retrieval system\ndeveloped and experimented using the data and ground truths provided by the\nTREC 2017 precision medicine track. The key aspects of our system include:\nmeaningful integration of various disease, gene, and drug name ontologies,\ntraining of a novel perceptron model for article relevance labeling, a ranking\nmodule that considers additional factors such as journal impact and article\npublication year, and comprehensive query matching rules. Experimental results\ndemonstrate that our proposed system considerably outperforms the results of\nthe best participating system of the TREC 2017 precision medicine challenge.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.11252,regular,pre_llm,2020,2,"{'ai_likelihood': 4.6690305074055995e-06, 'text': 'AutoEmb: Automated Embedding Dimensionality Search in Streaming\n  Recommendations\n\n  Deep learning based recommender systems (DLRSs) often have embedding layers,\nwhich are utilized to lessen the dimensionality of categorical variables (e.g.\nuser/item identifiers) and meaningfully transform them in the low-dimensional\nspace. The majority of existing DLRSs empirically pre-define a fixed and\nunified dimension for all user/item embeddings. It is evident from recent\nresearches that different embedding sizes are highly desired for different\nusers/items according to their popularity. However, manually selecting\nembedding sizes in recommender systems can be very challenging due to the large\nnumber of users/items and the dynamic nature of their popularity. Thus, in this\npaper, we propose an AutoML based end-to-end framework (AutoEmb), which can\nenable various embedding dimensions according to the popularity in an automated\nand dynamic manner. To be specific, we first enhance a typical DLRS to allow\nvarious embedding dimensions; then we propose an end-to-end differentiable\nframework that can automatically select different embedding dimensions\naccording to user/item popularity; finally we propose an AutoML based\noptimization algorithm in a streaming recommendation setting. The experimental\nresults based on widely used benchmark datasets demonstrate the effectiveness\nof the AutoEmb framework.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.00107,regular,pre_llm,2020,2,"{'ai_likelihood': 8.940696716308594e-07, 'text': 'NewsStand CoronaViz: A Map Query Interface for Spatio-Temporal and\n  Spatio-Textual Monitoring of Disease Spread\n\n  With the rapid continuing spread of COVID-19, it is clearly important to be\nable to track the progress of the virus over time in order to be better\nprepared to anticipate its emergence and spread in new regions as well as\ndeclines in its presence in regions thereby leading to or justifying\n""reopening"" decisions. There are many applications and web sites that monitor\nofficially released numbers of cases which are likely to be the most accurate\nmethods for tracking the progress of the virus; however, they will not\nnecessarily paint a complete picture. To begin filling any gaps in official\nreports, we have developed the NewsStand CoronaViz web application\n(https://coronaviz.umiacs.io) that can run on desktops and mobile devices that\nallows users to explore the geographic spread in discussions about the virus\nthrough analysis of keyword prevalence in geotagged news articles and tweets in\nrelation to the real spread of the virus as measured by confirmed case numbers\nreported by the appropriate authorities. NewsStand CoronaViz users have access\nto dynamic variants of the disease-related variables corresponding to the\nnumbers of confirmed cases, active cases, deaths, and recoveries (where they\nare provided) via a map query interface. It has the ability to step forward and\nbackward in time using both a variety of temporal window sizes (day, week,\nmonth, or combinations thereof) in addition to user-defined varying spatial\nwindow sizes specified by direct manipulation actions (e.g., pan, zoom, and\nhover) as well as textually (e.g., by the name of the containing country, state\nor province, or county as well as textually-specified spatially-adjacent\ncombinations thereof), and finally by the amount of spatio-temporally-varying\nnews and tweet volume involving COVID-19.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.11844,regular,pre_llm,2020,2,"{'ai_likelihood': 2.284844716389974e-06, 'text': 'The hypergeometric test performs comparably to TF-IDF on standard text\n  analysis tasks\n\n  Term frequency-inverse document frequency, or TF-IDF for short, and its many\nvariants form a class of term weighting functions the members of which are\nwidely used in text analysis applications. While TF-IDF was originally proposed\nas a heuristic, theoretical justifications grounded in information theory,\nprobability, and the divergence from randomness paradigm have been advanced. In\nthis work, we present an empirical study showing that TF-IDF corresponds very\nnearly with the hypergeometric test of statistical significance on selected\nreal-data document retrieval, summarization, and classification tasks. These\nfindings suggest that a fundamental mathematical connection between TF-IDF and\nthe negative logarithm of the hypergeometric test P-value (i.e., a\nhypergeometric distribution tail probability) remains to be elucidated. We\nadvance the empirical analyses herein as a first step toward explaining the\nlong-standing effectiveness of TF-IDF from a statistical significance testing\nlens. It is our aspiration that these results will open the door to the\nsystematic evaluation of significance testing derived term weighting functions\nin text analysis applications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.00097,regular,pre_llm,2020,2,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'Jointly Learning to Recommend and Advertise\n\n  Online recommendation and advertising are two major income channels for\nonline recommendation platforms (e.g. e-commerce and news feed site). However,\nmost platforms optimize recommending and advertising strategies by different\nteams separately via different techniques, which may lead to suboptimal overall\nperformances. To this end, in this paper, we propose a novel two-level\nreinforcement learning framework to jointly optimize the recommending and\nadvertising strategies, where the first level generates a list of\nrecommendations to optimize user experience in the long run; then the second\nlevel inserts ads into the recommendation list that can balance the immediate\nadvertising revenue from advertisers and the negative influence of ads on\nlong-term user experience. To be specific, the first level tackles high\ncombinatorial action space problem that selects a subset items from the large\nitem space; while the second level determines three internally related tasks,\ni.e., (i) whether to insert an ad, and if yes, (ii) the optimal ad and (iii)\nthe optimal location to insert. The experimental results based on real-world\ndata demonstrate the effectiveness of the proposed framework. We have released\nthe implementation code to ease reproductivity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.03124,regular,pre_llm,2020,2,"{'ai_likelihood': 1.817941665649414e-05, 'text': 'Predict your Click-out: Modeling User-Item Interactions and Session\n  Actions in an Ensemble Learning Fashion\n\n  This paper describes the solution of the POLINKS team to the RecSys Challenge\n2019 that focuses on the task of predicting the last click-out in a\nsession-based interaction. We propose an ensemble approach comprising a matrix\nfactorization for modeling the interaction user-item, and a session-aware\nlearning model implemented with a recurrent neural network. This method appears\nto be effective in predicting the last click-out scoring a 0.60277 of Mean\nReciprocal Rank on the local test set.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.07993,regular,pre_llm,2020,2,"{'ai_likelihood': 6.854534149169922e-06, 'text': ""Beyond Clicks: Modeling Multi-Relational Item Graph for Session-Based\n  Target Behavior Prediction\n\n  Session-based target behavior prediction aims to predict the next item to be\ninteracted with specific behavior types (e.g., clicking). Although existing\nmethods for session-based behavior prediction leverage powerful representation\nlearning approaches to encode items' sequential relevance in a low-dimensional\nspace, they suffer from several limitations. Firstly, they focus on only\nutilizing the same type of user behavior for prediction, but ignore the\npotential of taking other behavior data as auxiliary information. This is\nparticularly crucial when the target behavior is sparse but important (e.g.,\nbuying or sharing an item). Secondly, item-to-item relations are modeled\nseparately and locally in one behavior sequence, and they lack a principled way\nto globally encode these relations more effectively. To overcome these\nlimitations, we propose a novel Multi-relational Graph Neural Network model for\nSession-based target behavior Prediction, namely MGNN-SPred for short.\nSpecifically, we build a Multi-Relational Item Graph (MRIG) based on all\nbehavior sequences from all sessions, involving target and auxiliary behavior\ntypes. Based on MRIG, MGNN-SPred learns global item-to-item relations and\nfurther obtains user preferences w.r.t. current target and auxiliary behavior\nsequences, respectively. In the end, MGNN-SPred leverages a gating mechanism to\nadaptively fuse user representations for predicting next item interacted with\ntarget behavior. The extensive experiments on two real-world datasets\ndemonstrate the superiority of MGNN-SPred by comparing with state-of-the-art\nsession-based prediction methods, validating the benefits of leveraging\nauxiliary behavior and learning item-to-item relations over MRIG.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.01554,regular,pre_llm,2020,2,"{'ai_likelihood': 1.8543667263454863e-06, 'text': 'Relaxed N-Pairs Loss for Context-Aware Recommendations of Television\n  Content\n\n  This paper studies context-aware recommendations in the television domain by\nproposing a deep learning-based method for learning joint context-content\nembeddings (JCCE). The method builds on recent developments within\nrecommendations using latent representations and deep metric learning, in order\nto effectively represent contextual settings of viewing situations as well as\navailable content in a shared latent space. This embedding space is used for\nexploring relevant content in various viewing settings by applying an N-pairs\nloss objective as well as a relaxed variant proposed in this paper. Experiments\nconfirm the recommendation ability of JCCE, achieving improvements when\ncompared to state-of-the-art methods. Further experiments display useful\nstructures in the learned embeddings that can be used for gaining valuable\nknowledge of underlying variables in the relationship between contextual\nsettings and content properties.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.09102,regular,pre_llm,2020,2,"{'ai_likelihood': 2.5828679402669272e-06, 'text': ""Estimation-Action-Reflection: Towards Deep Interaction Between\n  Conversational and Recommender Systems\n\n  Recommender systems are embracing conversational technologies to obtain user\npreferences dynamically, and to overcome inherent limitations of their static\nmodels. A successful Conversational Recommender System (CRS) requires proper\nhandling of interactions between conversation and recommendation. We argue that\nthree fundamental problems need to be solved: 1) what questions to ask\nregarding item attributes, 2) when to recommend items, and 3) how to adapt to\nthe users' online feedback. To the best of our knowledge, there lacks a unified\nframework that addresses these problems.\n  In this work, we fill this missing interaction framework gap by proposing a\nnew CRS framework named Estimation-Action-Reflection, or EAR, which consists of\nthree stages to better converse with users. (1) Estimation, which builds\npredictive models to estimate user preference on both items and item\nattributes; (2) Action, which learns a dialogue policy to determine whether to\nask attributes or recommend items, based on Estimation stage and conversation\nhistory; and (3) Reflection, which updates the recommender model when a user\nrejects the recommendations made by the Action stage. We present two\nconversation scenarios on binary and enumerated questions, and conduct\nextensive experiments on two datasets from Yelp and LastFM, for each scenario,\nrespectively. Our experiments demonstrate significant improvements over the\nstate-of-the-art method CRM [32], corresponding to fewer conversation turns and\na higher level of recommendation hits.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.08577,review,pre_llm,2020,2,"{'ai_likelihood': 7.814831203884549e-06, 'text': ""Towards a Soft Faceted Browsing Scheme for Information Access\n\n  Faceted browsing is a commonly supported feature of user interfaces for\naccess to information. Existing interfaces generally treat facet values\nselected by a user as hard filters and respond to the user by only displaying\ninformation items strictly satisfying the filters and in their original ranking\norder. We propose a novel alternative strategy for faceted browsing, called\nsoft faceted browsing, where the system also includes some possibly relevant\nitems outside the selected filter in a non-intrusive way and re-ranks the items\nto better satisfy the user's information need. Such a soft faceted browsing\nstrategy can be beneficial when the user does not have a very confident and\nstrict preference for the selected facet values, and is especially appropriate\nfor applications such as e-commerce search where the user would like to explore\na larger space before finalizing a purchasing decision. We propose a\nprobabilistic framework for modeling and solving the soft faceted browsing\nproblem, and apply the framework to study the case of facet filter selection in\ne-commerce search engines. Preliminary experiment results demonstrate the soft\nfaceted browsing scheme is better than the traditional faceted browsing scheme\nin terms of its efficiency in helping users navigate in the information space.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.01854,regular,pre_llm,2020,2,"{'ai_likelihood': 4.006756676567926e-06, 'text': ""Interpretable & Time-Budget-Constrained Contextualization for Re-Ranking\n\n  Search engines operate under a strict time constraint as a fast response is\nparamount to user satisfaction. Thus, neural re-ranking models have a limited\ntime-budget to re-rank documents. Given the same amount of time, a faster\nre-ranking model can incorporate more documents than a less efficient one,\nleading to a higher effectiveness. To utilize this property, we propose TK\n(Transformer-Kernel): a neural re-ranking model for ad-hoc search using an\nefficient contextualization mechanism. TK employs a very small number of\nTransformer layers (up to three) to contextualize query and document word\nembeddings. To score individual term interactions, we use a document-length\nenhanced kernel-pooling, which enables users to gain insight into the model. TK\noffers an optimal ratio between effectiveness and efficiency: under realistic\ntime constraints (max. 200 ms per query) TK achieves the highest effectiveness\nin comparison to BERT and other re-ranking models. We demonstrate this on three\nlarge-scale ranking collections: MSMARCO-Passage, MSMARCO-Document, and TREC\nCAR. In addition, to gain insight into TK, we perform a clustered query\nanalysis of TK's results, highlighting its strengths and weaknesses on queries\nwith different types of information need and we show how to interpret the cause\nof ranking differences of two documents by comparing their internal scores.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.07461,regular,pre_llm,2020,3,"{'ai_likelihood': 8.54333241780599e-06, 'text': 'Identifying Notable News Stories\n\n  The volume of news content has increased significantly in recent years and\nsystems to process and deliver this information in an automated fashion at\nscale are becoming increasingly prevalent. One critical component that is\nrequired in such systems is a method to automatically determine how notable a\ncertain news story is, in order to prioritize these stories during delivery.\nOne way to do so is to compare each story in a stream of news stories to a\nnotable event. In other words, the problem of detecting notable news can be\ndefined as a ranking task; given a trusted source of notable events and a\nstream of candidate news stories, we aim to answer the question: ""Which of the\ncandidate news stories is most similar to the notable one?"". We employ\ndifferent combinations of features and learning to rank (LTR) models and gather\nrelevance labels using crowdsourcing. In our approach, we use structured\nrepresentations of candidate news stories (triples) and we link them to\ncorresponding entities. Our evaluation shows that the features in our proposed\nmethod outperform standard ranking methods, and that the trained model\ngeneralizes well to unseen news stories.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.09086,regular,pre_llm,2020,3,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'A^2-GCN: An Attribute-aware Attentive GCN Model for Recommendation\n\n  As important side information, attributes have been widely exploited in the\nexisting recommender system for better performance. In the real-world\nscenarios, it is common that some attributes of items/users are missing (e.g.,\nsome movies miss the genre data). Prior studies usually use a default value\n(i.e., ""other"") to represent the missing attribute, resulting in sub-optimal\nperformance. To address this problem, in this paper, we present an\nattribute-aware attentive graph convolution network (A${^2}$-GCN). In\nparticular, we first construct a graph, whereby users, items, and attributes\nare three types of nodes and their associations are edges. Thereafter, we\nleverage the graph convolution network to characterize the complicated\ninteractions among <users, items, attributes>. To learn the node\nrepresentation, we turn to the message-passing strategy to aggregate the\nmessage passed from the other directly linked types of nodes (e.g., a user or\nan attribute). To this end, we are capable of incorporating associate\nattributes to strengthen the user and item representations, and thus naturally\nsolve the attribute missing problem. Considering the fact that for different\nusers, the attributes of an item have different influence on their preference\nfor this item, we design a novel attention mechanism to filter the message\npassed from an item to a target user by considering the attribute information.\nExtensive experiments have been conducted on several publicly accessible\ndatasets to justify our model. Results show that our model outperforms several\nstate-of-the-art methods and demonstrate the effectiveness of our attention\nmethod.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.09592,regular,pre_llm,2020,3,"{'ai_likelihood': 4.5365757412380644e-06, 'text': 'Privacy-Preserving News Recommendation Model Learning\n\n  News recommendation aims to display news articles to users based on their\npersonal interest. Existing news recommendation methods rely on centralized\nstorage of user behavior data for model training, which may lead to privacy\nconcerns and risks due to the privacy-sensitive nature of user behaviors. In\nthis paper, we propose a privacy-preserving method for news recommendation\nmodel training based on federated learning, where the user behavior data is\nlocally stored on user devices. Our method can leverage the useful information\nin the behaviors of massive number users to train accurate news recommendation\nmodels and meanwhile remove the need of centralized storage of them. More\nspecifically, on each user device we keep a local copy of the news\nrecommendation model, and compute gradients of the local model based on the\nuser behaviors in this device. The local gradients from a group of randomly\nselected users are uploaded to server, which are further aggregated to update\nthe global model in the server. Since the model gradients may contain some\nimplicit private information, we apply local differential privacy (LDP) to them\nbefore uploading for better privacy protection. The updated global model is\nthen distributed to each user device for local model update. We repeat this\nprocess for multiple rounds. Extensive experiments on a real-world dataset show\nthe effectiveness of our method in news recommendation model training with\nprivacy protection.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.07027,regular,pre_llm,2020,3,"{'ai_likelihood': 8.245309193929037e-06, 'text': ""Eating Healthier: Exploring Nutrition Information for Healthier Recipe\n  Recommendation\n\n  With the booming of personalized recipe sharing networks (e.g., Yummly), a\ndeluge of recipes from different cuisines could be obtained easily. In this\npaper, we aim to solve a problem which many home-cooks encounter when searching\nfor recipes online. Namely, finding recipes which best fit a handy set of\ningredients while at the same time follow healthy eating guidelines. This task\nis especially difficult since the lions share of online recipes have been shown\nto be unhealthy. In this paper we propose a novel framework named NutRec, which\nmodels the interactions between ingredients and their proportions within\nrecipes for the purpose of offering healthy recommendation. Specifically,\nNutRec consists of three main components: 1) using an embedding-based\ningredient predictor to predict the relevant ingredients with user-defined\ninitial ingredients, 2) predicting the amounts of the relevant ingredients with\na multi-layer perceptron-based network, 3) creating a healthy pseudo-recipe\nwith a list of ingredients and their amounts according to the nutritional\ninformation and recommending the top similar recipes with the pseudo-recipe. We\nconduct the experiments on two recipe datasets, including Allrecipes with\n36,429 recipes and Yummly with 89,413 recipes, respectively. The empirical\nresults support the framework's intuition and showcase its ability to retrieve\nhealthier recipes.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.08276,review,pre_llm,2020,3,"{'ai_likelihood': 4.635916815863716e-06, 'text': 'Supporting Interoperability Between Open-Source Search Engines with the\n  Common Index File Format\n\n  There exists a natural tension between encouraging a diverse ecosystem of\nopen-source search engines and supporting fair, replicable comparisons across\nthose systems. To balance these two goals, we examine two approaches to\nproviding interoperability between the inverted indexes of several systems. The\nfirst takes advantage of internal abstractions around index structures and\nbuilding wrappers that allow one system to directly read the indexes of\nanother. The second involves sharing indexes across systems via a data exchange\nspecification that we have developed, called the Common Index File Format\n(CIFF). We demonstrate the first approach with the Java systems Anserini and\nTerrier, and the second approach with Anserini, JASSv2, OldDog, PISA, and\nTerrier. Together, these systems provide a wide range of implementations and\nfeatures, with different research goals. Overall, we recommend CIFF as a\nlow-effort approach to support independent innovation while enabling the types\nof fair evaluations that are critical for driving the field forward.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.13481,regular,pre_llm,2020,3,"{'ai_likelihood': 7.616149054633247e-07, 'text': ""Concept-aware Geographic Information Retrieval\n\n  Textual queries are largely employed in information retrieval to let users\nspecify search goals in a natural way. However, differences in user and system\nterminologies can challenge the identification of the user's information needs,\nand thus the generation of relevant results. We argue that the explicit\nmanagement of ontological knowledge, and of the meaning of concepts (by\nintegrating linguistic and encyclopedic knowledge in the system ontology), can\nimprove the analysis of search queries, because it enables a flexible\nidentification of the topics the user is searching for, regardless of the\nadopted vocabulary. This paper proposes an information retrieval support model\nbased on semantic concept identification. Starting from the recognition of the\nontology concepts that the search query refers to, this model exploits the\nqualifiers specified in the query to select information items on the basis of\npossibly fine-grained features. Moreover, it supports query expansion and\nreformulation by suggesting the exploration of semantically similar concepts,\nas well as of concepts related to those referred in the query through thematic\nrelations. A test on a data-set collected using the OnToMap Participatory GIS\nhas shown that this approach provides accurate results.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.04628,review,pre_llm,2020,3,"{'ai_likelihood': 3.940529293484158e-06, 'text': 'Large-Scale Evaluation of Keyphrase Extraction Models\n\n  Keyphrase extraction models are usually evaluated under different, not\ndirectly comparable, experimental setups. As a result, it remains unclear how\nwell proposed models actually perform, and how they compare to each other. In\nthis work, we address this issue by presenting a systematic large-scale\nanalysis of state-of-the-art keyphrase extraction models involving multiple\nbenchmark datasets from various sources and domains. Our main results reveal\nthat state-of-the-art models are in fact still challenged by simple baselines\non some datasets. We also present new insights about the impact of using\nauthor- or reader-assigned keyphrases as a proxy for gold standard, and give\nrecommendations for strong baselines and reliable benchmark datasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.13474,regular,pre_llm,2020,3,"{'ai_likelihood': 3.1789143880208335e-06, 'text': 'Extending a Tag-based Collaborative Recommender with Co-occurring\n  Information Interests\n\n  Collaborative Filtering is largely applied to personalize item recommendation\nbut its performance is affected by the sparsity of rating data. In order to\naddress this issue, recent systems have been developed to improve\nrecommendation by extracting latent factors from the rating matrices, or by\nexploiting trust relations established among users in social networks. In this\nwork, we are interested in evaluating whether other sources of preference\ninformation than ratings and social ties can be used to improve recommendation\nperformance. Specifically, we aim at testing whether the integration of\nfrequently co-occurring interests in information search logs can improve\nrecommendation performance in User-to-User Collaborative Filtering (U2UCF). For\nthis purpose, we propose the Extended Category-based Collaborative Filtering\n(ECCF) recommender, which enriches category-based user profiles derived from\nthe analysis of rating behavior with data categories that are frequently\nsearched together by people in search sessions. We test our model using a big\nrating dataset and a log of a largely used search engine to extract the\nco-occurrence of interests. The experiments show that ECCF outperforms U2UCF\nand category-based collaborative recommendation in accuracy, MRR, diversity of\nrecommendations and user coverage. Moreover, it outperforms the SVD++ Matrix\nFactorization algorithm in accuracy and diversity of recommendation lists.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.10699,regular,pre_llm,2020,3,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'Utilizing Human Memory Processes to Model Genre Preferences for\n  Personalized Music Recommendations\n\n  In this paper, we introduce a psychology-inspired approach to model and\npredict the music genre preferences of different groups of users by utilizing\nhuman memory processes. These processes describe how humans access information\nunits in their memory by considering the factors of (i) past usage frequency,\n(ii) past usage recency, and (iii) the current context. Using a publicly\navailable dataset of more than a billion music listening records shared on the\nmusic streaming platform Last.fm, we find that our approach provides\nsignificantly better prediction accuracy results than various baseline\nalgorithms for all evaluated user groups, i.e., (i) low-mainstream music\nlisteners, (ii) medium-mainstream music listeners, and (iii) high-mainstream\nmusic listeners. Furthermore, our approach is based on a simple psychological\nmodel, which contributes to the transparency and explainability of the\ncalculated predictions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.03975,regular,pre_llm,2020,3,"{'ai_likelihood': 4.238552517361111e-06, 'text': ""Price-aware Recommendation with Graph Convolutional Networks\n\n  In recent years, much research effort on recommendation has been devoted to\nmining user behaviors, i.e., collaborative filtering, along with the general\ninformation which describes users or items, e.g., textual attributes,\ncategorical demographics, product images, and so on. Price, an important factor\nin marketing --- which determines whether a user will make the final purchase\ndecision on an item --- surprisingly, has received relatively little scrutiny.\n  In this work, we aim at developing an effective method to predict user\npurchase intention with the focus on the price factor in recommender systems.\nThe main difficulties are two-fold: 1) the preference and sensitivity of a user\non item price are unknown, which are only implicitly reflected in the items\nthat the user has purchased, and 2) how the item price affects a user's\nintention depends largely on the product category, that is, the perception and\naffordability of a user on item price could vary significantly across\ncategories. Towards the first difficulty, we propose to model the transitive\nrelationship between user-to-item and item-to-price, taking the inspiration\nfrom the recently developed Graph Convolution Networks (GCN). The key idea is\nto propagate the influence of price on users with items as the bridge, so as to\nmake the learned user representations be price-aware. For the second\ndifficulty, we further integrate item categories into the propagation progress\nand model the possible pairwise interactions for predicting user-item\ninteractions. We conduct extensive experiments on two real-world datasets,\ndemonstrating the effectiveness of our GCN-based method in learning the\nprice-aware preference of users. Further analysis reveals that modeling the\nprice awareness is particularly useful for predicting user preference on items\nof unexplored categories.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.11634,regular,pre_llm,2020,3,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Unfair Exposure of Artists in Music Recommendation\n\n  Fairness in machine learning has been studied by many researchers. In\nparticular, fairness in recommender systems has been investigated to ensure the\nrecommendations meet certain criteria with respect to certain sensitive\nfeatures such as race, gender etc. However, often recommender systems are\nmulti-stakeholder environments in which the fairness towards all stakeholders\nshould be taken care of. It is well-known that the recommendation algorithms\nsuffer from popularity bias; few popular items are over-recommended which leads\nto the majority of other items not getting proportionate attention. This bias\nhas been investigated from the perspective of the users and how it makes the\nfinal recommendations skewed towards popular items in general. In this paper,\nhowever, we investigate the impact of popularity bias in recommendation\nalgorithms on the provider of the items (i.e. the entities who are behind the\nrecommended items). Using a music dataset for our experiments, we show that,\ndue to some biases in the algorithms, different groups of artists with varying\ndegrees of popularity are systematically and consistently treated differently\nthan others.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.13401,regular,pre_llm,2020,4,"{'ai_likelihood': 1.158979203965929e-06, 'text': 'CmnRec: Sequential Recommendations with Chunk-accelerated Memory Network\n\n  Recently, Memory-based Neural Recommenders (MNR) have demonstrated superior\npredictive accuracy in the task of sequential recommendations, particularly for\nmodeling long-term item dependencies. However, typical MNR requires complex\nmemory access operations, i.e., both writing and reading via a controller\n(e.g., RNN) at every time step. Those frequent operations will dramatically\nincrease the network training time, resulting in the difficulty in being\ndeployed on industrial-scale recommender systems. In this paper, we present a\nnovel general Chunk framework to accelerate MNR significantly. Specifically,\nour framework divides proximal information units into chunks, and performs\nmemory access at certain time steps, whereby the number of memory operations\ncan be greatly reduced. We investigate two ways to implement effective\nchunking, i.e., PEriodic Chunk (PEC) and Time-Sensitive Chunk (TSC), to\npreserve and recover important recurrent signals in the sequence. Since\nchunk-accelerated MNR models take into account more proximal information units\nthan that from a single timestep, it can remove the influence of noise in the\nitem sequence to a large extent, and thus improve the stability of MNR. In this\nway, the proposed chunk mechanism can lead to not only faster training and\nprediction, but even slightly better results. The experimental results on three\nreal-world datasets (weishi, ml-10M and ml-latest) show that our chunk\nframework notably reduces the running time (e.g., with up to 7x for training &\n10x for inference on ml-latest) of MNR, and meantime achieves competitive\nperformance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.09424,review,pre_llm,2020,4,"{'ai_likelihood': 3.907415601942274e-06, 'text': 'Learning a Fine-Grained Review-based Transformer Model for Personalized\n  Product Search\n\n  Product search has been a crucial entry point to serve people shopping\nonline. Most existing personalized product models follow the paradigm of\nrepresenting and matching user intents and items in the semantic space, where\nfiner-grained matching is totally discarded and the ranking of an item cannot\nbe explained further than just user/item level similarity. In addition, while\nsome models in existing studies have created dynamic user representations based\non search context, their representations for items are static across all search\nsessions. This makes every piece of information about the item always equally\nimportant in representing the item during matching with various user intents.\nAware of the above limitations, we propose a review-based transformer model\n(RTM) for personalized product search, which encodes the sequence of query,\nuser reviews, and item reviews with a transformer architecture. RTM conducts\nreview-level matching between the user and item, where each review has a\ndynamic effect according to the context in the sequence. This makes it possible\nto identify useful reviews to explain the scoring. Experimental results show\nthat RTM significantly outperforms state-of-the-art personalized product search\nbaselines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.14714,regular,pre_llm,2020,4,"{'ai_likelihood': 4.900826348198785e-06, 'text': ""A Deep Recurrent Survival Model for Unbiased Ranking\n\n  Position bias is a critical problem in information retrieval when dealing\nwith implicit yet biased user feedback data. Unbiased ranking methods typically\nrely on causality models and debias the user feedback through inverse\npropensity weighting. While practical, these methods still suffer from two\nmajor problems. First, when inferring a user click, the impact of the\ncontextual information, such as documents that have been examined, is often\nignored. Second, only the position bias is considered but other issues resulted\nfrom user browsing behaviors are overlooked. In this paper, we propose an\nend-to-end Deep Recurrent Survival Ranking (DRSR), a unified framework to\njointly model user's various behaviors, to (i) consider the rich contextual\ninformation in the ranking list; and (ii) address the hidden issues underlying\nuser behaviors, i.e., to mine observe pattern in queries without any click\n(non-click queries), and to model tracking logs which cannot truly reflect the\nuser browsing intents (untrusted observation). Specifically, we adopt a\nrecurrent neural network to model the contextual information and estimates the\nconditional likelihood of user feedback at each position. We then incorporate\nsurvival analysis techniques with the probability chain rule to mathematically\nrecover the unbiased joint probability of one user's various behaviors. DRSR\ncan be easily incorporated with both point-wise and pair-wise learning\nobjectives. The extensive experiments over two large-scale industrial datasets\ndemonstrate the significant performance gains of our model comparing with the\nstate-of-the-arts.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.13136,regular,pre_llm,2020,4,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'The Effect of the Multi-Layer Text Summarization Model on the Efficiency\n  and Relevancy of the Vector Space-based Information Retrieval\n\n  The massive upload of text on the internet creates a huge inverted index in\ninformation retrieval systems, which hurts their efficiency. The purpose of\nthis research is to measure the effect of the Multi-Layer Similarity model of\nthe automatic text summarization on building an informative and condensed\ninvert index in the IR systems. To achieve this purpose, we summarized a\nconsiderable number of documents using the Multi-Layer Similarity model, and we\nbuilt the inverted index from the automatic summaries that were generated from\nthis model. A series of experiments were held to test the performance in terms\nof efficiency and relevancy. The experiments include comparisons with three\nexisting text summarization models; the Jaccard Coefficient Model, the Vector\nSpace Model, and the Latent Semantic Analysis model. The experiments examined\nthree groups of queries with manual and automatic relevancy assessment. The\npositive effect of the Multi-Layer Similarity in the efficiency of the IR\nsystem was clear without noticeable loss in the relevancy results. However, the\nevaluation showed that the traditional statistical models without semantic\ninvestigation failed to improve the information retrieval efficiency. Comparing\nwith the previous publications that addressed the use of summaries as a source\nof the index, the relevancy assessment of our work was higher, and the\nMulti-Layer Similarity retrieval constructed an inverted index that was 58%\nsmaller than the main corpus inverted index.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.14245,regular,pre_llm,2020,4,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Expansion via Prediction of Importance with Contextualization\n\n  The identification of relevance with little textual context is a primary\nchallenge in passage retrieval. We address this problem with a\nrepresentation-based ranking approach that: (1) explicitly models the\nimportance of each term using a contextualized language model; (2) performs\npassage expansion by propagating the importance to similar terms; and (3)\ngrounds the representations in the lexicon, making them interpretable. Passage\nrepresentations can be pre-computed at index time to reduce query-time latency.\nWe call our approach EPIC (Expansion via Prediction of Importance with\nContextualization). We show that EPIC significantly outperforms prior\nimportance-modeling and document expansion approaches. We also observe that the\nperformance is additive with the current leading first-stage retrieval methods,\nfurther narrowing the gap between inexpensive and cost-prohibitive passage\nranking approaches. Specifically, EPIC achieves a MRR@10 of 0.304 on the\nMS-MARCO passage ranking dataset with 78ms average query latency on commodity\nhardware. We also find that the latency is further reduced to 68ms by pruning\ndocument representations, with virtually no difference in effectiveness.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.13313,regular,pre_llm,2020,4,"{'ai_likelihood': 4.255109363132053e-05, 'text': 'Modularized Transfomer-based Ranking Framework\n\n  Recent innovations in Transformer-based ranking models have advanced the\nstate-of-the-art in information retrieval. However, these Transformers are\ncomputationally expensive, and their opaque hidden states make it hard to\nunderstand the ranking process. In this work, we modularize the Transformer\nranker into separate modules for text representation and interaction. We show\nhow this design enables substantially faster ranking using offline pre-computed\nrepresentations and light-weight online interactions. The modular design is\nalso easier to interpret and sheds light on the ranking process in Transformer\nrankers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.14269,regular,pre_llm,2020,4,"{'ai_likelihood': 1.3841523064507379e-05, 'text': 'Training Curricula for Open Domain Answer Re-Ranking\n\n  In precision-oriented tasks like answer ranking, it is more important to rank\nmany relevant answers highly than to retrieve all relevant answers. It follows\nthat a good ranking strategy would be to learn how to identify the easiest\ncorrect answers first (i.e., assign a high ranking score to answers that have\ncharacteristics that usually indicate relevance, and a low ranking score to\nthose with characteristics that do not), before incorporating more complex\nlogic to handle difficult cases (e.g., semantic matching or reasoning). In this\nwork, we apply this idea to the training of neural answer rankers using\ncurriculum learning. We propose several heuristics to estimate the difficulty\nof a given training sample. We show that the proposed heuristics can be used to\nbuild a training curriculum that down-weights difficult samples early in the\ntraining process. As the training process progresses, our approach gradually\nshifts to weighting all samples equally, regardless of difficulty. We present a\ncomprehensive evaluation of our proposed idea on three answer ranking datasets.\nResults show that our approach leads to superior performance of two leading\nneural ranking architectures, namely BERT and ConvKNRM, using both pointwise\nand pairwise losses. When applied to a BERT-based ranker, our method yields up\nto a 4% improvement in MRR and a 9% improvement in P@1 (compared to the model\ntrained without a curriculum). This results in models that can achieve\ncomparable performance to more expensive state-of-the-art techniques.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.02127,regular,pre_llm,2020,4,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Image understanding and the web\n\n  The contextual information of Web images is investigated to address the issue\nof characterizing their content with semantic descriptors and therefore bridge\nthe semantic gap, i.e. the gap between their automated low-level representation\nin terms of colors, textures, shapes. . . and their semantic interpretation.\nSuch characterization allows for understanding the image content and is crucial\nin important Web-based tasks such as image indexing and retrieval. Although we\nare highly motivated by the availability of rich knowledge on the Web and the\nrelative success achieved by commercial search engines in automatically\ncharacterizing the image content using contextual information in Web pages, we\nare aware that the unpredictable quality of the contextual information is a\nmajor limiting factor. Among the reasons explaining the difficulty to leverage\non the image contextual information, some problems are related to the\ncharacterization and extraction of this information. Indeed, the first issue is\nthe lack of large-scale studies to highlight what is considered the relevant\ncontextual information of an image, where it is located in a Web page and\nwhether it is consistent across Web pages of different types, content layouts\nand domains. Also, the matter related to the extraction of this contextual\ninformation is topical as state-of-the-art automated extraction tools are\nunable to handle the heterogeneous Web. As far as the processing of the\ncontextual information is concerned, problems linked to the syntactic and\nsemantic characterizations of the textual components are important to address\nin order to tackle the semantic gap. Furthermore, questions pertaining to the\norganization of these textual components into coherent structures that are\nusable in image indexing and retrieval frameworks shall arise.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.06389,regular,pre_llm,2020,4,"{'ai_likelihood': 3.973642985026042e-07, 'text': ""Tag Embedding Based Personalized Point Of Interest Recommendation System\n\n  Personalized Point of Interest recommendation is very helpful for satisfying\nusers' needs at new places. In this article, we propose a tag embedding based\nmethod for Personalized Recommendation of Point Of Interest. We model the\nrelationship between tags corresponding to Point Of Interest. The model\nprovides representative embedding corresponds to a tag in a way that related\ntags will be closer. We model Point of Interest-based on tag embedding and also\nmodel the users (user profile) based on the Point Of Interest rated by them.\nfinally, we rank the user's candidate Point Of Interest based on cosine\nsimilarity between user's embedding and Point of Interest's embedding. Further,\nwe find the parameters required to model user by discrete optimizing over\ndifferent measures (like ndcg@5, MRR, ...). We also analyze the result while\nconsidering the same parameters for all users and individual parameters for\neach user. Along with it we also analyze the effect on the result while\nchanging the dataset to model the relationship between tags. Our method also\nminimizes the privacy leak issue. We used TREC Contextual Suggestion 2016 Phase\n2 dataset and have significant improvement over all the measures on the state\nof the art method. It improves ndcg@5 by 12.8%, p@5 by 4.3%, and MRR by 7.8%,\nwhich shows the effectiveness of the method.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.12563,regular,pre_llm,2020,4,"{'ai_likelihood': 2.947118547227648e-06, 'text': 'Automatic Textual Evidence Mining in COVID-19 Literature\n\n  We created this EVIDENCEMINER system for automatic textual evidence mining in\nCOVID-19 literature. EVIDENCEMINER is a web-based system that lets users query\na natural language statement and automatically retrieves textual evidence from\na background corpora for life sciences. It is constructed in a completely\nautomated way without any human effort for training data annotation.\nEVIDENCEMINER is supported by novel data-driven methods for distantly\nsupervised named entity recognition and open information extraction. The named\nentities and meta-patterns are pre-computed and indexed offline to support fast\nonline evidence retrieval. The annotation results are also highlighted in the\noriginal document for better visualization. EVIDENCEMINER also includes\nanalytic functionalities such as the most frequent entity and relation\nsummarization.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.08476,regular,pre_llm,2020,4,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Learning-to-Rank with BERT in TF-Ranking\n\n  This paper describes a machine learning algorithm for document (re)ranking,\nin which queries and documents are firstly encoded using BERT [1], and on top\nof that a learning-to-rank (LTR) model constructed with TF-Ranking (TFR) [2] is\napplied to further optimize the ranking performance. This approach is proved to\nbe effective in a public MS MARCO benchmark [3]. Our first two submissions\nachieve the best performance for the passage re-ranking task [4], and the\nsecond best performance for the passage full-ranking task as of April 10, 2020\n[5]. To leverage the lately development of pre-trained language models, we\nrecently integrate RoBERTa [6] and ELECTRA [7]. Our latest submissions improve\nour previously state-of-the-art re-ranking performance by 4.3% [8], and achieve\nthe third best performance for the full-ranking task [9] as of June 8, 2020.\nBoth of them demonstrate the effectiveness of combining ranking losses with\nBERT representations for document ranking.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.11718,review,pre_llm,2020,4,"{'ai_likelihood': 2.284844716389974e-06, 'text': ""Graph Learning Approaches to Recommender Systems: A Review\n\n  Recent years have witnessed the fast development of the emerging topic of\nGraph Learning based Recommender Systems (GLRS). GLRS mainly employ the\nadvanced graph learning approaches to model users' preferences and intentions\nas well as items' characteristics and popularity for Recommender Systems (RS).\nDifferently from conventional RS, including content based filtering and\ncollaborative filtering, GLRS are built on simple or complex graphs where\nvarious objects, e.g., users, items, and attributes, are explicitly or\nimplicitly connected. With the rapid development of graph learning, exploring\nand exploiting homogeneous or heterogeneous relations in graphs is a promising\ndirection for building advanced RS. In this paper, we provide a systematic\nreview of GLRS, on how they obtain the knowledge from graphs to improve the\naccuracy, reliability and explainability for recommendations. First, we\ncharacterize and formalize GLRS, and then summarize and categorize the key\nchallenges in this new research area. Then, we survey the most recent and\nimportant developments in the area. Finally, we share some new research\ndirections in this vibrant area.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.14255,regular,pre_llm,2020,4,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Efficient Document Re-Ranking for Transformers by Precomputing Term\n  Representations\n\n  Deep pretrained transformer networks are effective at various ranking tasks,\nsuch as question answering and ad-hoc document ranking. However, their\ncomputational expenses deem them cost-prohibitive in practice. Our proposed\napproach, called PreTTR (Precomputing Transformer Term Representations),\nconsiderably reduces the query-time latency of deep transformer networks (up to\na 42x speedup on web document ranking) making these networks more practical to\nuse in a real-time ranking scenario. Specifically, we precompute part of the\ndocument term representations at indexing time (without a query), and merge\nthem with the query representation at query time to compute the final ranking\nscore. Due to the large size of the token representations, we also propose an\neffective approach to reduce the storage requirement by training a compression\nlayer to match attention scores. Our compression technique reduces the storage\nrequired up to 95% and it can be applied without a substantial degradation in\nranking performance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.11588,review,pre_llm,2020,4,"{'ai_likelihood': 2.2351741790771484e-05, 'text': 'Learning Hierarchical Review Graph Representations for Recommendation\n\n  The user review data have been demonstrated to be effective in solving\ndifferent recommendation problems. Previous review-based recommendation methods\nusually employ sophisticated compositional models, such as Recurrent Neural\nNetworks (RNN) and Convolutional Neural Networks (CNN), to learn semantic\nrepresentations from the review data for recommendation. However, these methods\nmainly capture the local dependency between neighbouring words in a word\nwindow, and they treat each review equally. Therefore, they may not be\neffective in capturing the global dependency between words, and tend to be\neasily biased by noise review information. In this paper, we propose a novel\nreview-based recommendation model, named Review Graph Neural Network (RGNN).\nSpecifically, RGNN builds a specific review graph for each individual\nuser/item, which provides a global view about the user/item properties to help\nweaken the biases caused by noise review information. A type-aware graph\nattention mechanism is developed to learn semantic embeddings of words.\nMoreover, a personalized graph pooling operator is proposed to learn\nhierarchical representations of the review graph to form the semantic\nrepresentation for each user/item. We compared RGNN with state-of-the-art\nreview-based recommendation approaches on two real-world datasets. The\nexperimental results indicate that RGNN consistently outperforms baseline\nmethods, in terms of Mean Square Error (MSE).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.12118,regular,pre_llm,2020,4,"{'ai_likelihood': 1.5894571940104167e-06, 'text': ""Inter-sequence Enhanced Framework for Personalized Sequential\n  Recommendation\n\n  Modeling the sequential correlation of users' historical interactions is\nessential in sequential recommendation. However, the majority of the approaches\nmainly focus on modeling the \\emph{intra-sequence} item correlation within each\nindividual sequence but neglect the \\emph{inter-sequence} item correlation\nacross different user interaction sequences. Though several studies have been\naware of this issue, their method is either simple or implicit. To make better\nuse of such information, we propose an inter-sequence enhanced framework for\nthe Sequential Recommendation (ISSR). In ISSR, both inter-sequence and\nintra-sequence item correlation are considered. Firstly, we equip graph neural\nnetworks in the inter-sequence correlation encoder to capture the high-order\nitem correlation from the user-item bipartite graph and the item-item graph.\nThen, based on the inter-sequence correlation encoder, we build GRU network and\nattention network in the intra-sequence correlation encoder to model the item\nsequential correlation within each individual sequence and temporal dynamics\nfor predicting users' preferences over candidate items. Additionally, we\nconduct extensive experiments on three real-world datasets. The experimental\nresults demonstrate the superiority of ISSR over many state-of-the-art methods\nand the effectiveness of the inter-sequence correlation encoder.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.11529,regular,pre_llm,2020,4,"{'ai_likelihood': 7.722112867567275e-05, 'text': ""Contextualized Graph Attention Network for Recommendation with Item\n  Knowledge Graph\n\n  Graph neural networks (GNN) have recently been applied to exploit knowledge\ngraph (KG) for recommendation. Existing GNN-based methods explicitly model the\ndependency between an entity and its local graph context in KG (i.e., the set\nof its first-order neighbors), but may not be effective in capturing its\nnon-local graph context (i.e., the set of most related high-order neighbors).\nIn this paper, we propose a novel recommendation framework, named\nContextualized Graph Attention Network (CGAT), which can explicitly exploit\nboth local and non-local graph context information of an entity in KG.\nSpecifically, CGAT captures the local context information by a user-specific\ngraph attention mechanism, considering a user's personalized preferences on\nentities. Moreover, CGAT employs a biased random walk sampling process to\nextract the non-local context of an entity, and utilizes a Recurrent Neural\nNetwork (RNN) to model the dependency between the entity and its non-local\ncontextual entities. To capture the user's personalized preferences on items,\nan item-specific attention mechanism is also developed to model the dependency\nbetween a target item and the contextual items extracted from the user's\nhistorical behaviors. Experimental results on real datasets demonstrate the\neffectiveness of CGAT, compared with state-of-the-art KG-based recommendation\nmethods.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.00291,regular,pre_llm,2020,4,"{'ai_likelihood': 3.642506069607205e-07, 'text': ""Recommandation ontologique multicrit\\`ere pour la m\\'etrologie\n\n  Matchmaking and information ranking are helping process for users, by\noffering them the best answers possible at their request. When there is no\nexact answer, giving them the closest proposition available is an efficient\nupgrade of that helping process. With a reasearch platform on metrology as a\nframework, we will discuss about ranking with knowledge representation, with an\napproach based on Description Logic, ontologies and multricriteria comparison.\nWe present a reasonning to compare each proposition with the other, with\nsemantic and syntaxic difference, by troncating the information in distinct\ncomponent.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.02156,regular,pre_llm,2020,4,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'A User Study to Investigate Semantically Relevant Contextual Information\n  of WWW Images\n\n  The contextual information of Web images is investigated to address the issue\nof enriching their index characterizations with semantic descriptors and\ntherefore bridge the semantic gap (i.e. the gap between the low-level\ncontent-based description of images and their semantic interpretation).\nAlthough we are highly motivated by the availability of rich knowledge on the\nWeb and the relative success achieved by commercial search engines in indexing\nimages using surrounding text-based information in webpages, we are aware that\nthe unpredictable quality of the surrounding text is a major limiting factor.\nIn order to improve its quality, we highlight contextual information which is\nrelevant for the semantic characterization of Web images and study its\nstatistical properties in terms of its location and nature considering a\nclassification into five semantic concept classes: signal, object, scene,\nabstract and relational. A user study is conducted to validate the results. The\nresults suggest that there are several locations that consistently contain\nrelevant textual information with respect to the image. The importance of each\nlocation is influenced by the type of webpage as the results show the different\ndistribution of relevant contextual information across the locations for\ndifferent webpage types. The frequently found semantic concept classes are\nobject and abstract. Another important outcome of the user study shows that a\nwebpage is not an atomic unit and can be further partitioned into smaller\nsegments. Segments containing images are of interest and termed as image\nsegments. We observe that users typically single out textual information which\nthey consider relevant to the image from the textual information bounded within\nthe image segment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.11699,regular,pre_llm,2020,4,"{'ai_likelihood': 9.040037790934246e-06, 'text': 'An approach based on Combination of Features for automatic news\n  retrieval\n\n  Nowadays, according to the increasingly increasing information, the\nimportance of its presentation is also increasing. The internet has become one\nof the main sources of information for users and their favorite topics. It also\nprovides access to more information. Understanding this information is very\nimportant for providing the best set of information resources for users.\nContent providers now need a precise and efficient way to retrieve news with\nthe least human help. Data mining has led to the emergence of new methods for\ndetecting related and unrelated documents. Although the conceptual relationship\nbetween documents may be negligible, it is important to provide useful\ninformation and relevant content to users. In this paper, a new approach based\non the Combination of Features (CoF) for information retrieval operations is\nintroduced. Along with introducing this new approach, we proposed a dataset by\nidentifying the most commonly used keywords in documents and using the most\nappropriate documents to help them with the abundance of vocabulary. Then,\nusing the proposed approach, techniques of text categorization, evaluation\ncriteria and ranking algorithms, the data were analyzed and examined. The\nevaluation results show that using the combination of features approach\nimproves the quality and effects on efficient ranking.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.13969,regular,pre_llm,2020,4,"{'ai_likelihood': 2.947118547227648e-06, 'text': 'Complementing Lexical Retrieval with Semantic Residual Embedding\n\n  This paper presents CLEAR, a retrieval model that seeks to complement\nclassical lexical exact-match models such as BM25 with semantic matching\nsignals from a neural embedding matching model. CLEAR explicitly trains the\nneural embedding to encode language structures and semantics that lexical\nretrieval fails to capture with a novel residual-based embedding learning\nmethod. Empirical evaluations demonstrate the advantages of CLEAR over\nstate-of-the-art retrieval models, and that it can substantially improve the\nend-to-end accuracy and efficiency of reranking pipelines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.10084,regular,pre_llm,2020,5,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Context-Aware Learning to Rank with Self-Attention\n\n  Learning to rank is a key component of many e-commerce search engines. In\nlearning to rank, one is interested in optimising the global ordering of a list\nof items according to their utility for users.Popular approaches learn a\nscoring function that scores items individually (i.e. without the context of\nother items in the list) by optimising a pointwise, pairwise or listwise loss.\nThe list is then sorted in the descending order of the scores. Possible\ninteractions between items present in the same list are taken into account in\nthe training phase at the loss level. However, during inference, items are\nscored individually, and possible interactions between them are not considered.\nIn this paper, we propose a context-aware neural network model that learns item\nscores by applying a self-attention mechanism. The relevance of a given item is\nthus determined in the context of all other items present in the list, both in\ntraining and in inference. We empirically demonstrate significant performance\ngains of self-attention based neural architecture over Multi-LayerPerceptron\nbaselines, in particular on a dataset coming from search logs of a large scale\ne-commerce marketplace, Allegro.pl. This effect is consistent across popular\npointwise, pairwise and listwise losses.Finally, we report new state-of-the-art\nresults on MSLR-WEB30K, the learning to rank benchmark.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.12371,review,pre_llm,2020,5,"{'ai_likelihood': 8.808241950141059e-06, 'text': ""Reputation (In)dependence in Ranking Systems: Demographics Influence\n  Over Output Disparities\n\n  Recent literature on ranking systems (RS) has considered users' exposure when\nthey are the object of the ranking. Although items are the object of\nreputation-based RS, users have a central role also in this class of\nalgorithms. Indeed, when ranking the items, user preferences are weighted by\nhow relevant this user is in the platform (i.e., their reputation). In this\npaper, we formulate the concept of disparate reputation (DR) and study if users\ncharacterized by sensitive attributes systematically get a lower reputation,\nleading to a final ranking that reflects less their preferences. We consider\ntwo demographic attributes, i.e., gender and age, and show that DR\nsystematically occurs. Then, we propose mitigation, which ensures that\nreputation is independent of the users' sensitive attributes. Experiments on\nreal-world data show that our approach can overcome DR and also improve ranking\neffectiveness.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.12002,regular,pre_llm,2020,5,"{'ai_likelihood': 7.682376437717014e-06, 'text': 'ATBRG: Adaptive Target-Behavior Relational Graph Network for Effective\n  Recommendation\n\n  Recommender system (RS) devotes to predicting user preference to a given item\nand has been widely deployed in most web-scale applications. Recently,\nknowledge graph (KG) attracts much attention in RS due to its abundant\nconnective information. Existing methods either explore independent meta-paths\nfor user-item pairs over KG, or employ graph neural network (GNN) on whole KG\nto produce representations for users and items separately. Despite\neffectiveness, the former type of methods fails to fully capture structural\ninformation implied in KG, while the latter ignores the mutual effect between\ntarget user and item during the embedding propagation. In this work, we propose\na new framework named Adaptive Target-Behavior Relational Graph network (ATBRG\nfor short) to effectively capture structural relations of target user-item\npairs over KG. Specifically, to associate the given target item with user\nbehaviors over KG, we propose the graph connect and graph prune techniques to\nconstruct adaptive target-behavior relational graph. To fully distill\nstructural information from the sub-graph connected by rich relations in an\nend-to-end fashion, we elaborate on the model design of ATBRG, equipped with\nrelation-aware extractor layer and representation activation layer. We perform\nextensive experiments on both industrial and benchmark datasets. Empirical\nresults show that ATBRG consistently and significantly outperforms\nstate-of-the-art methods. Moreover, ATBRG has also achieved a performance\nimprovement of 5.1% on CTR metric after successful deployment in one popular\nrecommendation scenario of Taobao APP.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.14255,regular,pre_llm,2020,5,"{'ai_likelihood': 4.7021441989474825e-06, 'text': 'Towards Question-based Recommender Systems\n\n  Conversational and question-based recommender systems have gained increasing\nattention in recent years, with users enabled to converse with the system and\nbetter control recommendations. Nevertheless, research in the field is still\nlimited, compared to traditional recommender systems. In this work, we propose\na novel Question-based recommendation method, Qrec, to assist users to find\nitems interactively, by answering automatically constructed and algorithmically\nchosen questions. Previous conversational recommender systems ask users to\nexpress their preferences over items or item facets. Our model, instead, asks\nusers to express their preferences over descriptive item features. The model is\nfirst trained offline by a novel matrix factorization algorithm, and then\niteratively updates the user and item latent factors online by a closed-form\nsolution based on the user answers. Meanwhile, our model infers the underlying\nuser belief and preferences over items to learn an optimal question-asking\nstrategy by using Generalized Binary Search, so as to ask a sequence of\nquestions to the user. Our experimental results demonstrate that our proposed\nmatrix factorization model outperforms the traditional Probabilistic Matrix\nFactorization model. Further, our proposed Qrec model can greatly improve the\nperformance of state-of-the-art baselines, and it is also effective in the case\nof cold-start user and item recommendations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.13829,review,pre_llm,2020,5,"{'ai_likelihood': 6.357828776041667e-06, 'text': ""A Re-visit of the Popularity Baseline in Recommender Systems\n\n  Popularity is often included in experimental evaluation to provide a\nreference performance for a recommendation task. To understand how popularity\nbaseline is defined and evaluated, we sample 12 papers from top-tier\nconferences including KDD, WWW, SIGIR, and RecSys, and 6 open source toolkits.\nWe note that the widely adopted MostPop baseline simply ranks items based on\nthe number of interactions in the training data. We argue that the current\nevaluation of popularity (i) does not reflect the popular items at the time\nwhen a user interacts with the system, and (ii) may recommend items released\nafter a user's last interaction with the system. On the widely used MovieLens\ndataset, we show that the performance of popularity could be significantly\nimproved by 70% or more, if we consider the popular items at the time point\nwhen a user interacts with the system. We further show that, on MovieLens\ndataset, the users having lower tendencies on movies tend to follow the crowd\nand rate more popular movies. Movie lovers who rate a large number of movies,\nrate movies based on their own preferences and interests. Through this study,\nwe call for a re-visit of the popularity baseline in recommender system to\nbetter reflect its effectiveness.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.11992,regular,pre_llm,2020,5,"{'ai_likelihood': 1.5695889790852867e-05, 'text': 'MPSUM: Entity Summarization with Predicate-based Matching\n\n  With the development of Semantic Web, entity summarization has become an\nemerging task to generate concrete summaries for real world entities. To solve\nthis problem, we propose an approach named MPSUM that extends a probabilistic\ntopic model by integrating the idea of predicate-uniqueness and\nobject-importance for ranking triples. The approach aims at generating brief\nbut representative summaries for entities. We compare our approach with the\nstate-of-the-art methods using DBpedia and LinkedMDB datasets.The experimental\nresults show that our work improves the quality of entity summarization.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.14271,regular,pre_llm,2020,5,"{'ai_likelihood': 8.311536577012805e-06, 'text': 'Relation Extraction with Explanation\n\n  Recent neural models for relation extraction with distant supervision\nalleviate the impact of irrelevant sentences in a bag by learning importance\nweights for the sentences. Efforts thus far have focused on improving\nextraction accuracy but little is known about their explainability. In this\nwork we annotate a test set with ground-truth sentence-level explanations to\nevaluate the quality of explanations afforded by the relation extraction\nmodels. We demonstrate that replacing the entity mentions in the sentences with\ntheir fine-grained entity types not only enhances extraction accuracy but also\nimproves explanation. We also propose to automatically generate ""distractor""\nsentences to augment the bags and train the model to ignore the distractors.\nEvaluations on the widely used FB-NYT dataset show that our methods achieve new\nstate-of-the-art accuracy while improving model explainability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.12989,regular,pre_llm,2020,5,"{'ai_likelihood': 4.967053731282552e-07, 'text': ""Ranking-Incentivized Quality Preserving Content Modification\n\n  The Web is a canonical example of a competitive retrieval setting where many\ndocuments' authors consistently modify their documents to promote them in\nrankings. We present an automatic method for quality-preserving modification of\ndocument content -- i.e., maintaining content quality -- so that the document\nis ranked higher for a query by a non-disclosed ranking function whose rankings\ncan be observed. The method replaces a passage in the document with some other\npassage. To select the two passages, we use a learning-to-rank approach with a\nbi-objective optimization criterion: rank promotion and content-quality\nmaintenance. We used the approach as a bot in content-based ranking\ncompetitions. Analysis of the competitions demonstrates the merits of our\napproach with respect to human content modifications in terms of rank\npromotion, content-quality maintenance and relevance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.10545,regular,pre_llm,2020,5,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'ESAM: Discriminative Domain Adaptation with Non-Displayed Items to\n  Improve Long-Tail Performance\n\n  Most of ranking models are trained only with displayed items (most are hot\nitems), but they are utilized to retrieve items in the entire space which\nconsists of both displayed and non-displayed items (most are long-tail items).\nDue to the sample selection bias, the long-tail items lack sufficient records\nto learn good feature representations, i.e. data sparsity and cold start\nproblems. The resultant distribution discrepancy between displayed and\nnon-displayed items would cause poor long-tail performance. To this end, we\npropose an entire space adaptation model (ESAM) to address this problem from\nthe perspective of domain adaptation (DA). ESAM regards displayed and\nnon-displayed items as source and target domains respectively. Specifically, we\ndesign the attribute correlation alignment that considers the correlation\nbetween high-level attributes of the item to achieve distribution alignment.\nFurthermore, we introduce two effective regularization strategies, i.e.\n\\textit{center-wise clustering} and \\textit{self-training} to improve DA\nprocess. Without requiring any auxiliary information and auxiliary domains,\nESAM transfers the knowledge from displayed items to non-displayed items for\nalleviating the distribution inconsistency. Experiments on two public datasets\nand a large-scale industrial dataset collected from Taobao demonstrate that\nESAM achieves state-of-the-art performance, especially in the long-tail space.\nBesides, we deploy ESAM to the Taobao search engine, leading to significant\nimprovement on online performance. The code is available at\n\\url{https://github.com/A-bone1/ESAM.git}\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.00127,regular,pre_llm,2020,5,"{'ai_likelihood': 2.5497542487250434e-06, 'text': 'Automatic Generation of Topic Labels\n\n  Topic modelling is a popular unsupervised method for identifying the\nunderlying themes in document collections that has many applications in\ninformation retrieval. A topic is usually represented by a list of terms ranked\nby their probability but, since these can be difficult to interpret, various\napproaches have been developed to assign descriptive labels to topics. Previous\nwork on the automatic assignment of labels to topics has relied on a two-stage\napproach: (1) candidate labels are retrieved from a large pool (e.g. Wikipedia\narticle titles); and then (2) re-ranked based on their semantic similarity to\nthe topic terms. However, these extractive approaches can only assign candidate\nlabels from a restricted set that may not include any suitable ones. This paper\nproposes using a sequence-to-sequence neural-based approach to generate labels\nthat does not suffer from this limitation. The model is trained over a new\nlarge synthetic dataset created using distant supervision. The method is\nevaluated by comparing the labels it generates to ones rated by humans.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.09252,regular,pre_llm,2020,5,"{'ai_likelihood': 8.940696716308594e-06, 'text': 'Multi-Modal Summary Generation using Multi-Objective Optimization\n\n  Significant development of communication technology over the past few years\nhas motivated research in multi-modal summarization techniques. A majority of\nthe previous works on multi-modal summarization focus on text and images. In\nthis paper, we propose a novel extractive multi-objective optimization based\nmodel to produce a multi-modal summary containing text, images, and videos.\nImportant objectives such as intra-modality salience, cross-modal redundancy\nand cross-modal similarity are optimized simultaneously in a multi-objective\noptimization framework to produce effective multi-modal output. The proposed\nmodel has been evaluated separately for different modalities, and has been\nfound to perform better than state-of-the-art approaches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.14171,regular,pre_llm,2020,5,"{'ai_likelihood': 1.2252065870496963e-06, 'text': ""User Behavior Retrieval for Click-Through Rate Prediction\n\n  Click-through rate (CTR) prediction plays a key role in modern online\npersonalization services. In practice, it is necessary to capture user's\ndrifting interests by modeling sequential user behaviors to build an accurate\nCTR prediction model. However, as the users accumulate more and more behavioral\ndata on the platforms, it becomes non-trivial for the sequential models to make\nuse of the whole behavior history of each user. First, directly feeding the\nlong behavior sequence will make online inference time and system load\ninfeasible. Second, there is much noise in such long histories to fail the\nsequential model learning. The current industrial solutions mainly truncate the\nsequences and just feed recent behaviors to the prediction model, which leads\nto a problem that sequential patterns such as periodicity or long-term\ndependency are not embedded in the recent several behaviors but in far back\nhistory. To tackle these issues, in this paper we consider it from the data\nperspective instead of just designing more sophisticated yet complicated models\nand propose User Behavior Retrieval for CTR prediction (UBR4CTR) framework. In\nUBR4CTR, the most relevant and appropriate user behaviors will be firstly\nretrieved from the entire user history sequence using a learnable search\nmethod. These retrieved behaviors are then fed into a deep model to make the\nfinal prediction instead of simply using the most recent ones. It is highly\nfeasible to deploy UBR4CTR into industrial model pipeline with low cost.\nExperiments on three real-world large-scale datasets demonstrate the\nsuperiority and efficacy of our proposed framework and models.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.05768,regular,pre_llm,2020,5,"{'ai_likelihood': 1.8212530348036024e-05, 'text': ""Interpreting Neural Ranking Models using Grad-CAM\n\n  Recently, applying deep neural networks in IR has become an important and\ntimely topic. For instance, Neural Ranking Models(NRMs) have shown promising\nperformance compared to the traditional ranking models. However, explaining the\nranking results has become even more difficult with NRM due to the complex\nstructure of neural networks. On the other hand, a great deal of research is\nunder progress on Interpretable Machine Learning(IML), including Grad-CAM.\nGrad-CAM is an attribution method and it can visualize the input regions that\ncontribute to the network's output. In this paper, we adopt Grad-CAM for\ninterpreting the ranking results of NRM. By adopting Grad-CAM, we analyze how\neach query-document term pair contributes to the matching score for a given\npair of query and document. The visualization results provide insights on why a\ncertain document is relevant to the given query. Also, the results show that\nneural ranking model captures the subtle notion of relevance. Our\ninterpretation method and visualization results can be used for snippet\ngeneration and user-query intent analysis.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.09344,regular,pre_llm,2020,5,"{'ai_likelihood': 1.7881393432617188e-06, 'text': ""Try This Instead: Personalized and Interpretable Substitute\n  Recommendation\n\n  As a fundamental yet significant process in personalized recommendation,\ncandidate generation and suggestion effectively help users spot the most\nsuitable items for them. Consequently, identifying substitutable items that are\ninterchangeable opens up new opportunities to refine the quality of generated\ncandidates. When a user is browsing a specific type of product (e.g., a laptop)\nto buy, the accurate recommendation of substitutes (e.g., better equipped\nlaptops) can offer the user more suitable options to choose from, thus\nsubstantially increasing the chance of a successful purchase. However, existing\nmethods merely treat this problem as mining pairwise item relationships without\nthe consideration of users' personal preferences. Moreover, the substitutable\nrelationships are implicitly identified through the learned latent\nrepresentations of items, leading to uninterpretable recommendation results. In\nthis paper, we propose attribute-aware collaborative filtering (A2CF) to\nperform substitute recommendation by addressing issues from both\npersonalization and interpretability perspectives. Instead of directly\nmodelling user-item interactions, we extract explicit and polarized item\nattributes from user reviews with sentiment analysis, whereafter the\nrepresentations of attributes, users, and items are simultaneously learned.\nThen, by treating attributes as the bridge between users and items, we can\nthoroughly model the user-item preferences (i.e., personalization) and\nitem-item relationships (i.e., substitution) for recommendation. In addition,\nA2CF is capable of generating intuitive interpretations by analyzing which\nattributes a user currently cares the most and comparing the recommended\nsubstitutes with her/his currently browsed items at an attribute level. The\nrecommendation effectiveness and interpretation quality of A2CF are\ndemonstrated via extensive experiments on three real datasets.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.12566,regular,pre_llm,2020,5,"{'ai_likelihood': 1.2252065870496963e-06, 'text': ""Hierarchical Fashion Graph Network for Personalized Outfit\n  Recommendation\n\n  Fashion outfit recommendation has attracted increasing attentions from online\nshopping services and fashion communities.Distinct from other scenarios (e.g.,\nsocial networking or content sharing) which recommend a single item (e.g., a\nfriend or picture) to a user, outfit recommendation predicts user preference on\na set of well-matched fashion items.Hence, performing high-quality personalized\noutfit recommendation should satisfy two requirements -- 1) the nice\ncompatibility of fashion items and 2) the consistence with user preference.\nHowever, present works focus mainly on one of the requirements and only\nconsider either user-outfit or outfit-item relationships, thereby easily\nleading to suboptimal representations and limiting the performance. In this\nwork, we unify two tasks, fashion compatibility modeling and personalized\noutfit recommendation. Towards this end, we develop a new framework,\nHierarchical Fashion Graph Network(HFGN), to model relationships among users,\nitems, and outfits simultaneously. In particular, we construct a hierarchical\nstructure upon user-outfit interactions and outfit-item mappings. We then get\ninspirations from recent graph neural networks, and employ the embedding\npropagation on such hierarchical graph, so as to aggregate item information\ninto an outfit representation, and then refine a user's representation via\nhis/her historical outfits. Furthermore, we jointly train these two tasks to\noptimize these representations. To demonstrate the effectiveness of HFGN, we\nconduct extensive experiments on a benchmark dataset, and HFGN achieves\nsignificant improvements over the state-of-the-art compatibility matching\nmodels like NGNN and outfit recommenders like FHN.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.04456,regular,pre_llm,2020,5,"{'ai_likelihood': 3.029902776082357e-05, 'text': ""Rethinking Item Importance in Session-based Recommendation\n\n  Session-based recommendation aims to predict users' based on anonymous\nsessions. Previous work mainly focuses on the transition relationship between\nitems during an ongoing session. They generally fail to pay enough attention to\nthe importance of the items in terms of their relevance to user's main intent.\nIn this paper, we propose a Session-based Recommendation approach with an\nImportance Extraction Module, i.e., SR-IEM, that considers both a user's\nlong-term and recent behavior in an ongoing session. We employ a modified\nself-attention mechanism to estimate item importance in a session, which is\nthen used to predict user's long-term preference. Item recommendations are\nproduced by combining the user's long-term preference and current interest as\nconveyed by the last interacted item. Experiments conducted on two benchmark\ndatasets validate that SR-IEM outperforms the start-of-the-art in terms of\nRecall and MRR and has a reduced computational complexity.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.01148,regular,pre_llm,2020,5,"{'ai_likelihood': 3.7749608357747397e-06, 'text': ""FairMatch: A Graph-based Approach for Improving Aggregate Diversity in\n  Recommender Systems\n\n  Recommender systems are often biased toward popular items. In other words,\nfew items are frequently recommended while the majority of items do not get\nproportionate attention. That leads to low coverage of items in recommendation\nlists across users (i.e. low aggregate diversity) and unfair distribution of\nrecommended items. In this paper, we introduce FairMatch, a general graph-based\nalgorithm that works as a post-processing approach after recommendation\ngeneration for improving aggregate diversity. The algorithm iteratively finds\nitems that are rarely recommended yet are high-quality and add them to the\nusers' final recommendation lists. This is done by solving the maximum flow\nproblem on the recommendation bipartite graph. While we focus on aggregate\ndiversity and fair distribution of recommended items, the algorithm can be\nadapted to other recommendation scenarios using different underlying\ndefinitions of fairness. A comprehensive set of experiments on two datasets and\ncomparison with state-of-the-art baselines show that FairMatch, while\nsignificantly improving aggregate diversity, provides comparable recommendation\naccuracy.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.13007,regular,pre_llm,2020,5,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'DimensionRank: Personal Neural Representations for Personalized General\n  Search\n\n  Web Search and Social Media have always been two of the most important\napplications on the internet. We begin by giving a unified framework, called\ngeneral search, of which which all search and social media products can be seen\nas instances.\n  DimensionRank is our main contribution. This is an algorithm for personalized\ngeneral search, based on neural networks. DimensionRank\'s bold innovation is to\nmodel and represent each user using their own unique personal neural\nrepresentation vector, a learned representation in a real-valued\nmultidimensional vector space. This is the first internet service we are aware\nof that to model each user with their own independent representation vector.\nThis is also the first service we are aware of to attempt personalization for\ngeneral web search. Also, neural representations allows us to present the first\nReddit-style algorithm, that is immune to the problem of ""brigading"". We\nbelieve personalized general search will yield a search product orders of\nmagnitude better than Google\'s one-size-fits-all web search algorithm.\n  Finally, we announce Deep Revelations, a new search and social network\ninternet application based on DimensionRank.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.04908,regular,pre_llm,2020,5,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Local Self-Attention over Long Text for Efficient Document Retrieval\n\n  Neural networks, particularly Transformer-based architectures, have achieved\nsignificant performance improvements on several retrieval benchmarks. When the\nitems being retrieved are documents, the time and memory cost of employing\nTransformers over a full sequence of document terms can be prohibitive. A\npopular strategy involves considering only the first n terms of the document.\nThis can, however, result in a biased system that under retrieves longer\ndocuments. In this work, we propose a local self-attention which considers a\nmoving window over the document terms and for each term attends only to other\nterms in the same window. This local attention incurs a fraction of the compute\nand memory cost of attention over the whole document. The windowed approach\nalso leads to more compact packing of padded documents in minibatches resulting\nin additional savings. We also employ a learned saturation function and a\ntwo-staged pooling strategy to identify relevant regions of the document. The\nTransformer-Kernel pooling model with these changes can efficiently elicit\nrelevance information from documents with thousands of tokens. We benchmark our\nproposed modifications on the document ranking task from the TREC 2019 Deep\nLearning track and observe significant improvements in retrieval quality as\nwell as increased retrieval of longer documents at moderate increase in compute\nand memory costs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.00617,regular,pre_llm,2020,5,"{'ai_likelihood': 5.298190646701389e-07, 'text': ""Content-aware Neural Hashing for Cold-start Recommendation\n\n  Content-aware recommendation approaches are essential for providing\nmeaningful recommendations for \\textit{new} (i.e., \\textit{cold-start}) items\nin a recommender system. We present a content-aware neural hashing-based\ncollaborative filtering approach (NeuHash-CF), which generates binary hash\ncodes for users and items, such that the highly efficient Hamming distance can\nbe used for estimating user-item relevance. NeuHash-CF is modelled as an\nautoencoder architecture, consisting of two joint hashing components for\ngenerating user and item hash codes. Inspired from semantic hashing, the item\nhashing component generates a hash code directly from an item's content\ninformation (i.e., it generates cold-start and seen item hash codes in the same\nmanner). This contrasts existing state-of-the-art models, which treat the two\nitem cases separately. The user hash codes are generated directly based on user\nid, through learning a user embedding matrix. We show experimentally that\nNeuHash-CF significantly outperforms state-of-the-art baselines by up to 12\\%\nNDCG and 13\\% MRR in cold-start recommendation settings, and up to 4\\% in both\nNDCG and MRR in standard settings where all items are present while training.\nOur approach uses 2-4x shorter hash codes, while obtaining the same or better\nperformance compared to the state of the art, thus consequently also enabling a\nnotable storage reduction.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.15939,regular,pre_llm,2020,6,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'TFNet: Multi-Semantic Feature Interaction for CTR Prediction\n\n  The CTR (Click-Through Rate) prediction plays a central role in the domain of\ncomputational advertising and recommender systems. There exists several kinds\nof methods proposed in this field, such as Logistic Regression (LR),\nFactorization Machines (FM) and deep learning based methods like Wide&Deep,\nNeural Factorization Machines (NFM) and DeepFM. However, such approaches\ngenerally use the vector-product of each pair of features, which have ignored\nthe different semantic spaces of the feature interactions. In this paper, we\npropose a novel Tensor-based Feature interaction Network (TFNet) model, which\nintroduces an operating tensor to elaborate feature interactions via\nmulti-slice matrices in multiple semantic spaces. Extensive offline and online\nexperiments show that TFNet: 1) outperforms the competitive compared methods on\nthe typical Criteo and Avazu datasets; 2) achieves large improvement of revenue\nand click rate in online A/B tests in the largest Chinese App recommender\nsystem, Tencent MyApp.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.01926,regular,pre_llm,2020,6,"{'ai_likelihood': 9.834766387939453e-06, 'text': 'Would You Like to Hear the News? Investigating Voice-BasedSuggestions\n  for Conversational News Recommendation\n\n  One of the key benefits of voice-based personal assistants is the potential\nto proactively recommend relevant and interesting information. One of the most\nvaluable sources of such information is the News. However, in order for the\nuser to hear the news that is useful and relevant to them, it must be\nrecommended in an interesting and informative way. However, to the best of our\nknowledge, how to present a news item for a voice-based recommendation remains\nan open question. In this paper, we empirically compare different ways of\nrecommending news, or specific news items, in a voice-based conversational\nsetting. Specifically, we study the user engagement and satisfaction with five\ndifferent variants of presenting news recommendations: (1) a generic news\nbriefing; (2) news about a specific entity relevant to the current\nconversation; (3) news about an entity from a past conversation; (4) news on a\ntrending news topic; and (5) the default - a suggestion to talk about news in\ngeneral. Our results show that entity-based news recommendations exhibit 29%\nhigher acceptance compared to briefing recommendations, and almost 100% higher\nacceptance compared to recommending generic or trending news. Our investigation\ninto the presentation of news recommendations and the resulting insights could\nmake voice assistants more informative and engaging.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.13864,regular,pre_llm,2020,6,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'Community-Based Data Integration of Course and Job Data in Support of\n  Personalized Career-Education Recommendations\n\n  How does your education impact your professional career? Ideally, the courses\nyou take help you identify, get hired for, and perform the job you always\nwanted. However, not all courses provide skills that transfer to existing and\nfuture jobs; skill terms used in course descriptions might be different from\nthose listed in job advertisements; and there might exist a considerable skill\ngap between what is taught in courses and what is needed for a job. In this\nstudy, we propose a novel method to integrate extensive course description and\njob advertisement data by leveraging heterogeneous data integration and\ncommunity detection. The innovative heterogeneous graph approach along with\nidentified skill communities enables cross-domain information recommendation,\ne.g., given an educational profile, job recommendations can be provided\ntogether with suggestions on education opportunities for re- and upskilling in\nsupport of lifelong learning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.16742,regular,pre_llm,2020,6,"{'ai_likelihood': 1.4139546288384332e-05, 'text': ""FairRec: Fairness-aware News Recommendation with Decomposed Adversarial\n  Learning\n\n  News recommendation is important for online news services. Existing news\nrecommendation models are usually learned from users' news click behaviors.\nUsually the behaviors of users with the same sensitive attributes (e.g.,\ngenders) have similar patterns and news recommendation models can easily\ncapture these patterns. It may lead to some biases related to sensitive user\nattributes in the recommendation results, e.g., always recommending sports news\nto male users, which is unfair since users may not receive diverse news\ninformation. In this paper, we propose a fairness-aware news recommendation\napproach with decomposed adversarial learning and orthogonality regularization,\nwhich can alleviate unfairness in news recommendation brought by the biases of\nsensitive user attributes. In our approach, we propose to decompose the user\ninterest model into two components. One component aims to learn a bias-aware\nuser embedding that captures the bias information on sensitive user attributes,\nand the other aims to learn a bias-free user embedding that only encodes\nattribute-independent user interest information for fairness-aware news\nrecommendation. In addition, we propose to apply an attribute prediction task\nto the bias-aware user embedding to enhance its ability on bias modeling, and\nwe apply adversarial learning to the bias-free user embedding to remove the\nbias information from it. Moreover, we propose an orthogonality regularization\nmethod to encourage the bias-free user embeddings to be orthogonal to the\nbias-aware one to better distinguish the bias-free user embedding from the\nbias-aware one. For fairness-aware news ranking, we only use the bias-free user\nembedding. Extensive experiments on benchmark dataset show that our approach\ncan effectively improve fairness in news recommendation with minor performance\nloss.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.05563,regular,pre_llm,2020,6,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Using BibTeX to Automatically Generate Labeled Data for Citation Field\n  Extraction\n\n  Accurate parsing of citation reference strings is crucial to automatically\nconstruct scholarly databases such as Google Scholar or Semantic Scholar.\nCitation field extraction (CFE) is precisely this task---given a reference\nlabel which tokens refer to the authors, venue, title, editor, journal, pages,\netc. Most methods for CFE are supervised and rely on training from labeled\ndatasets that are quite small compared to the great variety of reference\nformats. BibTeX, the widely used reference management tool, provides a natural\nmethod to automatically generate and label training data for CFE. In this\npaper, we describe a technique for using BibTeX to generate, automatically, a\nlarge-scale 41M labeled strings), labeled dataset, that is four orders of\nmagnitude larger than the current largest CFE dataset, namely the UMass\nCitation Field Extraction dataset [Anzaroot and McCallum, 2013]. We\nexperimentally demonstrate how our dataset can be used to improve the\nperformance of the UMass CFE using a RoBERTa-based [Liu et al., 2019] model. In\ncomparison to previous SoTA, we achieve a 24.48% relative error reduction,\nachieving span level F1-scores of 96.3%.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.10389,review,pre_llm,2020,6,"{'ai_likelihood': 2.185503641764323e-06, 'text': 'Interactive Recommender System via Knowledge Graph-enhanced\n  Reinforcement Learning\n\n  Interactive recommender system (IRS) has drawn huge attention because of its\nflexible recommendation strategy and the consideration of optimal long-term\nuser experiences. To deal with the dynamic user preference and optimize\naccumulative utilities, researchers have introduced reinforcement learning (RL)\ninto IRS. However, RL methods share a common issue of sample efficiency, i.e.,\nhuge amount of interaction data is required to train an effective\nrecommendation policy, which is caused by the sparse user responses and the\nlarge action space consisting of a large number of candidate items. Moreover,\nit is infeasible to collect much data with explorative policies in online\nenvironments, which will probably harm user experience. In this work, we\ninvestigate the potential of leveraging knowledge graph (KG) in dealing with\nthese issues of RL methods for IRS, which provides rich side information for\nrecommendation decision making. Instead of learning RL policies from scratch,\nwe make use of the prior knowledge of the item correlation learned from KG to\n(i) guide the candidate selection for better candidate item retrieval, (ii)\nenrich the representation of items and user states, and (iii) propagate user\npreferences among the correlated items over KG to deal with the sparsity of\nuser feedback. Comprehensive experiments have been conducted on two real-world\ndatasets, which demonstrate the superiority of our approach with significant\nimprovements against state-of-the-arts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.05009,regular,pre_llm,2020,6,"{'ai_likelihood': 5.265076955159506e-06, 'text': 'Few-Shot Generative Conversational Query Rewriting\n\n  Conversational query rewriting aims to reformulate a concise conversational\nquery to a fully specified, context-independent query that can be effectively\nhandled by existing information retrieval systems. This paper presents a\nfew-shot generative approach to conversational query rewriting. We develop two\nmethods, based on rules and self-supervised learning, to generate weak\nsupervision data using large amounts of ad hoc search sessions, and to\nfine-tune GPT-2 to rewrite conversational queries. On the TREC Conversational\nAssistance Track, our weakly supervised GPT-2 rewriter improves the\nstate-of-the-art ranking accuracy by 12%, only using very limited amounts of\nmanual query rewrites. In the zero-shot learning setting, the rewriter still\ngives a comparable result to previous state-of-the-art systems. Our analyses\nreveal that GPT-2 effectively picks up the task syntax and learns to capture\ncontext dependencies, even for hard cases that involve group references and\nlong-turn dependencies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.11632,regular,pre_llm,2020,6,"{'ai_likelihood': 4.503462049696181e-06, 'text': ""Embedding-based Retrieval in Facebook Search\n\n  Search in social networks such as Facebook poses different challenges than in\nclassical web search: besides the query text, it is important to take into\naccount the searcher's context to provide relevant results. Their social graph\nis an integral part of this context and is a unique aspect of Facebook search.\nWhile embedding-based retrieval (EBR) has been applied in eb search engines for\nyears, Facebook search was still mainly based on a Boolean matching model. In\nthis paper, we discuss the techniques for applying EBR to a Facebook Search\nsystem. We introduce the unified embedding framework developed to model\nsemantic embeddings for personalized search, and the system to serve\nembedding-based retrieval in a typical search system based on an inverted\nindex. We discuss various tricks and experiences on end-to-end optimization of\nthe whole system, including ANN parameter tuning and full-stack optimization.\nFinally, we present our progress on two selected advanced topics about\nmodeling. We evaluated EBR on verticals for Facebook Search with significant\nmetrics gains observed in online A/B experiments. We believe this paper will\nprovide useful insights and experiences to help people on developing\nembedding-based retrieval systems in search engines.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.14827,regular,pre_llm,2020,6,"{'ai_likelihood': 6.4240561591254345e-06, 'text': 'Memory-efficient Embedding for Recommendations\n\n  Practical large-scale recommender systems usually contain thousands of\nfeature fields from users, items, contextual information, and their\ninteractions. Most of them empirically allocate a unified dimension to all\nfeature fields, which is memory inefficient. Thus it is highly desired to\nassign different embedding dimensions to different feature fields according to\ntheir importance and predictability. Due to the large amounts of feature fields\nand the nuanced relationship between embedding dimensions with feature\ndistributions and neural network architectures, manually allocating embedding\ndimensions in practical recommender systems can be very difficult. To this end,\nwe propose an AutoML based framework (AutoDim) in this paper, which can\nautomatically select dimensions for different feature fields in a data-driven\nfashion. Specifically, we first proposed an end-to-end differentiable framework\nthat can calculate the weights over various dimensions for feature fields in a\nsoft and continuous manner with an AutoML based optimization algorithm; then we\nderive a hard and discrete embedding component architecture according to the\nmaximal weights and retrain the whole recommender framework. We conduct\nextensive experiments on benchmark datasets to validate the effectiveness of\nthe AutoDim framework.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.04153,regular,pre_llm,2020,6,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Denoising Implicit Feedback for Recommendation\n\n  The ubiquity of implicit feedback makes them the default choice to build\nonline recommender systems. While the large volume of implicit feedback\nalleviates the data sparsity issue, the downside is that they are not as clean\nin reflecting the actual satisfaction of users. For example, in E-commerce, a\nlarge portion of clicks do not translate to purchases, and many purchases end\nup with negative reviews. As such, it is of critical importance to account for\nthe inevitable noises in implicit feedback for recommender training. However,\nlittle work on recommendation has taken the noisy nature of implicit feedback\ninto consideration.\n  In this work, we explore the central theme of denoising implicit feedback for\nrecommender training. We find serious negative impacts of noisy implicit\nfeedback,i.e., fitting the noisy data prevents the recommender from learning\nthe actual user preference. Our target is to identify and prune noisy\ninteractions, so as to improve the quality of recommender training. By\nobserving the process of normal recommender training, we find that noisy\nfeedback typically has large loss values in the early stages. Inspired by this\nobservation, we propose a new training strategy namedAdaptive Denoising\nTraining(ADT), which adaptively prunes noisy interactions during training.\nSpecifically, we devise two paradigms for adaptive loss formulation: Truncated\nLoss that discards the large-loss samples with a dynamic threshold in each\niteration; and reweighted Loss that adaptively lowers the weight of large-loss\nsamples. We instantiate the two paradigms on the widely used binary\ncross-entropy loss and test the proposed ADT strategies on three representative\nrecommenders. Extensive experiments on three benchmarks demonstrate that ADT\nsignificantly improves the quality of recommendation over normal training.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.02785,regular,pre_llm,2020,6,"{'ai_likelihood': 1.8874804178873699e-06, 'text': 'What Makes a Top-Performing Precision Medicine Search Engine? Tracing\n  Main System Features in a Systematic Way\n\n  From 2017 to 2019 the Text REtrieval Conference (TREC) held a challenge task\non precision medicine using documents from medical publications (PubMed) and\nclinical trials. Despite lots of performance measurements carried out in these\nevaluation campaigns, the scientific community is still pretty unsure about the\nimpact individual system features and their weights have on the overall system\nperformance. In order to overcome this explanatory gap, we first determined\noptimal feature configurations using the Sequential Model-based Algorithm\nConfiguration (SMAC) program and applied its output to a BM25-based search\nengine. We then ran an ablation study to systematically assess the individual\ncontributions of relevant system features: BM25 parameters, query type and\nweighting schema, query expansion, stop word filtering, and keyword boosting.\nFor evaluation, we employed the gold standard data from the three TREC-PM\ninstallments to evaluate the effectiveness of different features using the\ncommonly shared infNDCG metric.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.04282,review,pre_llm,2020,6,"{'ai_likelihood': 4.13921144273546e-06, 'text': ""Equality of Learning Opportunity via Individual Fairness in Personalized\n  Recommendations\n\n  Online educational platforms are playing a primary role in mediating the\nsuccess of individuals' careers. Therefore, while building overlying content\nrecommendation services, it becomes essential to guarantee that learners are\nprovided with equal recommended learning opportunities, according to the\nplatform values, context, and pedagogy. Though the importance of ensuring\nequality of learning opportunities has been well investigated in traditional\ninstitutions, how this equality can be operationalized in online learning\necosystems through recommender systems is still under-explored. In this paper,\nwe formalize educational principles that model recommendations' learning\nproperties, and a novel fairness metric that combines them in order to monitor\nthe equality of recommended learning opportunities among learners. Then, we\nenvision a scenario wherein an educational platform should be arranged in such\na way that the generated recommendations meet each principle to a certain\ndegree for all learners, constrained to their individual preferences. Under\nthis view, we explore the learning opportunities provided by recommender\nsystems in a large-scale course platform, uncovering systematic inequalities.\nTo reduce this effect, we propose a novel post-processing approach that\nbalances personalization and equality of recommended opportunities. Experiments\nshow that our approach leads to higher equality, with a negligible loss in\npersonalization. Our study moves a step forward in operationalizing the ethics\nof human learning in recommendations, a core unit of intelligent educational\nsystems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.08732,regular,pre_llm,2020,6,"{'ai_likelihood': 3.543164994981554e-06, 'text': 'Evaluating Conversational Recommender Systems via User Simulation\n\n  Conversational information access is an emerging research area. Currently,\nhuman evaluation is used for end-to-end system evaluation, which is both very\ntime and resource intensive at scale, and thus becomes a bottleneck of\nprogress. As an alternative, we propose automated evaluation by means of\nsimulating users. Our user simulator aims to generate responses that a real\nhuman would give by considering both individual preferences and the general\nflow of interaction with the system. We evaluate our simulation approach on an\nitem recommendation task by comparing three existing conversational recommender\nsystems. We show that preference modeling and task-specific interaction models\nboth contribute to more realistic simulations, and can help achieve high\ncorrelation between automatic evaluation measures and manual human assessments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.02282,regular,pre_llm,2020,6,"{'ai_likelihood': 9.934107462565105e-07, 'text': ""Towards Personalized and Semantic Retrieval: An End-to-End Solution for\n  E-commerce Search via Embedding Learning\n\n  Nowadays e-commerce search has become an integral part of many people's\nshopping routines. Two critical challenges stay in today's e-commerce search:\nhow to retrieve items that are semantically relevant but not exact matching to\nquery terms, and how to retrieve items that are more personalized to different\nusers for the same search query. In this paper, we present a novel approach\ncalled DPSR, which stands for Deep Personalized and Semantic Retrieval, to\ntackle this problem. Explicitly, we share our design decisions on how to\narchitect a retrieval system so as to serve industry-scale traffic efficiently\nand how to train a model so as to learn query and item semantics accurately.\nBased on offline evaluations and online A/B test with live traffics, we show\nthat DPSR model outperforms existing models, and DPSR system can retrieve more\npersonalized and semantically relevant items to significantly improve users'\nsearch experience by +1.29% conversion rate, especially for long tail queries\nby +10.03%. As a result, our DPSR system has been successfully deployed into\nJD.com's search production since 2019.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.04084,regular,pre_llm,2020,6,"{'ai_likelihood': 3.8080745273166235e-06, 'text': ""SERank: Optimize Sequencewise Learning to Rank Using\n  Squeeze-and-Excitation Network\n\n  Learning-to-rank (LTR) is a set of supervised machine learning algorithms\nthat aim at generating optimal ranking order over a list of items. A lot of\nranking models have been studied during the past decades. And most of them\ntreat each query document pair independently during training and inference.\nRecently, there are a few methods have been proposed which focused on mining\ninformation across ranking candidates list for further improvements, such as\nlearning multivariant scoring function or learning contextual embedding.\nHowever, these methods usually greatly increase computational cost during\nonline inference, especially when with large candidates size in real-world web\nsearch systems. What's more, there are few studies that focus on novel design\nof model structure for leveraging information across ranking candidates. In\nthis work, we propose an effective and efficient method named as SERank which\nis a Sequencewise Ranking model by using Squeeze-and-Excitation network to take\nadvantage of cross-document information. Moreover, we examine our proposed\nmethods on several public benchmark datasets, as well as click logs collected\nfrom a commercial Question Answering search engine, Zhihu. In addition, we also\nconduct online A/B testing at Zhihu search engine to further verify the\nproposed approach. Results on both offline datasets and online A/B testing\ndemonstrate that our method contributes to a significant improvement.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.07073,regular,pre_llm,2020,6,"{'ai_likelihood': 2.615981631808811e-06, 'text': 'Feedback Clustering for Online Travel Agencies Searches: a Case Study\n\n  Understanding choices performed by online customers is a growing need in the\ntravel industry. In many practical situations, the only available information\nis the flight search query performed by the customer with no additional profile\nknowledge. In general, customer flight bookings are driven by prices, duration,\nnumber of connections, and so on. However, not all customers might assign the\nsame importance to each of those criteria. Here comes the need of grouping\ntogether all flight searches performed by the same kind of customer, that is\nhaving the same booking criteria. The effectiveness of some set of\nrecommendations, for a single cluster, can be measured in terms of the number\nof bookings historically performed. This effectiveness measure plays the role\nof a feedback, that is an external knowledge which can be recombined to\niteratively obtain a final segmentation. In this paper, we describe our Online\nTravel Agencies (OTA) flight search use case and highlight its specific\nfeatures. We address the flight search segmentation problem motivated above by\nproposing a novel algorithm called Split-or-Merge (S/M). This algorithm is a\nvariation of the Split-Merge-Evolve (SME) method. The SME method has already\nbeen introduced in the community as an iterative process updating a clustering\ngiven by the K-means algorithm by splitting and merging clusters subject to\nfeedback independent evaluations. No previous application of the SME method to\nthe real-word data is reported in literature to the best of our knowledge.\nHere, we provide experimental evaluations over real-world data to the SME and\nthe S/M methods. The impact on our domain-specific metrics obtained under the\nSME and the S/M methods suggests that feedback clustering techniques can be\nvery promising in the handling of the domain of OTA flight searches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.15772,regular,pre_llm,2020,6,"{'ai_likelihood': 1.0265244377983941e-05, 'text': ""Multi-sided Exposure Bias in Recommendation\n\n  Academic research in recommender systems has been greatly focusing on the\naccuracy-related measures of recommendations. Even when non-accuracy measures\nsuch as popularity bias, diversity, and novelty are studied, it is often solely\nfrom the users' perspective. However, many real-world recommenders are often\nmulti-stakeholder environments in which the needs and interests of several\nstakeholders should be addressed in the recommendation process. In this paper,\nwe focus on the popularity bias problem which is a well-known property of many\nrecommendation algorithms where few popular items are over-recommended while\nthe majority of other items do not get proportional attention and address its\nimpact on different stakeholders. Using several recommendation algorithms and\ntwo publicly available datasets in music and movie domains, we empirically show\nthe inherent popularity bias of the algorithms and how this bias impacts\ndifferent stakeholders such as users and suppliers of the items. We also\npropose metrics to measure the exposure bias of recommendation algorithms from\nthe perspective of different stakeholders.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.116,regular,pre_llm,2020,6,"{'ai_likelihood': 1.466936535305447e-05, 'text': 'Enhancing Factorization Machines with Generalized Metric Learning\n\n  Factorization Machines (FMs) are effective in incorporating side information\nto overcome the cold-start and data sparsity problems in recommender systems.\nTraditional FMs adopt the inner product to model the second-order interactions\nbetween different attributes, which are represented via feature vectors. The\nproblem is that the inner product violates the triangle inequality property of\nfeature vectors. As a result, it cannot well capture fine-grained attribute\ninteractions, resulting in sub-optimal performance. Recently, the Euclidean\ndistance is exploited in FMs to replace the inner product and has delivered\nbetter performance. However, previous FM methods including the ones equipped\nwith the Euclidean distance all focus on the attribute-level interaction\nmodeling, ignoring the critical intrinsic feature correlations inside\nattributes. Thereby, they fail to model the complex and rich interactions\nexhibited in the real-world data. To tackle this problem, in this paper, we\npropose a FM framework equipped with generalized metric learning techniques to\nbetter capture these feature correlations. In particular, based on this\nframework, we present a Mahalanobis distance and a deep neural network (DNN)\nmethods, which can effectively model the linear and non-linear correlations\nbetween features, respectively. Besides, we design an efficient approach for\nsimplifying the model functions. Experiments on several benchmark datasets\ndemonstrate that our proposed framework outperforms several state-of-the-art\nbaselines by a large margin. Moreover, we collect a new large-scale dataset on\nsecond-hand trading to justify the effectiveness of our method over cold-start\nand data sparsity problems in recommender systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.15498,regular,pre_llm,2020,6,"{'ai_likelihood': 3.245141771104601e-06, 'text': 'RepBERT: Contextualized Text Embeddings for First-Stage Retrieval\n\n  Although exact term match between queries and documents is the dominant\nmethod to perform first-stage retrieval, we propose a different approach,\ncalled RepBERT, to represent documents and queries with fixed-length\ncontextualized embeddings. The inner products of query and document embeddings\nare regarded as relevance scores. On MS MARCO Passage Ranking task, RepBERT\nachieves state-of-the-art results among all initial retrieval techniques. And\nits efficiency is comparable to bag-of-words methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.15679,regular,pre_llm,2020,6,"{'ai_likelihood': 8.940696716308594e-07, 'text': ""Kernel Density Estimation based Factored Relevance Model for\n  Multi-Contextual Point-of-Interest Recommendation\n\n  An automated contextual suggestion algorithm is likely to recommend\ncontextually appropriate and personalized 'points-of-interest' (POIs) to a\nuser, if it can extract information from the user's preference history\n(exploitation) and effectively blend it with the user's current contextual\ninformation (exploration) to predict a POI's 'appropriateness' in the current\ncontext. To balance this trade-off between exploitation and exploration, we\npropose an unsupervised, generic framework involving a factored relevance model\n(FRLM), constituting two distinct components, one pertaining to historical\ncontexts, and the other corresponding to the current context. We further\ngeneralize the proposed FRLM by incorporating the semantic relationships\nbetween terms in POI descriptors using kernel density estimation (KDE) on\nembedded word vectors. Additionally, we show that trip-qualifiers, (e.g.\n'trip-type', 'accompanied-by') are potentially useful information sources that\ncould be used to improve the recommendation effectiveness. Using such\ninformation is not straight forward since users' texts/reviews of visited POIs\ntypically do not explicitly contain such annotations. We undertake a weakly\nsupervised approach to predict the associations between the review-texts in a\nuser profile and the likely trip contexts. Our experiments, conducted on the\nTREC contextual suggestion 2016 dataset, demonstrate that factorization,\nKDE-based generalizations, and trip-qualifier enriched contexts of the\nrelevance model improve POI recommendation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.11682,regular,pre_llm,2020,7,"{'ai_likelihood': 1.294745339287652e-05, 'text': 'Assessing top-$k$ preferences\n\n  Assessors make preference judgments faster and more consistently than graded\njudgments. Preference judgments can also recognize distinctions between items\nthat appear equivalent under graded judgments. Unfortunately, preference\njudgments can require more than linear effort to fully order a pool of items,\nand evaluation measures for preference judgments are not as well established as\nthose for graded judgments, such as NDCG. In this paper, we explore the\nassessment process for partial preference judgments, with the aim of\nidentifying and ordering the top items in the pool, rather than fully ordering\nthe entire pool. To measure the performance of a ranker, we compare its output\nto this preferred ordering by applying a rank similarity measure.We demonstrate\nthe practical feasibility of this approach by crowdsourcing partial preferences\nfor the TREC 2019 Conversational Assistance Track, replacing NDCG with a new\nmeasure named ""compatibility"". This new measure has its most striking impact\nwhen comparing modern neural rankers, where it is able to recognize significant\nimprovements in quality that would otherwise be missed by NDCG.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.1223,regular,pre_llm,2020,7,"{'ai_likelihood': 7.583035363091363e-06, 'text': ""Addressing the Multistakeholder Impact of Popularity Bias in\n  Recommendation Through Calibration\n\n  Popularity bias is a well-known phenomenon in recommender systems: popular\nitems are recommended even more frequently than their popularity would warrant,\namplifying long-tail effects already present in many recommendation domains.\nPrior research has examined various approaches for mitigating popularity bias\nand enhancing the recommendation of long-tail items overall. The effectiveness\nof these approaches, however, has not been assessed in multistakeholder\nenvironments where in addition to the users who receive the recommendations,\nthe utility of the suppliers of the recommended items should also be\nconsidered. In this paper, we propose the concept of popularity calibration\nwhich measures the match between the popularity distribution of items in a\nuser's profile and that of the recommended items. We also develop an algorithm\nthat optimizes this metric. In addition, we demonstrate that existing\nevaluation metrics for popularity bias do not reflect the performance of the\nalgorithms when it is measured from the perspective of different stakeholders.\nUsing music and movie datasets, we empirically show that our approach\noutperforms the existing state-of-the-art approaches in addressing popularity\nbias by calibrating the recommendations to users' preferences. We also show\nthat our proposed algorithm has a secondary effect of improving supplier\nfairness.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.13019,regular,pre_llm,2020,7,"{'ai_likelihood': 3.642506069607205e-07, 'text': ""Feedback Loop and Bias Amplification in Recommender Systems\n\n  Recommendation algorithms are known to suffer from popularity bias; a few\npopular items are recommended frequently while the majority of other items are\nignored. These recommendations are then consumed by the users, their reaction\nwill be logged and added to the system: what is generally known as a feedback\nloop. In this paper, we propose a method for simulating the users interaction\nwith the recommenders in an offline setting and study the impact of feedback\nloop on the popularity bias amplification of several recommendation algorithms.\nWe then show how this bias amplification leads to several other problems such\nas declining the aggregate diversity, shifting the representation of users'\ntaste over time and also homogenization of the users experience. In particular,\nwe show that the impact of feedback loop is generally stronger for the users\nwho belong to the minority group.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.05911,regular,pre_llm,2020,7,"{'ai_likelihood': 5.993578169080946e-06, 'text': ""Graph Factorization Machines for Cross-Domain Recommendation\n\n  Recently, graph neural networks (GNNs) have been successfully applied to\nrecommender systems. In recommender systems, the user's feedback behavior on an\nitem is usually the result of multiple factors acting at the same time.\nHowever, a long-standing challenge is how to effectively aggregate multi-order\ninteractions in GNN. In this paper, we propose a Graph Factorization Machine\n(GFM) which utilizes the popular Factorization Machine to aggregate multi-order\ninteractions from neighborhood for recommendation. Meanwhile, cross-domain\nrecommendation has emerged as a viable method to solve the data sparsity\nproblem in recommender systems. However, most existing cross-domain\nrecommendation methods might fail when confronting the graph-structured data.\nIn order to tackle the problem, we propose a general cross-domain\nrecommendation framework which can be applied not only to the proposed GFM, but\nalso to other GNN models. We conduct experiments on four pairs of datasets to\ndemonstrate the superior performance of the GFM. Besides, based on general\ncross-domain recommendation experiments, we also demonstrate that our\ncross-domain framework could not only contribute to the cross-domain\nrecommendation task with the GFM, but also be universal and expandable for\nvarious existing GNN models.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.02747,regular,pre_llm,2020,7,"{'ai_likelihood': 6.887647840711806e-06, 'text': 'GAG: Global Attributed Graph Neural Network for Streaming Session-based\n  Recommendation\n\n  Streaming session-based recommendation (SSR) is a challenging task that\nrequires the recommender system to do the session-based recommendation (SR) in\nthe streaming scenario. In the real-world applications of e-commerce and social\nmedia, a sequence of user-item interactions generated within a certain period\nare grouped as a session, and these sessions consecutively arrive in the form\nof streams. Most of the recent SR research has focused on the static setting\nwhere the training data is first acquired and then used to train a\nsession-based recommender model. They need several epochs of training over the\nwhole dataset, which is infeasible in the streaming setting. Besides, they can\nhardly well capture long-term user interests because of the neglect or the\nsimple usage of the user information. Although some streaming recommendation\nstrategies have been proposed recently, they are designed for streams of\nindividual interactions rather than streams of sessions. In this paper, we\npropose a Global Attributed Graph (GAG) neural network model with a Wasserstein\nreservoir for the SSR problem. On one hand, when a new session arrives, a\nsession graph with a global attribute is constructed based on the current\nsession and its associate user. Thus, the GAG can take both the global\nattribute and the current session into consideration to learn more\ncomprehensive representations of the session and the user, yielding a better\nperformance in the recommendation. On the other hand, for the adaptation to the\nstreaming session scenario, a Wasserstein reservoir is proposed to help\npreserve a representative sketch of the historical data. Extensive experiments\non two real-world datasets have been conducted to verify the superiority of the\nGAG model compared with the state-of-the-art methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.13829,regular,pre_llm,2020,7,"{'ai_likelihood': 4.0398703681098095e-06, 'text': ""On using Product-Specific Schema.org from Web Data Commons: An Empirical\n  Set of Best Practices\n\n  Schema.org has experienced high growth in recent years. Structured\ndescriptions of products embedded in HTML pages are now not uncommon,\nespecially on e-commerce websites. The Web Data Commons (WDC) project has\nextracted schema.org data at scale from webpages in the Common Crawl and made\nit available as an RDF `knowledge graph' at scale. The portion of this data\nthat specifically describes products offers a golden opportunity for\nresearchers and small companies to leverage it for analytics and downstream\napplications. Yet, because of the broad and expansive scope of this data, it is\nnot evident whether the data is usable in its raw form. In this paper, we do a\ndetailed empirical study on the product-specific schema.org data made available\nby WDC. Rather than simple analysis, the goal of our study is to devise an\nempirically grounded set of best practices for using and consuming WDC\nproduct-specific schema.org data. Our studies reveal six best practices, each\nof which is justified by experimental data and analysis.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.13207,review,pre_llm,2020,7,"{'ai_likelihood': 4.76837158203125e-06, 'text': 'Neural-Symbolic Reasoning over Knowledge Graph for Multi-stage\n  Explainable Recommendation\n\n  Recent work on recommender systems has considered external knowledge graphs\nas valuable sources of information, not only to produce better recommendations\nbut also to provide explanations of why the recommended items were chosen. Pure\nrule-based symbolic methods provide a transparent reasoning process over\nknowledge graph but lack generalization ability to unseen examples, while deep\nlearning models enhance powerful feature representation ability but are hard to\ninterpret. Moreover, direct reasoning over large-scale knowledge graph can be\ncostly due to the huge search space of pathfinding. We approach the problem\nthrough a novel coarse-to-fine neural symbolic reasoning method called NSER. It\nfirst generates a coarse-grained explanation to capture abstract user\nbehavioral pattern, followed by a fined-grained explanation accompanying with\nexplicit reasoning paths and recommendations inferred from knowledge graph. We\nextensively experiment on four real-world datasets and observe substantial\ngains of recommendation performance compared with state-of-the-art methods as\nwell as more diversified explanations in different granularity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.15084,regular,pre_llm,2020,7,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'Improving Performance of Relation Extraction Algorithm via Leveled\n  Adversarial PCNN and Database Expansion\n\n  This study introduces database expansion using the Minimum Description Length\n(MDL) algorithm to expand the database for better relation extraction.\nDifferent from other previous relation extraction researches, our method\nimproves system performance by expanding data. The goal of database expansion,\ntogether with a robust deep learning classifier, is to diminish wrong labels\ndue to the incomplete or not found nature of relation instances in the relation\ndatabase (e.g., Freebase). The study uses a deep learning method (Piecewise\nConvolutional Neural Network or PCNN) as the base classifier of our proposed\napproach: the leveled adversarial attention neural networks (LATTADV-ATT). In\nthe database expansion process, the semantic entity identification is used to\nenlarge new instances using the most similar itemsets of the most common\npatterns of the data to get its pairs of entities. About the deep learning\nmethod, the use of attention of selective sentences in PCNN can reduce noisy\nsentences. Also, the use of adversarial perturbation training is useful to\nimprove the robustness of system performance. The performance even further is\nimproved using a combination of leveled strategy and database expansion. There\nare two issues: 1) database expansion method: rule generation by allowing step\nsizes on selected strong semantic of most similar itemsets with aims to find\nentity pair for generating instances, 2) a better classifier model for relation\nextraction. Experimental result has shown that the use of the database\nexpansion is beneficial. The MDL database expansion helps improvements in all\nmethods compared to the unexpanded method. The LATTADV-ATT performs as a good\nclassifier with high precision P@100=0.842 (at no expansion). It is even better\nwhile implemented on the expansion data with P@100=0.891 (at expansion factor\nk=7).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.08308,regular,pre_llm,2020,7,"{'ai_likelihood': 2.3510720994737412e-06, 'text': 'Collaborative Adversarial Learning for RelationalLearning on Multiple\n  Bipartite Graphs\n\n  Relational learning aims to make relation inference by exploiting the\ncorrelations among different types of entities. Exploring relational learning\non multiple bipartite graphs has been receiving attention because of its\npopular applications such as recommendations. How to make efficient relation\ninference with few observed links is the main problem on multiple bipartite\ngraphs. Most existing approaches attempt to solve the sparsity problem via\nlearning shared representations to integrate knowledge from multi-source data\nfor shared entities. However, they merely model the correlations from one\naspect (e.g. distribution, representation), and cannot impose sufficient\nconstraints on different relations of the shared entities. One effective way of\nmodeling the multi-domain data is to learn the joint distribution of the shared\nentities across domains.In this paper, we propose Collaborative Adversarial\nLearning (CAL) that explicitly models the joint distribution of the shared\nentities across multiple bipartite graphs. The objective of CAL is formulated\nfrom a variational lower bound that maximizes the joint log-likelihoods of the\nobservations. In particular, CAL consists of distribution-level and\nfeature-level alignments for knowledge from multiple bipartite graphs. The\ntwo-level alignment acts as two different constraints on different relations of\nthe shared entities and facilitates better knowledge transfer for relational\nlearning on multiple bipartite graphs. Extensive experiments on two real-world\ndatasets have shown that the proposed model outperforms the existing methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.07102,regular,pre_llm,2020,7,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'A Framework for Capturing and Analyzing Unstructured and Semi-structured\n  Data for a Knowledge Management System\n\n  Mainstream knowledge management researchers generally agree that knowledge\nextracted from unstructured data and semi-structured data have become\nimperative for organizational strategic decision making. In this research, we\ndevelop a framework that captures and analyses unstructured data using machine\nlearning techniques and integrates knowledge and insight gained from the data\ninto traditional knowledge management systems. Unlike most frameworks published\nin the literature that focuses on a specific type of unstructured data, our\nframeworks cut across the varieties of unstructured data ranging from textual\ndata from social network sites, online forums, discussion boards, reviews to\naudio data, image data and video data. We highlight some pre-processing and\nprocessing techniques for these data and also highlight some standard output.\nWe evaluate the framework by developing a textual data application programming\ninterface (API) using python and beautiful soup and we perform sentiment\nanalysis on the students review data collected through the API.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.15091,regular,pre_llm,2020,7,"{'ai_likelihood': 3.410710228814019e-06, 'text': 'Finding Local Experts for Dynamic Recommendations Using Lazy Random Walk\n\n  Statistics based privacy-aware recommender systems make suggestions more\npowerful by extracting knowledge from the log of social contacts interactions,\nbut unfortunately, they are static. Moreover, advice from local experts\neffective in finding specific business categories in a particular area. We\npropose a dynamic recommender algorithm based on a lazy random walk that\nrecommends top-rank shopping places to potentially interested visitors. We\nconsider local authority and topical authority. The algorithm tested on\nFourSquare shopping data sets of 5 cities in Indonesia with k-steps of 5,7,9 of\n(lazy) random walks and compared the results with other state-of-the-art\nranking techniques. The results show that it can reach high score precisions\n(0.5, 0.37, and 0.26 respectively on precision at 1, precision at 3, and\nprecision at 5 for k=5). The algorithm also shows scalability concerning\nexecution time. The advantage of dynamicity is the database used to power the\nrecommender system; no need to be very frequently updated to produce a good\nrecommendation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.00216,regular,pre_llm,2020,7,"{'ai_likelihood': 3.7087334526909726e-06, 'text': 'An Efficient Neighborhood-based Interaction Model for Recommendation on\n  Heterogeneous Graph\n\n  There is an influx of heterogeneous information network (HIN) based\nrecommender systems in recent years since HIN is capable of characterizing\ncomplex graphs and contains rich semantics. Although the existing approaches\nhave achieved performance improvement, while practical, they still face the\nfollowing problems. On one hand, most existing HIN-based methods rely on\nexplicit path reachability to leverage path-based semantic relatedness between\nusers and items, e.g., metapath-based similarities. These methods are hard to\nuse and integrate since path connections are sparse or noisy, and are often of\ndifferent lengths. On the other hand, other graph-based methods aim to learn\neffective heterogeneous network representations by compressing node together\nwith its neighborhood information into single embedding before prediction. This\nweakly coupled manner in modeling overlooks the rich interactions among nodes,\nwhich introduces an early summarization issue. In this paper, we propose an\nend-to-end Neighborhood-based Interaction Model for Recommendation (NIRec) to\naddress the above problems. Specifically, we first analyze the significance of\nlearning interactions in HINs and then propose a novel formulation to capture\nthe interactive patterns between each pair of nodes through their\nmetapath-guided neighborhoods. Then, to explore complex interactions between\nmetapaths and deal with the learning complexity on large-scale networks, we\nformulate interaction in a convolutional way and learn efficiently with fast\nFourier transform. The extensive experiments on four different types of\nheterogeneous graphs demonstrate the performance gains of NIRec comparing with\nstate-of-the-arts. To the best of our knowledge, this is the first work\nproviding an efficient neighborhood-based interaction model in the HIN-based\nrecommendations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.13237,regular,pre_llm,2020,7,"{'ai_likelihood': 6.953875223795574e-07, 'text': 'Exploring Data Splitting Strategies for the Evaluation of Recommendation\n  Models\n\n  Effective methodologies for evaluating recommender systems are critical, so\nthat such systems can be compared in a sound manner. A commonly overlooked\naspect of recommender system evaluation is the selection of the data splitting\nstrategy. In this paper, we both show that there is no standard splitting\nstrategy and that the selection of splitting strategy can have a strong impact\non the ranking of recommender systems. In particular, we perform experiments\ncomparing three common splitting strategies, examining their impact over seven\nstate-of-the-art recommendation models for two datasets. Our results\ndemonstrate that the splitting strategy employed is an important confounding\nvariable that can markedly alter the ranking of state-of-the-art systems,\nmaking much of the currently published literature non-comparable, even when the\nsame dataset and metrics are used.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.13273,regular,pre_llm,2020,7,"{'ai_likelihood': 1.4901161193847656e-06, 'text': ""Measuring similarity in co-occurrence data using ego-networks\n\n  The co-occurrence association is widely observed in many empirical data.\nMining the information in co-occurrence data is essential for advancing our\nunderstanding of systems such as social networks, ecosystem, and brain network.\nMeasuring similarity of entities is one of the important tasks, which can\nusually be achieved using a network-based approach. Here we show that\ntraditional methods based on the aggregated network can bring unwanted\nin-directed relationship. To cope with this issue, we propose a similarity\nmeasure based on the ego network of each entity, which effectively considers\nthe change of an entity's centrality from one ego network to another. The index\nproposed is easy to calculate and has a clear physical meaning. Using two\ndifferent data sets, we compare the new index with other existing ones. We find\nthat the new index outperforms the traditional network-based similarity\nmeasures, and it can sometimes surpass the embedding method. In the meanwhile,\nthe measure by the new index is weakly correlated with those by other methods,\nhence providing a different dimension to quantify similarities in co-occurrence\ndata. Altogether, our work makes an extension in the network-based similarity\nmeasure and can be potentially applied in several related tasks.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.15104,review,pre_llm,2020,7,"{'ai_likelihood': 4.2319297790527344e-05, 'text': ""Social Influences in Recommendation Systems\n\n  Social networking sites such as Flickr and Facebook allow users to share\ncontent with family, friends, and interest groups. Also, tags can often assign\nto resources. In the previous research using few association rules FAR, we have\nseen that high-quality and efficient association-based tag recommendation is\npossible, but the set-up that we considered was very generic and did not take\nsocial information into account. The proposed method in the previous paper,\nFAR, in particular, exhibited a favorable trade-off between recommendation\nquality and runtime. Unfortunately, recommendation quality is unlikely to be\noptimal because the algorithms are not aware of any social information that may\nbe available. Two proposed approaches take a more social view on tag\nrecommendation regarding the issue: social contact variants and social groups\nof interest. The user data is varied and used as a source of associations. The\nadoption of social contact variants has two approaches. The first social\nvariant is User-centered Knowledge, to contrast Collective Knowledge. It\nimproves tag recommendation by grouping historic tag data according to friend\nrelationships and interests. The second variant is dubbed 'social batched\npersonomy' and attempts to address both quality and scalability issues by\nprocessing queries in batches instead of individually, such as done in a\nconventional personomy approach. For the social group of interest, 'community\nbatched personomy' is proposed to provide better accuracy groups of\nrecommendation systems in contrast also to Collective Knowledge. By taking\nsocial information into account can enhance the performance of recommendation\nsystems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.09703,regular,pre_llm,2020,7,"{'ai_likelihood': 5.496872795952691e-06, 'text': ""A curated collection of COVID-19 online datasets\n\n  One of the defining moments of the year 2020 is the outbreak of Coronavirus\nDisease (Covid-19), a deadly virus affecting the body's respiratory system to\nthe point of needing a breathing aid via ventilators. As of June 21, 2020 there\nare 12,929,306 confirmed cases and 569,738 confirmed deaths across 216\ncountries, areas or territories. The scale of spread and impact of the pandemic\nleft many nations grappling with preventive and curative approaches. The\ninfamous lockdown measure introduced to mitigate the virus spread has altered\nmany aspects of our social routines in which demand for online-based services\nskyrocketed. As the virus propagate, so does misinformation and fake news\naround it via online social media, which seems to favour virality over\nveracity. With a majority of the populace confined to their homes for a long\nperiod, vulnerability to the toxic impact of online misinformation is high. A\ncase in point is the various myths and disinformation associated with the\nCovid-19, which, if left unchecked, could lead to a catastrophic outcome and\nhamper the fight against the virus.\n  While the scientific community is actively engaged in identifying the virus\ntreatment, there is a growing interest in combating the associated harmful\ninfodemic. To this end, researchers have been curating and documenting various\ndatasets about Covid-19. In line with existing studies, we provide an expansive\ncollection of curated datasets to support the fight against the pandemic,\nespecially concerning misinformation. The collection consists of 3 categories\nof Twitter data, information about standard practices from credible sources and\na chronicle of global situation reports. We describe how to retrieve the\nhydrated version of the data and proffer some research problems that could be\naddressed using the data.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.01978,review,pre_llm,2020,7,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Building benchmarking frameworks for supporting replicability and\n  reproducibility: spatial and textual analysis as an example\n\n  Replicability and reproducibility (R&R) are critical for the long-term\nprosperity of a scientific discipline. In GIScience, researchers have discussed\nR&R related to different research topics and problems, such as local spatial\nstatistics, digital earth, and metadata (Fotheringham, 2009; Goodchild, 2012;\nAnselin et al., 2014). This position paper proposes to further support R&R by\nbuilding benchmarking frameworks in order to facilitate the replication of\nprevious research for effective and effcient comparisons of methods and\nsoftware tools developed for addressing the same or similar problems.\nParticularly, this paper will use geoparsing, an important research problem in\nspatial and textual analysis, as an example to explain the values of such\nbenchmarking frameworks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.07987,regular,pre_llm,2020,7,"{'ai_likelihood': 9.238719940185547e-06, 'text': 'Deep Reinforced Query Reformulation for Information Retrieval\n\n  Query reformulations have long been a key mechanism to alleviate the\nvocabulary-mismatch problem in information retrieval, for example by expanding\nthe queries with related query terms or by generating paraphrases of the\nqueries. In this work, we propose a deep reinforced query reformulation (DRQR)\nmodel to automatically generate new reformulations of the query. To encourage\nthe model to generate queries which can achieve high performance when\nperforming the retrieval task, we incorporate query performance prediction into\nour reward function. In addition, to evaluate the quality of the reformulated\nquery in the context of information retrieval, we first train our DRQR model,\nthen apply the retrieval ranking model on the obtained reformulated query.\nExperiments are conducted on the TREC 2020 Deep Learning track MSMARCO document\nranking dataset. Our results show that our proposed model outperforms several\nquery reformulation model baselines when performing retrieval task. In\naddition, improvements are also observed when combining with various retrieval\nmodels, such as query expansion and BERT.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.14271,regular,pre_llm,2020,7,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Declarative Experimentation in Information Retrieval using PyTerrier\n\n  The advent of deep machine learning platforms such as Tensorflow and Pytorch,\ndeveloped in expressive high-level languages such as Python, have allowed more\nexpressive representations of deep neural network architectures. We argue that\nsuch a powerful formalism is missing in information retrieval (IR), and propose\na framework called PyTerrier that allows advanced retrieval pipelines to be\nexpressed, and evaluated, in a declarative manner close to their conceptual\ndesign. Like the aforementioned frameworks that compile deep learning\nexperiments into primitive GPU operations, our framework targets IR platforms\nas backends in order to execute and evaluate retrieval pipelines. Further, we\ncan automatically optimise the retrieval pipelines to increase their efficiency\nto suite a particular IR platform backend. Our experiments, conducted on TREC\nRobust and ClueWeb09 test collections, demonstrate the efficiency benefits of\nthese optimisations for retrieval pipelines involving both the Anserini and\nTerrier IR platforms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.15731,regular,pre_llm,2020,7,"{'ai_likelihood': 4.635916815863716e-07, 'text': 'Is there something I\'m missing? Topic Modeling in eDiscovery\n\n  In legal eDiscovery, the parties are required to search through their\nelectronically stored information to find documents that are relevant to a\nspecific case. Negotiations over the scope of these searches are often based on\na fear that something will be missed. This paper continues an argument that\ndiscovery should be based on identifying the facts of a case. If a search\nprocess is less than complete (if it has Recall less than 100%), it may still\nbe complete in presenting all of the relevant available topics. In this study,\nLatent Dirichlet Allocation was used to identify 100 topics from all of the\nknown relevant documents. The documents were then categorized to about 80%\nRecall (i.e., 80% of the relevant documents were found by the categorizer,\ndesignated the hit set and 20% were missed, designated the missed set). Despite\nthe fact that less than all of the relevant documents were identified by the\ncategorizer, the documents that were identified contained all of the topics\nderived from the full set of documents. This same pattern held whether the\ncategorizer was a na\\""ive Bayes categorizer trained on a random selection of\ndocuments or a Support Vector Machine trained with Continuous Active Learning\n(which focuses evaluation on the most-likely-to-be-relevant documents). No\ntopics were identified in either categorizer\'s missed set that were not already\nseen in the hit set. Not only is a computer-assisted search process reasonable\n(as required by the Federal Rules of Civil Procedure), it is also complete when\nmeasured by topics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.01969,regular,pre_llm,2020,8,"{'ai_likelihood': 1.026524437798394e-06, 'text': ""Retrieve Synonymous keywords for Frequent Queries in Sponsored Search in\n  a Data Augmentation Way\n\n  In sponsored search, retrieving synonymous keywords is of great importance\nfor accurately targeted advertising. The semantic gap between queries and\nkeywords and the extremely high precision requirements (>= 95\\%) are two major\nchallenges to this task. To the best of our knowledge, the problem has not been\nopenly discussed. In an industrial sponsored search system, the retrieved\nkeywords for frequent queries are usually done ahead of time and stored in a\nlookup table. Considering these results as a seed dataset, we propose a\ndata-augmentation-like framework to improve the synonymous retrieval\nperformance for these frequent queries. This framework comprises two steps:\ntranslation-based retrieval and discriminant-based filtering. Firstly, we\ndevise a Trie-based translation model to make a data increment. In this phase,\na Bag-of-Core-Words trick is conducted, which increased the data increment's\nvolume 4.2 times while keeping the original precision. Then we use a BERT-based\ndiscriminant model to filter out nonsynonymous pairs, which exceeds the\ntraditional feature-driven GBDT model with 11\\% absolute AUC improvement. This\nmethod has been successfully applied to Baidu's sponsored search system, which\nhas yielded a significant improvement in revenue. In addition, a commercial\nChinese dataset containing 500K synonymous pairs with a precision of 95\\% is\nreleased to the public for paraphrase study\n(http://ai.baidu.com/broad/subordinate?dataset=paraphrasing).\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.0818,regular,pre_llm,2020,8,"{'ai_likelihood': 2.9802322387695312e-06, 'text': 'Semantic Product Search for Matching Structured Product Catalogs in\n  E-Commerce\n\n  Retrieving all semantically relevant products from the product catalog is an\nimportant problem in E-commerce. Compared to web documents, product catalogs\nare more structured and sparse due to multi-instance fields that encode\nheterogeneous aspects of products (e.g. brand name and product dimensions). In\nthis paper, we propose a new semantic product search algorithm that learns to\nrepresent and aggregate multi-instance fields into a document representation\nusing state of the art transformers as encoders. Our experiments investigate\ntwo aspects of the proposed approach: (1) effectiveness of field\nrepresentations and structured matching; (2) effectiveness of adding lexical\nfeatures to semantic search. After training our models using user click logs\nfrom a well-known E-commerce platform, we show that our results provide useful\ninsights for improving product search. Lastly, we present a detailed error\nanalysis to show which types of queries benefited the most by fielded\nrepresentations and structured matching.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.02197,regular,pre_llm,2020,8,"{'ai_likelihood': 1.7881393432617188e-06, 'text': ""One word at a time: adversarial attacks on retrieval models\n\n  Adversarial examples, generated by applying small perturbations to input\nfeatures, are widely used to fool classifiers and measure their robustness to\nnoisy inputs. However, little work has been done to evaluate the robustness of\nranking models through adversarial examples. In this work, we present a\nsystematic approach of leveraging adversarial examples to measure the\nrobustness of popular ranking models. We explore a simple method to generate\nadversarial examples that forces a ranker to incorrectly rank the documents.\nUsing this approach, we analyze the robustness of various ranking models and\nthe quality of perturbations generated by the adversarial attacker across two\ndatasets. Our findings suggest that with very few token changes (1-3), the\nattacker can yield semantically similar perturbed documents that can fool\ndifferent rankers into changing a document's score, lowering its rank by\nseveral positions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.01246,regular,pre_llm,2020,8,"{'ai_likelihood': 2.6490953233506944e-06, 'text': 'Noise Contrastive Estimation for Autoencoding-based One-Class\n  Collaborative Filtering\n\n  One-class collaborative filtering (OC-CF) is a common class of recommendation\nproblem where only the positive class is explicitly observed (e.g., purchases,\nclicks). Autoencoder based recommenders such as AutoRec and variants\ndemonstrate strong performance on many OC-CF benchmarks, but also empirically\nsuffer from a strong popularity bias. While a careful choice of negative\nsamples in the OC-CF setting can mitigate popularity bias, Negative Sampling\n(NS) is often better for training embeddings than for the end task itself. To\naddress this, we propose a two-headed AutoRec to first train an embedding layer\nvia one head using Negative Sampling then to train for the final task via the\nsecond head. While this NS-AutoRec improves results for AutoRec and outperforms\nmany state-of-the-art baselines on OC-CF problems, we notice that Negative\nSampling can still take a large amount of time to train. Since Negative\nSampling is known to be a special case of Noise Contrastive Estimation (NCE),\nwe adapt a recently proposed closed-form NCE solution for collaborative\nfiltering to AutoRec yielding NCE-AutoRec. Overall, we show that our novel\ntwo-headed AutoRec models (NCE-AutoRec and NS-AutoRec) successfully mitigate\nthe popularity bias issue and maintain competitive performance in comparison to\nstate-of-the-art recommenders on multiple real-world datasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.07178,regular,pre_llm,2020,8,"{'ai_likelihood': 3.046459621853299e-06, 'text': 'Disentangled Item Representation for Recommender Systems\n\n  Item representations in recommendation systems are expected to reveal the\nproperties of items. Collaborative recommender methods usually represent an\nitem as one single latent vector. Nowadays the e-commercial platforms provide\nvarious kinds of attribute information for items (e.g., category, price and\nstyle of clothing). Utilizing these attribute information for better item\nrepresentations is popular in recent years. Some studies use the given\nattribute information as side information, which is concatenated with the item\nlatent vector to augment representations. However, the mixed item\nrepresentations fail to fully exploit the rich attribute information or provide\nexplanation in recommender systems. To this end, we propose a fine-grained\nDisentangled Item Representation (DIR) for recommender systems in this paper,\nwhere the items are represented as several separated attribute vectors instead\nof a single latent vector. In this way, the items are represented at the\nattribute level, which can provide fine-grained information of items in\nrecommendation. We introduce a learning strategy, LearnDIR, which can allocate\nthe corresponding attribute vectors to items. We show how DIR can be applied to\ntwo typical models, Matrix Factorization (MF) and Recurrent Neural Network\n(RNN). Experimental results on two real-world datasets show that the models\ndeveloped under the framework of DIR are effective and efficient. Even using\nfewer parameters, the proposed model can outperform the state-of-the-art\nmethods, especially in the cold-start situation. In addition, we make\nvisualizations to show that our proposition can provide explanation for users\nin real-world applications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.10242,regular,pre_llm,2020,8,"{'ai_likelihood': 8.046627044677734e-06, 'text': 'When Inverse Propensity Scoring does not Work: Affine Corrections for\n  Unbiased Learning to Rank\n\n  Besides position bias, which has been well-studied, trust bias is another\ntype of bias prevalent in user interactions with rankings: users are more\nlikely to click incorrectly w.r.t. their preferences on highly ranked items\nbecause they trust the ranking system. While previous work has observed this\nbehavior in users, we prove that existing Counterfactual Learning to Rank\n(CLTR) methods do not remove this bias, including methods specifically designed\nto mitigate this type of bias. Moreover, we prove that Inverse Propensity\nScoring (IPS) is principally unable to correct for trust bias under non-trivial\ncircumstances. Our main contribution is a new estimator based on affine\ncorrections: it both reweights clicks and penalizes items displayed on ranks\nwith high trust bias. Our estimator is the first estimator that is proven to\nremove the effect of both trust bias and position bias. Furthermore, we show\nthat our estimator is a generalization of the existing CLTR framework: if no\ntrust bias is present, it reduces to the original IPS estimator. Our\nsemi-synthetic experiments indicate that by removing the effect of trust bias\nin addition to position bias, CLTR can approximate the optimal ranking system\neven closer than previously possible.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.04185,regular,pre_llm,2020,8,"{'ai_likelihood': 2.5431315104166668e-05, 'text': 'Path-Based Reasoning over Heterogeneous Networks for Recommendation via\n  Bidirectional Modeling\n\n  Heterogeneous Information Network (HIN) is a natural and general\nrepresentation of data in recommender systems. Combining HIN and recommender\nsystems can not only help model user behaviors but also make the recommendation\nresults explainable by aligning the users/items with various types of entities\nin the network. Over the past few years, path-based reasoning models have shown\ngreat capacity in HIN-based recommendation. The basic idea of these models is\nto explore HIN with predefined path schemes. Despite their effectiveness, these\nmodels are often confronted with the following limitations: (1) Most prior\npath-based reasoning models only consider the influence of the predecessors on\nthe subsequent nodes when modeling the sequences, and ignore the reciprocity\nbetween the nodes in a path; (2) The weights of nodes in the same path instance\nare usually assumed to be constant, whereas varied weights of nodes can bring\nmore flexibility and lead to expressive modeling; (3) User-item interactions\nare noisy, but they are often indiscriminately exploited. To overcome the\naforementioned issues, in this paper, we propose a novel path-based reasoning\napproach for recommendation over HIN. Concretely, we use a bidirectional LSTM\nto enable the two-way modeling of paths and capture the reciprocity between\nnodes. Then an attention mechanism is employed to learn the dynamical influence\nof nodes in different contexts. Finally, the adversarial regularization terms\nare imposed on the loss function of the model to mitigate the effects of noise\nand enhance HIN-based recommendation. Extensive experiments conducted on three\npublic datasets show that our model outperforms the state-of-the-art baselines.\nThe case study further demonstrates the feasibility of our model on the\nexplainable recommendation task.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.10874,regular,pre_llm,2020,8,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Continual Domain Adaptation for Machine Reading Comprehension\n\n  Machine reading comprehension (MRC) has become a core component in a variety\nof natural language processing (NLP) applications such as question answering\nand dialogue systems. It becomes a practical challenge that an MRC model needs\nto learn in non-stationary environments, in which the underlying data\ndistribution changes over time. A typical scenario is the domain drift, i.e.\ndifferent domains of data come one after another, where the MRC model is\nrequired to adapt to the new domain while maintaining previously learned\nability. To tackle such a challenge, in this work, we introduce the\n\\textit{Continual Domain Adaptation} (CDA) task for MRC. So far as we know,\nthis is the first study on the continual learning perspective of MRC. We build\ntwo benchmark datasets for the CDA task, by re-organizing existing MRC\ncollections into different domains with respect to context type and question\ntype, respectively. We then analyze and observe the catastrophic forgetting\n(CF) phenomenon of MRC under the CDA setting. To tackle the CDA task, we\npropose several BERT-based continual learning MRC models using either\nregularization-based methodology or dynamic-architecture paradigm. We analyze\nthe performance of different continual learning MRC models under the CDA task\nand show that the proposed dynamic-architecture based model achieves the best\nperformance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.08302,regular,pre_llm,2020,8,"{'ai_likelihood': 3.642506069607205e-07, 'text': ""E-commerce Recommendation with Weighted Expected Utility\n\n  Different from shopping at retail stores, consumers on e-commerce platforms\nusually cannot touch or try products before purchasing, which means that they\nhave to make decisions when they are uncertain about the outcome (e.g.,\nsatisfaction level) of purchasing a product. To study people's preferences,\neconomics researchers have proposed the hypothesis of Expected Utility (EU)\nthat models the subject value associated with an individual's choice as the\nstatistical expectations of that individual's valuations of the outcomes of\nthis choice. Despite its success in studies of game theory and decision theory,\nthe effectiveness of EU, however, is mostly unknown in e-commerce\nrecommendation systems. Previous research on e-commerce recommendation\ninterprets the utility of purchase decisions either as a function of the\nconsumed quantity of the product or as the gain of sellers/buyers in the\nmonetary sense. As most consumers just purchase one unit of a product at a time\nand most alternatives have similar prices, such modeling of purchase utility is\nlikely to be inaccurate in practice. In this paper, we interpret purchase\nutility as the satisfaction level a consumer gets from a product and propose a\nrecommendation framework using EU to model consumers' behavioral patterns. We\nassume that consumer estimates the expected utilities of all the alternatives\nand choose products with maximum expected utility for each purchase. To deal\nwith the potential psychological biases of each consumer, we introduce the\nusage of Probability Weight Function (PWF) and design our algorithm based on\nWeighted Expected Utility (WEU). Empirical study on real-world e-commerce\ndatasets shows that our proposed ranking-based recommendation framework\nachieves statistically significant improvement against both classical\nCollaborative Filtering/Latent Factor Models and state-of-the-art deep models\nin top-K recommendation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.08958,regular,pre_llm,2020,8,"{'ai_likelihood': 4.2054388258192275e-06, 'text': 'Keyword Search Engine Enriched by Expert System Features\n\n  Keyword search engines are essential elements of large information spaces.\nThe largest information space is the Web, and keyword search engines play\ncrucial role there. The advent of keyword search engines has provided a quantum\nleap in the development of the Web. Since then, the Web has continued to\nevolve, and keyword search systems have proven inadequate. A new quantum leap\nin the development of keyword search engines is needed. This quantum leap can\nbe provided with more intellectual keyword search engines. The increased\nintelligence of such keyword search engines can be achieved through a\ncombination of keyword search engines and expert systems. The paper reveals how\nit can be done.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.12039,regular,pre_llm,2020,8,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'SciLens News Platform: A System for Real-Time Evaluation of News\n  Articles\n\n  We demonstrate the SciLens News Platform, a novel system for evaluating the\nquality of news articles. The SciLens News Platform automatically collects\ncontextual information about news articles in real-time and provides quality\nindicators about their validity and trustworthiness. These quality indicators\nderive from i) social media discussions regarding news articles, showcasing the\nreach and stance towards these articles, and ii) their content and their\nreferenced sources, showcasing the journalistic foundations of these articles.\nFurthermore, the platform enables domain-experts to review articles and rate\nthe quality of news sources. This augmented view of news articles, which\ncombines automatically extracted indicators and domain-expert reviews, has\nprovably helped the platform users to have a better consensus about the quality\nof the underlying articles. The platform is built in a distributed and robust\nfashion and runs operationally handling daily thousands of news articles. We\nevaluate the SciLens News Platform on the emerging topic of COVID-19 where we\nhighlight the discrepancies between low and high-quality news outlets based on\nthree axes, namely their newsroom activity, evidence seeking and social\nengagement. A live demonstration of the platform can be found here:\nhttp://scilens.epfl.ch.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.0186,regular,pre_llm,2020,8,"{'ai_likelihood': 1.5861458248562285e-05, 'text': ""A Comprehensive Pipeline for Hotel Recommendation System\n\n  This paper addresses a comprehensive pipeline to build a hotel recommendation\nsystem with the raw data collected by Apps in users' smartphones. The pipeline\nmainly consists of pre-processing of the raw data and training prediction\nmodels. We use two methods, Support Vector Machine (SVM) and Recurrent Neural\nNetwork (RNN). The results show that two methods achieved a reasonable accuracy\nwith the pre-processing of the raw data. Therefore, we conclude that this paper\nprovides a comprehensive pipeline, in which a hotel recommendation system was\nsuccessfully built from the raw data to specific applications.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.03917,regular,pre_llm,2020,8,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Beyond Lexical: A Semantic Retrieval Framework for Textual SearchEngine\n\n  Search engine has become a fundamental component in various web and mobile\napplications. Retrieving relevant documents from the massive datasets is\nchallenging for a search engine system, especially when faced with verbose or\ntail queries. In this paper, we explore a vector space search framework for\ndocument retrieval. Specifically, we trained a deep semantic matching model so\nthat each query and document can be encoded as a low dimensional embedding. Our\nmodel was trained based on BERT architecture. We deployed a fast\nk-nearest-neighbor index service for online serving. Both offline and online\nmetrics demonstrate that our method improved retrieval performance and search\nquality considerably, particularly for tail\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.09093,review,pre_llm,2020,8,"{'ai_likelihood': 7.947285970052084e-07, 'text': ""PARADE: Passage Representation Aggregation for Document Reranking\n\n  Pretrained transformer models, such as BERT and T5, have shown to be highly\neffective at ad-hoc passage and document ranking. Due to inherent sequence\nlength limits of these models, they need to be run over a document's passages,\nrather than processing the entire document sequence at once. Although several\napproaches for aggregating passage-level signals have been proposed, there has\nyet to be an extensive comparison of these techniques. In this work, we explore\nstrategies for aggregating relevance signals from a document's passages into a\nfinal ranking score. We find that passage representation aggregation techniques\ncan significantly improve over techniques proposed in prior work, such as\ntaking the maximum passage score. We call this new approach PARADE. In\nparticular, PARADE can significantly improve results on collections with broad\ninformation needs where relevance signals can be spread throughout the document\n(such as TREC Robust04 and GOV2). Meanwhile, less complex aggregation\ntechniques may work better on collections with an information need that can\noften be pinpointed to a single passage (such as TREC DL and TREC Genomics). We\nalso conduct efficiency analyses, and highlight several strategies for\nimproving transformer-based aggregation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.0768,regular,pre_llm,2020,8,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'An Annotated Corpus of Webtables for Information Extraction Tasks\n\n  Information Extraction is a well-researched area of Natural Language\nProcessing with applications in web search and question answering concerned\nwith identifying entities and relationships between them as expressed in a\ngiven context, usually a sentence of a paragraph of running text. Given the\nimportance of the task, several datasets and benchmarks have been curated over\nthe years. However, focusing on running text alone leaves out tables which are\ncommon in many structured documents and in which pairs of entities also\nco-occur in context (e.g., the same row of the table). While there are recent\npapers on relation extraction from tables in the literature, their experimental\nevaluations have been on ad-hoc datasets for the lack of a standard benchmark.\nThis paper helps close that gap. We introduce an annotation framework and a\ndataset of 217,834 tables from Wikipedia which are annotated with 28 relations,\nusing both classifiers and carefully designed queries over a reference\nknowledge graph. Binary classifiers are then applied to the resulting dataset\nto remove false positives, resulting in an average annotation accuracy of 94%.\nThe resulting dataset is the first of its kind to be made publicly available.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.07226,regular,pre_llm,2020,8,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Exploring Longitudinal Effects of Session-based Recommendations\n\n  Session-based recommendation is a problem setting where the task of a\nrecommender system is to make suitable item suggestions based only on a few\nobserved user interactions in an ongoing session. The lack of long-term\npreference information about individual users in such settings usually results\nin a limited level of personalization, where a small set of popular items may\nbe recommended to many users. This repeated exposure of such a subset of the\nitems through the recommendations may in turn lead to a reinforcement effect\nover time, and to a system which is not able to help users discover new content\nanymore to the desirable extent.\n  In this work, we investigate such potential longitudinal effects of\nsession-based recommendations in a simulation-based approach. Specifically, we\nanalyze to what extent algorithms of different types may lead to concentration\neffects over time. Our experiments in the music domain reveal that all\ninvestigated algorithms---both neural and heuristic ones---may lead to lower\nitem coverage and to a higher concentration on a subset of the items.\nAdditional simulation experiments however also indicate that relatively simple\nre-ranking strategies, e.g., by avoiding too many repeated recommendations in\nthe music domain, may help to deal with this problem.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.08551,review,pre_llm,2020,8,"{'ai_likelihood': 3.940529293484158e-06, 'text': ""Popularity Bias in Recommendation: A Multi-stakeholder Perspective\n\n  Traditionally, especially in academic research in recommender systems, the\nfocus has been solely on the satisfaction of the end-user. While user\nsatisfaction has, indeed, been associated with the success of the business, it\nis not the only factor. In many recommendation domains, there are other\nstakeholders whose needs should be taken into account in the recommendation\ngeneration and evaluation. In this dissertation, I describe the notion of\nmulti-stakeholder recommendation. In particular, I study one of the most\nimportant challenges in recommendation research, popularity bias, from a\nmulti-stakeholder perspective since, as I show later in this dissertation, it\nimpacts different stakeholders in a recommender system. Popularity bias is a\nwell-known phenomenon in recommender systems where popular items are\nrecommended even more frequently than their popularity would warrant,\namplifying long-tail effects already present in many recommendation domains.\nPrior research has examined various approaches for mitigating popularity bias\nand enhancing the recommendation of long-tail items overall. The effectiveness\nof these approaches, however, has not been assessed in multi-stakeholder\nenvironments. In this dissertation, I study the impact of popularity bias in\nrecommender systems from a multi-stakeholder perspective. In addition, I\npropose several algorithms each approaching the popularity bias mitigation from\na different angle and compare their performances using several metrics with\nsome other state-of-the-art approaches in the literature. I show that, often,\nthe standard evaluation measures of popularity bias mitigation in the\nliterature do not reflect the real picture of an algorithm's performance when\nit is evaluated from a multi-stakeholder point of view.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.13368,regular,pre_llm,2020,8,"{'ai_likelihood': 3.410710228814019e-06, 'text': ""PT-Ranking: A Benchmarking Platform for Neural Learning-to-Rank\n\n  Deep neural networks has become the first choice for researchers working on\nalgorithmic aspects of learning-to-rank. Unfortunately, it is not trivial to\nfind the optimal setting of hyper-parameters that achieves the best ranking\nperformance. As a result, it becomes more and more difficult to develop a new\nmodel and conduct a fair comparison with prior methods, especially for\nnewcomers. In this work, we propose PT-Ranking, an open-source project based on\nPyTorch for developing and evaluating learning-to-rank methods using deep\nneural networks as the basis to construct a scoring function. On one hand,\nPT-Ranking includes many representative learning-to-rank methods. Besides the\ntraditional optimization framework via empirical risk minimization, adversarial\noptimization framework is also integrated. Furthermore, PT-Ranking's modular\ndesign provides a set of building blocks that users can leverage to develop new\nranking models. On the other hand, PT-Ranking supports to compare different\nlearning-to-rank methods based on the widely used datasets (e.g., MSLR-WEB30K,\nYahoo!LETOR and Istella LETOR) in terms of different metrics, such as\nprecision, MAP, nDCG, nERR. By randomly masking the ground-truth labels with a\nspecified ratio, PT-Ranking allows to examine to what extent the ratio of\nunlabelled query-document pairs affects the performance of different\nlearning-to-rank methods. We further conducted a series of demo experiments to\nclearly show the effect of different factors on neural learning-to-rank\nmethods, such as the activation function, the number of layers and the\noptimization strategy.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.06487,regular,pre_llm,2020,8,"{'ai_likelihood': 2.682209014892578e-06, 'text': ""Negative Confidence-Aware Weakly Supervised Binary Classification for\n  Effective Review Helpfulness Classification\n\n  The incompleteness of positive labels and the presence of many unlabelled\ninstances are common problems in binary classification applications such as in\nreview helpfulness classification. Various studies from the classification\nliterature consider all unlabelled instances as negative examples. However, a\nclassification model that learns to classify binary instances with incomplete\npositive labels while assuming all unlabelled data to be negative examples will\noften generate a biased classifier. In this work, we propose a novel Negative\nConfidence-aware Weakly Supervised approach (NCWS), which customises a binary\nclassification loss function by discriminating the unlabelled examples with\ndifferent negative confidences during the classifier's training. We use the\nreview helpfulness classification as a test case for examining the\neffectiveness of our NCWS approach. We thoroughly evaluate NCWS by using three\ndifferent datasets, namely one from Yelp (venue reviews), and two from Amazon\n(Kindle and Electronics reviews). Our results show that NCWS outperforms strong\nbaselines from the literature including an existing SVM-based approach (i.e.\nSVM-P), the positive and unlabelled learning-based approach (i.e. C-PU) and the\npositive confidence-based approach (i.e. P-conf) in addressing the classifier's\nbias problem. Moreover, we further examine the effectiveness of NCWS by using\nits classified helpful reviews in a state-of-the-art review-based venue\nrecommendation model (i.e. DeepCoNN) and demonstrate the benefits of using NCWS\nin enhancing venue recommendation effectiveness in comparison to the baselines.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.13281,regular,pre_llm,2020,8,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Beyond Next Item Recommendation: Recommending and Evaluating List of\n  Sequences\n\n  Recommender systems (RS) suggest items-based on the estimated preferences of\nusers. Recent RS methods utilise vector space embeddings and deep learning\nmethods to make efficient recommendations. However, most of these methods\noverlook the sequentiality feature and consider each interaction, e.g.,\ncheck-in, independent from each other. The proposed method considers the\nsequentiality of the interactions of users with items and uses them to make\nrecommendations of a list of multi-item sequences. The proposed method uses\nFastText \\cite{bojanowski2016enriching}, a well-known technique in natural\nlanguage processing (NLP), to model the relationship among the subunits of\nsequences, e.g., tracks, playlists, and utilises the trained representation as\nan input to a traditional recommendation method. The recommended lists of\nmulti-item sequences are evaluated by the ROUGE\n\\cite{lin2003automatic,lin2004rouge} metric, which is also commonly used in the\nNLP literature. The current experimental results reveal that it is possible to\nrecommend a list of multi-item sequences, in addition to the traditional next\nitem recommendation. Also, the usage of FastText, which utilise sub-units of\nthe input sequences, helps to overcome cold-start user problem.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.08948,regular,pre_llm,2020,9,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'A New Citation Recommendation Strategy Based on Term Functions in\n  Related Studies Section\n\n  Purpose: Researchers frequently encounter the following problems when writing\nscientific articles: (1) Selecting appropriate citations to support the\nresearch idea is challenging. (2) The literature review is not conducted\nextensively, which leads to working on a research problem that others have well\naddressed. This study focuses on citation recommendation in the related studies\nsection by applying the term function of a citation context, potentially\nimproving the efficiency of writing a literature review.\nDesign/methodology/approach: We present nine term functions with three newly\ncreated and six identified from existing literature. Using these term functions\nas labels, we annotate 531 research papers in three topics to evaluate our\nproposed recommendation strategy. BM25 and Word2vec with VSM are implemented as\nthe baseline models for the recommendation. Then the term function information\nis applied to enhance the performance. Findings: The experiments show that the\nterm function-based methods outperform the baseline methods regarding the\nrecall, precision, and F1-score measurement, demonstrating that term functions\nare useful in identifying valuable citations. Research limitations: The dataset\nis insufficient due to the complexity of annotating citation functions for\nparagraphs in the related studies section. More recent deep learning models\nshould be performed to future validate the proposed approach. Practical\nimplications: The citation recommendation strategy can be helpful for valuable\ncitation discovery, semantic scientific retrieval, and automatic literature\nreview generation. Originality/value: The proposed citation function-based\ncitation recommendation can generate intuitive explanations of the results for\nusers, improving the transparency, persuasiveness, and effectiveness of\nrecommender systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.10791,regular,pre_llm,2020,9,"{'ai_likelihood': 7.28501213921441e-06, 'text': 'Using the Hammer Only on Nails: A Hybrid Method for Evidence Retrieval\n  for Question Answering\n\n  Evidence retrieval is a key component of explainable question answering (QA).\nWe argue that, despite recent progress, transformer network-based approaches\nsuch as universal sentence encoder (USE-QA) do not always outperform\ntraditional information retrieval (IR) methods such as BM25 for evidence\nretrieval for QA. We introduce a lexical probing task that validates this\nobservation: we demonstrate that neural IR methods have the capacity to capture\nlexical differences between questions and answers, but miss obvious lexical\noverlap signal. Learning from this probing analysis, we introduce a hybrid\napproach for evidence retrieval that combines the advantages of both IR\ndirections. Our approach uses a routing classifier that learns when to direct\nincoming questions to BM25 vs. USE-QA for evidence retrieval using very simple\nstatistics, which can be efficiently extracted from the top candidate evidence\nsentences produced by a BM25 model. We demonstrate that this hybrid evidence\nretrieval generally performs better than either individual retrieval strategy\non three QA datasets: OpenBookQA, ReQA SQuAD, and ReQA NQ. Furthermore, we show\nthat the proposed routing strategy is considerably faster than neural methods,\nwith a runtime that is up to 5 times faster than USE-QA.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.04964,review,pre_llm,2020,9,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'News Recommender System: A review of recent progress, challenges, and\n  opportunities\n\n  Nowadays, more and more news readers tend to read news online where they have\naccess to millions of news articles from multiple sources. In order to help\nusers to find the right and relevant content, news recommender systems (NRS)\nare developed to relieve the information overload problem and suggest news\nitems that users might be interested in. In this paper, we highlight the major\nchallenges faced by the news recommendation domain and identify the possible\nsolutions from the state-of-the-art. Due to the rapid growth of building\nrecommender systems using deep learning models, we divide our discussion in two\nparts. In the first part, we present an overview of the conventional\nrecommendation solutions, datasets, evaluation criteria beyond accuracy and\nrecommendation platforms being used in NRS. In the second part, we explain the\ndeep learning-based recommendation solutions applied in NRS. Different from\nprevious surveys, we also study the effects of news recommendations on user\nbehavior and try to suggest the possible remedies to mitigate these effects. By\nproviding the state-of-the-art knowledge, this survey can help researchers and\npractical professionals in their understanding of developments in news\nrecommendation algorithms. It also sheds light on potential new directions\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.11576,regular,pre_llm,2020,9,"{'ai_likelihood': 1.2252065870496963e-06, 'text': 'ArXivDigest: A Living Lab for Personalized Scientific Literature\n  Recommendation\n\n  Providing personalized recommendations that are also accompanied by\nexplanations as to why an item is recommended is a research area of growing\nimportance. At the same time, progress is limited by the availability of open\nevaluation resources. In this work, we address the task of scientific\nliterature recommendation. We present arXivDigest, which is an online service\nproviding personalized arXiv recommendations to end users and operates as a\nliving lab for researchers wishing to work on explainable scientific literature\nrecommendations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.03668,regular,pre_llm,2020,9,"{'ai_likelihood': 3.112687004937066e-06, 'text': 'IAI MovieBot: A Conversational Movie Recommender System\n\n  Conversational recommender systems support users in accomplishing\nrecommendation-related goals via multi-turn conversations. To better model\ndynamically changing user preferences and provide the community with a reusable\ndevelopment framework, we introduce IAI MovieBot, a conversational recommender\nsystem for movies. It features a task-specific dialogue flow, a multi-modal\nchat interface, and an effective way to deal with dynamically changing user\npreferences. The system is made available open source and is operated as a\nchannel on Telegram.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.10128,regular,pre_llm,2020,9,"{'ai_likelihood': 5.1657358805338544e-06, 'text': 'Claraprint: a chord and melody based fingerprint for western classical\n  music cover detection\n\n  Cover song detection has been an active field in the Music Information\nRetrieval (MIR) community during the past decades. Most of the research\ncommunity focused in solving it for a wide range of music genres with diverse\ncharacteristics. Western classical music, a genre heavily based on the\nrecording of ""cover songs"", or musical works, represents a large heritage,\noffering immediate application for an efficient fingerprint algorithm. We\npropose an engineering approach for retrieving a cover song from a reference\ndatabase thanks to a fingerprint designed for classical musical works. We open\na new data set to encourage the scientific community to use it for further\nresearches regarding this genre.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.01311,review,pre_llm,2020,9,"{'ai_likelihood': 4.635916815863716e-07, 'text': 'Comparing Fair Ranking Metrics\n\n  Ranked lists are frequently used by information retrieval (IR) systems to\npresent results believed to be relevant to the users information need. Fairness\nis a relatively new but important aspect of these rankings to measure, joining\na rich set of metrics that go beyond traditional accuracy or utility constructs\nto provide a more holistic understanding of IR system behavior. In the last few\nyears, several metrics have been proposed to quantify the (un)fairness of\nrankings, particularly with respect to particular group(s) of content\nproviders, but comparative analyses of these metrics -- particularly for IR --\nis lacking. There is limited guidance, therefore, to decide what fairness\nmetrics are applicable to a specific scenario, or assessment of the extent to\nwhich metrics agree or disagree applied to real data. In this paper, we\ndescribe several fair ranking metrics from existing literature in a common\nnotation, enabling direct comparison of their assumptions, goals, and design\nchoices; we then empirically compare them on multiple data sets covering both\nsearch and recommendation tasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.11796,review,pre_llm,2020,9,"{'ai_likelihood': 1.3245476616753472e-06, 'text': ""Automatic Extraction of Agriculture Terms from Domain Text: A Survey of\n  Tools and Techniques\n\n  Agriculture is a key component in any country's development. Domain-specific\nknowledge resources serve to gain insight into the domain. Existing knowledge\nresources such as AGROVOC and NAL Thesaurus are developed and maintained by the\ndomain experts. Population of terms into these knowledge resources can be\nautomated by using automatic term extraction tools for processing unstructured\nagricultural text. Automatic term extraction is also a key component in many\nsemantic web applications, such as ontology creation, recommendation systems,\nsentiment classification, query expansion among others. The primary goal of an\nautomatic term extraction system is to maximize the number of valid terms and\nminimize the number of invalid terms extracted from the input set of documents.\nDespite its importance in various applications, the availability of online\ntools for the said purpose is rather limited. Moreover, the performance of the\nmost popular ones among them varies significantly. As a consequence, selection\nof the right term extraction tool is perceived as a serious problem for\ndifferent knowledge-based applications. This paper presents an analysis of\nthree commonly used term extraction tools, viz. RAKE, TerMine, TermRaider and\ncompares their performance in terms of precision and recall, vis-a-vis RENT, a\nmore recent term extractor developed by these authors for agriculture domain.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.08206,regular,pre_llm,2020,9,"{'ai_likelihood': 2.1523899502224394e-06, 'text': ""Learning to Personalize for Web Search Sessions\n\n  The task of session search focuses on using interaction data to improve\nrelevance for the user's next query at the session level. In this paper, we\nformulate session search as a personalization task under the framework of\nlearning to rank. Personalization approaches re-rank results to match a user\nmodel. Such user models are usually accumulated over time based on the user's\nbrowsing behaviour. We use a pre-computed and transparent set of user models\nbased on concepts from the social science literature. Interaction data are used\nto map each session to these user models. Novel features are then estimated\nbased on such models as well as sessions' interaction data. Extensive\nexperiments on test collections from the TREC session track show statistically\nsignificant improvements over current session search algorithms.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.02637,regular,pre_llm,2020,9,"{'ai_likelihood': 2.1192762586805556e-06, 'text': 'Detecting User Community in Sparse Domain via Cross-Graph Pairwise\n  Learning\n\n  Cyberspace hosts abundant interactions between users and different kinds of\nobjects, and their relations are often encapsulated as bipartite graphs.\nDetecting user community in such heterogeneous graphs is an essential task to\nuncover user information needs and to further enhance recommendation\nperformance. While several main cyber domains carrying high-quality graphs,\nunfortunately, most others can be quite sparse. However, as users may appear in\nmultiple domains (graphs), their high-quality activities in the main domains\ncan supply community detection in the sparse ones, e.g., user behaviors on\nGoogle can help thousands of applications to locate his/her local community\nwhen s/he uses Google ID to login those applications. In this paper, our model,\nPairwise Cross-graph Community Detection (PCCD), is proposed to cope with the\nsparse graph problem by involving external graph knowledge to learn user\npairwise community closeness instead of detecting direct communities.\nParticularly in our model, to avoid taking excessive propagated information, a\ntwo-level filtering module is utilized to select the most informative\nconnections through both community and node level filters. Subsequently, a\nCommunity Recurrent Unit (CRU) is designed to estimate pairwise user community\ncloseness. Extensive experiments on two real-world graph datasets validate our\nmodel against several strong alternatives. Supplementary experiments also\nvalidate its robustness on graphs with varied sparsity scales.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.04915,regular,pre_llm,2020,9,"{'ai_likelihood': 1.7881393432617188e-06, 'text': 'Sanitizing Synthetic Training Data Generation for Question Answering\n  over Knowledge Graphs\n\n  Synthetic data generation is important to training and evaluating neural\nmodels for question answering over knowledge graphs. The quality of the data\nand the partitioning of the datasets into training, validation and test splits\nimpact the performance of the models trained on this data. If the synthetic\ndata generation depends on templates, as is the predominant approach for this\ntask, there may be a leakage of information via a shared basis of templates\nacross data splits if the partitioning is not performed hygienically. This\npaper investigates the extent of such information leakage across data splits,\nand the ability of trained models to generalize to test data when the leakage\nis controlled. We find that information leakage indeed occurs and that it\naffects performance. At the same time, the trained models do generalize to test\ndata under the sanitized partitioning presented here. Importantly, these\nfindings extend beyond the particular flavor of question answering task we\nstudied and raise a series of difficult questions around template-based\nsynthetic data generation that will necessitate additional research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.05183,regular,pre_llm,2020,9,"{'ai_likelihood': 1.8212530348036025e-06, 'text': 'TRec: Sequential Recommender Based On Latent Item Trend Information\n\n  Recommendation system plays an important role in online web applications.\nSequential recommender further models user short-term preference through\nexploiting information from latest user-item interaction history. Most of the\nsequential recommendation methods neglect the importance of ever-changing item\npopularity. We propose the model from the intuition that items with most user\ninteractions may be popular in the past but could go out of fashion in recent\ndays. To this end, this paper proposes a novel sequential recommendation\napproach dubbed TRec, TRec learns item trend information from implicit user\ninteraction history and incorporates item trend information into next item\nrecommendation tasks. Then a self-attention mechanism is used to learn better\nnode representation. Our model is trained via pairwise rank-based optimization.\nWe conduct extensive experiments with seven baseline methods on four benchmark\ndatasets, The empirical result shows our approach outperforms other\nstateof-the-art methods while maintains a superiorly low runtime cost. Our\nstudy demonstrates the importance of item trend information in recommendation\nsystem designs, and our method also possesses great efficiency which enables it\nto be practical in real-world scenarios.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.13724,regular,pre_llm,2020,9,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'One Person, One Model, One World: Learning Continual User Representation\n  without Forgetting\n\n  Learning user representations is a vital technique toward effective user\nmodeling and personalized recommender systems. Existing approaches often derive\nan individual set of model parameters for each task by training on separate\ndata. However, the representation of the same user potentially has some\ncommonalities, such as preference and personality, even in different tasks. As\nsuch, these separately trained representations could be suboptimal in\nperformance as well as inefficient in terms of parameter sharing.\n  In this paper, we delve on research to continually learn user representations\ntask by task, whereby new tasks are learned while using partial parameters from\nold ones. A new problem arises since when new tasks are trained, previously\nlearned parameters are very likely to be modified, and as a result, an\nartificial neural network (ANN)-based model may lose its capacity to serve for\nwell-trained previous tasks forever, this issue is termed catastrophic\nforgetting. To address this issue, we present \\emph{Conure} the first\n\\underline{con}tinual, or lifelong, \\underline{u}ser \\underline{re}presentation\nlearner -- i.e., learning new tasks over time without forgetting old ones.\nSpecifically, we propose iteratively removing less important weights of old\ntasks in a deep user representation model, motivated by the fact that neural\nnetwork models are usually over-parameterized. In this way, we could learn many\ntasks with a single model by reusing the important weights, and modifying the\nless important weights to adapt to new tasks. We conduct extensive experiments\non two real-world datasets with nine tasks and show that \\emph{Conure} largely\nexceeds the standard model that does not purposely preserve such old\n""knowledge"", and performs competitively or sometimes better than models which\nare trained either individually for each task or simultaneously by merging all\ntask data.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.02684,regular,pre_llm,2020,9,"{'ai_likelihood': 1.2252065870496963e-06, 'text': 'An Improved Algorithm for Fast K-Word Proximity Search Based on\n  Multi-Component Key Indexes\n\n  A search query consists of several words. In a proximity full-text search, we\nwant to find documents that contain these words near each other. This task\nrequires much time when the query consists of high-frequently occurring words.\nIf we cannot avoid this task by excluding high-frequently occurring words from\nconsideration by declaring them as stop words, then we can optimize our\nsolution by introducing additional indexes for faster execution. In a previous\nwork, we discussed how to decrease the search time with multi-component key\nindexes. We had shown that additional indexes can be used to improve the\naverage query execution time up to 130 times if queries consisted of\nhigh-frequently occurring words. In this paper, we present another search\nalgorithm that overcomes some limitations of our previous algorithm and\nprovides even more performance gain.\n  This is a pre-print of a contribution published in Arai K., Kapoor S., Bhatia\nR. (eds) Intelligent Systems and Applications. IntelliSys 2020. Advances in\nIntelligent Systems and Computing, vol 1251, published by Springer, Cham. The\nfinal authenticated version is available online at:\nhttps://doi.org/10.1007/978-3-030-55187-2_37\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.02865,regular,pre_llm,2020,9,"{'ai_likelihood': 6.291601392957899e-07, 'text': 'CAVA: A Visual Analytics System for Exploratory Columnar Data\n  Augmentation Using Knowledge Graphs\n\n  Most visual analytics systems assume that all foraging for data happens\nbefore the analytics process; once analysis begins, the set of data attributes\nconsidered is fixed. Such separation of data construction from analysis\nprecludes iteration that can enable foraging informed by the needs that arise\nin-situ during the analysis. The separation of the foraging loop from the data\nanalysis tasks can limit the pace and scope of analysis. In this paper, we\npresent CAVA, a system that integrates data curation and data augmentation with\nthe traditional data exploration and analysis tasks, enabling information\nforaging in-situ during analysis. Identifying attributes to add to the dataset\nis difficult because it requires human knowledge to determine which available\nattributes will be helpful for the ensuing analytical tasks. CAVA crawls\nknowledge graphs to provide users with a a broad set of attributes drawn from\nexternal data to choose from. Users can then specify complex operations on\nknowledge graphs to construct additional attributes. CAVA shows how visual\nanalytics can help users forage for attributes by letting users visually\nexplore the set of available data, and by serving as an interface for query\nconstruction. It also provides visualizations of the knowledge graph itself to\nhelp users understand complex joins such as multi-hop aggregations. We assess\nthe ability of our system to enable users to perform complex data combinations\nwithout programming in a user study over two datasets. We then demonstrate the\ngeneralizability of CAVA through two additional usage scenarios. The results of\nthe evaluation confirm that CAVA is effective in helping the user perform data\nforaging that leads to improved analysis outcomes, and offer evidence in\nsupport of integrating data augmentation as a part of the visual analytics\npipeline.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.03679,regular,pre_llm,2020,9,"{'ai_likelihood': 8.17908181084527e-06, 'text': 'Proximity full-text searches of frequently occurring words with a\n  response time guarantee\n\n  Full-text search engines are important tools for information retrieval. In a\nproximity full-text search, a document is relevant if it contains query terms\nnear each other, especially if the query terms are frequently occurring words.\nFor each word in the text, we use additional indexes to store information about\nnearby words at distances from the given word of less than or equal to\nMaxDistance, which is a parameter. A search algorithm for the case when the\nquery consists of high-frequently used words is discussed. In addition, we\npresent results of experiments with different values of MaxDistance to evaluate\nthe search speed dependence on the value of MaxDistance. These results show\nthat the average time of the query execution with our indexes is 94.7-45.9\ntimes (depending on the value of MaxDistance) less than that with standard\ninverted files when queries that contain high-frequently occurring words are\nevaluated.\n  This is a pre-print of a contribution published in Pinelas S., Kim A., Vlasov\nV. (eds) Mathematical Analysis With Applications. CONCORD-90 2018. Springer\nProceedings in Mathematics & Statistics, vol 318, published by Springer, Cham.\nThe final authenticated version is available online at:\nhttps://doi.org/10.1007/978-3-030-42176-2_37\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.12468,regular,pre_llm,2020,9,"{'ai_likelihood': 2.715322706434462e-06, 'text': ""Investigating Misinformation in Online Marketplaces: An Audit Study on\n  Amazon\n\n  Search and recommendation systems are ubiquitous and irreplaceable tools in\nour daily lives. Despite their critical role in selecting and ranking the most\nrelevant information, they typically do not consider the veracity of\ninformation presented to the user. In this paper, we introduce an audit\nmethodology to investigate the extent of misinformation presented in search\nresults and recommendations on online marketplaces. We investigate the factors\nand personalization attributes that influence the amount of misinformation in\nsearches and recommendations. Recently, several media reports criticized Amazon\nfor hosting and recommending items that promote misinformation on topics such\nas vaccines. Motivated by those reports, we apply our algorithmic auditing\nmethodology on Amazon to verify those claims. Our audit study investigates (a)\nfactors that might influence the search algorithms of Amazon and (b)\npersonalization attributes that contribute to amplifying the amount of\nmisinformation recommended to users in their search results and\nrecommendations. Our audit study collected ~526k search results and ~182k\nhomepage recommendations, with ~8.5k unique items. Each item is annotated for\nits stance on vaccines' misinformation (pro, neutral, or anti). Our study\nreveals that (1) the selection and ranking by the default Featured search\nalgorithm of search results that have misinformation stances are positively\ncorrelated with the stance of search queries and customers' evaluation of items\n(ratings and reviews), (2) misinformation stances of search results are neither\naffected by users' activities nor by interacting (browsing, wish-listing,\nshopping) with items that have a misinformation stance, and (3) a filter bubble\nbuilt-in users' homepages have a misinformation stance positively correlated\nwith the misinformation stance of items that a user interacts with.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.01715,regular,pre_llm,2020,9,"{'ai_likelihood': 3.973642985026042e-07, 'text': ""Exploring Artist Gender Bias in Music Recommendation\n\n  Music Recommender Systems (mRS) are designed to give personalised and\nmeaningful recommendations of items (i.e. songs, playlists or artists) to a\nuser base, thereby reflecting and further complementing individual users'\nspecific music preferences. Whilst accuracy metrics have been widely applied to\nevaluate recommendations in mRS literature, evaluating a user's item utility\nfrom other impact-oriented perspectives, including their potential for\ndiscrimination, is still a novel evaluation practice in the music domain. In\nthis work, we center our attention on a specific phenomenon for which we want\nto estimate if mRS may exacerbate its impact: gender bias. Our work presents an\nexploratory study, analyzing the extent to which commonly deployed state of the\nart Collaborative Filtering(CF) algorithms may act to further increase or\ndecrease artist gender bias. To assess group biases introduced by CF, we deploy\na recently proposed metric of bias disparity on two listening event datasets:\nthe LFM-1b dataset, and the earlier constructed Celma's dataset. Our work\ntraces the causes of disparity to variations in input gender distributions and\nuser-item preferences, highlighting the effect such configurations can have on\nuser's gender bias after recommendation generation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.12097,regular,pre_llm,2020,9,"{'ai_likelihood': 4.6690305074055995e-06, 'text': ""Parsisanj: a semi-automatic component-based approach towards search\n  engine evaluation\n\n  Accessing to required data on the internet is wide via search engines in the\nlast two decades owing to the huge amount of available data and the high rate\nof new data is generating daily. Accordingly, search engines are encouraged to\nmake the most valuable existing data on the web searchable. Knowing how to\nhandle a large amount of data in each step of a search engines' procedure from\ncrawling to indexing and ranking is just one of the challenges that a\nprofessional search engine should solve. Moreover, it should also have the best\npractices in handling users' traffics, state-of-the-art natural language\nprocessing tools, and should also address many other challenges on the edge of\nscience and technology. As a result, evaluating these systems is too\nchallenging due to the level of internal complexity they have, and is crucial\nfor finding the improvement path of the existing system. Therefore, an\nevaluation procedure is a normal subsystem of a search engine that has the role\nof building its roadmap. Recently, several countries have developed national\nsearch engine programs to build an infrastructure to provide special services\nbased on their needs on the available data of their language on the web. This\nresearch is conducted accordingly to enlighten the advancement path of two\nIranian national search engines: Yooz and Parsijoo in comparison with two\ninternational ones, Google and Bing. Unlike related work, it is a\nsemi-automatic method to evaluate the search engines at the first pace.\nEventually, we obtained some interesting results which based on them the\ncomponent-based improvement roadmap of national search engines could be\nillustrated concretely.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.12298,review,pre_llm,2020,9,"{'ai_likelihood': 1.8543667263454863e-06, 'text': ""A review of metadata fields associated with podcast RSS feeds\n\n  Podcasts are traditionally shared through RSS feeds. As well as pointing to\nthe audio files, RSS gives a creator a way of providing metadata about the\npodcast shows and episodes. We investigate how certain metadata fields\nassociated with podcasts are currently being used and comment on their\napplicability to recommendations. Specifically, we find that many creators are\nnot using the itunes:type field in the expected fashion, and that using this\nfield for recommendations might not lead to an optimal user experience. We\nperform similar explorations for the season number and the category associated\nwith a podcast, and also find that the fields aren't being used in the expected\nfashion. Finally, we examine the notion that a single podcast show is the same\nas a single RSS feed. This also turns out to not be strictly true in all cases.\nIn short, the metadata associated with many podcasts isn't always reflective of\nthe show and should be used with caution.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.10685,regular,pre_llm,2020,10,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'Fact-Checking at Scale with DimensionRank\n\n  The most important problem that has emerged after twenty years of popular\ninternet usage is that of fact-checking at scale. This problem is experienced\nacutely in both of the major internet application platform types, web search\nand social media.\n  We offer a working definition of what a ""platform"" is. We critically\ndeconstruct what we call the ""PolitiFact"" model of fact checking, and show it\nto be inherently inferior for fact-checking at scale to a platform-b ased\nsolution.\n  Our central contribution is to show how to effectively platformize the\nproblem of fact-checking at scale. We show how a two-dimensional rating system,\nwith dimensions agreement and hotness allows us to create information-seeking\nqueries not possible with the on e-dimensional rating system predominating on\nexisting platforms. And, we show that, underlying our user-friendly\nuser-interface, lies a system that allows the creation of formal proofs in the\npropositional calculus.\n  Our algorithm is implemented in our open-source DimensionRank software\npackage available at ""https://thinkdifferentagain.art"".\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.12674,regular,pre_llm,2020,10,"{'ai_likelihood': 1.592768563164605e-05, 'text': ""Exploring task-based query expansion at the TREC-COVID track\n\n  We explore how to generate effective queries based on search tasks. Our\napproach has three main steps: 1) identify search tasks based on research\ngoals, 2) manually classify search queries according to those tasks, and 3)\ncompare three methods to improve search rankings based on the task context. The\nmost promising approach is based on expanding the user's query terms using task\nterms, which slightly improved the NDCG@20 scores over a BM25 baseline. Further\nimprovements might be gained if we can identify more specific search tasks.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.15363,regular,pre_llm,2020,10,"{'ai_likelihood': 2.4172994825575088e-06, 'text': ""Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias\n  in Recommender System\n\n  The general aim of the recommender system is to provide personalized\nsuggestions to users, which is opposed to suggesting popular items. However,\nthe normal training paradigm, i.e., fitting a recommender model to recover the\nuser behavior data with pointwise or pairwise loss, makes the model biased\ntowards popular items. This results in the terrible Matthew effect, making\npopular items be more frequently recommended and become even more popular.\nExisting work addresses this issue with Inverse Propensity Weighting (IPW),\nwhich decreases the impact of popular items on the training and increases the\nimpact of long-tail items. Although theoretically sound, IPW methods are highly\nsensitive to the weighting strategy, which is notoriously difficult to tune. In\nthis work, we explore the popularity bias issue from a novel and fundamental\nperspective -- cause-effect. We identify that popularity bias lies in the\ndirect effect from the item node to the ranking score, such that an item's\nintrinsic property is the cause of mistakenly assigning it a higher ranking\nscore. To eliminate popularity bias, it is essential to answer the\ncounterfactual question that what the ranking score would be if the model only\nuses item property. To this end, we formulate a causal graph to describe the\nimportant cause-effect relations in the recommendation process. During\ntraining, we perform multi-task learning to achieve the contribution of each\ncause; during testing, we perform counterfactual inference to remove the effect\nof item popularity. Remarkably, our solution amends the learning process of\nrecommendation which is agnostic to a wide range of models -- it can be easily\nimplemented in existing methods. We demonstrate it on Matrix Factorization (MF)\nand LightGCN [20]. Experiments on five real-world datasets demonstrate the\neffectiveness of our method.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.1237,regular,pre_llm,2020,10,"{'ai_likelihood': 1.556343502468533e-06, 'text': 'EventKG+Click: A Dataset of Language-specific Event-centric User\n  Interaction Traces\n\n  An increasing need to analyse event-centric cross-lingual information calls\nfor innovative user interaction models that assist users in crossing the\nlanguage barrier. However, datasets that reflect user interaction traces in\ncross-lingual settings required to train and evaluate the user interaction\nmodels are mostly missing. In this paper, we present the EventKG+Click dataset\nthat aims to facilitate the creation and evaluation of such interaction models.\nEventKG+Click builds upon the event-centric EventKG knowledge graph and\nlanguage-specific information on user interactions with events, entities, and\ntheir relations derived from the Wikipedia clickstream.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.1562,regular,pre_llm,2020,10,"{'ai_likelihood': 2.2186173333062066e-06, 'text': 'CAFE: Coarse-to-Fine Neural Symbolic Reasoning for Explainable\n  Recommendation\n\n  Recent research explores incorporating knowledge graphs (KG) into e-commerce\nrecommender systems, not only to achieve better recommendation performance, but\nmore importantly to generate explanations of why particular decisions are made.\nThis can be achieved by explicit KG reasoning, where a model starts from a user\nnode, sequentially determines the next step, and walks towards an item node of\npotential interest to the user. However, this is challenging due to the huge\nsearch space, unknown destination, and sparse signals over the KG, so\ninformative and effective guidance is needed to achieve a satisfactory\nrecommendation quality. To this end, we propose a CoArse-to-FinE neural\nsymbolic reasoning approach (CAFE). It first generates user profiles as coarse\nsketches of user behaviors, which subsequently guide a path-finding process to\nderive reasoning paths for recommendations as fine-grained predictions. User\nprofiles can capture prominent user behaviors from the history, and provide\nvaluable signals about which kinds of path patterns are more likely to lead to\npotential items of interest for the user. To better exploit the user profiles,\nan improved path-finding algorithm called Profile-guided Path Reasoning (PPR)\nis also developed, which leverages an inventory of neural symbolic reasoning\nmodules to effectively and efficiently find a batch of paths over a large-scale\nKG. We extensively experiment on four real-world benchmarks and observe\nsubstantial gains in the recommendation performance compared with\nstate-of-the-art methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.12256,regular,pre_llm,2020,10,"{'ai_likelihood': 4.2054388258192275e-06, 'text': ""NGAT4Rec: Neighbor-Aware Graph Attention Network For Recommendation\n\n  Learning informative representations (aka. embeddings) of users and items is\nthe core of modern recommender systems. Previous works exploit user-item\nrelationships of one-hop neighbors in the user-item interaction graph to\nimprove the quality of representation. Recently, the research of Graph Neural\nNetwork (GNN) for recommendation considers the implicit collaborative\ninformation of multi-hop neighbors to enrich the representation. However, most\nworks of GNN for recommendation systems do not consider the relational\ninformation which implies the expression differences of different neighbors in\nthe neighborhood explicitly. The influence of each neighboring item to the\nrepresentation of the user's preference can be represented by the correlation\nbetween the item and neighboring items of the user. Symmetrically, for a given\nitem, the correlation between one neighboring user and neighboring users can\nreflect the strength of signal about the item's characteristic. To modeling the\nimplicit correlations of neighbors in graph embedding aggregating, we propose a\nNeighbor-Aware Graph Attention Network for recommendation task, termed\nNGAT4Rec. It employs a novel neighbor-aware graph attention layer that assigns\ndifferent neighbor-aware attention coefficients to different neighbors of a\ngiven node by computing the attention among these neighbors pairwisely. Then\nNGAT4Rec aggregates the embeddings of neighbors according to the corresponding\nneighbor-aware attention coefficients to generate next layer embedding for\nevery node. Furthermore, we combine more neighbor-aware graph attention layer\nto gather the influential signals from multi-hop neighbors. We remove feature\ntransformation and nonlinear activation that proved to be useless on\ncollaborative filtering. Extensive experiments on three benchmark datasets show\nthat our model outperforms various state-of-the-art models consistently.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.01494,regular,pre_llm,2020,10,"{'ai_likelihood': 7.185671064588759e-06, 'text': 'PTUM: Pre-training User Model from Unlabeled User Behaviors via\n  Self-supervision\n\n  User modeling is critical for many personalized web services. Many existing\nmethods model users based on their behaviors and the labeled data of target\ntasks. However, these methods cannot exploit useful information in unlabeled\nuser behavior data, and their performance may be not optimal when labeled data\nis scarce. Motivated by pre-trained language models which are pre-trained on\nlarge-scale unlabeled corpus to empower many downstream tasks, in this paper we\npropose to pre-train user models from large-scale unlabeled user behaviors\ndata. We propose two self-supervision tasks for user model pre-training. The\nfirst one is masked behavior prediction, which can model the relatedness\nbetween historical behaviors. The second one is next $K$ behavior prediction,\nwhich can model the relatedness between past and future behaviors. The\npre-trained user models are finetuned in downstream tasks to learn\ntask-specific user representations. Experimental results on two real-world\ndatasets validate the effectiveness of our proposed user model pre-training\nmethod.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.10409,review,pre_llm,2020,10,"{'ai_likelihood': 5.1657358805338544e-06, 'text': ""Bias in Conversational Search: The Double-Edged Sword of the\n  Personalized Knowledge Graph\n\n  Conversational AI systems are being used in personal devices, providing users\nwith highly personalized content. Personalized knowledge graphs (PKGs) are one\nof the recently proposed methods to store users' information in a structured\nform and tailor answers to their liking. Personalization, however, is prone to\namplifying bias and contributing to the echo-chamber phenomenon. In this paper,\nwe discuss different types of biases in conversational search systems, with the\nemphasis on the biases that are related to PKGs. We review existing definitions\nof bias in the literature: people bias, algorithm bias, and a combination of\nthe two, and further propose different strategies for tackling these biases for\nconversational search systems. Finally, we discuss methods for measuring bias\nand evaluating user satisfaction.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.10508,regular,pre_llm,2020,10,"{'ai_likelihood': 5.298190646701389e-06, 'text': 'Polar Deconvolution of Mixed Signals\n\n  The signal demixing problem seeks to separate a superposition of multiple\nsignals into its constituent components. This paper studies a two-stage\napproach that first decompresses and subsequently deconvolves the noisy and\nundersampled observations of the superposition using two convex programs.\nProbabilistic error bounds are given on the accuracy with which this process\napproximates the individual signals. The theory of polar convolution of convex\nsets and gauge functions plays a central role in the analysis and solution\nprocess. If the measurements are random and the noise is bounded, this approach\nstably recovers low-complexity and mutually incoherent signals, with high\nprobability and with near-optimal sample complexity. We develop an efficient\nalgorithm, based on level-set and conditional-gradient methods, that solves the\nconvex optimization problems with sublinear iteration complexity and linear\nspace requirements. Numerical experiments on both real and synthetic data\nconfirm the theory and the efficiency of the approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.01195,regular,pre_llm,2020,10,"{'ai_likelihood': 9.139378865559896e-06, 'text': 'Leveraging Semantic and Lexical Matching to Improve the Recall of\n  Document Retrieval Systems: A Hybrid Approach\n\n  Search engines often follow a two-phase paradigm where in the first stage\n(the retrieval stage) an initial set of documents is retrieved and in the\nsecond stage (the re-ranking stage) the documents are re-ranked to obtain the\nfinal result list. While deep neural networks were shown to improve the\nperformance of the re-ranking stage in previous works, there is little\nliterature about using deep neural networks to improve the retrieval stage. In\nthis paper, we study the merits of combining deep neural network models and\nlexical models for the retrieval stage. A hybrid approach, which leverages both\nsemantic (deep neural network-based) and lexical (keyword matching-based)\nretrieval models, is proposed. We perform an empirical study, using a publicly\navailable TREC collection, which demonstrates the effectiveness of our approach\nand sheds light on the different characteristics of the semantic approach, the\nlexical approach, and their combination.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.14531,review,pre_llm,2020,10,"{'ai_likelihood': 2.615981631808811e-06, 'text': 'Assessing Viewpoint Diversity in Search Results Using Ranking Fairness\n  Metrics\n\n  The way pages are ranked in search results influences whether the users of\nsearch engines are exposed to more homogeneous, or rather to more diverse\nviewpoints. However, this viewpoint diversity is not trivial to assess. In this\npaper we use existing and novel ranking fairness metrics to evaluate viewpoint\ndiversity in search result rankings. We conduct a controlled simulation study\nthat shows how ranking fairness metrics can be used for viewpoint diversity,\nhow their outcome should be interpreted, and which metric is most suitable\ndepending on the situation. This paper lays out important ground work for\nfuture research to measure and assess viewpoint diversity in real search result\nrankings.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.14848,regular,pre_llm,2020,10,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Flexible retrieval with NMSLIB and FlexNeuART\n\n  Our objective is to introduce to the NLP community an existing k-NN search\nlibrary NMSLIB, a new retrieval toolkit FlexNeuART, as well as their\nintegration capabilities. NMSLIB, while being one the fastest k-NN search\nlibraries, is quite generic and supports a variety of distance/similarity\nfunctions. Because the library relies on the distance-based structure-agnostic\nalgorithms, it can be further extended by adding new distances. FlexNeuART is a\nmodular, extendible and flexible toolkit for candidate generation in IR and QA\napplications, which supports mixing of classic and neural ranking signals.\nFlexNeuART can efficiently retrieve mixed dense and sparse representations\n(with weights learned from training data), which is achieved by extending\nNMSLIB. In that, other retrieval systems work with purely sparse\nrepresentations (e.g., Lucene), purely dense representations (e.g., FAISS and\nAnnoy), or only perform mixing at the re-ranking stage.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.02666,regular,pre_llm,2020,10,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'Improving Efficient Neural Ranking Models with Cross-Architecture\n  Knowledge Distillation\n\n  Retrieval and ranking models are the backbone of many applications such as\nweb search, open domain QA, or text-based recommender systems. The latency of\nneural ranking models at query time is largely dependent on the architecture\nand deliberate choices by their designers to trade-off effectiveness for higher\nefficiency. This focus on low query latency of a rising number of efficient\nranking architectures make them feasible for production deployment. In machine\nlearning an increasingly common approach to close the effectiveness gap of more\nefficient models is to apply knowledge distillation from a large teacher model\nto a smaller student model. We find that different ranking architectures tend\nto produce output scores in different magnitudes. Based on this finding, we\npropose a cross-architecture training procedure with a margin focused loss\n(Margin-MSE), that adapts knowledge distillation to the varying score output\ndistributions of different BERT and non-BERT passage ranking architectures. We\napply the teachable information as additional fine-grained labels to existing\ntraining triples of the MSMARCO-Passage collection. We evaluate our procedure\nof distilling knowledge from state-of-the-art concatenated BERT models to four\ndifferent efficient architectures (TK, ColBERT, PreTT, and a BERT CLS dot\nproduct model). We show that across our evaluated architectures our Margin-MSE\nknowledge distillation significantly improves re-ranking effectiveness without\ncompromising their efficiency. Additionally, we show our general distillation\nmethod to improve nearest neighbor based index retrieval with the BERT dot\nproduct model, offering competitive results with specialized and much more\ncostly training methods. To benefit the community, we publish the teacher-score\ntraining files in a ready-to-use package.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.04484,review,pre_llm,2020,10,"{'ai_likelihood': 5.198849572075738e-06, 'text': 'Revisiting Alternative Experimental Settings for Evaluating Top-N Item\n  Recommendation Algorithms\n\n  Top-N item recommendation has been a widely studied task from implicit\nfeedback. Although much progress has been made with neural methods, there is\nincreasing concern on appropriate evaluation of recommendation algorithms. In\nthis paper, we revisit alternative experimental settings for evaluating top-N\nrecommendation algorithms, considering three important factors, namely dataset\nsplitting, sampled metrics and domain selection. We select eight representative\nrecommendation algorithms (covering both traditional and neural methods) and\nconstruct extensive experiments on a very large dataset. By carefully\nrevisiting different options, we make several important findings on the three\nfactors, which directly provide useful suggestions on how to appropriately set\nup the experiments for top-N item recommendation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.0324,review,pre_llm,2020,10,"{'ai_likelihood': 2.2186173333062066e-06, 'text': ""Bias and Debias in Recommender System: A Survey and Future Directions\n\n  While recent years have witnessed a rapid growth of research papers on\nrecommender system (RS), most of the papers focus on inventing machine learning\nmodels to better fit user behavior data. However, user behavior data is\nobservational rather than experimental. This makes various biases widely exist\nin the data, including but not limited to selection bias, position bias,\nexposure bias, and popularity bias. Blindly fitting the data without\nconsidering the inherent biases will result in many serious issues, e.g., the\ndiscrepancy between offline evaluation and online metrics, hurting user\nsatisfaction and trust on the recommendation service, etc. To transform the\nlarge volume of research models into practical improvements, it is highly\nurgent to explore the impacts of the biases and perform debiasing when\nnecessary. When reviewing the papers that consider biases in RS, we find that,\nto our surprise, the studies are rather fragmented and lack a systematic\norganization. The terminology ``bias'' is widely used in the literature, but\nits definition is usually vague and even inconsistent across papers. This\nmotivates us to provide a systematic survey of existing work on RS biases. In\nthis paper, we first summarize seven types of biases in recommendation, along\nwith their definitions and characteristics. We then provide a taxonomy to\nposition and organize the existing work on recommendation debiasing. Finally,\nwe identify some open challenges and envision some future directions, with the\nhope of inspiring more research work on this important yet less investigated\ntopic. The summary of debiasing methods reviewed in this survey can be found at\n\\url{https://github.com/jiawei-chen/RecDebiasing}.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.00768,regular,pre_llm,2020,10,"{'ai_likelihood': 7.318125830756294e-06, 'text': 'SparTerm: Learning Term-based Sparse Representation for Fast Text\n  Retrieval\n\n  Term-based sparse representations dominate the first-stage text retrieval in\nindustrial applications, due to its advantage in efficiency, interpretability,\nand exact term matching. In this paper, we study the problem of transferring\nthe deep knowledge of the pre-trained language model (PLM) to Term-based Sparse\nrepresentations, aiming to improve the representation capacity of\nbag-of-words(BoW) method for semantic-level matching, while still keeping its\nadvantages. Specifically, we propose a novel framework SparTerm to directly\nlearn sparse text representations in the full vocabulary space. The proposed\nSparTerm comprises an importance predictor to predict the importance for each\nterm in the vocabulary, and a gating controller to control the term activation.\nThese two modules cooperatively ensure the sparsity and flexibility of the\nfinal text representation, which unifies the term-weighting and expansion in\nthe same framework. Evaluated on MSMARCO dataset, SparTerm significantly\noutperforms traditional sparse methods and achieves state of the art ranking\nperformance among all the PLM-based sparse models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.10469,regular,pre_llm,2020,10,"{'ai_likelihood': 2.4504131740993925e-06, 'text': 'Learning To Retrieve: How to Train a Dense Retrieval Model Effectively\n  and Efficiently\n\n  Ranking has always been one of the top concerns in information retrieval\nresearch. For decades, lexical matching signal has dominated the ad-hoc\nretrieval process, but it also has inherent defects, such as the vocabulary\nmismatch problem. Recently, Dense Retrieval (DR) technique has been proposed to\nalleviate these limitations by capturing the deep semantic relationship between\nqueries and documents. The training of most existing Dense Retrieval models\nrelies on sampling negative instances from the corpus to optimize a pairwise\nloss function. Through investigation, we find that this kind of training\nstrategy is biased and fails to optimize full retrieval performance effectively\nand efficiently. To solve this problem, we propose a Learning To Retrieve\n(LTRe) training technique. LTRe constructs the document index beforehand. At\neach training iteration, it performs full retrieval without negative sampling\nand then updates the query representation model parameters. Through this\nprocess, it teaches the DR model how to retrieve relevant documents from the\nentire corpus instead of how to rerank a potentially biased sample of\ndocuments. Experiments in both passage retrieval and document retrieval tasks\nshow that: 1) in terms of effectiveness, LTRe significantly outperforms all\ncompetitive sparse and dense baselines. It even gains better performance than\nthe BM25-BERT cascade system under reasonable latency constraints. 2) in terms\nof training efficiency, compared with the previous state-of-the-art DR method,\nLTRe provides more than 170x speed-up in the training process. Training with a\ncompressed index further saves computing resources with minor performance loss.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.14057,regular,pre_llm,2020,10,"{'ai_likelihood': 1.4901161193847656e-06, 'text': 'An Intermediate Data-driven Methodology for Scientific Workflow\n  Management System to Support Reusability\n\n  In this thesis first we propose an intermediate data management scheme for a\nSWfMS. In our second attempt, we explored the possibilities and introduced an\nautomatic recommendation technique for a SWfMS from real-world workflow data\n(i.e Galaxy [1] workflows) where our investigations show that the proposed\ntechnique can facilitate 51% of workflow building in a SWfMS by reusing\nintermediate data of previous workflows and can reduce 74% execution time of\nworkflow buildings in a SWfMS. Later we propose an adaptive version of our\ntechnique by considering the states of tools in a SWfMS, which shows around 40%\nreusability for workflows. Consequently, in our fourth study, We have done\nseveral experiments for analyzing the performance and exploring the\neffectiveness of the technique in a SWfMS for various environments. The\ntechnique is introduced to emphasize on storing cost reduction, increase data\nreusability, and faster workflow execution, to the best of our knowledge, which\nis the first of its kind. Detail architecture and evaluation of the technique\nare presented in this thesis. We believe our findings and developed system will\ncontribute significantly to the research domain of SWfMSs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.09426,regular,pre_llm,2020,10,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'LANNS: A Web-Scale Approximate Nearest Neighbor Lookup System\n\n  Nearest neighbor search (NNS) has a wide range of applications in information\nretrieval, computer vision, machine learning, databases, and other areas.\nExisting state-of-the-art algorithm for nearest neighbor search, Hierarchical\nNavigable Small World Networks(HNSW), is unable to scale to large datasets of\n100M records in high dimensions. In this paper, we propose LANNS, an end-to-end\nplatform for Approximate Nearest Neighbor Search, which scales for web-scale\ndatasets. Library for Large Scale Approximate Nearest Neighbor Search (LANNS)\nis deployed in multiple production systems for identifying topK ($100 \\leq topK\n\\leq 200$) approximate nearest neighbors with a latency of a few milliseconds\nper query, high throughput of 2.5k Queries Per Second (QPS) on a single node,\non large ($\\sim$180M data points) high dimensional (50-2048 dimensional)\ndatasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.10137,regular,pre_llm,2020,10,"{'ai_likelihood': 6.828043195936416e-05, 'text': 'PROP: Pre-training with Representative Words Prediction for Ad-hoc\n  Retrieval\n\n  Recently pre-trained language representation models such as BERT have shown\ngreat success when fine-tuned on downstream tasks including information\nretrieval (IR). However, pre-training objectives tailored for ad-hoc retrieval\nhave not been well explored. In this paper, we propose Pre-training with\nRepresentative wOrds Prediction (PROP) for ad-hoc retrieval. PROP is inspired\nby the classical statistical language model for IR, specifically the query\nlikelihood model, which assumes that the query is generated as the piece of\ntext representative of the ""ideal"" document. Based on this idea, we construct\nthe representative words prediction (ROP) task for pre-training. Given an input\ndocument, we sample a pair of word sets according to the document language\nmodel, where the set with higher likelihood is deemed as more representative of\nthe document. We then pre-train the Transformer model to predict the pairwise\npreference between the two word sets, jointly with the Masked Language Model\n(MLM) objective. By further fine-tuning on a variety of representative\ndownstream ad-hoc retrieval tasks, PROP achieves significant improvements over\nbaselines without pre-training or with other pre-training methods. We also show\nthat PROP can achieve exciting performance under both the zero- and\nlow-resource IR settings. The code and pre-trained models are available at\nhttps://github.com/Albert-Ma/PROP.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.04137,regular,pre_llm,2020,11,"{'ai_likelihood': 1.953707800971137e-06, 'text': 'Automated data extraction of bar chart raster images\n\n  Objective: To develop software utilizing optical character recognition toward\nthe automatic extraction of data from bar charts for meta-analysis. Methods: We\nutilized a multistep data extraction approach that included figure extraction,\ntext detection, and image disassembly. PubMed Central papers that were\nprocessed in this manner included clinical trials regarding macular\ndegeneration, a disease causing blindness with a heavy disease burden and many\nclinical trials. Bar chart characteristics were extracted in both an automated\nand manual fashion. These two approaches were then compared for accuracy. These\ncharacteristics were then compared using a Bland-Altman analysis. Results:\nBased on Bland-Altman analysis, 91.8% of data points were within the limits of\nagreement. By comparing our automated data extraction with manual data\nextraction, automated data extraction yielded the following accuracies: X-axis\nlabels 79.5%, Y-tick values 88.6%, Y-axis label 88.6%, Bar value <5% error\n88.0%. Discussion: Based on our analysis, we achieved an agreement between\nautomated data extraction and manual data extraction. A major source of error\nwas the incorrect delineation of 7s as 2s by optical character recognition\nlibrary. We also would benefit from adding redundancy checks in the form of a\ndeep neural network to boost our bar detection accuracy. Further refinements to\nthis method are justified to extract tabulated and line graph data to\nfacilitate automated data gathering for meta-analysis.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.07363,regular,pre_llm,2020,11,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'RecTen: A Recursive Hierarchical Low Rank Tensor Factorization Method to\n  Discover Hierarchical Patterns in Multi-modal Data\n\n  How can we expand the tensor decomposition to reveal a hierarchical structure\nof the multi-modal data in a self-adaptive way? Current tensor decomposition\nprovides only a single layer of clusters. We argue that with the abundance of\nmultimodal data and time-evolving networks nowadays, the ability to identify\nemerging hierarchies is important. To this effect, we propose RecTen, a\nrecursive hierarchical soft clustering approach based on tensor decomposition.\nOur approach enables us to: (a) recursively decompose clusters identified in\nthe previous step, and (b) identify the right conditions for terminating this\nprocess. In the absence of proper ground truth, we evaluate our approach with\nsynthetic data and test its sensitivity to different parameters. We also apply\nRecTen on five real datasets which involve the activities of users in online\ndiscussion platforms, such as security forums. This analysis helps us reveal\nclusters of users with interesting behaviors, including but not limited to\nearly detection of some real events like ransomware outbreaks, the emergence of\na blackmarket of decryption tools, and romance scamming. To maximize the\nusefulness of our approach, we develop a tool which can help the data analysts\nand overall research community by identifying hierarchical structures. RecTen\nis an unsupervised approach which can be used to take the pulse of the large\nmulti-modal data and let the data discover its own hidden structures by itself.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.05119,regular,pre_llm,2020,11,"{'ai_likelihood': 4.933940039740669e-06, 'text': ""TRSM-RS: A Movie Recommender System Based on Users' Gender and New\n  Weighted Similarity Measure\n\n  With the growing data on the Internet, recommender systems have been able to\npredict users' preferences and offer related movies. Collaborative filtering is\none of the most popular algorithms in these systems. The main purpose of\ncollaborative filtering is to find the users or the same items using the rating\nmatrix. By increasing the number of users and items, this algorithm suffers\nfrom the scalability problem. On the other hand, due to the unavailability of a\nlarge number of user preferences for different items, there is a cold start\nproblem for a new user or item that has a significant impact on system\nperformance. The purpose of this paper is to design a movie recommender system\nnamed TRSM-RS using users' demographic information (just users' gender) along\nwith the new weighted similarity measure. By segmenting users based on their\ngender, the scalability problem is improved, and by considering the reliability\nof the users' similarity as the weight in the new similarity measure (Tanimoto\nReliability Similarity Measure, TRSM), the effect of the cold-start problem is\nundermined and the performance of the system is improved. Experiments were\nperformed on the MovieLens dataset and the system was evaluated using mean\nabsolute error (MAE), Accuracy, Precision, and Recall metrics. The results of\nthe experiments indicate improved performance (accuracy and precision) and\nsystem error rate compared to other research methods of the researchers. The\nmaximum improved MAE rate of the system for men and women is 5.5% and 13.8%,\nrespectively.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.01453,regular,pre_llm,2020,11,"{'ai_likelihood': 5.828009711371528e-06, 'text': 'Participation in TREC 2020 COVID Track Using Continuous Active Learning\n\n  We describe our participation in all five rounds of the TREC 2020 COVID Track\n(TREC-COVID). The goal of TREC-COVID is to contribute to the response to the\nCOVID-19 pandemic by identifying answers to many pressing questions and\nbuilding infrastructure to improve search systems [8]. All five rounds of this\nTrack challenged participants to perform a classic ad-hoc search task on the\nnew data collection CORD-19. Our solution addressed this challenge by applying\nthe Continuous Active Learning model (CAL) and its variations. Our results\nshowed us to be amongst the top scoring manual runs and we remained competitive\nwithin all categories of submissions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.05061,regular,pre_llm,2020,11,"{'ai_likelihood': 5.894237094455296e-06, 'text': ""Alleviating Cold-Start Problems in Recommendation through\n  Pseudo-Labelling over Knowledge Graph\n\n  Solving cold-start problems is indispensable to provide meaningful\nrecommendation results for new users and items. Under sparsely observed data,\nunobserved user-item pairs are also a vital source for distilling latent users'\ninformation needs. Most present works leverage unobserved samples for\nextracting negative signals. However, such an optimisation strategy can lead to\nbiased results toward already popular items by frequently handling new items as\nnegative instances. In this study, we tackle the cold-start problems for new\nusers/items by appropriately leveraging unobserved samples. We propose a\nknowledge graph (KG)-aware recommender based on graph neural networks, which\naugments labelled samples through pseudo-labelling. Our approach aggressively\nemploys unobserved samples as positive instances and brings new items into the\nspotlight. To avoid exhaustive label assignments to all possible pairs of users\nand items, we exploit a KG for selecting probably positive items for each user.\nWe also utilise an improved negative sampling strategy and thereby suppress the\nexacerbation of popularity biases. Through experiments, we demonstrate that our\napproach achieves improvements over the state-of-the-art KG-aware recommenders\nin a variety of scenarios; in particular, our methodology successfully improves\nrecommendation performance for cold-start users/items.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.00944,regular,pre_llm,2020,11,"{'ai_likelihood': 1.1622905731201172e-05, 'text': ""Deep Pairwise Hashing for Cold-start Recommendation\n\n  Recommendation efficiency and data sparsity problems have been regarded as\ntwo challenges of improving performance for online recommendation. Most of the\nprevious related work focus on improving recommendation accuracy instead of\nefficiency. In this paper, we propose a Deep Pairwise Hashing (DPH) to map\nusers and items to binary vectors in Hamming space, where a user's preference\nfor an item can be efficiently calculated by Hamming distance, which\nsignificantly improves the efficiency of online recommendation. To alleviate\ndata sparsity and cold-start problems, the user-item interactive information\nand item content information are unified to learn effective representations of\nitems and users. Specifically, we first pre-train robust item representation\nfrom item content data by a Denoising Auto-encoder instead of other\ndeterministic deep learning frameworks; then we finetune the entire framework\nby adding a pairwise loss objective with discrete constraints; moreover, DPH\naims to minimize a pairwise ranking loss that is consistent with the ultimate\ngoal of recommendation. Finally, we adopt the alternating optimization method\nto optimize the proposed model with discrete constraints. Extensive experiments\non three different datasets show that DPH can significantly advance the\nstate-of-the-art frameworks regarding data sparsity and item cold-start\nrecommendation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.07739,regular,pre_llm,2020,11,"{'ai_likelihood': 1.8212530348036025e-06, 'text': ""CoSam: An Efficient Collaborative Adaptive Sampler for Recommendation\n\n  Sampling strategies have been widely applied in many recommendation systems\nto accelerate model learning from implicit feedback data. A typical strategy is\nto draw negative instances with uniform distribution, which however will\nseverely affect model's convergency, stability, and even recommendation\naccuracy. A promising solution for this problem is to over-sample the\n``difficult'' (a.k.a informative) instances that contribute more on training.\nBut this will increase the risk of biasing the model and leading to non-optimal\nresults. Moreover, existing samplers are either heuristic, which require domain\nknowledge and often fail to capture real ``difficult'' instances; or rely on a\nsampler model that suffers from low efficiency.\n  To deal with these problems, we propose an efficient and effective\ncollaborative sampling method CoSam, which consists of: (1) a collaborative\nsampler model that explicitly leverages user-item interaction information in\nsampling probability and exhibits good properties of normalization, adaption,\ninteraction information awareness, and sampling efficiency; and (2) an\nintegrated sampler-recommender framework, leveraging the sampler model in\nprediction to offset the bias caused by uneven sampling. Correspondingly, we\nderive a fast reinforced training algorithm of our framework to boost the\nsampler performance and sampler-recommender collaboration. Extensive\nexperiments on four real-world datasets demonstrate the superiority of the\nproposed collaborative sampler model and integrated sampler-recommender\nframework.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.08431,regular,pre_llm,2020,11,"{'ai_likelihood': 4.526641633775499e-05, 'text': 'Association Rules Enhanced Knowledge Graph Attention Network\n\n  Most existing knowledge graphs suffer from incompleteness. Embedding\nknowledge graphs into continuous vector spaces has recently attracted\nincreasing interest in knowledge base completion. However, in most existing\nembedding methods, only fact triplets are utilized, and logical rules have not\nbeen thoroughly studied for the knowledge base completion task. To overcome the\nproblem, we propose an association rules enhanced knowledge graph attention\nnetwork (AR-KGAT). The AR-KGAT captures both entity and relation features for\nhigh-order neighborhoods of any given entity in an end-to-end manner under the\ngraph attention network framework. The major component of AR-KGAT is an encoder\nof an effective neighborhood aggregator, which addresses the problems by\naggregating neighbors with both association-rules-based and graph-based\nattention weights. Additionally, the proposed model also encapsulates the\nrepresentations from multi-hop neighbors of nodes to refine their embeddings.\nThe decoder enables AR-KGAT to be translational between entities and relations\nwhile keeping the superior link prediction performance. A logic-like inference\npattern is utilized as constraints for knowledge graph embedding. Then, the\nglobal loss is minimized over both atomic and complex formulas to achieve the\nembedding task. In this manner, we learn embeddings compatible with triplets\nand rules, which are certainly more predictive for knowledge acquisition and\ninference. We conduct extensive experiments on two benchmark datasets: WN18RR\nand FB15k-237, for two knowledge graph completion tasks: the link prediction\nand triplet classification to evaluate the proposed AR-KGAT model. The results\nshow that the proposed AR-KGAT model achieves significant and consistent\nimprovements over state-of-the-art methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.00565,regular,pre_llm,2020,11,"{'ai_likelihood': 2.8808911641438803e-06, 'text': 'CURE: Collection for Urdu Information Retrieval Evaluation and Ranking\n\n  Urdu is a widely spoken language with 163 million speakers worldwide across\nthe globe. Information Retrieval (IR) for Urdu entails special consideration of\nresearch community due to its rich morphological features and a large number of\nspeakers. In general, IR evaluation task is not extensively explored for Urdu.\nThe most important missing element is the availability of a standardized\nevaluation corpus specific to Urdu. In this research work, we propose and\nconstruct a standard test collection of Urdu documents for IR evaluation and\nnamed it Collection for Urdu Retrieval Evaluation (CURE). We select 1,096\nunique documents against 50 diverse queries from a large collection of 0.5\nmillion crawled documents using two IR models. The purpose of test collection\nis the evaluation of IR models, ranking algorithms, and different natural\nlanguage processing techniques. Next, we perform binary relevance judgment on\nthe selected documents. We also built two other language resources for\nlemmatization and query expansion specific to our test collection. Evaluation\nof test collection is carried out using four retrieval models as well using the\nstop-words list, lemmatization, and query expansion. Furthermore, error\nanalysis was performed for each query with different NLP techniques. To the\nbest of our knowledge, this work is the first attempt for preparing a\nstandardized information retrieval evaluation test collection for the Urdu\nlanguage.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.0055,regular,pre_llm,2020,11,"{'ai_likelihood': 4.404120975070529e-06, 'text': 'U-rank: Utility-oriented Learning to Rank with Implicit Feedback\n\n  Learning to rank with implicit feedback is one of the most important tasks in\nmany real-world information systems where the objective is some specific\nutility, e.g., clicks and revenue. However, we point out that existing methods\nbased on probabilistic ranking principle do not necessarily achieve the highest\nutility. To this end, we propose a novel ranking framework called U-rank that\ndirectly optimizes the expected utility of the ranking list. With a\nposition-aware deep click-through rate prediction model, we address the\nattention bias considering both query-level and item-level features. Due to the\nitem-specific attention bias modeling, the optimization for expected utility\ncorresponds to a maximum weight matching on the item-position bipartite graph.\nWe base the optimization of this objective in an efficient Lambdaloss\nframework, which is supported by both theoretical and empirical analysis. We\nconduct extensive experiments for both web search and recommender systems over\nthree benchmark datasets and two proprietary datasets, where the performance\ngain of U-rank over state-of-the-arts is demonstrated. Moreover, our proposed\nU-rank has been deployed on a large-scale commercial recommender and a large\nimprovement over the production baseline has been observed in an online A/B\ntesting.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.00479,regular,pre_llm,2020,11,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Cheap IR Evaluation: Fewer Topics, No Relevance Judgements, and\n  Crowdsourced Assessments\n\n  To evaluate Information Retrieval (IR) effectiveness, a possible approach is\nto use test collections, which are composed of a collection of documents, a set\nof description of information needs (called topics), and a set of relevant\ndocuments to each topic. Test collections are modelled in a competition\nscenario: for example, in the well known TREC initiative, participants run\ntheir own retrieval systems over a set of topics and they provide a ranked list\nof retrieved documents; some of the retrieved documents (usually the first\nranked) constitute the so called pool, and their relevance is evaluated by\nhuman assessors; the document list is then used to compute effectiveness\nmetrics and rank the participant systems. Private Web Search companies also run\ntheir in-house evaluation exercises; although the details are mostly unknown,\nand the aims are somehow different, the overall approach shares several issues\nwith the test collection approach.\n  The aim of this work is to: (i) develop and improve some state-of-the-art\nwork on the evaluation of IR effectiveness while saving resources, and (ii)\npropose a novel, more principled and engineered, overall approach to test\ncollection based effectiveness evaluation.\n  [...]\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.00422,regular,pre_llm,2020,11,"{'ai_likelihood': 1.4901161193847656e-06, 'text': ""Future-Aware Diverse Trends Framework for Recommendation\n\n  In recommender systems, modeling user-item behaviors is essential for user\nrepresentation learning. Existing sequential recommenders consider the\nsequential correlations between historically interacted items for capturing\nusers' historical preferences. However, since users' preferences are by nature\ntime-evolving and diversified, solely modeling the historical preference\n(without being aware of the time-evolving trends of preferences) can be\ninferior for recommending complementary or fresh items and thus hurt the\neffectiveness of recommender systems. In this paper, we bridge the gap between\nthe past preference and potential future preference by proposing the\nfuture-aware diverse trends (FAT) framework. By future-aware, for each\ninspected user, we construct the future sequences from other similar users,\nwhich comprise of behaviors that happen after the last behavior of the\ninspected user, based on a proposed neighbor behavior extractor. By diverse\ntrends, supposing the future preferences can be diversified, we propose the\ndiverse trends extractor and the time-aware mechanism to represent the possible\ntrends of preferences for a given user with multiple vectors. We leverage both\nthe representations of historical preference and possible future trends to\nobtain the final recommendation. The quantitative and qualitative results from\nrelatively extensive experiments on real-world datasets demonstrate the\nproposed framework not only outperforms the state-of-the-art sequential\nrecommendation methods across various metrics, but also makes complementary and\nfresh recommendations.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.12683,regular,pre_llm,2020,11,"{'ai_likelihood': 6.4240561591254345e-06, 'text': 'GraphHINGE: Learning Interaction Models of Structured Neighborhood on\n  Heterogeneous Information Network\n\n  Heterogeneous information network (HIN) has been widely used to characterize\nentities of various types and their complex relations. Recent attempts either\nrely on explicit path reachability to leverage path-based semantic relatedness\nor graph neighborhood to learn heterogeneous network representations before\npredictions. These weakly coupled manners overlook the rich interactions among\nneighbor nodes, which introduces an early summarization issue. In this paper,\nwe propose GraphHINGE (Heterogeneous INteract and aggreGatE), which captures\nand aggregates the interactive patterns between each pair of nodes through\ntheir structured neighborhoods. Specifically, we first introduce\nNeighborhood-based Interaction (NI) module to model the interactive patterns\nunder the same metapaths, and then extend it to Cross Neighborhood-based\nInteraction (CNI) module to deal with different metapaths. Next, in order to\naddress the complexity issue on large-scale networks, we formulate the\ninteraction modules via a convolutional framework and learn the parameters\nefficiently with fast Fourier transform. Furthermore, we design a novel\nneighborhood-based selection (NS) mechanism, a sampling strategy, to filter\nhigh-order neighborhood information based on their low-order performance. The\nextensive experiments on six different types of heterogeneous graphs\ndemonstrate the performance gains by comparing with state-of-the-arts in both\nclick-through rate prediction and top-N recommendation tasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.00953,regular,pre_llm,2020,11,"{'ai_likelihood': 5.0001674228244356e-06, 'text': 'Collaborative Generative Hashing for Marketing and Fast Cold-start\n  Recommendation\n\n  Cold-start has being a critical issue in recommender systems with the\nexplosion of data in e-commerce. Most existing studies proposed to alleviate\nthe cold-start problem are also known as hybrid recommender systems that learn\nrepresentations of users and items by combining user-item interactive and\nuser/item content information. However, previous hybrid methods regularly\nsuffered poor efficiency bottlenecking in online recommendations with\nlarge-scale items, because they were designed to project users and items into\ncontinuous latent space where the online recommendation is expensive. To this\nend, we propose a collaborative generated hashing (CGH) framework to improve\nthe efficiency by denoting users and items as binary codes, then fast hashing\nsearch techniques can be used to speed up the online recommendation. In\naddition, the proposed CGH can generate potential users or items for marketing\napplication where the generative network is designed with the principle of\nMinimum Description Length (MDL), which is used to learn compact and\ninformative binary codes. Extensive experiments on two public datasets show the\nadvantages for recommendations in various settings over competing baselines and\nanalyze its feasibility in marketing application.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.10173,regular,pre_llm,2020,11,"{'ai_likelihood': 1.771582497490777e-05, 'text': 'Exploring Global Information for Session-based Recommendation\n\n  Session-based recommendation (SBR) is a challenging task, which aims at\nrecommending items based on anonymous behavior sequences. Most existing SBR\nstudies model the user preferences based only on the current session while\nneglecting the item-transition information from the other sessions, which\nsuffer from the inability of modeling the complicated item-transition pattern.\nTo address the limitations, we introduce global item-transition information to\nstrength the modeling of the dynamic item-transition. For fully exploiting the\nglobal item-transition information, two ways of exploring global information\nfor SBR are studied in this work. Specifically, we first propose a basic\nGNN-based framework (BGNN), which solely uses session-level item-transition\ninformation on session graph. Based on BGNN, we propose a novel approach,\ncalled Session-based Recommendation with Global Information (SRGI), which\ninfers the user preferences via fully exploring global item-transitions over\nall sessions from two different perspectives: (i) Fusion-based Model (SRGI-FM),\nwhich recursively incorporates the neighbor embeddings of each node on global\ngraph into the learning process of session level item representation; and (ii)\nConstrained-based Model (SRGI-CM), which treats the global-level\nitem-transition information as a constraint to ensure the learned item\nembeddings are consistent with the global item-transition. Extensive\nexperiments conducted on three popular benchmark datasets demonstrate that both\nSRGI-FM and SRGI-CM outperform the state-of-the-art methods consistently.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.07734,regular,pre_llm,2020,11,"{'ai_likelihood': 3.013345930311415e-06, 'text': ""SamWalker++: recommendation with informative sampling strategy\n\n  Recommendation from implicit feedback is a highly challenging task due to the\nlack of reliable negative feedback data. Existing methods address this\nchallenge by treating all the un-observed data as negative (dislike) but\ndownweight the confidence of these data. However, this treatment causes two\nproblems: (1) Confidence weights of the unobserved data are usually assigned\nmanually, which lack flexibility and may create empirical bias on evaluating\nuser's preference. (2) To handle massive volume of the unobserved feedback\ndata, most of the existing methods rely on stochastic inference and data\nsampling strategies. However, since a user is only aware of a very small\nfraction of items in a large dataset, it is difficult for existing samplers to\nselect informative training instances in which the user really dislikes the\nitem rather than does not know it.\n  To address the above two problems, we propose two novel recommendation\nmethods SamWalker and SamWalker++ that support both adaptive confidence\nassignment and efficient model learning. SamWalker models data confidence with\na social network-aware function, which can adaptively specify different weights\nto different data according to users' social contexts. However, the social\nnetwork information may not be available in many recommender systems, which\nhinders application of SamWalker. Thus, we further propose SamWalker++, which\ndoes not require any side information and models data confidence with a\nconstructed pseudo-social network. We also develop fast random-walk-based\nsampling strategies for our SamWalker and SamWalker++ to adaptively draw\ninformative training instances, which can speed up gradient estimation and\nreduce sampling variance. Extensive experiments on five real-world datasets\ndemonstrate the superiority of the proposed SamWalker and SamWalker++.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.00056,regular,pre_llm,2020,11,"{'ai_likelihood': 2.947118547227648e-06, 'text': ""Diversifying Relevant Phrases\n\n  Diverse keyword suggestions for a given landing page or matching queries to\ndiverse documents is an active research area in online advertising. Modern\nsearch engines provide advertisers with products like Dynamic Search Ads and\nSmart Campaigns where they extract meaningful keywords/phrases from the\nadvertiser's product inventory. These keywords/phrases are representative of a\ndiverse spectrum of advertiser's interests. In this paper, we address the\nproblem of obtaining relevant yet diverse keywords/phrases for any given\ndocument. We formulate this as an optimization problem, maximizing the\nparameterized trade-off between diversity and relevance constrained over number\nof possible keywords/phrases. We show that this is a combinatorial NP-hard\noptimization problem. We propose two approaches based on convex relaxations\nvarying in complexity and performance. In the first approach, we show that the\noptimization problem reduces to an eigen value problem. In the second approach,\nwe show that the optimization problem reduces to minimizing a quadratic form\nover an l1-ball. Subsequently, we show that this is equivalent to a\nsemi-definite optimization problem. To prove the efficacy of our proposed\nformulation, we evaluate it on various real-world datasets and compare it to\nthe state-of-the-art heuristic approaches.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.01731,regular,pre_llm,2020,11,"{'ai_likelihood': 5.8081414964463976e-05, 'text': 'RecBole: Towards a Unified, Comprehensive and Efficient Framework for\n  Recommendation Algorithms\n\n  In recent years, there are a large number of recommendation algorithms\nproposed in the literature, from traditional collaborative filtering to deep\nlearning algorithms. However, the concerns about how to standardize open source\nimplementation of recommendation algorithms continually increase in the\nresearch community. In the light of this challenge, we propose a unified,\ncomprehensive and efficient recommender system library called RecBole, which\nprovides a unified framework to develop and reproduce recommendation algorithms\nfor research purpose. In this library, we implement 73 recommendation models on\n28 benchmark datasets, covering the categories of general recommendation,\nsequential recommendation, context-aware recommendation and knowledge-based\nrecommendation. We implement the RecBole library based on PyTorch, which is one\nof the most popular deep learning frameworks. Our library is featured in many\naspects, including general and extensible data structures, comprehensive\nbenchmark models and datasets, efficient GPU-accelerated execution, and\nextensive and standard evaluation protocols. We provide a series of auxiliary\nfunctions, tools, and scripts to facilitate the use of this library, such as\nautomatic parameter tuning and break-point resume. Such a framework is useful\nto standardize the implementation and evaluation of recommender systems. The\nproject and documents are released at https://recbole.io/.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.09752,regular,pre_llm,2020,11,"{'ai_likelihood': 3.907415601942274e-06, 'text': 'From Protocol to Screening: A Hybrid Learning Approach for\n  Technology-Assisted Systematic Literature Reviews\n\n  In the medical domain, a Systematic Literature Review (SLR) attempts to\ncollect all empirical evidence, that fit pre-specified eligibility criteria, in\norder to answer a specific research question. The process of preparing an SLR\nconsists of multiple tasks that are labor-intensive and time-consuming,\ninvolving large monetary costs. Technology-assisted review (TAR) methods\nautomate the different processes of creating an SLR and they are particularly\nfocused on reducing the burden of screening for reviewers. We present a novel\nmethod for TAR that implements a full pipeline from the research protocol to\nthe screening of the relevant papers. Our pipeline overcomes the need of a\nBoolean query constructed by specialists and consists of three different\ncomponents: the primary retrieval engine, the inter-review ranker and the\nintra-review ranker, combining learning-to-rank techniques with a relevance\nfeedback method. In addition, we contribute an updated version of the Task 2 of\nthe CLEF 2019 eHealth Lab dataset, which we make publicly available. Empirical\nresults on this dataset show that our approach can achieve state-of-the-art\nresults.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.05057,regular,pre_llm,2020,11,"{'ai_likelihood': 5.298190646701389e-07, 'text': 'The improved model of user similarity coefficients computation For\n  recommendation systems\n\n  The subject matter of the article is a model of calculating the user\nsimilarity coefficients of the recommendation systems. The goal is the\ndevelopment of the improved model of user similarity coefficients calculation\nfor recommendation systems to optimize the time of forming recommendation\nlists. The tasks to be solved are: to investigate the probability of changing\nuser preferences of a recommendation system by comparing their similarity\ncoefficients in time, to investigate which distribution function describes the\nchanges of similarity coefficients of users in time. The methods used are:\ngraph theory, probability theory, radioactivity theory, algorithm theory.\nConclusions. In the course of the researches, the model of user similarity\ncoefficients calculating for the recommendation systems has been improved. The\nmodel differs from the known ones in that it takes into account the\nrecalculation period of similarity coefficients for the individual user and\naverage recalculation period of similarity coefficients for all users of the\nsystem or a specific group of users. The software has been developed, in which\na series of experiments was conducted to test the effectiveness of the\ndeveloped method. The conducted experiments showed that the developed method in\ngeneral increases the quality of the recommendation system without significant\nfluctuations of Precision and Recall of the system. Precision and Recall can\ndecrease slightly or increase, depending on the characteristics of the incoming\ndata set. The use of the proposed solutions will increase the application\nperiod of the previously calculated similarity coefficients of users for the\nprediction of preferences without their recalculation and, accordingly, it will\nshorten the time of formation and issuance of recommendation lists up to 2\ntimes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.0965,regular,pre_llm,2020,12,"{'ai_likelihood': 1.6656186845567492e-05, 'text': 'A White Box Analysis of ColBERT\n\n  Transformer-based models are nowadays state-of-the-art in ad-hoc Information\nRetrieval, but their behavior is far from being understood. Recent work has\nclaimed that BERT does not satisfy the classical IR axioms. However, we propose\nto dissect the matching process of ColBERT, through the analysis of term\nimportance and exact/soft matching patterns. Even if the traditional axioms are\nnot formally verified, our analysis reveals that ColBERT: (i) is able to\ncapture a notion of term importance; (ii) relies on exact matches for important\nterms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.10185,review,pre_llm,2020,12,"{'ai_likelihood': 3.1458006964789497e-06, 'text': ""Recommenders with a mission: assessing diversity in newsrecommendations\n\n  News recommenders help users to find relevant online content and have the\npotential to fulfill a crucial role in a democratic society, directing the\nscarce attention of citizens towards the information that is most important to\nthem. Simultaneously, recent concerns about so-called filter bubbles,\nmisinformation and selective exposure are symptomatic of the disruptive\npotential of these digital news recommenders. Recommender systems can make or\nbreak filter bubbles, and as such can be instrumental in creating either a more\nclosed or a more open internet. Current approaches to evaluating recommender\nsystems are often focused on measuring an increase in user clicks and\nshort-term engagement, rather than measuring the user's longer term interest in\ndiverse and important information.\n  This paper aims to bridge the gap between normative notions of diversity,\nrooted in democratic theory, and quantitative metrics necessary for evaluating\nthe recommender system. We propose a set of metrics grounded in social science\ninterpretations of diversity and suggest ways for practical implementations.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.14803,regular,pre_llm,2020,12,"{'ai_likelihood': 1.0596381293402778e-06, 'text': ""Supporting Human Memory by Reconstructing Personal Episodic Narratives\n  from Digital Traces\n\n  Numerous applications capture in digital form aspects of people's lives. The\nresulting data, which we call Personal Digital Traces - PDTs, can be used to\nhelp reconstruct people's episodic memories and connect to their past personal\nevents. This reconstruction has several applications, from helping patients\nwith neurodegenerative diseases recall past events to gathering clues from\nmultiple sources to identify recent contacts and places visited - a critical\nnew application for the current health crisis. This paper takes steps towards\nintegrating, connecting and summarizing the heterogeneous collection of data\ninto episodic narratives using scripts - prototypical plans for everyday\nactivities. Specifically, we propose a matching algorithm that groups several\ndigital traces from many different sources into script instances (episodes),\nand we provide a technique for ranking the likelihood of candidate episodes. We\nreport on the results of a study based on the personal data of real users,\nwhich gives evidence that our episode reconstruction technique 1) successfully\nintegrates and combines traces from different sources into coherent episodes,\nand 2) augments users' memory of their past actions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.08787,regular,pre_llm,2020,12,"{'ai_likelihood': 2.384185791015625e-06, 'text': ""Query expansion with artificially generated texts\n\n  A well-known way to improve the performance of document retrieval is to\nexpand the user's query. Several approaches have been proposed in the\nliterature, and some of them are considered as yielding state-of-the-art\nresults in IR. In this paper, we explore the use of text generation to\nautomatically expand the queries. We rely on a well-known neural generative\nmodel, GPT-2, that comes with pre-trained models for English but can also be\nfine-tuned on specific corpora. Through different experiments, we show that\ntext generation is a very effective way to improve the performance of an IR\nsystem, with a large margin (+10% MAP gains), and that it outperforms strong\nbaselines also relying on query expansion (LM+RM3). This conceptually simple\napproach can easily be implemented on any IR system thanks to the availability\nof GPT code and models.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.09263,regular,pre_llm,2020,12,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Checking Fact Worthiness using Sentence Embeddings\n\n  Checking and confirming factual information in texts and speeches is vital to\ndetermine the veracity and correctness of the factual statements. This work was\npreviously done by journalists and other manual means but it is a\ntime-consuming task. With the advancements in Information Retrieval and NLP,\nresearch in the area of Fact-checking is getting attention for automating it.\nCLEF-2018 and 2019 organised tasks related to Fact-checking and invited\nparticipants. This project focuses on CLEF-2019 Task-1 Check-Worthiness and\nexperiments using the latest Sentence-BERT pre-trained embeddings, topic\nModeling and sentiment score are performed. Evaluation metrics such as MAP,\nMean Reciprocal Rank, Mean R-Precision and Mean Precision@N present the\nimprovement in the results using the techniques.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.15518,regular,pre_llm,2020,12,"{'ai_likelihood': 1.2583202785915798e-06, 'text': ""Design Knowledge Representation with Technology Semantic Network\n\n  Engineers often need to discover and learn designs from unfamiliar domains\nfor inspiration or other particular uses. However, the complexity of the\ntechnical design descriptions and the unfamiliarity to the domain make it hard\nfor engineers to comprehend the function, behavior, and structure of a design.\nTo help engineers quickly understand a complex technical design description new\nto them, one approach is to represent it as a network graph of the\ndesign-related entities and their relations as an abstract summary of the\ndesign. While graph or network visualizations are widely adopted in the\nengineering design literature, the challenge remains in retrieving the design\nentities and deriving their relations. In this paper, we propose a network\nmapping method that is powered by Technology Semantic Network (TechNet).\nThrough a case study, we showcase how TechNet's unique characteristic of being\ntrained on a large technology-related data source advantages itself over\ncommon-sense knowledge bases, such as WordNet and ConceptNet, for design\nknowledge representation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.11336,regular,pre_llm,2020,12,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'CODE: Contrastive Pre-training with Adversarial Fine-tuning for\n  Zero-shot Expert Linking\n\n  Expert finding, a popular service provided by many online websites such as\nExpertise Finder, LinkedIn, and AMiner, is beneficial to seeking candidate\nqualifications, consultants, and collaborators. However, its quality is\nsuffered from lack of ample sources of expert information. This paper employs\nAMiner as the basis with an aim at linking any external experts to the\ncounterparts on AMiner. As it is infeasible to acquire sufficient linkages from\narbitrary external sources, we explore the problem of zero-shot expert linking.\nIn this paper, we propose CODE, which first pre-trains an expert linking model\nby contrastive learning on AMiner such that it can capture the representation\nand matching patterns of experts without supervised signals, then it is\nfine-tuned between AMiner and external sources to enhance the models\ntransferability in an adversarial manner. For evaluation, we first design two\nintrinsic tasks, author identification and paper clustering, to validate the\nrepresentation and matching capability endowed by contrastive learning. Then\nthe final external expert linking performance on two genres of external sources\nalso implies the superiority of the adversarial fine-tuning method.\nAdditionally, we show the online deployment of CODE, and continuously improve\nits online performance via active learning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.03069,regular,pre_llm,2020,12,"{'ai_likelihood': 3.642506069607205e-06, 'text': 'Aligning geographic entities from historical maps for building knowledge\n  graphs\n\n  Historical maps contain rich geographic information about the past of a\nregion. They are sometimes the only source of information before the\navailability of digital maps. Despite their valuable content, it is often\nchallenging to access and use the information in historical maps, due to their\nforms of paper-based maps or scanned images. It is even more time-consuming and\nlabor-intensive to conduct an analysis that requires a synthesis of the\ninformation from multiple historical maps. To facilitate the use of the\ngeographic information contained in historical maps, one way is to build a\ngeographic knowledge graph (GKG) from them. This paper proposes a general\nworkflow for completing one important step of building such a GKG, namely\naligning the same geographic entities from different maps. We present this\nworkflow and the related methods for implementation, and systematically\nevaluate their performances using two different datasets of historical maps.\nThe evaluation results show that machine learning and deep learning models for\nmatching place names are sensitive to the thresholds learned from the training\ndata, and a combination of measures based on string similarity, spatial\ndistance, and approximate topological relation achieves the best performance\nwith an average F-score of 0.89.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.07064,regular,pre_llm,2020,12,"{'ai_likelihood': 7.384353213840061e-06, 'text': ""Pre-Training Graph Neural Networks for Cold-Start Users and Items\n  Representation\n\n  Cold-start problem is a fundamental challenge for recommendation tasks.\nDespite the recent advances on Graph Neural Networks (GNNs) incorporate the\nhigh-order collaborative signal to alleviate the problem, the embeddings of the\ncold-start users and items aren't explicitly optimized, and the cold-start\nneighbors are not dealt with during the graph convolution in GNNs. This paper\nproposes to pre-train a GNN model before applying it for recommendation. Unlike\nthe goal of recommendation, the pre-training GNN simulates the cold-start\nscenarios from the users/items with sufficient interactions and takes the\nembedding reconstruction as the pretext task, such that it can directly improve\nthe embedding quality and can be easily adapted to the new cold-start\nusers/items. To further reduce the impact from the cold-start neighbors, we\nincorporate a self-attention-based meta aggregator to enhance the aggregation\nability of each graph convolution step, and an adaptive neighbor sampler to\nselect the effective neighbors according to the feedbacks from the pre-training\nGNN model. Experiments on three public recommendation datasets show the\nsuperiority of our pre-training GNN model against the original GNN models on\nuser/item embedding inference and the recommendation task.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.01141,regular,pre_llm,2020,12,"{'ai_likelihood': 6.059805552164714e-06, 'text': 'Presenting a Dataset for Collaborator Recommending Systems in Academic\n  Social Network: a Case Study on ReseachGate\n\n  Collaborator finding systems are a special type of expert finding models.\nThere is a long-lasting challenge for research in the collaborator recommending\nresearch area, which is the lack of the structured dataset to be used by the\nresearchers. We introduce two datasets to fill this gap. The first dataset is\nprepared for designing a consistent, collaborator finding system. The next one,\ncalled a co-author finding model, models an academic social network as a table\nthat contains different relations between the pair of users. Both of them\nprovide an opportunity for introducing potential collaborators to each other.\nThese two models have been extracted from ResearchGate (RG) data set and are\navailable publicly. RG dataset has been collected from Jan. 2019 to April 2019\nand includes raw data of 3980 RG users. The dataset consists of almost complete\ninformation about users. In the preprocessing phase, the well-known Elmo was\nused for analyzing textual data. We call this as ResearchGate dataset for\nRecommending Systems (RGRS). For assessing the validity of data, we analyze\neach layer of data separately, and the results are reported. After preparing\ndata and evaluating the collaborator finding models, we have done some\nassessments on RGRS. Some of these assessments are co-author,\nfollowing-follower, and question answering relations. The outcomes indicate\nthat it is the best relation in propagating knowledge in the network. To the\nbest of our knowledge, there is no processed and analyzed dataset of this size.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.06209,regular,pre_llm,2020,12,"{'ai_likelihood': 7.318125830756294e-06, 'text': ""KOSMOS: Knowledge-graph Oriented Social media and Mainstream media\n  Overview System\n\n  We introduce KOSMOS, a knowledge retrieval system based on the constructed\nknowledge graph of social media and mainstream media documents. The system\nfirst identifies key events from the documents at each time frame through\nclustering, extracting a document to represent each cluster, then describing\nthe document in terms of 5W1H (Who, What, When, Where, Why, How). The event\ncentric knowledge graph is enhanced by relation triplets and entity\ndisambiguation from the representative document. This knowledge retrieval is\nsupported by a web interface that presents a graph visualisation of related\nnodes and relevant articles based on a user query. The interface facilitates\nunderstanding relationships between events reported in mainstream and social\nmedia journalism through the KOSMOS information extraction pipeline, which is\nvaluable to understand media slant and public opinions. Finally, we explore a\nuse case in extracting events and relations from documents to understand the\nmedia and community's view to the 2020 COVID19 pandemic.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.12522,regular,pre_llm,2020,12,"{'ai_likelihood': 2.053048875596788e-06, 'text': 'Design and Implementation of Curriculum System Based on Knowledge Graph\n\n  With the fact that the knowledge in each field in university is keeping\nincreasing, the number of university courses is becoming larger, and the\ncontent and curriculum system is becoming much more complicated than it used to\nbe, which bring many inconveniences to the course arrangement and analysis. In\nthis paper, we aim to construct a method to visualize all courses based on\nGoogle Knowledge Graph. By analysing the properties of the courses and their\npreceding requirements, we want to extract the relationship between the\nprecursors and the successors, so as to build the knowledge graph of the\ncurriculum system. Using the graph database Neo4j [7] as the core aspect for\ndata storage and display for our new curriculum system will be our approach to\nimplement our knowledge graph. Based on this graph, the venation relationship\nbetween courses can be clearly analysed, and some difficult information can be\nobtained, which can help to combine the outline of courses and the need to\nquickly query the venation information of courses.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.02629,regular,pre_llm,2020,12,"{'ai_likelihood': 0.0003017319573296441, 'text': ""Linear Regression Evaluation of Search Engine Automatic Search\n  Performance Based on Hadoop and R\n\n  The automatic search performance of search engines has become an essential\npart of measuring the difference in user experience. An efficient automatic\nsearch system can significantly improve the performance of search engines and\nincrease user traffic. Hadoop has strong data integration and analysis\ncapabilities, while R has excellent statistical capabilities in linear\nregression. This article will propose a linear regression based on Hadoop and R\nto quantify the efficiency of the automatic retrieval system. We use R's\nfunctional properties to transform the user's search results upon linear\ncorrelations. In this way, the final output results have multiple display forms\ninstead of web page preview interfaces. This article provides feasible\nsolutions to the drawbacks of current search engine algorithms lacking once or\ntwice search accuracies and multiple types of search results. We can conduct\npersonalized regression analysis for user's needs with public datasets and\noptimize resources integration for most relevant information.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.12498,regular,pre_llm,2020,12,"{'ai_likelihood': 3.8080745273166235e-06, 'text': 'Fake News Data Collection and Classification: Iterative Query Selection\n  for Opaque Search Engines with Pseudo Relevance Feedback\n\n  Retrieving information from an online search engine, is the first and most\nimportant step in many data mining tasks. Most of the search engines currently\navailable on the web, including all social media platforms, are black-boxes\n(a.k.a opaque) supporting short keyword queries. In these settings, retrieving\nall posts and comments discussing a particular news item automatically and at\nlarge scales is a challenging task. In this paper, we propose a method for\ngenerating short keyword queries given a prototype document. The proposed\niterative query selection algorithm (IQS) interacts with the opaque search\nengine to iteratively improve the query. It is evaluated on the Twitter TREC\nMicroblog 2012 and TREC-COVID 2019 datasets showing superior performance\ncompared to state-of-the-art. IQS is applied to automatically collect a\nlarge-scale fake news dataset of about 70K true and fake news items. The\ndataset, publicly available for research, includes more than 22M accounts and\n61M tweets in Twitter approved format. We demonstrate the usefulness of the\ndataset for fake news detection task achieving state-of-the-art performance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.08793,regular,pre_llm,2020,12,"{'ai_likelihood': 5.298190646701389e-07, 'text': 'Session-based k-NNs with Semantic Suggestions for Next-item Prediction\n\n  One of the most critical problems in e-commerce domain is the information\noverload problem. Usually, an enormous number of products is offered to a user.\nThe characteristics of this domain force researchers to opt for session-based\nrecommendation methods, from which nearest-neighbors-based (SkNN) approaches\nhave been shown to be competitive with and even outperform neural network-based\nmodels. Existing SkNN approaches, however, lack the ability to detect sudden\ninterest changes at a micro-level, i.e., during an individual session; and to\nadapt their recommendations to these changes. In this paper, we propose a\nconceptual (cSkNN) model extension for the next-item prediction allowing better\nadaptation to the interest changes via the semantic-level properties. We use an\nNLP technique to parse salient concepts from the product titles to create\nlinguistically based product generalizations that are used for change detection\nand a recommendation list post-filtering. We conducted experiments with two\nversions of our extension that differ in semantics derivation procedure while\nboth showing an improvement over the existing SkNN method on a sparse fashion\ne-commerce dataset.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.11876,regular,pre_llm,2020,12,"{'ai_likelihood': 2.814663781060113e-06, 'text': ""Intelligent Vector-based Customer Segmentation in the Banking Industry\n\n  Customer Segmentation is the process of dividing customers into groups based\non common characteristics. An intelligent Customer Segmentation will not only\nenable an organization to effectively allocate marketing resources (e.g.,\nRecommender Systems in the Banking sector) but also it will enable identifying\nthe customer cohorts that are most likely to benefit from a specific policy\n(e.g., to discover diverse patient groups in the Health sector). While there\nhas been a significant improvement in approaches to Customer Segmentation, the\nmain challenge remains to be the understanding of the reasons behind the\nsegmentation need. This task is challenging as it is subjective and depends on\nthe goal of segmentation as well as the analyst's perspective. To address this\nchallenge, in this paper, we present an intelligent vector-based customer\nsegmentation approach. The proposed approach will leverage feature engineering\nto enable analysts to identify important features (from a pool of features such\nas demographics, geography, psychographics, behavioral, and more) and feed them\ninto a neural embedding framework named Customer2Vec. The Customer2Vec combines\nthe neural network classification and clustering methods as supervised and\nunsupervised learning techniques to embed the customer vector. We adopt a\ntypical scenario in the Banking Sector to highlight how Customer2Vec\nsignificantly improves the quality of the segmentation and detecting customer\nsimilarities.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.08907,regular,pre_llm,2020,12,"{'ai_likelihood': 4.635916815863716e-07, 'text': 'Information retrieval system for silte language using BM25 weighting\n\n  The main aim of an information retrieval system is to extract appropriate\ninformation from an enormous collection of data based on users need. The basic\nconcept of the information retrieval system is that when a user sends out a\nquery, the system would try to generate a list of related documents ranked in\norder, according to their degree of relevance. Digital unstructured Silte text\ndocuments increase from time to time. The growth of digital text information\nmakes the utilization and access of the right information difficult. Thus,\ndeveloping an information retrieval system for Silte language allows searching\nand retrieving relevant documents that satisfy information need of users. In\nthis research, we design probabilistic information retrieval system for Silte\nlanguage. The system has both indexing and searching part was created. In these\nmodules, different text operations such as tokenization, stemming, stop word\nremoval and synonym is included.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.13114,regular,pre_llm,2020,12,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'A Frequency-Based Learning-To-Rank Approach for Personal Digital Traces\n\n  Personal digital traces are constantly produced by connected devices,\ninternet services and interactions. These digital traces are typically small,\nheterogeneous and stored in various locations in the cloud or on local devices,\nmaking it a challenge for users to interact with and search their own data. By\nadopting a multidimensional data model based on the six natural questions --\nwhat, when, where, who, why and how -- to represent and unify heterogeneous\npersonal digital traces, we can propose a learning-to-rank approach using the\nstate of the art LambdaMART algorithm and frequency-based features that\nleverage the correlation between content (what), users (who), time (when),\nlocation (where) and data source (how) to improve the accuracy of search\nresults. Due to the lack of publicly available personal training data, a\ncombination of known-item query generation techniques and an unsupervised\nranking model (field-based BM25) is used to build our own training sets.\nExperiments performed over a publicly available email collection and a personal\ndigital data trace collection from a real user show that the frequency-based\nlearning approach improves search accuracy when compared with traditional\nsearch tools.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.15151,regular,pre_llm,2020,12,"{'ai_likelihood': 8.940696716308594e-07, 'text': 'Per-Instance Algorithm Selection for Recommender Systems via Instance\n  Clustering\n\n  Recommendation algorithms perform differently if the users, recommendation\ncontexts, applications, and user interfaces vary even slightly. It is similarly\nobserved in other fields, such as combinatorial problem solving, that\nalgorithms perform differently for each instance presented. In those fields,\nmeta-learning is successfully used to predict an optimal algorithm for each\ninstance, to improve overall system performance. Per-instance algorithm\nselection has thus far been unsuccessful for recommender systems. In this paper\nwe propose a per-instance meta-learner that clusters data instances and\npredicts the best algorithm for unseen instances according to cluster\nmembership. We test our approach using 10 collaborative- and 4 content-based\nfiltering algorithms, for varying clustering parameters, and find a significant\nimprovement over the best performing base algorithm at alpha=0.053 (MAE: 0.7107\nvs LightGBM 0.7214; t-test). We also explore the performances of our base\nalgorithms on a ratings dataset and empirically show that the error of a\nperfect algorithm selector monotonically decreases for larger pools of\nalgorithm. To the best of our knowledge, this is the first effective\nmeta-learning technique for per-instance algorithm selection in recommender\nsystems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.06576,review,pre_llm,2020,12,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'Learning from User Interactions with Rankings: A Unification of the\n  Field\n\n  Ranking systems form the basis for online search engines and recommendation\nservices. They process large collections of items, for instance web pages or\ne-commerce products, and present the user with a small ordered selection. The\ngoal of a ranking system is to help a user find the items they are looking for\nwith the least amount of effort. Thus the rankings they produce should place\nthe most relevant or preferred items at the top of the ranking. Learning to\nrank is a field within machine learning that covers methods which optimize\nranking systems w.r.t. this goal. Traditional supervised learning to rank\nmethods utilize expert-judgements to evaluate and learn, however, in many\nsituations such judgements are impossible or infeasible to obtain. As a\nsolution, methods have been introduced that perform learning to rank based on\nuser clicks instead. The difficulty with clicks is that they are not only\naffected by user preferences, but also by what rankings were displayed.\nTherefore, these methods have to prevent being biased by other factors than\nuser preference. This thesis concerns learning to rank methods based on user\nclicks and specifically aims to unify the different families of these methods.\n  As a whole, the second part of this thesis proposes a framework that bridges\nmany gaps between areas of online, counterfactual, and supervised learning to\nrank. It has taken approaches, previously considered independent, and unified\nthem into a single methodology for widely applicable and effective learning to\nrank from user clicks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.01317,regular,pre_llm,2021,1,"{'ai_likelihood': 2.8974480099148222e-05, 'text': ""Contrastive Learning for Recommender System\n\n  Recommender systems, which analyze users' preference patterns to suggest\npotential targets, are indispensable in today's society. Collaborative\nFiltering (CF) is the most popular recommendation model. Specifically, Graph\nNeural Network (GNN) has become a new state-of-the-art for CF. In the GNN-based\nrecommender system, message dropout is usually used to alleviate the selection\nbias in the user-item bipartite graph. However, message dropout might\ndeteriorate the recommender system's performance due to the randomness of\ndropping out the outgoing messages based on the user-item bipartite graph. To\nsolve this problem, we propose a graph contrastive learning module for a\ngeneral recommender system that learns the embeddings in a self-supervised\nmanner and reduces the randomness of message dropout. Besides, many recommender\nsystems optimize models with pairwise ranking objectives, such as the Bayesian\nPairwise Ranking (BPR) based on a negative sampling strategy. However, BPR has\nthe following problems: suboptimal sampling and sample bias. We introduce a new\ndebiased contrastive loss to solve these problems, which provides sufficient\nnegative samples and applies a bias correction probability to alleviate the\nsample bias. We integrate the proposed framework, including graph contrastive\nmodule and debiased contrastive module with several Matrix Factorization(MF)\nand GNN-based recommendation models. Experimental results on three public\nbenchmarks demonstrate the effectiveness of our framework.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.04328,regular,pre_llm,2021,1,"{'ai_likelihood': 2.7020772298177086e-05, 'text': 'Neural News Recommendation with Negative Feedback\n\n  News recommendation is important for online news services. Precise user\ninterest modeling is critical for personalized news recommendation. Existing\nnews recommendation methods usually rely on the implicit feedback of users like\nnews clicks to model user interest. However, news click may not necessarily\nreflect user interests because users may click a news due to the attraction of\nits title but feel disappointed at its content. The dwell time of news reading\nis an important clue for user interest modeling, since short reading dwell time\nusually indicates low and even negative interest. Thus, incorporating the\nnegative feedback inferred from the dwell time of news reading can improve the\nquality of user modeling. In this paper, we propose a neural news\nrecommendation approach which can incorporate the implicit negative user\nfeedback. We propose to distinguish positive and negative news clicks according\nto their reading dwell time, and respectively learn user representations from\npositive and negative news clicks via a combination of Transformer and additive\nattention network. In addition, we propose to compute a positive click score\nand a negative click score based on the relevance between candidate news\nrepresentations and the user representations learned from the positive and\nnegative news clicks. The final click score is a combination of positive and\nnegative click scores. Besides, we propose an interactive news modeling method\nto consider the relatedness between title and body in news modeling. Extensive\nexperiments on real-world dataset validate that our approach can achieve more\naccurate user interest modeling for news recommendation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.07481,regular,pre_llm,2021,1,"{'ai_likelihood': 3.1789143880208335e-06, 'text': 'Density-Ratio Based Personalised Ranking from Implicit Feedback\n\n  Learning from implicit user feedback is challenging as we can only observe\npositive samples but never access negative ones. Most conventional methods cope\nwith this issue by adopting a pairwise ranking approach with negative sampling.\nHowever, the pairwise ranking approach has a severe disadvantage in the\nconvergence time owing to the quadratically increasing computational cost with\nrespect to the sample size; it is problematic, particularly for large-scale\ndatasets and complex models such as neural networks. By contrast, a pointwise\napproach does not directly solve a ranking problem, and is therefore inferior\nto a pairwise counterpart in top-K ranking tasks; however, it is generally\nadvantageous in regards to the convergence time. This study aims to establish\nan approach to learn personalised ranking from implicit feedback, which\nreconciles the training efficiency of the pointwise approach and ranking\neffectiveness of the pairwise counterpart. The key idea is to estimate the\nranking of items in a pointwise manner; we first reformulate the conventional\npointwise approach based on density ratio estimation and then incorporate the\nessence of ranking-oriented approaches (e.g. the pairwise approach) into our\nformulation. Through experiments on three real-world datasets, we demonstrate\nthat our approach not only dramatically reduces the convergence time (one to\ntwo orders of magnitude faster) but also significantly improving the ranking\nperformance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.11916,regular,pre_llm,2021,1,"{'ai_likelihood': 2.2186173333062066e-06, 'text': ""Perceptions of Diversity in Electronic Music: the Impact of Listener,\n  Artist, and Track Characteristics\n\n  Shared practices to assess the diversity of retrieval system results are\nstill debated in the Information Retrieval community, partly because of the\nchallenges of determining what diversity means in specific scenarios, and of\nunderstanding how diversity is perceived by end-users. The field of Music\nInformation Retrieval is not exempt from this issue. Even if fields such as\nMusicology or Sociology of Music have a long tradition in questioning the\nrepresentation and the impact of diversity in cultural environments, such\nknowledge has not been yet embedded into the design and development of music\ntechnologies. In this paper, focusing on electronic music, we investigate the\ncharacteristics of listeners, artists, and tracks that are influential in the\nperception of diversity. Specifically, we center our attention on 1)\nunderstanding the relationship between perceived diversity and computational\nmethods to measure diversity, and 2) analyzing how listeners' domain knowledge\nand familiarity influence such perceived diversity. To accomplish this, we\ndesign a user-study in which listeners are asked to compare pairs of lists of\ntracks and artists, and to select the most diverse list from each pair. We\ncompare participants' ratings with results obtained through computational\nmodels built using audio tracks' features and artist attributes. We find that\nsuch models are generally aligned with participants' choices when most of them\nagree that one list is more diverse than the other, while they present a mixed\nbehaviour in cases where participants have little agreement. Moreover, we\nobserve how differences in domain knowledge, familiarity, and demographics can\ninfluence the level of agreement among listeners, and between listeners and\ndiversity metrics computed automatically.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.10912,regular,pre_llm,2021,1,"{'ai_likelihood': 4.635916815863716e-07, 'text': 'Cloud-based traffic data fusion for situation evaluation of handover\n  scenarios\n\n  Upcoming vehicles introduce functions at the level of conditional automation\nwhere a driver no longer must supervise the system but must be able to take\nover the driving function when the system request it. This leads to the\nsituation that the driver does not concentrate on the road but is reading mails\nfor example. In this case, the driver is not able to take over the driving\nfunction immediately because she must first orient herself in the current\ntraffic situation. In an urban scenario a situation that an automated vehicle\nis not able to steer further can arise quickly. To find suitable handover\nsituations, data from traffic infrastructure systems, vehicles, and drivers is\nfused in a cloud-based situation to provide the hole traffic environment as\nbase for the decision when the driving function should be transferred best and\npossibly even before a critical situation arises\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.03654,regular,pre_llm,2021,1,"{'ai_likelihood': 5.099508497450087e-06, 'text': 'Disentangled Self-Attentive Neural Networks for Click-Through Rate\n  Prediction\n\n  Click-Through Rate (CTR) prediction, whose aim is to predict the probability\nof whether a user will click on an item, is an essential task for many online\napplications. Due to the nature of data sparsity and high dimensionality of CTR\nprediction, a key to making effective prediction is to model high-order feature\ninteraction. An efficient way to do this is to perform inner product of feature\nembeddings with self-attentive neural networks. To better model complex feature\ninteraction, in this paper we propose a novel DisentanglEd Self-atTentIve\nNEtwork (DESTINE) framework for CTR prediction that explicitly decouples the\ncomputation of unary feature importance from pairwise interaction.\nSpecifically, the unary term models the general importance of one feature on\nall other features, whereas the pairwise interaction term contributes to\nlearning the pure impact for each feature pair. We conduct extensive\nexperiments using two real-world benchmark datasets. The results show that\nDESTINE not only maintains computational efficiency but achieves consistent\nimprovements over state-of-the-art baselines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.06327,regular,pre_llm,2021,1,"{'ai_likelihood': 1.4238887363009983e-06, 'text': ""Controlling the Risk of Conversational Search via Reinforcement Learning\n\n  Users often formulate their search queries with immature language without\nwell-developed keywords and complete structures. Such queries fail to express\ntheir true information needs and raise ambiguity as fragmental language often\nyield various interpretations and aspects. This gives search engines a hard\ntime processing and understanding the query, and eventually leads to\nunsatisfactory retrieval results. An alternative approach to direct answer\nwhile facing an ambiguous query is to proactively ask clarifying questions to\nthe user. Recent years have seen many works and shared tasks from both NLP and\nIR community about identifying the need for asking clarifying question and\nmethodology to generate them. An often neglected fact by these works is that\nalthough sometimes the need for clarifying questions is correctly recognized,\nthe clarifying questions these system generate are still off-topic and\ndissatisfaction provoking to users and may just cause users to leave the\nconversation.\n  In this work, we propose a risk-aware conversational search agent model to\nbalance the risk of answering user's query and asking clarifying questions. The\nagent is fully aware that asking clarifying questions can potentially collect\nmore information from user, but it will compare all the choices it has and\nevaluate the risks. Only after then, it will make decision between answering or\nasking. To demonstrate that our system is able to retrieve better answers, we\nconduct experiments on the MSDialog dataset which contains real-world customer\nservice conversations from Microsoft products community. We also purpose a\nreinforcement learning strategy which allows us to train our model on the\noriginal dataset directly and saves us from any further data annotation\nefforts. Our experiment results show that our risk-aware conversational search\nagent is able to significantly outperform strong non-risk-aware baselines.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.12549,regular,pre_llm,2021,1,"{'ai_likelihood': 1.953707800971137e-06, 'text': ""Graph Embedding for Recommendation against Attribute Inference Attacks\n\n  In recent years, recommender systems play a pivotal role in helping users\nidentify the most suitable items that satisfy personal preferences. As\nuser-item interactions can be naturally modelled as graph-structured data,\nvariants of graph convolutional networks (GCNs) have become a well-established\nbuilding block in the latest recommenders. Due to the wide utilization of\nsensitive user profile data, existing recommendation paradigms are likely to\nexpose users to the threat of privacy breach, and GCN-based recommenders are no\nexception. Apart from the leakage of raw user data, the fragility of current\nrecommenders under inference attacks offers malicious attackers a backdoor to\nestimate users' private attributes via their behavioral footprints and the\nrecommendation results. However, little attention has been paid to developing\nrecommender systems that can defend such attribute inference attacks, and\nexisting works achieve attack resistance by either sacrificing considerable\nrecommendation accuracy or only covering specific attack models or protected\ninformation. In our paper, we propose GERAI, a novel differentially private\ngraph convolutional network to address such limitations. Specifically, in\nGERAI, we bind the information perturbation mechanism in differential privacy\nwith the recommendation capability of graph convolutional networks.\nFurthermore, based on local differential privacy and functional mechanism, we\ninnovatively devise a dual-stage encryption paradigm to simultaneously enforce\nprivacy guarantee on users' sensitive features and the model optimization\nprocess. Extensive experiments show the superiority of GERAI in terms of its\nresistance to attribute inference attacks and recommendation effectiveness.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.00166,regular,pre_llm,2021,1,"{'ai_likelihood': 3.675619761149089e-06, 'text': 'OpenMatch: An Open Source Library for Neu-IR Research\n\n  OpenMatch is a Python-based library that serves for Neural Information\nRetrieval (Neu-IR) research. It provides self-contained neural and traditional\nIR modules, making it easy to build customized and higher-capacity IR systems.\nIn order to develop the advantages of Neu-IR models for users, OpenMatch\nprovides implementations of recent neural IR models, complicated experiment\ninstructions, and advanced few-shot training methods. OpenMatch reproduces\ncorresponding ranking results of previous work on widely-used IR benchmarks,\nliberating users from surplus labor in baseline reimplementation. Our\nOpenMatch-based solutions conduct top-ranked empirical results on various\nranking tasks, such as ad hoc retrieval and conversational retrieval,\nillustrating the convenience of OpenMatch to facilitate building an effective\nIR system. The library, experimental methodologies and results of OpenMatch are\nall publicly available at https://github.com/thunlp/OpenMatch.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.08595,regular,pre_llm,2021,1,"{'ai_likelihood': 1.4768706427680123e-05, 'text': 'Fast Clustering of Short Text Streams Using Efficient Cluster Indexing\n  and Dynamic Similarity Thresholds\n\n  Short text stream clustering is an important but challenging task since\nmassive amount of text is generated from different sources such as\nmicro-blogging, question-answering, and social news aggregation websites. One\nof the major challenges of clustering such massive amount of text is to cluster\nthem within a reasonable amount of time. The existing state-of-the-art short\ntext stream clustering methods can not cluster such massive amount of text\nwithin a reasonable amount of time as they compute similarities between a text\nand all the existing clusters to assign that text to a cluster. To overcome\nthis challenge, we propose a fast short text stream clustering method (called\nFastStream) that efficiently index the clusters using inverted index and\ncompute similarity between a text and a selected number of clusters while\nassigning a text to a cluster. In this way, we not only reduce the running time\nof our proposed method but also reduce the running time of several\nstate-of-the-art short text stream clustering methods. FastStream assigns a\ntext to a cluster (new or existing) using the dynamically computed similarity\nthresholds based on statistical measure. Thus our method efficiently deals with\nthe concept drift problem. Experimental results demonstrate that FastStream\noutperforms the state-of-the-art short text stream clustering methods by a\nsignificant margin on several short text datasets. In addition, the running\ntime of FastStream is several orders of magnitude faster than that of the\nstate-of-the-art methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.02051,regular,pre_llm,2021,1,"{'ai_likelihood': 2.2517310248480903e-06, 'text': ""Transformer-based approach towards music emotion recognition from lyrics\n\n  The task of identifying emotions from a given music track has been an active\npursuit in the Music Information Retrieval (MIR) community for years. Music\nemotion recognition has typically relied on acoustic features, social tags, and\nother metadata to identify and classify music emotions. The role of lyrics in\nmusic emotion recognition remains under-appreciated in spite of several studies\nreporting superior performance of music emotion classifiers based on features\nextracted from lyrics. In this study, we use the transformer-based approach\nmodel using XLNet as the base architecture which, till date, has not been used\nto identify emotional connotations of music based on lyrics. Our proposed\napproach outperforms existing methods for multiple datasets. We used a robust\nmethodology to enhance web-crawlers' accuracy for extracting lyrics. This study\nhas important implications in improving applications involved in playlist\ngeneration of music based on emotions in addition to improving music\nrecommendation systems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.06387,regular,pre_llm,2021,1,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'A Zero Attentive Relevance Matching Networkfor Review Modeling in\n  Recommendation System\n\n  User and item reviews are valuable for the construction of recommender\nsystems. In general, existing review-based methods for recommendation can be\nbroadly categorized into two groups: the siamese models that build static user\nand item representations from their reviews respectively, and the\ninteraction-based models that encode user and item dynamically according to the\nsimilarity or relationships of their reviews. Although the interaction-based\nmodels have more model capacity and fit human purchasing behavior better,\nseveral problematic model designs and assumptions of the existing\ninteraction-based models lead to its suboptimal performance compared to\nexisting siamese models. In this paper, we identify three problems of the\nexisting interaction-based recommendation models and propose a couple of\nsolutions as well as a new interaction-based model to incorporate review data\nfor rating prediction. Our model implements a relevance matching model with\nregularized training losses to discover user relevant information from long\nitem reviews, and it also adapts a zero attention strategy to dynamically\nbalance the item-dependent and item-independent information extracted from user\nreviews. Empirical experiments and case studies on Amazon Product Benchmark\ndatasets show that our model can extract effective and interpretable user/item\nrepresentations from their reviews and outperforms multiple types of\nstate-of-the-art review-based recommendation models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.06286,review,pre_llm,2021,1,"{'ai_likelihood': 1.953707800971137e-06, 'text': 'Reinforcement learning based recommender systems: A survey\n\n  Recommender systems (RSs) have become an inseparable part of our everyday\nlives. They help us find our favorite items to purchase, our friends on social\nnetworks, and our favorite movies to watch. Traditionally, the recommendation\nproblem was considered to be a classification or prediction problem, but it is\nnow widely agreed that formulating it as a sequential decision problem can\nbetter reflect the user-system interaction. Therefore, it can be formulated as\na Markov decision process (MDP) and be solved by reinforcement learning (RL)\nalgorithms. Unlike traditional recommendation methods, including collaborative\nfiltering and content-based filtering, RL is able to handle the sequential,\ndynamic user-system interaction and to take into account the long-term user\nengagement. Although the idea of using RL for recommendation is not new and has\nbeen around for about two decades, it was not very practical, mainly because of\nscalability problems of traditional RL algorithms. However, a new trend has\nemerged in the field since the introduction of deep reinforcement learning\n(DRL), which made it possible to apply RL to the recommendation problem with\nlarge state and action spaces. In this paper, a survey on reinforcement\nlearning based recommender systems (RLRSs) is presented. Our aim is to present\nan outlook on the field and to provide the reader with a fairly complete\nknowledge of key concepts of the field. We first recognize and illustrate that\nRLRSs can be generally classified into RL- and DRL-based methods. Then, we\npropose an RLRS framework with four components, i.e., state representation,\npolicy optimization, reward formulation, and environment building, and survey\nRLRS algorithms accordingly. We highlight emerging topics and depict important\ntrends using various graphs and tables. Finally, we discuss important aspects\nand challenges that can be addressed in the future.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.09018,regular,pre_llm,2021,1,"{'ai_likelihood': 1.2252065870496963e-06, 'text': 'Network Clustering for Multi-task Learning\n\n  The Multi-Task Learning (MTL) technique has been widely studied by word-wide\nresearchers. The majority of current MTL studies adopt the hard parameter\nsharing structure, where hard layers tend to learn general representations over\nall tasks and specific layers are prone to learn specific representations for\neach task. Since the specific layers directly follow the hard layers, the MTL\nmodel needs to estimate this direct change (from general to specific) as well.\nTo alleviate this problem, we introduce the novel cluster layer, which groups\ntasks into clusters during training procedures. In a cluster layer, the tasks\nin the same cluster are further required to share the same network. By this\nway, the cluster layer produces the general presentation for the same cluster,\nwhile produces relatively specific presentations for different clusters. As\ntransitions the cluster layers are used between the hard layers and the\nspecific layers. The MTL model thus learns general representations to specific\nrepresentations gradually. We evaluate our model with MTL document\nclassification and the results demonstrate the cluster layer is quite efficient\nin MTL.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.06927,regular,pre_llm,2021,1,"{'ai_likelihood': 1.5861458248562285e-05, 'text': ""Robustness of Meta Matrix Factorization Against Strict Privacy\n  Constraints\n\n  In this paper, we explore the reproducibility of MetaMF, a meta matrix\nfactorization framework introduced by Lin et al. MetaMF employs meta learning\nfor federated rating prediction to preserve users' privacy. We reproduce the\nexperiments of Lin et al. on five datasets, i.e., Douban, Hetrec-MovieLens,\nMovieLens 1M, Ciao, and Jester. Also, we study the impact of meta learning on\nthe accuracy of MetaMF's recommendations. Furthermore, in our work, we\nacknowledge that users may have different tolerances for revealing information\nabout themselves. Hence, in a second strand of experiments, we investigate the\nrobustness of MetaMF against strict privacy constraints. Our study illustrates\nthat we can reproduce most of Lin et al.'s results. Plus, we provide strong\nevidence that meta learning is essential for MetaMF's robustness against strict\nprivacy constraints.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.09159,review,pre_llm,2021,1,"{'ai_likelihood': 2.384185791015625e-06, 'text': 'Misplaced trust? The relationship between trust, ability to identify\n  commercially influenced results, and search engine preference\n\n  People have a high level of trust in search engines, especially Google, but\nonly limited knowledge of them, as numerous studies have shown. This leads to\nthe question: To what extent is this trust justified considering the lack of\nfamiliarity among users with how search engines work and the business models\nthey are founded on? We assume that trust in Google, search engine preferences,\nand knowledge of result types are interrelated. To examine this assumption, we\nconducted a representative online survey with n = 2,012 German internet users.\nWe show that users with little search engine knowledge are more likely to trust\nand use Google than users with more knowledge. A contradiction revealed itself\n- users strongly trust Google, yet they are unable to adequately evaluate\nsearch results. This may be problematic since it can potentially affect\nknowledge acquisition. Consequently, there is a need to promote user\ninformation literacy to create a more solid foundation for user trust in search\nengines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.03327,regular,pre_llm,2021,1,"{'ai_likelihood': 2.053048875596788e-06, 'text': 'Selection of Optimal Parameters in the Fast K-Word Proximity Search\n  Based on Multi-component Key Indexes\n\n  Proximity full-text search is commonly implemented in contemporary full-text\nsearch systems. Let us assume that the search query is a list of words. It is\nnatural to consider a document as relevant if the queried words are near each\nother in the document. The proximity factor is even more significant for the\ncase where the query consists of frequently occurring words. Proximity\nfull-text search requires the storage of information for every occurrence in\ndocuments of every word that the user can search. For every occurrence of every\nword in a document, we employ additional indexes to store information about\nnearby words, that is, the words that occur in the document at distances from\nthe given word of less than or equal to the MaxDistance parameter. We showed in\nprevious works that these indexes can be used to improve the average query\nexecution time by up to 130 times for queries that consist of words occurring\nwith high-frequency. In this paper, we consider how both the search performance\nand the search quality depend on the value of MaxDistance and other parameters.\nWell-known GOV2 text collection is used in the experiments for reproducibility\nof the results. We propose a new index schema after the analysis of the results\nof the experiments.\n  This is a pre-print of a contribution published in Supplementary Proceedings\nof the XXII International Conference on Data Analytics and Management in Data\nIntensive Domains (DAMDID/RCDL 2020), Voronezh, Russia, October 13-16, 2020, P.\n336-350, published by CEUR Workshop Proceedings. The final authenticated\nversion is available online at: http://ceur-ws.org/Vol-2790/\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.08394,review,pre_llm,2021,1,"{'ai_likelihood': 4.569689432779948e-06, 'text': ""Templates of generic geographic information for answering\n  where-questions\n\n  In everyday communication, where-questions are answered by place\ndescriptions. To answer where-questions automatically, computers should be able\nto generate relevant place descriptions that satisfy inquirers' information\nneeds. Human-generated answers to where-questions constructed based on a few\nanchor places that characterize the location of inquired places. The challenge\nfor automatically generating such relevant responses stems from selecting\nrelevant anchor places. In this paper, we present templates that allow to\ncharacterize the human-generated answers and to imitate their structure. These\ntemplates are patterns of generic geographic information derived and encoded\nfrom the largest available machine comprehension dataset, MS MARCO v2.1. In our\napproach, the toponyms in the questions and answers of the dataset are encoded\ninto sequences of generic information. Next, sequence prediction methods are\nused to model the relation between the generic information in the questions and\ntheir answers. Finally, we evaluate the performance of predicting templates for\nanswers to where-questions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.09807,regular,pre_llm,2021,1,"{'ai_likelihood': 2.4868382347954644e-05, 'text': ""Estimating the Total Volume of Queries to a Search Engine\n\n  We study the problem of estimating the total number of searches (volume) of\nqueries in a specific domain, which were submitted to a search engine in a\ngiven time period. Our statistical model assumes that the distribution of\nsearches follows a Zipf's law, and that the observed sample volumes are biased\naccordingly to three possible scenarios. These assumptions are consistent with\nempirical data, with keyword research practices, and with approximate\nalgorithms used to take counts of query frequencies. A few estimators of the\nparameters of the distribution are devised and experimented, based on the\nnature of the empirical/simulated data. For continuous data, we recommend using\nnonlinear least square regression (NLS) on the top-volume queries, where the\nbound on the volume is obtained from the well-known Clauset, Shalizi and Newman\n(CSN) estimation of power-law parameters. For binned data, we propose using a\nChi-square minimization approach restricted to the top-volume queries, where\nthe bound is obtained by the binned version of the CSN method. Estimations are\nthen derived for the total number of queries and for the total volume of the\npopulation, including statistical error bounds. We apply the methods on the\ndomain of recipes and cooking queries searched in Italian in 2017. The observed\nvolumes of sample queries are collected from Google Trends (continuous data)\nand SearchVolume (binned data). The estimated total number of queries and total\nvolume are computed for the two cases, and the results are compared and\ndiscussed.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.03617,regular,pre_llm,2021,1,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'Transfer Learning and Augmentation for Word Sense Disambiguation\n\n  Many downstream NLP tasks have shown significant improvement through\ncontinual pre-training, transfer learning and multi-task learning.\nState-of-the-art approaches in Word Sense Disambiguation today benefit from\nsome of these approaches in conjunction with information sources such as\nsemantic relationships and gloss definitions contained within WordNet. Our work\nbuilds upon these systems and uses data augmentation along with extensive\npre-training on various different NLP tasks and datasets. Our transfer learning\nand augmentation pipeline achieves state-of-the-art single model performance in\nWSD and is at par with the best ensemble results.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.01922,regular,pre_llm,2021,2,"{'ai_likelihood': 1.0000334845648871e-05, 'text': ""Session-based Recommendation with Self-Attention Networks\n\n  Session-based recommendation aims to predict user's next behavior from\ncurrent session and previous anonymous sessions. Capturing long-range\ndependencies between items is a vital challenge in session-based\nrecommendation. A novel approach is proposed for session-based recommendation\nwith self-attention networks (SR-SAN) as a remedy. The self-attention networks\n(SAN) allow SR-SAN capture the global dependencies among all items of a session\nregardless of their distance. In SR-SAN, a single item latent vector is used to\ncapture both current interest and global interest instead of session embedding\nwhich is composed of current interest embedding and global interest embedding.\nSome experiments have been performed on some open benchmark datasets.\nExperimental results show that the proposed method outperforms some\nstate-of-the-arts by comparisons.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.03135,regular,pre_llm,2021,2,"{'ai_likelihood': 3.9769543541802304e-05, 'text': 'Graph Attention Collaborative Similarity Embedding for Recommender\n  System\n\n  We present Graph Attention Collaborative Similarity Embedding (GACSE), a new\nrecommendation framework that exploits collaborative information in the\nuser-item bipartite graph for representation learning. Our framework consists\nof two parts: the first part is to learn explicit graph collaborative filtering\ninformation such as user-item association through embedding propagation with\nattention mechanism, and the second part is to learn implicit graph\ncollaborative information such as user-user similarities and item-item\nsimilarities through auxiliary loss. We design a new loss function that\ncombines BPR loss with adaptive margin and similarity loss for the similarities\nlearning. Extensive experiments on three benchmarks show that our model is\nconsistently better than the latest state-of-the-art models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.03089,regular,pre_llm,2021,2,"{'ai_likelihood': 3.245141771104601e-06, 'text': ""Leveraging Review Properties for Effective Recommendation\n\n  Many state-of-the-art recommendation systems leverage explicit item reviews\nposted by users by considering their usefulness in representing the users'\npreferences and describing the items' attributes. These posted reviews may have\nvarious associated properties, such as their length, their age since they were\nposted, or their item rating. However, it remains unclear how these different\nreview properties contribute to the usefulness of their corresponding reviews\nin addressing the recommendation task. In particular, users show distinct\npreferences when considering different aspects of the reviews (i.e. properties)\nfor making decisions about the items. Hence, it is important to model the\nrelationship between the reviews' properties and the usefulness of reviews\nwhile learning the users' preferences and the items' attributes. Therefore, we\npropose to model the reviews with their associated available properties. We\nintroduce a novel review properties-based recommendation model (RPRM) that\nlearns which review properties are more important than others in capturing the\nusefulness of reviews, thereby enhancing the recommendation results.\nFurthermore, inspired by the users' information adoption framework, we\nintegrate two loss functions and a negative sampling strategy into our proposed\nRPRM model, to ensure that the properties of reviews are correlated with the\nusers' preferences. We examine the effectiveness of RPRM using the well-known\nYelp and Amazon datasets. Our results show that RPRM significantly outperforms\na classical and five state-of-the-art baselines. Moreover, we experimentally\nshow the advantages of using our proposed loss functions and negative sampling\nstrategy, which further enhance the recommendation performances of RPRM.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.09182,regular,pre_llm,2021,2,"{'ai_likelihood': 6.622738308376736e-08, 'text': ""Testing Lotka's Law and Pattern of Author Productivity in the Scholarly\n  Publications of Artificial Intelligence\n\n  Artificial intelligence has changed our day to day life in multitude ways. AI\ntechnology is rearing itself as a driving force to be reckoned with in the\nlargest industries in the world. AI has already engulfed our educational\nsystem, our businesses and our financial establishments. The future is definite\nthat machines with artificial intelligence will soon be captivating over\ntrained manual work that now is mostly cared by humans. Machines can carry out\nhuman-like tasks by new inputs as artificial intelligence makes it possible for\nmachines to learn from experience. AI data from web of science database from\n2008 to 2017 have been mapped to depict the average growth rate, relative\ngrowth rate, contribution made by authors in the view of research productivity,\nauthorship pattern and collaboration of AI literature. The Lotka's law on\nauthorship productivity of AI literature has been tested to confirm the\napplicability of the law to the present data set. A K-S test was applied to\nmeasure the degree of agreement between the distribution of the observed set of\ndata against the inverse general power relationship and the theoretical value\nof {\\alpha} =2. It is found that the inverse square law of Lotka follow as\nsuch.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.12998,regular,pre_llm,2021,2,"{'ai_likelihood': 1.357661353217231e-06, 'text': 'Deep NMF Topic Modeling\n\n  Nonnegative matrix factorization (NMF) based topic modeling methods do not\nrely on model- or data-assumptions much. However, they are usually formulated\nas difficult optimization problems, which may suffer from bad local minima and\nhigh computational complexity. In this paper, we propose a deep NMF (DNMF)\ntopic modeling framework to alleviate the aforementioned problems. It first\napplies an unsupervised deep learning method to learn latent hierarchical\nstructures of documents, under the assumption that if we could learn a good\nrepresentation of documents by, e.g. a deep model, then the topic word\ndiscovery problem can be boosted. Then, it takes the output of the deep model\nto constrain a topic-document distribution for the discovery of the\ndiscriminant topic words, which not only improves the efficacy but also reduces\nthe computational complexity over conventional unsupervised NMF methods. We\nconstrain the topic-document distribution in three ways, which takes the\nadvantages of the three major sub-categories of NMF -- basic NMF, structured\nNMF, and constrained NMF respectively. To overcome the weaknesses of deep\nneural networks in unsupervised topic modeling, we adopt a non-neural-network\ndeep model -- multilayer bootstrap network. To our knowledge, this is the first\ntime that a deep NMF model is used for unsupervised topic modeling. We have\ncompared the proposed method with a number of representative references\ncovering major branches of topic modeling on a variety of real-world text\ncorpora. Experimental results illustrate the effectiveness of the proposed\nmethod under various evaluation metrics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.10745,regular,pre_llm,2021,2,"{'ai_likelihood': 1.996755599975586e-05, 'text': ""Feature-level Attentive ICF for Recommendation\n\n  Item-based collaborative filtering (ICF) enjoys the advantages of high\nrecommendation accuracy and ease in online penalization and thus is favored by\nthe industrial recommender systems. ICF recommends items to a target user based\non their similarities to the previously interacted items of the user. Great\nprogresses have been achieved for ICF in recent years by applying advanced\nmachine learning techniques (e.g., deep neural networks) to learn the item\nsimilarity from data. The early methods simply treat all the historical items\nequally and recently proposed methods attempt to distinguish the different\nimportance of historical items when recommending a target item. Despite the\nprogress, we argue that those ICF models neglect the diverse intents of users\non adopting items (e.g., watching a movie because of the director, leading\nactors, or the visual effects). As a result, they fail to estimate the item\nsimilarity on a finer-grained level to predict the user's preference to an\nitem, resulting in sub-optimal recommendation. In this work, we propose a\ngeneral feature-level attention method for ICF models. The key of our method is\nto distinguish the importance of different factors when computing the item\nsimilarity for a prediction. To demonstrate the effectiveness of our method, we\ndesign a light attention neural network to integrate both item-level and\nfeature-level attention for neural ICF models. It is model-agnostic and\neasy-to-implement. We apply it to two baseline ICF models and evaluate its\neffectiveness on six public datasets. Extensive experiments show the\nfeature-level attention enhanced models consistently outperform their\ncounterparts, demonstrating the potential of differentiating user intents on\nthe feature-level for ICF recommendation models.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.12057,regular,pre_llm,2021,2,"{'ai_likelihood': 9.073151482476129e-06, 'text': ""Revisit Recommender System in the Permutation Prospective\n\n  Recommender systems (RS) work effective at alleviating information overload\nand matching user interests in various web-scale applications. Most RS retrieve\nthe user's favorite candidates and then rank them by the rating scores in the\ngreedy manner. In the permutation prospective, however, current RS come to\nreveal the following two limitations: 1) They neglect addressing the\npermutation-variant influence within the recommended results; 2) Permutation\nconsideration extends the latent solution space exponentially, and current RS\nlack the ability to evaluate the permutations. Both drive RS away from the\npermutation-optimal recommended results and better user experience. To\napproximate the permutation-optimal recommended results effectively and\nefficiently, we propose a novel permutation-wise framework PRS in the\nre-ranking stage of RS, which consists of Permutation-Matching (PMatch) and\nPermutation-Ranking (PRank) stages successively. Specifically, the PMatch stage\nis designed to obtain the candidate list set, where we propose the FPSA\nalgorithm to generate multiple candidate lists via the permutation-wise and\ngoal-oriented beam search algorithm. Afterwards, for the candidate list set,\nthe PRank stage provides a unified permutation-wise ranking criterion named LR\nmetric, which is calculated by the rating scores of elaborately designed\npermutation-wise model DPWN. Finally, the list with the highest LR score is\nrecommended to the user. Empirical results show that PRS consistently and\nsignificantly outperforms state-of-the-art methods. Moreover, PRS has achieved\na performance improvement of 11.0% on PV metric and 8.7% on IPV metric after\nthe successful deployment in one popular recommendation scenario of Taobao\napplication.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.1056,regular,pre_llm,2021,2,"{'ai_likelihood': 4.172325134277344e-06, 'text': ""A Concept Knowledge-Driven Keywords Retrieval Framework for Sponsored\n  Search\n\n  In sponsored search, retrieving synonymous keywords for exact match type is\nimportant for accurately targeted advertising. Data-driven deep learning-based\nmethod has been proposed to tackle this problem. An apparent disadvantage of\nthis method is its poor generalization performance on entity-level long-tail\ninstances, even though they might share similar concept-level patterns with\nfrequent instances. With the help of a large knowledge base, we find that most\ncommercial synonymous query-keyword pairs can be abstracted into meaningful\nconceptual patterns through concept tagging. Based on this fact, we propose a\nnovel knowledge-driven conceptual retrieval framework to mitigate this problem,\nwhich consists of three parts: data conceptualization, matching via conceptual\npatterns and concept-augmented discrimination. Both offline and online\nexperiments show that our method is very effective. This framework has been\nsuccessfully applied to Baidu's sponsored search system, which yields a\nsignificant improvement in revenue.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.04274,regular,pre_llm,2021,2,"{'ai_likelihood': 1.457002427842882e-06, 'text': ""Privacy-Preserving Near Neighbor Search via Sparse Coding with\n  Ambiguation\n\n  In this paper, we propose a framework for privacy-preserving approximate near\nneighbor search via stochastic sparsifying encoding. The core of the framework\nrelies on sparse coding with ambiguation (SCA) mechanism that introduces the\nnotion of inherent shared secrecy based on the support intersection of sparse\ncodes. This approach is `fairness-aware', in the sense that any point in the\nneighborhood has an equiprobable chance to be chosen. Our approach can be\napplied to raw data, latent representation of autoencoders, and aggregated\nlocal descriptors. The proposed method is tested on both synthetic i.i.d data\nand real large-scale image databases.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.07619,regular,pre_llm,2021,2,"{'ai_likelihood': 8.344650268554688e-06, 'text': ""MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models\n  by Instance-Guided Mask\n\n  Click-Through Rate(CTR) estimation has become one of the most fundamental\ntasks in many real-world applications and it's important for ranking models to\neffectively capture complex high-order features. Shallow feed-forward network\nis widely used in many state-of-the-art DNN models such as FNN, DeepFM and\nxDeepFM to implicitly capture high-order feature interactions. However, some\nresearch has proved that addictive feature interaction, particular feed-forward\nneural networks, is inefficient in capturing common feature interaction. To\nresolve this problem, we introduce specific multiplicative operation into DNN\nranking system by proposing instance-guided mask which performs element-wise\nproduct both on the feature embedding and feed-forward layers guided by input\ninstance. We also turn the feed-forward layer in DNN model into a mixture of\naddictive and multiplicative feature interactions by proposing MaskBlock in\nthis paper. MaskBlock combines the layer normalization, instance-guided mask,\nand feed-forward layer and it is a basic building block to be used to design\nnew ranking model under various configurations. The model consisting of\nMaskBlock is called MaskNet in this paper and two new MaskNet models are\nproposed to show the effectiveness of MaskBlock as basic building block for\ncomposing high performance ranking systems. The experiment results on three\nreal-world datasets demonstrate that our proposed MaskNet models outperform\nstate-of-the-art models such as DeepFM and xDeepFM significantly, which implies\nMaskBlock is an effective basic building unit for composing new high\nperformance ranking systems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.057,regular,pre_llm,2021,2,"{'ai_likelihood': 1.7881393432617188e-06, 'text': 'ELSKE: Efficient Large-Scale Keyphrase Extraction\n\n  Keyphrase extraction methods can provide insights into large collections of\ndocuments such as social media posts. Existing methods, however, are less\nsuited for the real-time analysis of streaming data, because they are\ncomputationally too expensive or require restrictive constraints regarding the\nstructure of keyphrases. We propose an efficient approach to extract keyphrases\nfrom large document collections and show that the method also performs\ncompetitively on individual documents.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.12188,regular,pre_llm,2021,2,"{'ai_likelihood': 4.006756676567926e-06, 'text': ""Support the Underground: Characteristics of Beyond-Mainstream Music\n  Listeners\n\n  Music recommender systems have become an integral part of music streaming\nservices such as Spotify and Last.fm to assist users navigating the extensive\nmusic collections offered by them. However, while music listeners interested in\nmainstream music are traditionally served well by music recommender systems,\nusers interested in music beyond the mainstream (i.e., non-popular music)\nrarely receive relevant recommendations. In this paper, we study the\ncharacteristics of beyond-mainstream music and music listeners and analyze to\nwhat extent these characteristics impact the quality of music recommendations\nprovided. Therefore, we create a novel dataset consisting of Last.fm listening\nhistories of several thousand beyond-mainstream music listeners, which we\nenrich with additional metadata describing music tracks and music listeners.\nOur analysis of this dataset shows four subgroups within the group of\nbeyond-mainstream music listeners that differ not only with respect to their\npreferred music but also with their demographic characteristics. Furthermore,\nwe evaluate the quality of music recommendations that these subgroups are\nprovided with four different recommendation algorithms where we find\nsignificant differences between the groups. Specifically, our results show a\npositive correlation between a subgroup's openness towards music listened to by\nmembers of other subgroups and recommendation accuracy. We believe that our\nfindings provide valuable insights for developing improved user models and\nrecommendation approaches to better serve beyond-mainstream music listeners.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.10315,regular,pre_llm,2021,2,"{'ai_likelihood': 8.609559800889758e-06, 'text': 'EXTRA: Explanation Ranking Datasets for Explainable Recommendation\n\n  Recently, research on explainable recommender systems has drawn much\nattention from both academia and industry, resulting in a variety of\nexplainable models. As a consequence, their evaluation approaches vary from\nmodel to model, which makes it quite difficult to compare the explainability of\ndifferent models. To achieve a standard way of evaluating recommendation\nexplanations, we provide three benchmark datasets for EXplanaTion RAnking\n(denoted as EXTRA), on which explainability can be measured by ranking-oriented\nmetrics. Constructing such datasets, however, poses great challenges. First,\nuser-item-explanation triplet interactions are rare in existing recommender\nsystems, so how to find alternatives becomes a challenge. Our solution is to\nidentify nearly identical sentences from user reviews. This idea then leads to\nthe second challenge, i.e., how to efficiently categorize the sentences in a\ndataset into different groups, since it has quadratic runtime complexity to\nestimate the similarity between any two sentences. To mitigate this issue, we\nprovide a more efficient method based on Locality Sensitive Hashing (LSH) that\ncan detect near-duplicates in sub-linear time for a given query. Moreover, we\nmake our code publicly available to allow researchers in the community to\ncreate their own datasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.06137,regular,pre_llm,2021,2,"{'ai_likelihood': 1.2715657552083334e-05, 'text': 'Task-adaptive Neural Process for User Cold-Start Recommendation\n\n  User cold-start recommendation is a long-standing challenge for recommender\nsystems due to the fact that only a few interactions of cold-start users can be\nexploited. Recent studies seek to address this challenge from the perspective\nof meta learning, and most of them follow a manner of parameter initialization,\nwhere the model parameters can be learned by a few steps of gradient updates.\nWhile these gradient-based meta-learning models achieve promising performances\nto some extent, a fundamental problem of them is how to adapt the global\nknowledge learned from previous tasks for the recommendations of cold-start\nusers more effectively. In this paper, we develop a novel meta-learning\nrecommender called task-adaptive neural process (TaNP). TaNP is a new member of\nthe neural process family, where making recommendations for each user is\nassociated with a corresponding stochastic process. TaNP directly maps the\nobserved interactions of each user to a predictive distribution, sidestepping\nsome training issues in gradient-based meta-learning models. More importantly,\nto balance the trade-off between model capacity and adaptation reliability, we\nintroduce a novel task-adaptive mechanism. It enables our model to learn the\nrelevance of different tasks and customize the global knowledge to the\ntask-related decoder parameters for estimating user preferences. We validate\nTaNP on multiple benchmark datasets in different experimental settings.\nEmpirical results demonstrate that TaNP yields consistent improvements over\nseveral state-of-the-art meta-learning recommenders.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.12887,review,pre_llm,2021,2,"{'ai_likelihood': 2.4504131740993925e-06, 'text': 'Significant Improvements over the State of the Art? A Case Study of the\n  MS MARCO Document Ranking Leaderboard\n\n  Leaderboards are a ubiquitous part of modern research in applied machine\nlearning. By design, they sort entries into some linear order, where the\ntop-scoring entry is recognized as the ""state of the art"" (SOTA). Due to the\nrapid progress being made in information retrieval today, particularly with\nneural models, the top entry in a leaderboard is replaced with some regularity.\nThese are touted as improvements in the state of the art. Such pronouncements,\nhowever, are almost never qualified with significance testing. In the context\nof the MS MARCO document ranking leaderboard, we pose a specific question: How\ndo we know if a run is significantly better than the current SOTA? We ask this\nquestion against the backdrop of recent IR debates on scale types: in\nparticular, whether commonly used significance tests are even mathematically\npermissible. Recognizing these potential pitfalls in evaluation methodology,\nour study proposes an evaluation framework that explicitly treats certain\noutcomes as distinct and avoids aggregating them into a single-point metric.\nEmpirical analysis of SOTA runs from the MS MARCO document ranking leaderboard\nreveals insights about how one run can be ""significantly better"" than another\nthat are obscured by the current official evaluation metric (MRR@100).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.10073,regular,pre_llm,2021,2,"{'ai_likelihood': 8.44399134318034e-06, 'text': 'Pyserini: An Easy-to-Use Python Toolkit to Support Replicable IR\n  Research with Sparse and Dense Representations\n\n  Pyserini is an easy-to-use Python toolkit that supports replicable IR\nresearch by providing effective first-stage retrieval in a multi-stage ranking\narchitecture. Our toolkit is self-contained as a standard Python package and\ncomes with queries, relevance judgments, pre-built indexes, and evaluation\nscripts for many commonly used IR test collections. We aim to support, out of\nthe box, the entire research lifecycle of efforts aimed at improving ranking\nwith modern neural approaches. In particular, Pyserini supports sparse\nretrieval (e.g., BM25 scoring using bag-of-words representations), dense\nretrieval (e.g., nearest-neighbor search on transformer-encoded\nrepresentations), as well as hybrid retrieval that integrates both approaches.\nThis paper provides an overview of toolkit features and presents empirical\nresults that illustrate its effectiveness on two popular ranking tasks. We also\ndescribe how our group has built a culture of replicability through shared\nnorms and tools that enable rigorous automated testing.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.07831,regular,pre_llm,2021,2,"{'ai_likelihood': 2.053048875596788e-06, 'text': ""NeuralNDCG: Direct Optimisation of a Ranking Metric via Differentiable\n  Relaxation of Sorting\n\n  Learning to Rank (LTR) algorithms are usually evaluated using Information\nRetrieval metrics like Normalised Discounted Cumulative Gain (NDCG) or Mean\nAverage Precision. As these metrics rely on sorting predicted items' scores\n(and thus, on items' ranks), their derivatives are either undefined or zero\neverywhere. This makes them unsuitable for gradient-based optimisation, which\nis the usual method of learning appropriate scoring functions. Commonly used\nLTR loss functions are only loosely related to the evaluation metrics, causing\na mismatch between the optimisation objective and the evaluation criterion. In\nthis paper, we address this mismatch by proposing NeuralNDCG, a novel\ndifferentiable approximation to NDCG. Since NDCG relies on the\nnon-differentiable sorting operator, we obtain NeuralNDCG by relaxing that\noperator using NeuralSort, a differentiable approximation of sorting. As a\nresult, we obtain a new ranking loss function which is an arbitrarily accurate\napproximation to the evaluation metric, thus closing the gap between the\ntraining and the evaluation of LTR models. We introduce two variants of the\nproposed loss function. Finally, the empirical evaluation shows that our\nproposed method outperforms previous work aimed at direct optimisation of NDCG\nand is competitive with the state-of-the-art methods.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.12793,regular,pre_llm,2021,2,"{'ai_likelihood': 6.5896246168348525e-06, 'text': 'Learning to Truncate Ranked Lists for Information Retrieval\n\n  Ranked list truncation is of critical importance in a variety of professional\ninformation retrieval applications such as patent search or legal search. The\ngoal is to dynamically determine the number of returned documents according to\nsome user-defined objectives, in order to reach a balance between the overall\nutility of the results and user efforts. Existing methods formulate this task\nas a sequential decision problem and take some pre-defined loss as a proxy\nobjective, which suffers from the limitation of local decision and non-direct\noptimization. In this work, we propose a global decision based truncation model\nnamed AttnCut, which directly optimizes user-defined objectives for the ranked\nlist truncation. Specifically, we take the successful transformer architecture\nto capture the global dependency within the ranked list for truncation\ndecision, and employ the reward augmented maximum likelihood (RAML) for direct\noptimization. We consider two types of user-defined objectives which are of\npractical usage. One is the widely adopted metric such as F1 which acts as a\nbalanced objective, and the other is the best F1 under some minimal recall\nconstraint which represents a typical objective in professional search.\nEmpirical results over the Robust04 and MQ2007 datasets demonstrate the\neffectiveness of our approach as compared with the state-of-the-art baselines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.04447,regular,pre_llm,2021,2,"{'ai_likelihood': 8.940696716308594e-07, 'text': ""Applying the Affective Aware Pseudo Association Method to Enhance the\n  Top-N Recommendations Distribution to Users in Group Emotion Recommender\n  Systems\n\n  Recommender Systems are a subclass of information retrieval systems, or more\nsuccinctly, a class of information filtering systems that seeks to predict how\nclose is the match of the user's preference to a recommended item. A common\napproach for making recommendations for a user group is to extend Personalized\nRecommender Systems' capability. This approach gives the impression that group\nrecommendations are retrofits of the Personalized Recommender Systems.\nMoreover, such an approach not taken the dynamics of group emotion and\nindividual emotion into the consideration in making top_N recommendations.\nRecommending items to a group of two or more users has certainly raised unique\nchallenges in group behaviors that influence group decision-making that\nresearchers only partially understand. This study applies the Affective Aware\nPseudo Association Method in studying group formation and dynamics in group\ndecision-making. The method shows its adaptability to group's moods change when\nmaking recommendations.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.00532,regular,pre_llm,2021,2,"{'ai_likelihood': 2.914004855685764e-06, 'text': 'An Efficient Indexing and Searching Technique for Information Retrieval\n  for Urdu Language\n\n  Indexing techniques are used to improve retrieval of data in response to\ncertain search condition. Inverted files are mostly used for creating indexes.\nThis paper proposes indexing technique for Urdu language. Language processing\nstep in Index creation is different for a particular language. We discuss index\ncreation steps specifically for Urdu language. We explore morphological rules\nfor Urdu language and implement these rules to create Urdu stemmer. We\nimplement our proposed technique with different implementations and compare\nresults. We suggest that indexes should be created without stop words and also\nindex file should be an order index file.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.05256,regular,pre_llm,2021,3,"{'ai_likelihood': 6.556510925292969e-06, 'text': 'CEQE: Contextualized Embeddings for Query Expansion\n\n  In this work we leverage recent advances in context-sensitive language models\nto improve the task of query expansion. Contextualized word representation\nmodels, such as ELMo and BERT, are rapidly replacing static embedding models.\nWe propose a new model, Contextualized Embeddings for Query Expansion (CEQE),\nthat utilizes query-focused contextualized embedding vectors. We study the\nbehavior of contextual representations generated for query expansion in ad-hoc\ndocument retrieval. We conduct our experiments on probabilistic retrieval\nmodels as well as in combination with neural ranking models. We evaluate CEQE\non two standard TREC collections: Robust and Deep Learning. We find that CEQE\noutperforms static embedding-based expansion methods on multiple collections\n(by up to 18% on Robust and 31% on Deep Learning on average precision) and also\nimproves over proven probabilistic pseudo-relevance feedback (PRF) models. We\nfurther find that multiple passes of expansion and reranking result in\ncontinued gains in effectiveness with CEQE-based approaches outperforming other\napproaches. The final model incorporating neural and CEQE-based expansion score\nachieves gains of up to 5% in P@20 and 2% in AP on Robust over the\nstate-of-the-art transformer-based re-ranking model, Birch.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.07849,regular,pre_llm,2021,3,"{'ai_likelihood': 2.6490953233506946e-07, 'text': ""Fairness-aware Personalized Ranking Recommendation via Adversarial\n  Learning\n\n  Recommendation algorithms typically build models based on historical\nuser-item interactions (e.g., clicks, likes, or ratings) to provide a\npersonalized ranked list of items. These interactions are often distributed\nunevenly over different groups of items due to varying user preferences.\nHowever, we show that recommendation algorithms can inherit or even amplify\nthis imbalanced distribution, leading to unfair recommendations to item groups.\nConcretely, we formalize the concepts of ranking-based statistical parity and\nequal opportunity as two measures of fairness in personalized ranking\nrecommendation for item groups. Then, we empirically show that one of the most\nwidely adopted algorithms -- Bayesian Personalized Ranking -- produces unfair\nrecommendations, which motivates our effort to propose the novel fairness-aware\npersonalized ranking model. The debiased model is able to improve the two\nproposed fairness metrics while preserving recommendation performance.\nExperiments on three public datasets show strong fairness improvement of the\nproposed model versus state-of-the-art alternatives.\n  This is paper is an extended and reorganized version of our SIGIR\n2020~\\cite{zhu2020measuring} paper. In this paper, we re-frame the studied\nproblem as `item recommendation fairness' in personalized ranking\nrecommendation systems, and provide more details about the training process of\nthe proposed model and details of experiment setup.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.13235,regular,pre_llm,2021,3,"{'ai_likelihood': 2.715322706434462e-06, 'text': 'Web Mining for Estimating Regulatory Blockchain Readiness\n\n  The regulatory framework of cryptocurrencies (and, in general, blockchain\ntokens) is of paramount importance. This framework drives nearly all key\ndecisions in the respective business areas. In this work, a computational model\nis proposed for quantitatively estimating the regulatory stance of countries\nwith respect to cryptocurrencies. This is conducted via web mining utilizing\nweb search engines. The proposed model is experimentally validated. In\naddition, unsupervised learning (clustering) is applied for better analyzing\nthe automatically derived estimations. Overall, very good performance is\nachieved by the proposed algorithmic approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.0259,regular,pre_llm,2021,3,"{'ai_likelihood': 4.801485273573134e-06, 'text': 'Elliot: a Comprehensive and Rigorous Framework for Reproducible\n  Recommender Systems Evaluation\n\n  Recommender Systems have shown to be an effective way to alleviate the\nover-choice problem and provide accurate and tailored recommendations. However,\nthe impressive number of proposed recommendation algorithms, splitting\nstrategies, evaluation protocols, metrics, and tasks, has made rigorous\nexperimental evaluation particularly challenging. Puzzled and frustrated by the\ncontinuous recreation of appropriate evaluation benchmarks, experimental\npipelines, hyperparameter optimization, and evaluation procedures, we have\ndeveloped an exhaustive framework to address such needs. Elliot is a\ncomprehensive recommendation framework that aims to run and reproduce an entire\nexperimental pipeline by processing a simple configuration file. The framework\nloads, filters, and splits the data considering a vast set of strategies (13\nsplitting methods and 8 filtering approaches, from temporal training-test\nsplitting to nested K-folds Cross-Validation). Elliot optimizes hyperparameters\n(51 strategies) for several recommendation algorithms (50), selects the best\nmodels, compares them with the baselines providing intra-model statistics,\ncomputes metrics (36) spanning from accuracy to beyond-accuracy, bias, and\nfairness, and conducts statistical analysis (Wilcoxon and Paired t-test). The\naim is to provide the researchers with a tool to ease (and make them\nreproducible) all the experimental evaluation phases, from data reading to\nresults collection. Elliot is available on GitHub\n(https://github.com/sisinflab/elliot).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.16669,review,pre_llm,2021,3,"{'ai_likelihood': 6.291601392957899e-07, 'text': 'An In-depth Analysis of Passage-Level Label Transfer for Contextual\n  Document Ranking\n\n  Pre-trained contextual language models such as BERT, GPT, and XLnet work\nquite well for document retrieval tasks. Such models are fine-tuned based on\nthe query-document/query-passage level relevance labels to capture the ranking\nsignals. However, the documents are longer than the passages and such document\nranking models suffer from the token limitation (512) of BERT. Researchers\nproposed ranking strategies that either truncate the documents beyond the token\nlimit or chunk the documents into units that can fit into the BERT. In the\nlater case, the relevance labels are either directly transferred from the\noriginal query-document pair or learned through some external model. In this\npaper, we conduct a detailed study of the design decisions about splitting and\nlabel transfer on retrieval effectiveness and efficiency. We find that direct\ntransfer of relevance labels from documents to passages introduces label noise\nthat strongly affects retrieval effectiveness for large training datasets. We\nalso find that query processing times are adversely affected by fine-grained\nsplitting schemes. As a remedy, we propose a careful passage level labelling\nscheme using weak supervision that delivers improved performance (3-14% in\nterms of nDCG score) over most of the recently proposed models for ad-hoc\nretrieval while maintaining manageable computational complexity on four diverse\ndocument retrieval datasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.029,regular,pre_llm,2021,3,"{'ai_likelihood': 9.602970547146267e-07, 'text': 'The effects of having lists of synonyms on the performance of Afaan\n  Oromo Text Retrieval system\n\n  Obtaining relevant information from a collection of informational resources\nin Afaan Oromo is very important for Afaan Oromo speakers, developing a system\nthat help users of Afaan Oromo is mandatory. That is why this study is\nenvisioned to make possible retrieval of Afaan Oromo text documents by applying\ntechniques of modern information retrieval system. In the developed Afaan Oromo\nprototype, Probabilistic approach was used as an information retrieval models\nand precision and recall measurement were used as the performance measurement\nor evaluation technique. Apache Solr was also used as an environmental\nprogramming language to achieve the evaluation goal. Afaan Oromo text retrieval\nis evaluated using 158 documents and 13 arbitrarily selected queries that can\ndetermine the effectiveness of retrieval using the precision-recall. The\naverage result obtained by our evaluation before the addition of synonymy was\n72.91% precision and 86.8% recall respectively. After the addition of synonymy,\nthe value was changed to 71.39% average precision and 90.5% average recall. The\nF-measure for the evaluation before synonymy addition was 79.25% and after\naddition changed to 79.82%. The addition of synonymy improves the system\nperformance by 0.57%. The study therefore, experimentally proves that the\naddition of the thesaurus system can improve the system performance.\nSpellchecking, pagination, hit highlighting and autosuggestion is also possible\nin the developed prototype for Afaan Oromo.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.07901,regular,pre_llm,2021,3,"{'ai_likelihood': 4.5365757412380644e-06, 'text': 'TripClick: The Log Files of a Large Health Web Search Engine\n\n  Click logs are valuable resources for a variety of information retrieval (IR)\ntasks. This includes query understanding/analysis, as well as learning\neffective IR models particularly when the models require large amounts of\ntraining data. We release a large-scale domain-specific dataset of click logs,\nobtained from user interactions of the Trip Database health web search engine.\nOur click log dataset comprises approximately 5.2 million user interactions\ncollected between 2013 and 2020. We use this dataset to create a standard IR\nevaluation benchmark -- TripClick -- with around 700,000 unique free-text\nqueries and 1.3 million pairs of query-document relevance signals, whose\nrelevance is estimated by two click-through models. As such, the collection is\none of the few datasets offering the necessary data richness and scale to train\nneural IR models with a large amount of parameters, and notably the first in\nthe health domain. Using TripClick, we conduct experiments to evaluate a\nvariety of IR models, showing the benefits of exploiting this data to train\nneural architectures. In particular, the evaluation results show that the best\nperforming neural IR model significantly improves the performance by a large\nmargin relative to classical IR models, especially for more frequent queries.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.02462,regular,pre_llm,2021,3,"{'ai_likelihood': 7.28501213921441e-06, 'text': 'University of Copenhagen Participation in TREC Health Misinformation\n  Track 2020\n\n  In this paper, we describe our participation in the TREC Health\nMisinformation Track 2020. We submitted $11$ runs to the Total Recall Task and\n13 runs to the Ad Hoc task. Our approach consists of 3 steps: (1) we create an\ninitial run with BM25 and RM3; (2) we estimate credibility and misinformation\nscores for the documents in the initial run; (3) we merge the relevance,\ncredibility and misinformation scores to re-rank documents in the initial run.\nTo estimate credibility scores, we implement a classifier which exploits\nfeatures based on the content and the popularity of a document. To compute the\nmisinformation score, we apply a stance detection approach with a pretrained\nTransformer language model. Finally, we use different approaches to merge\nscores: weighted average, the distance among score vectors and rank fusion.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.12286,regular,pre_llm,2021,3,"{'ai_likelihood': 4.6690305074055995e-06, 'text': 'Automated Discovery of Real-Time Network Camera Data From Heterogeneous\n  Web Pages\n\n  Reduction in the cost of Network Cameras along with a rise in connectivity\nenables entities all around the world to deploy vast arrays of camera networks.\nNetwork cameras offer real-time visual data that can be used for studying\ntraffic patterns, emergency response, security, and other applications.\nAlthough many sources of Network Camera data are available, collecting the data\nremains difficult due to variations in programming interface and website\nstructures. Previous solutions rely on manually parsing the target website,\ntaking many hours to complete. We create a general and automated solution for\naggregating Network Camera data spread across thousands of uniquely structured\nweb pages. We analyze heterogeneous web page structures and identify common\ncharacteristics among 73 sample Network Camera websites (each website has\nmultiple web pages). These characteristics are then used to build an automated\ncamera discovery module that crawls and aggregates Network Camera data. Our\nsystem successfully extracts 57,364 Network Cameras from 237,257 unique web\npages.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.04831,review,pre_llm,2021,3,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'Semantic Models for the First-stage Retrieval: A Comprehensive Review\n\n  Multi-stage ranking pipelines have been a practical solution in modern search\nsystems, where the first-stage retrieval is to return a subset of candidate\ndocuments, and latter stages attempt to re-rank those candidates. Unlike\nre-ranking stages going through quick technique shifts during past decades, the\nfirst-stage retrieval has long been dominated by classical term-based models.\nUnfortunately, these models suffer from the vocabulary mismatch problem, which\nmay block re-ranking stages from relevant documents at the very beginning.\nTherefore, it has been a long-term desire to build semantic models for the\nfirst-stage retrieval that can achieve high recall efficiently. Recently, we\nhave witnessed an explosive growth of research interests on the first-stage\nsemantic retrieval models. We believe it is the right time to survey current\nstatus, learn from existing methods, and gain some insights for future\ndevelopment. In this paper, we describe the current landscape of the\nfirst-stage retrieval models under a unified framework to clarify the\nconnection between classical term-based retrieval methods, early semantic\nretrieval methods and neural semantic retrieval methods. Moreover, we identify\nsome open challenges and envision some future directions, with the hope of\ninspiring more researches on these important yet less investigated topics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.1242,review,pre_llm,2021,3,"{'ai_likelihood': 3.6093923780653213e-06, 'text': 'HSEarch: semantic search system for workplace accident reports\n\n  Semantic search engines, which integrate the output of text mining (TM)\nmethods, can significantly increase the ease and efficiency of finding relevant\ndocuments and locating important information within them. We present a novel\nsearch engine for the construction industry, HSEarch\n(http://www.nactem.ac.uk/hse/), which uses TM methods to provide\nsemantically-enhanced, faceted search over a repository of workplace accident\nreports. Compared to previous TM-driven search engines for the construction\nindustry, HSEarch provides a more interactive means for users to explore the\ncontents of the repository, to review documents more systematically and to\nlocate relevant knowledge within them.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.0439,regular,pre_llm,2021,3,"{'ai_likelihood': 3.344482845730252e-06, 'text': 'RevDet: Robust and Memory Efficient Event Detection and Tracking in\n  Large News Feeds\n\n  With the ever-growing volume of online news feeds, event-based organization\nof news articles has many practical applications including better information\nnavigation and the ability to view and analyze events as they develop.\nAutomatically tracking the evolution of events in large news corpora still\nremains a challenging task, and the existing techniques for Event Detection and\nTracking do not place a particular focus on tracking events in very large and\nconstantly updating news feeds. Here, we propose a new method for robust and\nefficient event detection and tracking, which we call RevDet algorithm. RevDet\nadopts an iterative clustering approach for tracking events. Even though many\nevents continue to develop for many days or even months, RevDet is able to\ndetect and track those events while utilizing only a constant amount of space\non main memory. We also devise a redundancy removal strategy which effectively\neliminates duplicate news articles and substantially reduces the size of data.\nWe construct a large, comprehensive new ground truth dataset specifically for\nevent detection and tracking approaches by augmenting two existing datasets:\nw2e and GDELT. We implement RevDet algorithm and evaluate its performance on\nthe ground truth event chains. We discover that our algorithm is able to\naccurately recover event chains in the ground-truth dataset. We also compare\nthe memory efficiency of our algorithm with the standard single pass clustering\napproach, and demonstrate the appropriateness of our algorithm for event\ndetection and tracking task in large news feeds.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.06909,review,pre_llm,2021,3,"{'ai_likelihood': 7.28501213921441e-07, 'text': ""Toward the Next Generation of News Recommender Systems\n\n  This paper proposes a vision and research agenda for the next generation of\nnews recommender systems (RS), called the table d'hote approach. A table d'hote\n(translates as host's table) meal is a sequence of courses that create a\nbalanced and enjoyable dining experience for a guest. Likewise, we believe news\nRS should strive to create a similar experience for the users by satisfying the\nnews-diet needs of a user. While extant news RS considers criteria such as\ndiversity and serendipity, and RS bundles have been studied for other contexts\nsuch as tourism, table d'hote goes further by ensuring the recommended articles\nsatisfy a diverse set of user needs in the right proportions and in a specific\norder. In table d'hote, available articles need to be stratified based on the\ndifferent ways that news can create value for the reader, building from\ntheories and empirical research in journalism and user engagement. Using\ntheories and empirical research from communication on the uses and\ngratifications (U&G) consumers derive from media, we define two main strata in\na table d'hote news RS, each with its own substrata: 1) surveillance, which\nconsists of information the user needs to know, and 2) serendipity, which are\nthe articles offering unexpected surprises. The diversity of the articles\naccording to the defined strata and the order of the articles within the list\nof recommendations are also two important aspects of the table d'hote in order\nto give the users the most effective reading experience. We propose our vision,\nlink it to the existing concepts in the RS literature, and identify challenges\nfor future research.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.00956,regular,pre_llm,2021,3,"{'ai_likelihood': 1.1887815263536242e-05, 'text': 'A Linguistic Study on Relevance Modeling in Information Retrieval\n\n  Relevance plays a central role in information retrieval (IR), which has\nreceived extensive studies starting from the 20th century. The definition and\nthe modeling of relevance has always been critical challenges in both\ninformation science and computer science research areas. Along with the debate\nand exploration on relevance, IR has already become a core task in many\nreal-world applications, such as Web search engines, question answering\nsystems, conversational bots, and so on. While relevance acts as a unified\nconcept in all these retrieval tasks, the inherent definitions are quite\ndifferent due to the heterogeneity of these tasks. This raises a question to\nus: Do these different forms of relevance really lead to different modeling\nfocuses? To answer this question, in this work, we conduct an empirical study\non relevance modeling in three representative IR tasks, i.e., document\nretrieval, answer retrieval, and response retrieval. Specifically, we attempt\nto study the following two questions: 1) Does relevance modeling in these tasks\nreally show differences in terms of natural language understanding (NLU)? We\nemploy 16 linguistic tasks to probe a unified retrieval model over these three\nretrieval tasks to answer this question. 2) If there do exist differences, how\ncan we leverage the findings to enhance the relevance modeling? We proposed\nthree intervention methods to investigate how to leverage different modeling\nfocuses of relevance to improve these IR tasks. We believe the way we study the\nproblem as well as our findings would be beneficial to the IR community.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.14866,regular,pre_llm,2021,3,"{'ai_likelihood': 2.0662943522135418e-05, 'text': 'Multi-Facet Recommender Networks with Spherical Optimization\n\n  Implicit feedback is widely explored by modern recommender systems. Since the\nfeedback is often sparse and imbalanced, it poses great challenges to the\nlearning of complex interactions among users and items. Metric learning has\nbeen proposed to capture user-item interactions from implicit feedback, but\nexisting methods only represent users and items in a single metric space,\nignoring the fact that users can have multiple preferences and items can have\nmultiple properties, which leads to potential conflicts limiting their\nperformance in recommendation. To capture the multiple facets of user\npreferences and item properties while resolving their potential conflicts, we\npropose the novel framework of Multi-fAcet Recommender networks with Spherical\noptimization (MARS). By designing a cross-facet similarity measurement, we\nproject users and items into multiple metric spaces for fine-grained\nrepresentation learning, and compare them only in the proper spaces.\nFurthermore, we devise a spherical optimization strategy to enhance the\neffectiveness and robustness of the multi-facet recommendation framework.\nExtensive experiments on six real-world benchmark datasets show drastic\nperformance gains brought by MARS, which constantly achieves up to 40\\%\nimprovements over the state-of-the-art baselines regarding both HR and nDCG\nmetrics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.10683,regular,pre_llm,2021,3,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'Discovering Redundant Activities in Event Logs for the Simplification of\n  Process Models\n\n  Process mining acts as a valuable tool to analyse the behaviour of an\norganisation by offering techniques to discover, monitor and enhance real\nprocesses. The key to process mining is to discovery understandable process\nmodels. However, real-life logs can be complex with redundant activities, which\nshare similar behaviour but have different syntax. We show that the existence\nof such redundant activities heavily affects the quality of discovered process\nmodels. Existing approaches filter activities by frequency, which cannot solve\nproblems caused by redundant activities. In this paper, we propose first to\ndiscover redundant activities in the log level and, then, use the discovery\nresults to simplify event logs. Two publicly available data sets are used to\nevaluate the usability of our approach in real-life processes. Our approach can\nbe adopted as a preprocessing step before applying any discovery algorithms to\nproduce simplify models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.10474,regular,pre_llm,2021,3,"{'ai_likelihood': 5.1657358805338544e-06, 'text': 'Dynamic Model for Query-Document Expansion towards Improving Retrieval\n  Relevance\n\n  Getting relevant information from search engines has been the heart of\nresearch works in information retrieval. Query expansion is a retrieval\ntechnique that has been studied and proved to yield positive results in\nrelevance. Users are required to express their queries as a shortlist of words,\nsentences, or questions. With this short format, a huge amount of information\nis lost in the process of translating the information need from the actual\nquery size since the user cannot convey all his thoughts in a few words. This\nmostly leads to poor query representation which contributes to undesired\nretrieval effectiveness. This loss of information has made the study of query\nexpansion technique a strong area of study. This research work focuses on two\nmethods of retrieval for both tweet-length queries and sentence-length queries.\nTwo algorithms have been proposed and the implementation is expected to produce\na better relevance retrieval model than most state-the-art relevance models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.14934,regular,pre_llm,2021,3,"{'ai_likelihood': 7.682376437717014e-06, 'text': ""Community-based Cyberreading for Information Understanding\n\n  Although the content in scientific publications is increasingly challenging,\nit is necessary to investigate another important problem, that of scientific\ninformation understanding. For this proposed problem, we investigate novel\nmethods to assist scholars (readers) to better understand scientific\npublications by enabling physical and virtual collaboration. For physical\ncollaboration, an algorithm will group readers together based on their profiles\nand reading behavior, and will enable the cyberreading collaboration within a\nonline reading group. For virtual collaboration, instead of pushing readers to\ncommunicate with others, we cluster readers based on their estimated\ninformation needs. For each cluster, a learning to rank model will be generated\nto recommend readers' communitized resources (i.e., videos, slides, and wikis)\nto help them understand the target publication.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.05673,regular,pre_llm,2021,3,"{'ai_likelihood': 5.298190646701389e-07, 'text': 'u-cf2vec: Representation Learning for Personalized Algorithm Selection\n  in Recommender Systems\n\n  Collaborative Filtering (CF) has become the standard approach to solve\nrecommendation systems (RS) problems. Collaborative Filtering algorithms try to\nmake predictions about interests of a user by collecting the personal interests\nfrom multiple users. There are multiple CF algorithms, each one of them with\nits own biases. It is the Machine Learning practitioner that has to choose the\nbest algorithm for each task beforehand. In Recommender Systems, different\nalgorithms have different performance for different users within the same\ndataset. Meta Learning (MtL) has been used to choose the best algorithm for a\ngiven problem. Meta Learning is usually applied to select algorithms for a\nwhole dataset. Adapting it to select the to the algorithm for a single user in\na RS involves several challenges. The most important is the design of the\nmetafeatures which, in typical meta learning, characterize datasets while here,\nthey must characterize a single user. This work presents a new meta-learning\nbased framework named $\\mu$-cf2vec to select the best algorithm for each user.\nWe propose using Representation Learning techniques to extract the\nmetafeatures. Representation Learning tries to extract representations that can\nbe reused in other learning tasks. In this work we also implement the framework\nusing different RL techniques to evaluate which one can be more useful to solve\nthis task. In the meta level, the meta learning model will use the metafeatures\nto extract knowledge that will be used to predict the best algorithm for each\nuser. We evaluated an implementation of this framework using MovieLens 20M\ndataset. Our implementation achieved consistent gains in the meta level,\nhowever, in the base level we only achieved marginal gains.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.0656,regular,pre_llm,2021,3,"{'ai_likelihood': 3.1391779581705734e-05, 'text': ""Heterogeneous Information Network-based Interest Composition with Graph\n  Neural Network for Recommendation\n\n  Heterogeneous information networks (HINs) are widely applied to\nrecommendation systems due to their capability of modeling various auxiliary\ninformation with meta-paths. However, existing HIN-based recommendation models\nusually fuse the information from various meta-paths by simple weighted sum or\nconcatenation, which limits performance improvement because it lacks the\ncapability of interest compositions among meta-paths. In this article, we\npropose an HIN-based Interest Composition model for Recommendation (HicRec).\nSpecifically, user and item representations are learned with a graph neural\nnetwork on both the graph structure and features in each meta-path, and a\nparameter sharing mechanism is utilized here to ensure that the user and item\nrepresentations are in the same latent space. Then, users' interests in each\nitem from each pair of related meta-paths are calculated by a combination of\nthe user and item representations. The composed user interests are obtained by\ntheir single interest from both intra- and inter-meta-paths for recommendation.\nExtensive experiments are conducted on three real-world datasets and the\nresults demonstrate that our proposed HicRec model outperforms the baselines.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.08707,regular,pre_llm,2021,4,"{'ai_likelihood': 3.410710228814019e-06, 'text': 'Contextualized Query Embeddings for Conversational Search\n\n  This paper describes a compact and effective model for low-latency passage\nretrieval in conversational search based on learned dense representations.\nPrior to our work, the state-of-the-art approach uses a multi-stage pipeline\ncomprising conversational query reformulation and information retrieval\nmodules. Despite its effectiveness, such a pipeline often includes multiple\nneural models that require long inference times. In addition, independently\noptimizing each module ignores dependencies among them. To address these\nshortcomings, we propose to integrate conversational query reformulation\ndirectly into a dense retrieval model. To aid in this goal, we create a dataset\nwith pseudo-relevance labels for conversational search to overcome the lack of\ntraining data and to explore different training strategies. We demonstrate that\nour model effectively rewrites conversational queries as dense representations\nin conversational search and open-domain question answering datasets. Finally,\nafter observing that our model learns to adjust the $L_2$ norm of query token\nembeddings, we leverage this property for hybrid retrieval and to support error\nanalysis.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.0736,regular,pre_llm,2021,4,"{'ai_likelihood': 8.4837277730306e-05, 'text': ""DebiasedRec: Bias-aware User Modeling and Click Prediction for\n  Personalized News Recommendation\n\n  News recommendation is critical for personalized news access. Existing news\nrecommendation methods usually infer users' personal interest based on their\nhistorical clicked news, and train the news recommendation models by predicting\nfuture news clicks. A core assumption behind these methods is that news click\nbehaviors can indicate user interest. However, in practical scenarios, beyond\nthe relevance between user interest and news content, the news click behaviors\nmay also be affected by other factors, such as the bias of news presentation in\nthe online platform. For example, news with higher positions and larger sizes\nare usually more likely to be clicked. The bias of clicked news may bring\nnoises to user interest modeling and model training, which may hurt the\nperformance of the news recommendation model.\n  In this paper, we propose a bias-aware personalized news recommendation\nmethod named DebiasRec, which can handle the bias information for more accurate\nuser interest inference and model training. The core of our method includes a\nbias representation module, a bias-aware user modeling module, and a bias-aware\nclick prediction module. The bias representation module is used to model\ndifferent kinds of news bias and their interactions to capture their joint\neffect on click behaviors. The bias-aware user modeling module aims to infer\nusers' debiased interest from the clicked news articles by using their bias\ninformation to calibrate the interest model. The bias-aware click prediction\nmodule is used to train a debiased news recommendation model from the biased\nclick behaviors, where the click score is decomposed into a preference score\nindicating user's interest in the news content and a news bias score inferred\nfrom its different bias features. Experiments on two real-world datasets show\nthat our method can effectively improve the performance of news recommendation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.06312,regular,pre_llm,2021,4,"{'ai_likelihood': 1.7053551144070097e-05, 'text': 'A Non-sequential Approach to Deep User Interest Model for CTR Prediction\n\n  Click-Through Rate (CTR) prediction plays an important role in many\nindustrial applications, and recently a lot of attention is paid to the deep\ninterest models which use attention mechanism to capture user interests from\nhistorical behaviors. However, most current models are based on sequential\nmodels which truncate the behavior sequences by a fixed length, thus have\ndifficulties in handling very long behavior sequences. Another big problem is\nthat sequences with the same length can be quite different in terms of time,\ncarrying completely different meanings. In this paper, we propose a\nnon-sequential approach to tackle the above problems. Specifically, we first\nrepresent the behavior data in a sparse key-vector format, where the vector\ncontains rich behavior info such as time, count and category. Next, we enhance\nthe Deep Interest Network to take such rich information into account by a novel\nattention network. The sparse representation makes it practical to handle large\nscale long behavior sequences. Finally, we introduce a multidimensional\npartition framework to mine behavior interactions. The framework can partition\ndata into custom designed time buckets to capture the interactions among\ninformation aggregated in different time buckets. Similarly, it can also\npartition the data into different categories and capture the interactions among\nthem. Experiments are conducted on two public datasets: one is an advertising\ndataset and the other is a production recommender dataset. Our models\noutperform other state-of-the-art models on both datasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.1205,regular,pre_llm,2021,4,"{'ai_likelihood': 1.0960631900363498e-05, 'text': 'Attention on Global-Local Representation Spaces in Recommender Systems\n\n  In this study, we present a novel clustering-based collaborative filtering\n(CF) method for recommender systems. Clustering-based CF methods can\neffectively deal with data sparsity and scalability problems. However, most of\nthem are applied to a single representation space, which might not characterize\ncomplex user-item interactions well. We argue that the user-item interactions\nshould be observed from multiple views and characterized in an adaptive way. To\naddress this issue, we leveraged the global and local properties to construct\nmultiple representation spaces by learning various training datasets and loss\nfunctions. An attention network was built to generate a blended representation\naccording to the relative importance of the representation spaces for each\nuser-item pair, providing a flexible way to characterize diverse user-item\ninteractions. Substantial experiments were evaluated on four popular benchmark\ndatasets. The results show that the proposed method is superior to several CF\nmethods where only one representation space is considered.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.08558,regular,pre_llm,2021,4,"{'ai_likelihood': 5.993578169080946e-06, 'text': ""ASBERT: Siamese and Triplet network embedding for open question\n  answering\n\n  Answer selection (AS) is an essential subtask in the field of natural\nlanguage processing with an objective to identify the most likely answer to a\ngiven question from a corpus containing candidate answer sentences. A common\napproach to address the AS problem is to generate an embedding for each\ncandidate sentence and query. Then, select the sentence whose vector\nrepresentation is closest to the query's. A key drawback is the low quality of\nthe embeddings, hitherto, based on its performance on AS benchmark datasets. In\nthis work, we present ASBERT, a framework built on the BERT architecture that\nemploys Siamese and Triplet neural networks to learn an encoding function that\nmaps a text to a fixed-size vector in an embedded space. The notion of distance\nbetween two points in this space connotes similarity in meaning between two\ntexts. Experimental results on the WikiQA and TrecQA datasets demonstrate that\nour proposed approach outperforms many state-of-the-art baseline methods.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.12302,regular,pre_llm,2021,4,"{'ai_likelihood': 1.1920928955078125e-06, 'text': 'A unified Neural Network Approach to E-CommerceRelevance Learning\n\n  Result relevance scoring is critical to e-commerce search user experience.\nTraditional information retrieval methods focus on keyword matching and\nhand-crafted or counting-based numeric features, with limited understanding of\nitem semantic relevance. We describe a highly-scalable feed-forward neural\nmodel to provide relevance score for (query, item) pairs, using only user query\nand item title as features, and both user click feedback as well as limited\nhuman ratings as labels. Several general enhancements were applied to further\noptimize eval/test metrics, including Siamese pairwise architecture, random\nbatch negative co-training, and point-wise fine-tuning. We found significant\nimprovement over GBDT baseline as well as several off-the-shelf deep-learning\nbaselines on an independently constructed ratings dataset. The GBDT model\nrelies on 10 times more features. We also present metrics for select subset\ncombinations of techniques mentioned above.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.07414,regular,pre_llm,2021,4,"{'ai_likelihood': 2.21199459499783e-05, 'text': 'Hyperbolic Neural Collaborative Recommender\n\n  This paper explores the use of hyperbolic geometry and deep learning\ntechniques for recommendation. We present Hyperbolic Neural Collaborative\nRecommender (HNCR), a deep hyperbolic representation learning method that\nexploits mutual semantic relations among users/items for collaborative\nfiltering (CF) tasks. HNCR contains two major phases: neighbor construction and\nrecommendation framework. The first phase introduces a neighbor construction\nstrategy to construct a semantic neighbor set for each user and item according\nto the user-item historical interaction. In the second phase, we develop a deep\nframework based on hyperbolic geometry to integrate constructed neighbor sets\ninto recommendation. Via a series of extensive experiments, we show that HNCR\noutperforms its Euclidean counterpart and state-of-the-art baselines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.04556,regular,pre_llm,2021,4,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'A Probabilistic Framework for Lexicon-based Keyword Spotting in\n  Handwritten Text Images\n\n  Query by String Keyword Spotting (KWS) is here considered as a key technology\nfor indexing large collections of handwritten text images to allow fast textual\naccess to the contents of these collections. Under this perspective, a\nprobabilistic framework for lexicon-based KWS in text images is presented. The\npresentation aims at providing a tutorial view that helps to understand the\nrelations between classical statements of KWS and the relative challenges\nentailed by these statements. More specifically, the development of the\nproposed framework makes it self-evident that word recognition or\nclassification implicitly or explicitly underlies any formulation of KWS.\nMoreover, it clearly suggests that the same statistical models and training\nmethods successfully used for handwriting text recognition can advantageously\nbe used also for KWS, even though KWS does not generally require or rely on any\nkind of previously produced image transcripts. These ideas are developed into a\nspecific, probabilistically sound approach for segmentation-free,\nlexicon-based, query-by-string KWS. Experiments carried out using this approach\nare presented, which support the consistency and general interest of the\nproposed framework. Several datasets, traditionally used for KWS benchmarking\nare considered, with results significantly better than those previously\npublished for these datasets. In addition, results on two new, larger\nhandwritten text image datasets are reported, showing the great potential of\nthe methods proposed in this paper for indexing and textual search in large\ncollections of handwritten documents.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.00867,regular,pre_llm,2021,4,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Online Product Feature Recommendations with Interpretable Machine\n  Learning\n\n  Product feature recommendations are critical for online customers to purchase\nthe right products based on the right features. For a customer, selecting the\nproduct that has the best trade-off between price and functionality is a\ntime-consuming step in an online shopping experience, and customers can be\noverwhelmed by the available choices. However, determining the set of product\nfeatures that most differentiate a particular product is still an open question\nin online recommender systems. In this paper, we focus on using interpretable\nmachine learning methods to tackle this problem. First, we identify this unique\nproduct feature recommendation problem from a business perspective on a major\nUS e-commerce site. Second, we formulate the problem into a price-driven\nsupervised learning problem to discover the product features that could best\nexplain the price of a product in a given product category. We build machine\nlearning models with a model-agnostic method Shapley Values to understand the\nimportance of each feature, rank and recommend the most essential features.\nThird, we leverage human experts to evaluate its relevancy. The results show\nthat our method is superior to a strong baseline method based on customer\nbehavior and significantly boosts the coverage by 45%. Finally, our proposed\nmethod shows comparable conversion rate against the baseline in online A/B\ntests.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.10083,regular,pre_llm,2021,4,"{'ai_likelihood': 8.033381568060981e-05, 'text': 'Personalized News Recommendation with Knowledge-aware Interactive\n  Matching\n\n  The most important task in personalized news recommendation is accurate\nmatching between candidate news and user interest. Most of existing news\nrecommendation methods model candidate news from its textual content and user\ninterest from their clicked news in an independent way. However, a news article\nmay cover multiple aspects and entities, and a user usually has different kinds\nof interest. Independent modeling of candidate news and user interest may lead\nto inferior matching between news and users. In this paper, we propose a\nknowledge-aware interactive matching method for news recommendation. Our method\ninteractively models candidate news and user interest to facilitate their\naccurate matching. We design a knowledge-aware news co-encoder to interactively\nlearn representations for both clicked news and candidate news by capturing\ntheir relatedness in both semantic and entities with the help of knowledge\ngraphs. We also design a user-news co-encoder to learn candidate news-aware\nuser interest representation and user-aware candidate news representation for\nbetter interest matching. Experiments on two real-world datasets validate that\nour method can effectively improve the performance of news recommendation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.1208,regular,pre_llm,2021,4,"{'ai_likelihood': 1.0927518208821616e-06, 'text': ""AdsGNN: Behavior-Graph Augmented Relevance Modeling in Sponsored Search\n\n  Sponsored search ads appear next to search results when people look for\nproducts and services on search engines. In recent years, they have become one\nof the most lucrative channels for marketing. As the fundamental basis of\nsearch ads, relevance modeling has attracted increasing attention due to the\nsignificant research challenges and tremendous practical value. Most existing\napproaches solely rely on the semantic information in the input query-ad pair,\nwhile the pure semantic information in the short ads data is not sufficient to\nfully identify user's search intents. Our motivation lies in incorporating the\ntremendous amount of unsupervised user behavior data from the historical search\nlogs as the complementary graph to facilitate relevance modeling. In this\npaper, we extensively investigate how to naturally fuse the semantic textual\ninformation with the user behavior graph, and further propose three novel\nAdsGNN models to aggregate topological neighborhood from the perspectives of\nnodes, edges and tokens. Furthermore, two critical but rarely investigated\nproblems, domain-specific pre-training and long-tail ads matching, are studied\nthoroughly. Empirically, we evaluate the AdsGNN models over the large industry\ndataset, and the experimental results of online/offline tests consistently\ndemonstrate the superiority of our proposal.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.07969,regular,pre_llm,2021,4,"{'ai_likelihood': 8.27842288547092e-07, 'text': ""Hierarchical Topic Presence Models\n\n  Topic models analyze text from a set of documents. Documents are modeled as a\nmixture of topics, with topics defined as probability distributions on words.\nInferences of interest include the most probable topics and characterization of\na topic by inspecting the topic's highest probability words. Motivated by a\ndata set of web pages (documents) nested in web sites, we extend the Poisson\nfactor analysis topic model to hierarchical topic presence models for analyzing\ntext from documents nested in known groups. We incorporate an unknown binary\ntopic presence parameter for each topic at the web site and/or the web page\nlevel to allow web sites and/or web pages to be sparse mixtures of topics and\nwe propose logistic regression modeling of topic presence conditional on web\nsite covariates. We introduce local topics into the Poisson factor analysis\nframework, where each web site has a local topic not found in other web sites.\nTwo data augmentation methods, the Chinese table distribution and\nP\\'{o}lya-Gamma augmentation, aid in constructing our sampler. We analyze text\nfrom web pages nested in United States local public health department web sites\nto abstract topical information and understand national patterns in topic\npresence.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.08926,review,pre_llm,2021,4,"{'ai_likelihood': 5.728668636745877e-06, 'text': 'Transitivity, Time Consumption, and Quality of Preference Judgments in\n  Crowdsourcing\n\n  Preference judgments have been demonstrated as a better alternative to graded\njudgments to assess the relevance of documents relative to queries. Existing\nwork has verified transitivity among preference judgments when collected from\ntrained judges, which reduced the number of judgments dramatically. Moreover,\nstrict preference judgments and weak preference judgments, where the latter\nadditionally allow judges to state that two documents are equally relevant for\na given query, are both widely used in literature. However, whether\ntransitivity still holds when collected from crowdsourcing, i.e., whether the\ntwo kinds of preference judgments behave similarly remains unclear. In this\nwork, we collect judgments from multiple judges using a crowdsourcing platform\nand aggregate them to compare the two kinds of preference judgments in terms of\ntransitivity, time consumption, and quality. That is, we look into whether\naggregated judgments are transitive, how long it takes judges to make them, and\nwhether judges agree with each other and with judgments from TREC. Our key\nfindings are that only strict preference judgments are transitive. Meanwhile,\nweak preference judgments behave differently in terms of transitivity, time\nconsumption, as well as of quality of judgment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.07407,regular,pre_llm,2021,4,"{'ai_likelihood': 3.1391779581705734e-05, 'text': 'MM-Rec: Multimodal News Recommendation\n\n  Accurate news representation is critical for news recommendation. Most of\nexisting news representation methods learn news representations only from news\ntexts while ignore the visual information in news like images. In fact, users\nmay click news not only because of the interest in news titles but also due to\nthe attraction of news images. Thus, images are useful for representing news\nand predicting user behaviors. In this paper, we propose a multimodal news\nrecommendation method, which can incorporate both textual and visual\ninformation of news to learn multimodal news representations. We first extract\nregion-of-interests (ROIs) from news images via object detection. Then we use a\npre-trained visiolinguistic model to encode both news texts and news image ROIs\nand model their inherent relatedness using co-attentional Transformers. In\naddition, we propose a crossmodal candidate-aware attention network to select\nrelevant historical clicked news for accurate user modeling by measuring the\ncrossmodal relatedness between clicked news and candidate news. Experiments\nvalidate that incorporating multimodal news information can effectively improve\nnews recommendation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.00919,regular,pre_llm,2021,4,"{'ai_likelihood': 2.947118547227648e-06, 'text': ""Fast-adapting and Privacy-preserving Federated Recommender System\n\n  In the mobile Internet era, the recommender system has become an\nirreplaceable tool to help users discover useful items, and thus alleviating\nthe information overload problem. Recent deep neural network (DNN)-based\nrecommender system research have made significant progress in improving\nprediction accuracy, which is largely attributed to the access to a large\namount of users' personal data collected from users' devices and then centrally\nstored in the cloud server. However, as there are rising concerns around the\nglobe on user privacy leakage in the online platform, the public is becoming\nanxious by such abuse of user privacy. Therefore, it is urgent and beneficial\nto develop a recommender system that can achieve both high prediction accuracy\nand high degree of user privacy protection.\n  To this end, we propose a DNN-based recommendation model called PrivRec\nrunning on the decentralized federated learning (FL) environment, which ensures\nthat a user's data never leaves his/her during the course of model training. On\nthe other hand, to better embrace the data heterogeneity commonly existing in\nFL, we innovatively introduce a first-order meta-learning method that enables\nfast in-device personalization with only few data points. Furthermore, to\ndefense from potential malicious participant that poses serious security threat\nto other users, we develop a user-level differentially private DP-PrivRec model\nso that it is unable to determine whether a particular user is present or not\nsolely based on the trained model. Finally, we conduct extensive experiments on\ntwo large-scale datasets in a simulated FL environment, and the results\nvalidate the superiority of our proposed PrivRec and DP-PrivRec.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.09791,regular,pre_llm,2021,4,"{'ai_likelihood': 7.682376437717014e-06, 'text': ""B-PROP: Bootstrapped Pre-training with Representative Words Prediction\n  for Ad-hoc Retrieval\n\n  Pre-training and fine-tuning have achieved remarkable success in many\ndownstream natural language processing (NLP) tasks. Recently, pre-training\nmethods tailored for information retrieval (IR) have also been explored, and\nthe latest success is the PROP method which has reached new SOTA on a variety\nof ad-hoc retrieval benchmarks. The basic idea of PROP is to construct the\n\\textit{representative words prediction} (ROP) task for pre-training inspired\nby the query likelihood model. Despite its exciting performance, the\neffectiveness of PROP might be bounded by the classical unigram language model\nadopted in the ROP task construction process. To tackle this problem, we\npropose a bootstrapped pre-training method (namely B-PROP) based on BERT for\nad-hoc retrieval. The key idea is to use the powerful contextual language model\nBERT to replace the classical unigram language model for the ROP task\nconstruction, and re-train BERT itself towards the tailored objective for IR.\nSpecifically, we introduce a novel contrastive method, inspired by the\ndivergence-from-randomness idea, to leverage BERT's self-attention mechanism to\nsample representative words from the document. By further fine-tuning on\ndownstream ad-hoc retrieval tasks, our method achieves significant improvements\nover baselines without pre-training or with other pre-training methods, and\nfurther pushes forward the SOTA on a variety of ad-hoc retrieval tasks.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.08912,regular,pre_llm,2021,4,"{'ai_likelihood': 5.960464477539062e-07, 'text': ""The Simpson's Paradox in the Offline Evaluation of Recommendation\n  Systems\n\n  Recommendation systems are often evaluated based on user's interactions that\nwere collected from an existing, already deployed recommendation system. In\nthis situation, users only provide feedback on the exposed items and they may\nnot leave feedback on other items since they have not been exposed to them by\nthe deployed system. As a result, the collected feedback dataset that is used\nto evaluate a new model is influenced by the deployed system, as a form of\nclosed loop feedback. In this paper, we show that the typical offline\nevaluation of recommender systems suffers from the so-called Simpson's paradox.\nSimpson's paradox is the name given to a phenomenon observed when a significant\ntrend appears in several different sub-populations of observational data but\ndisappears or is even reversed when these sub-populations are combined\ntogether. Our in-depth experiments based on stratified sampling reveal that a\nvery small minority of items that are frequently exposed by the deployed system\nplays a confounding factor in the offline evaluation of recommendation systems.\nIn addition, we propose a novel evaluation methodology that takes into account\nthe confounder, i.e the deployed system's characteristics. Using the relative\ncomparison of many recommendation models as in the typical offline evaluation\nof recommender systems, and based on the Kendall rank correlation coefficient,\nwe show that our proposed evaluation methodology exhibits statistically\nsignificant improvements of 14% and 40% on the examined open loop datasets\n(Yahoo! and Coat), respectively, in reflecting the true ranking of systems with\nan open loop (randomised) evaluation in comparison to the standard evaluation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.0086,regular,pre_llm,2021,4,"{'ai_likelihood': 3.344482845730252e-06, 'text': 'GRN: Generative Rerank Network for Context-wise Recommendation\n\n  Reranking is attracting incremental attention in the recommender systems,\nwhich rearranges the input ranking list into the final rank-ing list to better\nmeet user demands. Most existing methods greedily rerank candidates through the\nrating scores from point-wise or list-wise models. Despite effectiveness,\nneglecting the mutual influence between each item and its contexts in the final\nranking list often makes the greedy strategy based reranking methods\nsub-optimal. In this work, we propose a new context-wise reranking framework\nnamed Generative Rerank Network (GRN). Specifically, we first design the\nevaluator, which applies Bi-LSTM and self-attention mechanism to model the\ncontextual information in the labeled final ranking list and predict the\ninteraction probability of each item more precisely. Afterwards, we elaborate\non the generator, equipped with GRU, attention mechanism and pointer network to\nselect the item from the input ranking list step by step. Finally, we apply\ncross-entropy loss to train the evaluator and, subsequently, policy gradient to\noptimize the generator under the guidance of the evaluator. Empirical results\nshow that GRN consistently and significantly outperforms state-of-the-art\npoint-wise and list-wise methods. Moreover, GRN has achieved a performance\nimprovement of 5.2% on PV and 6.1% on IPV metric after the successful\ndeployment in one popular recommendation scenario of Taobao application.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.08523,regular,pre_llm,2021,4,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'Co-BERT: A Context-Aware BERT Retrieval Model Incorporating Local and\n  Query-specific Context\n\n  BERT-based text ranking models have dramatically advanced the\nstate-of-the-art in ad-hoc retrieval, wherein most models tend to consider\nindividual query-document pairs independently. In the mean time, the importance\nand usefulness to consider the cross-documents interactions and the\nquery-specific characteristics in a ranking model have been repeatedly\nconfirmed, mostly in the context of learning to rank. The BERT-based ranking\nmodel, however, has not been able to fully incorporate these two types of\nranking context, thereby ignoring the inter-document relationships from the\nranking and the differences among queries. To mitigate this gap, in this work,\nan end-to-end transformer-based ranking model, named Co-BERT, has been proposed\nto exploit several BERT architectures to calibrate the query-document\nrepresentations using pseudo relevance feedback before modeling the relevance\nof a group of documents jointly. Extensive experiments on two standard test\ncollections confirm the effectiveness of the proposed model in improving the\nperformance of text re-ranking over strong fine-tuned BERT-Base baselines. We\nplan to make our implementation open source to enable further comparisons.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.05796,regular,pre_llm,2021,4,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'On the instability of embeddings for recommender systems: the case of\n  Matrix Factorization\n\n  Most state-of-the-art top-N collaborative recommender systems work by\nlearning embeddings to jointly represent users and items. Learned embeddings\nare considered to be effective to solve a variety of tasks. Among others,\nproviding and explaining recommendations. In this paper we question the\nreliability of the embeddings learned by Matrix Factorization (MF). We\nempirically demonstrate that, by simply changing the initial values assigned to\nthe latent factors, the same MF method generates very different embeddings of\nitems and users, and we highlight that this effect is stronger for less popular\nitems. To overcome these drawbacks, we present a generalization of MF, called\nNearest Neighbors Matrix Factorization (NNMF). The new method propagates the\ninformation about items and users to their neighbors, speeding up the training\nprocedure and extending the amount of information that supports recommendations\nand representations. We describe the NNMF variants of three common MF\napproaches, and with extensive experiments on five different datasets we show\nthat they strongly mitigate the instability issues of the original MF versions\nand they improve the accuracy of recommendations on the long-tail.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.00062,regular,pre_llm,2021,5,"{'ai_likelihood': 4.006756676567926e-06, 'text': 'Controllable Gradient Item Retrieval\n\n  In this paper, we identify and study an important problem of gradient item\nretrieval. We define the problem as retrieving a sequence of items with a\ngradual change on a certain attribute, given a reference item and a\nmodification text. For example, after a customer saw a white dress, she/he\nwants to buy a similar one but more floral on it. The extent of ""more floral""\nis subjective, thus prompting one floral dress is hard to satisfy the\ncustomer\'s needs. A better way is to present a sequence of products with\nincreasingly floral attributes based on the white dress, and allow the customer\nto select the most satisfactory one from the sequence. Existing item retrieval\nmethods mainly focus on whether the target items appear at the top of the\nretrieved sequence, but ignore the demand for retrieving a sequence of products\nwith gradual change on a certain attribute. To deal with this problem, we\npropose a weakly-supervised method that can learn a disentangled item\nrepresentation from user-item interaction data and ground the semantic meaning\nof attributes to dimensions of the item representation. Our method takes a\nreference item and a modification as a query. During inference, we start from\nthe reference item and ""walk"" along the direction of the modification in the\nitem representation space to retrieve a sequence of items in a gradient manner.\nWe demonstrate our proposed method can achieve disentanglement through weak\nsupervision. Besides, we empirically show that an item sequence retrieved by\nour method is gradually changed on an indicated attribute and, in the item\nretrieval task, our method outperforms existing approaches on three different\ndatasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.08246,regular,pre_llm,2021,5,"{'ai_likelihood': 2.6490953233506946e-07, 'text': ""Path-based Deep Network for Candidate Item Matching in Recommenders\n\n  The large-scale recommender system mainly consists of two stages: matching\nand ranking. The matching stage (also known as the retrieval step) identifies a\nsmall fraction of relevant items from billion-scale item corpus in low latency\nand computational cost. Item-to-item collaborative filter (item-based CF) and\nembedding-based retrieval (EBR) have been long used in the industrial matching\nstage owing to its efficiency. However, item-based CF is hard to meet\npersonalization, while EBR has difficulty in satisfying diversity. In this\npaper, we propose a novel matching architecture, Path-based Deep Network (named\nPDN), which can incorporate both personalization and diversity to enhance\nmatching performance. Specifically, PDN is comprised of two modules: Trigger\nNet and Similarity Net. PDN utilizes Trigger Net to capture the user's interest\nin each of his/her interacted item, and Similarity Net to evaluate the\nsimilarity between each interacted item and the target item based on these\nitems' profile and CF information. The final relevance between the user and the\ntarget item is calculated by explicitly considering user's diverse interests,\n\\ie aggregating the relevance weights of the related two-hop paths (one hop of\na path corresponds to user-item interaction and the other to item-item\nrelevance). Furthermore, we describe the architecture design of a matching\nsystem with the proposed PDN in a leading real-world E-Commerce service (Mobile\nTaobao App). Based on offline evaluations and online A/B test, we show that PDN\noutperforms the existing solutions for the same task. The online results also\ndemonstrate that PDN can retrieve more personalized and more diverse relevant\nitems to significantly improve user engagement. Currently, PDN system has been\nsuccessfully deployed at Mobile Taobao App and handling major online traffic.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.06365,regular,pre_llm,2021,5,"{'ai_likelihood': 2.284844716389974e-06, 'text': 'Semantic Table Retrieval using Keyword and Table Queries\n\n  Tables on the Web contain a vast amount of knowledge in a structured form. To\ntap into this valuable resource, we address the problem of table retrieval:\nanswering an information need with a ranked list of tables. We investigate this\nproblem in two different variants, based on how the information need is\nexpressed: as a keyword query or as an existing table (""query-by-table""). The\nmain novel contribution of this work is a semantic table retrieval framework\nfor matching information needs (keyword or table queries) against tables.\nSpecifically, we (i) represent queries and tables in multiple semantic spaces\n(both discrete sparse and continuous dense vector representations) and (ii)\nintroduce various similarity measures for matching those semantic\nrepresentations. We consider all possible combinations of semantic\nrepresentations and similarity measures and use these as features in a\nsupervised learning model. Using two purpose-built test collections based on\nWikipedia tables, we demonstrate significant and substantial improvements over\nstate-of-the-art baselines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.04266,regular,pre_llm,2021,5,"{'ai_likelihood': 3.6093923780653213e-06, 'text': 'A Probabilistic Approach to Personalize Type-based Facet Ranking for POI\n  Suggestion\n\n  Faceted Search Systems (FSS) have become one of the main search interfaces\nused in vertical search systems, offering users meaningful facets to refine\ntheir search query and narrow down the results quickly to find the intended\nsearch target. This work focuses on the problem of ranking type-based facets.\nIn a structured information space, type-based facets (t-facets) indicate the\ncategory to which each object belongs. When they belong to a large multi-level\ntaxonomy, it is desirable to rank them separately before ranking other facet\ngroups. This helps the searcher in filtering the results according to their\ntype first. This also makes it easier to rank the rest of the facets once the\ntype of the intended search target is selected. Existing research employs the\nsame ranking methods for different facet groups. In this research, we propose a\ntwo-step approach to personalize t-facet ranking. The first step assigns a\nrelevance score to each individual leaf-node t-facet. The score is generated\nusing probabilistic models and it reflects t-facet relevance to the query and\nthe user profile. In the second step, this score is used to re-order and select\nthe sub-tree to present to the user. We investigate the usefulness of the\nproposed method to a Point Of Interest (POI) suggestion task. Our evaluation\naims at capturing the user effort required to fulfil her search needs by using\nthe ranked facets. The proposed approach achieved better results than other\nexisting personalized baselines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.14599,regular,pre_llm,2021,5,"{'ai_likelihood': 1.4238887363009983e-06, 'text': ""Personalization in E-Grocery: Top-N versus Top-k Rankings\n\n  Business success in e-commerce depends on customer perceived value. A\ncustomer with high perceived value buys, returns, and recommends items. The\nperceived value is at risk whenever the information load harms users' shopping\nexperience. In e-grocery, shoppers face an overwhelming number of items, the\nmajority of which is irrelevant for the shopper. Recommender systems (RS)\nenable businesses to master information overload (IO) by providing users with\nan item ranking by relevance. Prior work proposes RS with short personalized\nrankings (top-k). Given large order sizes and high user heterogeneity in\ne-grocery, top-k RS are insufficient to diminish IO in this domain. To fill\nthis gap and raise business performance, this paper introduces an RS with a\npersonalized long ranking (top-N). Undertaking a randomized field experiment,\nthe paper establishes the merit of shifting from top-k to top-N rankings.\nSpecifically, the proposed RS reduces IO by 29.4% and lowers users' search time\nby 3.3 seconds per item. The field experiment also reveals a 7% uplift in\nrevenue due to the top-N ranking. Substantial benefits for the customer and the\ncompany highlight the business value of top-N rankings as a new design\nrequirement for recommender systems in e-grocery.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.09009,review,pre_llm,2021,5,"{'ai_likelihood': 3.311369154188368e-08, 'text': ""An Overview of Computer Supported Query Formulation\n\n  Most present day organisations make use of some automated information system.\nThis usually means that a large body of vital corporate information is stored\nin these information systems. As a result, an essential function of information\nsystems should be the support of disclosure of this information.\n  We purposely use the term {\\em information disclosure} in this context. When\nusing the term information disclosure we envision a computer supported\nmechanism that allows for an easy and intuitive formulation of queries in a\nlanguage that is as close to the user's perception of the universe of discourse\nas possible.\n  From this point of view, it is only obvious that we do not consider a simple\nquery mechanism where users have to enter complex queries manually and look up\nwhat information is stored in a set of relational tables. Without a set of\nadequate information disclosure avenues an information system becomes worthless\nsince there is no use in storing information that will never be retrieved.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.06398,regular,pre_llm,2021,5,"{'ai_likelihood': 4.967053731282552e-07, 'text': '""Who can help me?"": Knowledge Infused Matching of Support Seekers and\n  Support Providers during COVID-19 on Reddit\n\n  During the ongoing COVID-19 crisis, subreddits on Reddit, such as\nr/Coronavirus saw a rapid growth in user\'s requests for help (support seekers -\nSSs) including individuals with varying professions and experiences with\ndiverse perspectives on care (support providers - SPs). Currently,\nknowledgeable human moderators match an SS with a user with relevant\nexperience, i.e, an SP on these subreddits. This unscalable process defers\ntimely care. We present a medical knowledge-infused approach to efficient\nmatching of SS and SPs validated by experts for the users affected by anxiety\nand depression, in the context of with COVID-19. After matching, each SP to an\nSS labeled as either supportive, informative, or similar (sharing experiences)\nusing the principles of natural language inference. Evaluation by 21 domain\nexperts indicates the efficacy of incorporated knowledge and shows the efficacy\nthe matching system.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.0479,regular,pre_llm,2021,5,"{'ai_likelihood': 4.967053731282553e-06, 'text': 'Learning to Warm Up Cold Item Embeddings for Cold-start Recommendation\n  with Meta Scaling and Shifting Networks\n\n  Recently, embedding techniques have achieved impressive success in\nrecommender systems. However, the embedding techniques are data demanding and\nsuffer from the cold-start problem. Especially, for the cold-start item which\nonly has limited interactions, it is hard to train a reasonable item ID\nembedding, called cold ID embedding, which is a major challenge for the\nembedding techniques. The cold item ID embedding has two main problems: (1) A\ngap is existing between the cold ID embedding and the deep model. (2) Cold ID\nembedding would be seriously affected by noisy interaction. However, most\nexisting methods do not consider both two issues in the cold-start problem,\nsimultaneously. To address these problems, we adopt two key ideas: (1) Speed up\nthe model fitting for the cold item ID embedding (fast adaptation). (2)\nAlleviate the influence of noise. Along this line, we propose Meta Scaling and\nShifting Networks to generate scaling and shifting functions for each item,\nrespectively. The scaling function can directly transform cold item ID\nembeddings into warm feature space which can fit the model better, and the\nshifting function is able to produce stable embeddings from the noisy\nembeddings. With the two meta networks, we propose Meta Warm Up Framework\n(MWUF) which learns to warm up cold ID embeddings. Moreover, MWUF is a general\nframework that can be applied upon various existing deep recommendation models.\nThe proposed model is evaluated on three popular benchmarks, including both\nrecommendation and advertising datasets. The evaluation results demonstrate its\nsuperior performance and compatibility.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.06067,review,pre_llm,2021,5,"{'ai_likelihood': 1.158979203965929e-06, 'text': 'Causal Intervention for Leveraging Popularity Bias in Recommendation\n\n  Recommender system usually faces popularity bias issues: from the data\nperspective, items exhibit uneven (long-tail) distribution on the interaction\nfrequency; from the method perspective, collaborative filtering methods are\nprone to amplify the bias by over-recommending popular items. It is undoubtedly\ncritical to consider popularity bias in recommender systems, and existing work\nmainly eliminates the bias effect. However, we argue that not all biases in the\ndata are bad -- some items demonstrate higher popularity because of their\nbetter intrinsic quality. Blindly pursuing unbiased learning may remove the\nbeneficial patterns in the data, degrading the recommendation accuracy and user\nsatisfaction.\n  This work studies an unexplored problem in recommendation -- how to leverage\npopularity bias to improve the recommendation accuracy. The key lies in two\naspects: how to remove the bad impact of popularity bias during training, and\nhow to inject the desired popularity bias in the inference stage that generates\ntop-K recommendations. This questions the causal mechanism of the\nrecommendation generation process. Along this line, we find that item\npopularity plays the role of confounder between the exposed items and the\nobserved interactions, causing the bad effect of bias amplification. To achieve\nour goal, we propose a new training and inference paradigm for recommendation\nnamed Popularity-bias Deconfounding and Adjusting (PDA). It removes the\nconfounding popularity bias in model training and adjusts the recommendation\nscore with desired popularity bias via causal intervention. We demonstrate the\nnew paradigm on latent factor model and perform extensive experiments on three\nreal-world datasets. Empirical studies validate that the deconfounded training\nis helpful to discover user real interests and the inference adjustment with\npopularity bias could further improve the recommendation accuracy.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.14566,regular,pre_llm,2021,5,"{'ai_likelihood': 4.602803124321832e-06, 'text': 'CNN Retrieval based Unsupervised Metric Learning for Near-Duplicated\n  Video Retrieval\n\n  As important data carriers, the drastically increasing number of multimedia\nvideos often brings many duplicate and near-duplicate videos in the top results\nof search. Near-duplicate video retrieval (NDVR) can cluster and filter out the\nredundant contents. In this paper, the proposed NDVR approach extracts the\nframe-level video representation based on convolutional neural network (CNN)\nfeatures from fully-connected layer and aggregated intermediate convolutional\nlayers. Unsupervised metric learning is used for similarity measurement and\nfeature matching. An efficient re-ranking algorithm combined with k-nearest\nneighborhood fuses the retrieval results from two levels of features and\nfurther improves the retrieval performance. Extensive experiments on the widely\nused CC\\_WEB\\_VIDEO dataset shows that the proposed approach exhibits superior\nperformance over the state-of-the-art.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.14909,regular,pre_llm,2021,5,"{'ai_likelihood': 4.801485273573134e-06, 'text': 'Generating Interesting Song-to-Song Segues With Dave\n\n  We introduce a novel domain-independent algorithm for generating interesting\nitem-to-item textual connections, or segues. Pivotal to our contribution is the\nintroduction of a scoring function for segues, based on their\n""interestingness"". We provide an implementation of our algorithm in the music\ndomain. We refer to our implementation as Dave. Dave is able to generate 1553\ndifferent types of segues, that can be broadly categorized as either\ninformative or funny. We evaluate Dave by comparing it against a curated source\nof song-to-song segues, called The Chain. In the case of informative segues, we\nfind that Dave can produce segues of the same quality, if not better, than\nthose to be found in The Chain. And, we report positive correlation between the\nvalues produced by our scoring function and human perceptions of segue quality.\nThe results highlight the validity of our method, and open future directions in\nthe application of segues to recommender systems research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.14688,regular,pre_llm,2021,5,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'Learning to Expand Audience via Meta Hybrid Experts and Critics for\n  Recommendation and Advertising\n\n  In recommender systems and advertising platforms, marketers always want to\ndeliver products, contents, or advertisements to potential audiences over media\nchannels such as display, video, or social. Given a set of audiences or\ncustomers (seed users), the audience expansion technique (look-alike modeling)\nis a promising solution to identify more potential audiences, who are similar\nto the seed users and likely to finish the business goal of the target\ncampaign. However, look-alike modeling faces two challenges: (1) In practice, a\ncompany could run hundreds of marketing campaigns to promote various contents\nwithin completely different categories every day, e.g., sports, politics,\nsociety. Thus, it is difficult to utilize a common method to expand audiences\nfor all campaigns. (2) The seed set of a certain campaign could only cover\nlimited users. Therefore, a customized approach based on such a seed set is\nlikely to be overfitting.\n  In this paper, to address these challenges, we propose a novel two-stage\nframework named Meta Hybrid Experts and Critics (MetaHeac) which has been\ndeployed in WeChat Look-alike System. In the offline stage, a general model\nwhich can capture the relationships among various tasks is trained from a\nmeta-learning perspective on all existing campaign tasks. In the online stage,\nfor a new campaign, a customized model is learned with the given seed set based\non the general model. According to both offline and online experiments, the\nproposed MetaHeac shows superior effectiveness for both content marketing\ncampaigns in recommender systems and advertising campaigns in advertising\nplatforms. Besides, MetaHeac has been successfully deployed in WeChat for the\npromotion of both contents and advertisements, leading to great improvement in\nthe quality of marketing. The code has been available at\n\\url{https://github.com/easezyc/MetaHeac}.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.04166,regular,pre_llm,2021,5,"{'ai_likelihood': 6.821420457628038e-06, 'text': ""Few-Shot Conversational Dense Retrieval\n\n  Dense retrieval (DR) has the potential to resolve the query understanding\nchallenge in conversational search by matching in the learned embedding space.\nHowever, this adaptation is challenging due to DR models' extra needs for\nsupervision signals and the long-tail nature of conversational search. In this\npaper, we present a Conversational Dense Retrieval system, ConvDR, that learns\ncontextualized embeddings for multi-turn conversational queries and retrieves\ndocuments solely using embedding dot products. In addition, we grant ConvDR\nfew-shot ability using a teacher-student framework, where we employ an ad hoc\ndense retriever as the teacher, inherit its document encodings, and learn a\nstudent query encoder to mimic the teacher embeddings on oracle reformulated\nqueries. Our experiments on TREC CAsT and OR-QuAC demonstrate ConvDR's\neffectiveness in both few-shot and fully-supervised settings. It outperforms\nprevious systems that operate in the sparse word space, matches the retrieval\naccuracy of oracle query reformulations, and is also more efficient thanks to\nits simplicity. Our analyses reveal that the advantages of ConvDR come from its\nability to capture informative context while ignoring the unrelated context in\nprevious conversation rounds. This makes ConvDR more effective as conversations\nevolve while previous systems may get confused by the increased noise from\nprevious turns. Our code is publicly available at\nhttps://github.com/thunlp/ConvDR.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.09179,review,pre_llm,2021,5,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'On Interpretation and Measurement of Soft Attributes for Recommendation\n\n  We address how to robustly interpret natural language refinements (or\ncritiques) in recommender systems. In particular, in human-human recommendation\nsettings people frequently use soft attributes to express preferences about\nitems, including concepts like the originality of a movie plot, the noisiness\nof a venue, or the complexity of a recipe. While binary tagging is extensively\nstudied in the context of recommender systems, soft attributes often involve\nsubjective and contextual aspects, which cannot be captured reliably in this\nway, nor be represented as objective binary truth in a knowledge base. This\nalso adds important considerations when measuring soft attribute ranking. We\npropose a more natural representation as personalized relative statements,\nrather than as absolute item properties. We present novel data collection\ntechniques and evaluation approaches, and a new public dataset. We also propose\na set of scoring approaches, from unsupervised to weakly supervised to fully\nsupervised, as a step towards interpreting and acting upon soft attribute based\ncritiques.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.03938,regular,pre_llm,2021,5,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Passage Retrieval for Outside-Knowledge Visual Question Answering\n\n  In this work, we address multi-modal information needs that contain text\nquestions and images by focusing on passage retrieval for outside-knowledge\nvisual question answering. This task requires access to outside knowledge,\nwhich in our case we define to be a large unstructured passage collection. We\nfirst conduct sparse retrieval with BM25 and study expanding the question with\nobject names and image captions. We verify that visual clues play an important\nrole and captions tend to be more informative than object names in sparse\nretrieval. We then construct a dual-encoder dense retriever, with the query\nencoder being LXMERT, a multi-modal pre-trained transformer. We further show\nthat dense retrieval significantly outperforms sparse retrieval that uses\nobject expansion. Moreover, dense retrieval matches the performance of sparse\nretrieval that leverages human-generated captions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.09981,regular,pre_llm,2021,5,"{'ai_likelihood': 5.430645412868924e-06, 'text': 'A Load Balanced Recommendation Approach\n\n  Recommender systems (RSs) are software tools and algorithms developed to\nalleviate the problem of information overload, which makes it difficult for a\nuser to make right decisions. Two main paradigms toward the recommendation\nproblem are collaborative filtering and content-based filtering, which try to\nrecommend the best items using ratings and content available. These methods\ntypically face infamous problems including cold-start, diversity, scalability,\nand great computational expense. We argue that the uptake of deep learning and\nreinforcement learning methods is also questionable due to their computational\ncomplexities and uninterpretability. In this paper, we approach the\nrecommendation problem from a new prospective. We borrow ideas from cluster\nhead selection algorithms in wireless sensor networks and adapt them to the\nrecommendation problem. In particular, we propose Load Balanced Recommender\nSystem (LBRS), which uses a probabilistic scheme for item recommendation.\nFurthermore, we factor in the importance of items in the recommendation\nprocess, which significantly improves the recommendation accuracy. We also\nintroduce a method that considers a heterogeneity among items, in order to\nbalance the similarity and diversity trade-off. Finally, we propose a new\nmetric for diversity, which emphasizes the importance of diversity not only\nfrom an intra-list perspective, but also from a between-list point of view.\nWith experiments in a simulation study performed on RecSim, we show that LBRS\nis effective and can outperform baseline methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.02354,regular,pre_llm,2021,5,"{'ai_likelihood': 9.834766387939453e-06, 'text': 'WTR: A Test Collection for Web Table Retrieval\n\n  We describe the development, characteristics and availability of a test\ncollection for the task of Web table retrieval, which uses a large-scale Web\nTable Corpora extracted from the Common Crawl. Since a Web table usually has\nrich context information such as the page title and surrounding paragraphs, we\nnot only provide relevance judgments of query-table pairs, but also the\nrelevance judgments of query-table context pairs with respect to a query, which\nare ignored by previous test collections. To facilitate future research with\nthis benchmark, we provide details about how the dataset is pre-processed and\nalso baseline results from both traditional and recently proposed table\nretrieval methods. Our experimental results show that proper usage of context\nlabels can benefit previous table retrieval methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.14068,regular,pre_llm,2021,5,"{'ai_likelihood': 3.7749608357747397e-06, 'text': 'Linear-Time Self Attention with Codeword Histogram for Efficient\n  Recommendation\n\n  Self-attention has become increasingly popular in a variety of sequence\nmodeling tasks from natural language processing to recommendation, due to its\neffectiveness. However, self-attention suffers from quadratic computational and\nmemory complexities, prohibiting its applications on long sequences. Existing\napproaches that address this issue mainly rely on a sparse attention context,\neither using a local window, or a permuted bucket obtained by\nlocality-sensitive hashing (LSH) or sorting, while crucial information may be\nlost. Inspired by the idea of vector quantization that uses cluster centroids\nto approximate items, we propose LISA (LInear-time Self Attention), which\nenjoys both the effectiveness of vanilla self-attention and the efficiency of\nsparse attention. LISA scales linearly with the sequence length, while enabling\nfull contextual attention via computing differentiable histograms of codeword\ndistributions. Meanwhile, unlike some efficient attention methods, our method\nposes no restriction on casual masking or sequence length. We evaluate our\nmethod on four real-world datasets for sequential recommendation. The results\nshow that LISA outperforms the state-of-the-art efficient attention methods in\nboth performance and speed; and it is up to 57x faster and 78x more memory\nefficient than vanilla self-attention.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.10124,regular,pre_llm,2021,5,"{'ai_likelihood': 3.443823920355903e-06, 'text': ""RLIRank: Learning to Rank with Reinforcement Learning for Dynamic Search\n\n  To support complex search tasks, where the initial information requirements\nare complex or may change during the search, a search engine must adapt the\ninformation delivery as the user's information requirements evolve. To support\nthis dynamic ranking paradigm effectively, search result ranking must\nincorporate both the user feedback received, and the information displayed so\nfar. To address this problem, we introduce a novel reinforcement learning-based\napproach, RLIrank. We first build an adapted reinforcement learning framework\nto integrate the key components of the dynamic search. Then, we implement a new\nLearning to Rank (LTR) model for each iteration of the dynamic search, using a\nrecurrent Long Short Term Memory neural network (LSTM), which estimates the\ngain for each next result, learning from each previously ranked document. To\nincorporate the user's feedback, we develop a word-embedding variation of the\nclassic Rocchio Algorithm, to help guide the ranking towards the high-value\ndocuments. Those innovations enable RLIrank to outperform the previously\nreported methods from the TREC Dynamic Domain Tracks 2017 and exceed all the\nmethods in 2016 TREC Dynamic Domain after multiple search iterations, advancing\nthe state of the art for dynamic search.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.05076,regular,pre_llm,2021,5,"{'ai_likelihood': 2.4172994825575088e-06, 'text': ""A Text Extraction-Based Smart Knowledge Graph Composition for\n  Integrating Lessons Learned during the Microchip Design\n\n  The production of microchips is a complex and thus well documented process.\nTherefore, available textual data about the production can be overwhelming in\nterms of quantity. This affects the visibility and retrieval of a certain piece\nof information when it is most needed. In this paper, we propose a dynamic\napproach to interlink the information extracted from multisource\nproduction-relevant documents through the creation of a knowledge graph. This\ngraph is constructed in order to support searchability and enhance user's\naccess to large-scale production information. Text mining methods are firstly\nutilized to extract data from multiple documentation sources. Document\nrelations are then mined and extracted for the composition of the knowledge\ngraph. Graph search functionality is then supported with a recommendation\nuse-case to enhance users' access to information that is related to the initial\ndocuments. The proposed approach is tailored to and tested on microchip\ndesign-relevant documents. It enhances the visibility and findability of\nprevious design-failure-cases during the process of a new chip design.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.03569,regular,pre_llm,2021,6,"{'ai_likelihood': 1.0596381293402777e-05, 'text': 'Socially-Aware Self-Supervised Tri-Training for Recommendation\n\n  Self-supervised learning (SSL), which can automatically generate ground-truth\nsamples from raw data, holds vast potential to improve recommender systems.\nMost existing SSL-based methods perturb the raw data graph with uniform\nnode/edge dropout to generate new data views and then conduct the\nself-discrimination based contrastive learning over different views to learn\ngeneralizable representations. Under this scheme, only a bijective mapping is\nbuilt between nodes in two different views, which means that the\nself-supervision signals from other nodes are being neglected. Due to the\nwidely observed homophily in recommender systems, we argue that the supervisory\nsignals from other nodes are also highly likely to benefit the representation\nlearning for recommendation. To capture these signals, a general socially-aware\nSSL framework that integrates tri-training is proposed in this paper.\nTechnically, our framework first augments the user data views with the user\nsocial information. And then under the regime of tri-training for multi-view\nencoding, the framework builds three graph encoders (one for recommendation)\nupon the augmented views and iteratively improves each encoder with\nself-supervision signals from other users, generated by the other two encoders.\nSince the tri-training operates on the augmented views of the same data sources\nfor self-supervision signals, we name it self-supervised tri-training.\nExtensive experiments on multiple real-world datasets consistently validate the\neffectiveness of the self-supervised tri-training framework for improving\nrecommendation. The code is released at https://github.com/Coder-Yu/QRec.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.0287,regular,pre_llm,2021,6,"{'ai_likelihood': 1.655684577094184e-06, 'text': ""Bidirectional Distillation for Top-K Recommender System\n\n  Recommender systems (RS) have started to employ knowledge distillation, which\nis a model compression technique training a compact model (student) with the\nknowledge transferred from a cumbersome model (teacher). The state-of-the-art\nmethods rely on unidirectional distillation transferring the knowledge only\nfrom the teacher to the student, with an underlying assumption that the teacher\nis always superior to the student. However, we demonstrate that the student\nperforms better than the teacher on a significant proportion of the test set,\nespecially for RS. Based on this observation, we propose Bidirectional\nDistillation (BD) framework whereby both the teacher and the student\ncollaboratively improve with each other. Specifically, each model is trained\nwith the distillation loss that makes to follow the other's prediction along\nwith its original loss function. For effective bidirectional distillation, we\npropose rank discrepancy-aware sampling scheme to distill only the informative\nknowledge that can fully enhance each other. The proposed scheme is designed to\neffectively cope with a large performance gap between the teacher and the\nstudent. Trained in the bidirectional way, it turns out that both the teacher\nand the student are significantly improved compared to when being trained\nseparately. Our extensive experiments on real-world datasets show that our\nproposed framework consistently outperforms the state-of-the-art competitors.\nWe also provide analyses for an in-depth understanding of BD and ablation\nstudies to verify the effectiveness of each proposed component.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.06244,regular,pre_llm,2021,6,"{'ai_likelihood': 7.947285970052084e-07, 'text': ""Predicting Knowledge Gain during Web Search based on Multimedia Resource\n  Consumption\n\n  In informal learning scenarios the popularity of multimedia content, such as\nvideo tutorials or lectures, has significantly increased. Yet, the users'\ninteractions, navigation behavior, and consequently learning outcome, have not\nbeen researched extensively. Related work in this field, also called search as\nlearning, has focused on behavioral or text resource features to predict\nlearning outcome and knowledge gain. In this paper, we investigate whether we\ncan exploit features representing multimedia resource consumption to predict of\nknowledge gain (KG) during Web search from in-session data, that is without\nprior knowledge about the learner. For this purpose, we suggest a set of\nmultimedia features related to image and video consumption. Our feature\nextraction is evaluated in a lab study with 113 participants where we collected\ndata for a given search as learning task on the formation of thunderstorms and\nlightning. We automatically analyze the monitored log data and utilize\nstate-of-the-art computer vision methods to extract features about the seen\nmultimedia resources. Experimental results demonstrate that multimedia features\ncan improve KG prediction. Finally, we provide an analysis on feature\nimportance (text and multimedia) for KG prediction.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.07316,regular,pre_llm,2021,6,"{'ai_likelihood': 1.953707800971137e-06, 'text': 'Exploiting Sentence-Level Representations for Passage Ranking\n\n  Recently, pre-trained contextual models, such as BERT, have shown to perform\nwell in language related tasks. We revisit the design decisions that govern the\napplicability of these models for the passage re-ranking task in open-domain\nquestion answering. We find that common approaches in the literature rely on\nfine-tuning a pre-trained BERT model and using a single, global representation\nof the input, discarding useful fine-grained relevance signals in token- or\nsentence-level representations. We argue that these discarded tokens hold\nuseful information that can be leveraged. In this paper, we explicitly model\nthe sentence-level representations by using Dynamic Memory Networks (DMNs) and\nconduct empirical evaluation to show improvements in passage re-ranking over\nfine-tuned vanilla BERT models by memory-enhanced explicit sentence modelling\non a diverse set of open-domain QA datasets. We further show that freezing the\nBERT model and only training the DMN layer still comes close to the original\nperformance, while improving training efficiency drastically. This indicates\nthat the usual fine-tuning step mostly helps to aggregate the inherent\ninformation in a single output token, as opposed to adapting the whole model to\nthe new task, and only achieves rather small gains.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.1212,regular,pre_llm,2021,6,"{'ai_likelihood': 1.1324882507324219e-05, 'text': 'Improving Transformer-based Sequential Recommenders through Preference\n  Editing\n\n  One of the key challenges in Sequential Recommendation (SR) is how to extract\nand represent user preferences. Traditional SR methods rely on the next item as\nthe supervision signal to guide preference extraction and representation. We\npropose a novel learning strategy, named preference editing. The idea is to\nforce the SR model to discriminate the common and unique preferences in\ndifferent sequences of interactions between users and the recommender system.\nBy doing so, the SR model is able to learn how to identify common and unique\nuser preferences, and thereby do better user preference extraction and\nrepresentation. We propose a transformer based SR model, named MrTransformer\n(Multi-preference Transformer), that concatenates some special tokens in front\nof the sequence to represent multiple user preferences and makes sure they\ncapture different aspects through a preference coverage mechanism. Then, we\ndevise a preference editing-based self-supervised learning mechanism for\ntraining MrTransformer which contains two main operations: preference\nseparation and preference recombination. The former separates the common and\nunique user preferences for a given pair of sequences. The latter swaps the\ncommon preferences to obtain recombined user preferences for each sequence.\nBased on the preference separation and preference recombination operations, we\ndefine two types of SSL loss that require that the recombined preferences are\nsimilar to the original ones, and the common preferences are close to each\nother.\n  We carry out extensive experiments on two benchmark datasets. MrTransformer\nwith preference editing significantly outperforms state-of-the-art SR methods\nin terms of Recall, MRR and NDCG. We find that long sequences whose user\npreferences are harder to extract and represent benefit most from preference\nediting.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.12085,regular,pre_llm,2021,6,"{'ai_likelihood': 1.556343502468533e-06, 'text': 'A Graph-based Method for Session-based Recommendations\n\n  We present a graph-based approach for the data management tasks and the\nefficient operation of a system for session-based next-item recommendations.\nThe proposed method can collect data continuously and incrementally from an\necommerce web site, thus seemingly prepare the necessary data infrastructure\nfor the recommendation algorithm to operate without any excessive training\nphase. Our work aims at developing a recommender method that represents a\nbalance between data processing and management efficiency requirements and the\neffectiveness of the recommendations produced. We use the Neo4j graph database\nto implement a prototype of such a system. Furthermore, we use an industry\ndataset corresponding to a typical e-commerce session-based scenario, and we\nreport on experiments using our graph-based approach and other state-of-the-art\nmachine learning and deep learning methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.07813,review,pre_llm,2021,6,"{'ai_likelihood': 5.3313043382432725e-06, 'text': ""To Infinity and Beyond! Accessibility is the Future for Kids' Search\n  Engines\n\n  Research in the area of search engines for children remains in its infancy.\nSeminal works have studied how children use mainstream search engines, as well\nas how to design and evaluate custom search engines explicitly for children.\nThese works, however, tend to take a one-size-fits-all view, treating children\nas a unit. Nevertheless, even at the same age, children are known to possess\nand exhibit different capabilities. These differences affect how children\naccess and use search engines. To better serve children, in this vision paper,\nwe spotlight accessibility and discuss why current research on children and\nsearch engines does not, but should, focus on this significant matter.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.06722,regular,pre_llm,2021,6,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Curriculum Pre-Training Heterogeneous Subgraph Transformer for Top-$N$\n  Recommendation\n\n  Due to the flexibility in modelling data heterogeneity, heterogeneous\ninformation network (HIN) has been adopted to characterize complex and\nheterogeneous auxiliary data in top-$N$ recommender systems, called\n\\emph{HIN-based recommendation}. HIN characterizes complex, heterogeneous data\nrelations, containing a variety of information that may not be related to the\nrecommendation task. Therefore, it is challenging to effectively leverage\nuseful information from HINs for improving the recommendation performance. To\naddress the above issue, we propose a Curriculum pre-training based\nHEterogeneous Subgraph Transformer (called \\emph{CHEST}) with new \\emph{data\ncharacterization}, \\emph{representation model} and \\emph{learning algorithm}.\n  Specifically, we consider extracting useful information from HIN to compose\nthe interaction-specific heterogeneous subgraph, containing both sufficient and\nrelevant context information for recommendation. Then we capture the rich\nsemantics (\\eg graph structure and path semantics) within the subgraph via a\nheterogeneous subgraph Transformer, where we encode the subgraph with\nmulti-slot sequence representations. Besides, we design a curriculum\npre-training strategy to provide an elementary-to-advanced learning process, by\nwhich we smoothly transfer basic semantics in HIN for modeling user-item\ninteraction relation.\n  Extensive experiments conducted on three real-world datasets demonstrate the\nsuperiority of our proposed method over a number of competitive baselines,\nespecially when only limited training data is available.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.08019,review,pre_llm,2021,6,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Towards Axiomatic Explanations for Neural Ranking Models\n\n  Recently, neural networks have been successfully employed to improve upon\nstate-of-the-art performance in ad-hoc retrieval tasks via machine-learned\nranking functions. While neural retrieval models grow in complexity and impact,\nlittle is understood about their correspondence with well-studied IR\nprinciples. Recent work on interpretability in machine learning has provided\ntools and techniques to understand neural models in general, yet there has been\nlittle progress towards explaining ranking models.\n  We investigate whether one can explain the behavior of neural ranking models\nin terms of their congruence with well understood principles of document\nranking by using established theories from axiomatic IR. Axiomatic analysis of\ninformation retrieval models has formalized a set of constraints on ranking\ndecisions that reasonable retrieval models should fulfill. We operationalize\nthis axiomatic thinking to reproduce rankings based on combinations of\nelementary constraints. This allows us to investigate to what extent the\nranking decisions of neural rankers can be explained in terms of retrieval\naxioms, and which axioms apply in which situations. Our experimental study\nconsiders a comprehensive set of axioms over several representative neural\nrankers. While the existing axioms can already explain the particularly\nconfident ranking decisions rather well, future work should extend the axiom\nset to also cover the other still ""unexplainable"" neural IR rank decisions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.08433,regular,pre_llm,2021,6,"{'ai_likelihood': 1.0728836059570312e-05, 'text': 'Combining Lexical and Dense Retrieval for Computationally Efficient\n  Multi-hop Question Answering\n\n  In simple open-domain question answering (QA), dense retrieval has become one\nof the standard approaches for retrieving the relevant passages to infer an\nanswer. Recently, dense retrieval also achieved state-of-the-art results in\nmulti-hop QA, where aggregating information from multiple pieces of information\nand reasoning over them is required. Despite their success, dense retrieval\nmethods are computationally intensive, requiring multiple GPUs to train. In\nthis work, we introduce a hybrid (lexical and dense) retrieval approach that is\nhighly competitive with the state-of-the-art dense retrieval models, while\nrequiring substantially less computational resources. Additionally, we provide\nan in-depth evaluation of dense retrieval methods on limited computational\nresource settings, something that is missing from the current literature.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.04494,regular,pre_llm,2021,6,"{'ai_likelihood': 5.9604644775390625e-06, 'text': 'Optimization of Service Addition in Multilevel Index Model for Edge\n  Computing\n\n  With the development of Edge Computing and Artificial Intelligence (AI)\ntechnologies, edge devices are witnessed to generate data at unprecedented\nvolume. The Edge Intelligence (EI) has led to the emergence of edge devices in\nvarious application domains. The EI can provide efficient services to\ndelay-sensitive applications, where the edge devices are deployed as edge nodes\nto host the majority of execution, which can effectively manage services and\nimprove service discovery efficiency. The multilevel index model is a\nwell-known model used for indexing service, such a model is being introduced\nand optimized in the edge environments to efficiently services discovery whilst\nmanaging large volumes of data. However, effectively updating the multilevel\nindex model by adding new services timely and precisely in the dynamic Edge\nComputing environments is still a challenge. Addressing this issue, this paper\nproposes a designated key selection method to improve the efficiency of adding\nservices in the multilevel index models. Our experimental results show that in\nthe partial index and the full index of multilevel index model, our method\nreduces the service addition time by around 84% and 76%, respectively when\ncompared with the original key selection method and by around 78% and 66%,\nrespectively when compared with the random selection method. Our proposed\nmethod significantly improves the service addition efficiency in the multilevel\nindex model, when compared with existing state-of-the-art key selection\nmethods, without compromising the service retrieval stability to any notable\nlevel.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.14072,review,pre_llm,2021,6,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Detecting race and gender bias in visual representation of AI on web\n  search engines\n\n  Web search engines influence perception of social reality by filtering and\nranking information. However, their outputs are often subjected to bias that\ncan lead to skewed representation of subjects such as professional occupations\nor gender. In our paper, we use a mixed-method approach to investigate presence\nof race and gender bias in representation of artificial intelligence (AI) in\nimage search results coming from six different search engines. Our findings\nshow that search engines prioritize anthropomorphic images of AI that portray\nit as white, whereas non-white images of AI are present only in non-Western\nsearch engines. By contrast, gender representation of AI is more diverse and\nless skewed towards a specific gender that can be attributed to higher\nawareness about gender bias in search outputs. Our observations indicate both\nthe the need and the possibility for addressing bias in representation of\nsocietally relevant subjects, such as technological innovation, and emphasize\nthe importance of designing new approaches for detecting bias in information\nretrieval systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.1246,regular,pre_llm,2021,6,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'Extractive Explanations for Interpretable Text Ranking\n\n  Neural document ranking models perform impressively well due to superior\nlanguage understanding gained from pre-training tasks. However, due to their\ncomplexity and large number of parameters, these (typically transformer-based)\nmodels are often non-interpretable in that ranking decisions can not be clearly\nattributed to specific parts of the input documents.\n  In this paper we propose ranking models that are inherently interpretable by\ngenerating explanations as a by-product of the prediction decision. We\nintroduce the Select-and-Rank paradigm for document ranking, where we first\noutput an explanation as a selected subset of sentences in a document.\nThereafter, we solely use the explanation or selection to make the prediction,\nmaking explanations first-class citizens in the ranking process. Technically,\nwe treat sentence selection as a latent variable trained jointly with the\nranker from the final output. To that end, we propose an end-to-end training\ntechnique for Select-and-Rank models utilizing reparameterizable subset\nsampling using the Gumbel-max trick.\n  We conduct extensive experiments to demonstrate that our approach is\ncompetitive to state-of-the-art methods. Our approach is broadly applicable to\nnumerous ranking tasks and furthers the goal of building models that are\ninterpretable by design. Finally, we present real-world applications that\nbenefit from our sentence selection method.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.09665,regular,pre_llm,2021,6,"{'ai_likelihood': 4.072984059651693e-06, 'text': ""Understanding the Effectiveness of Reviews in E-commerce Top-N\n  Recommendation\n\n  Modern E-commerce websites contain heterogeneous sources of information, such\nas numerical ratings, textual reviews and images. These information can be\nutilized to assist recommendation. Through textual reviews, a user explicitly\nexpress her affinity towards the item. Previous researchers found that by using\nthe information extracted from these reviews, we can better profile the users'\nexplicit preferences as well as the item features, leading to the improvement\nof recommendation performance. However, most of the previous algorithms were\nonly utilizing the review information for explicit-feedback problem i.e. rating\nprediction, and when it comes to implicit-feedback ranking problem such as\ntop-N recommendation, the usage of review information has not been fully\nexplored. Seeing this gap, in this work, we investigate the effectiveness of\ntextual review information for top-N recommendation under E-commerce settings.\nWe adapt several SOTA review-based rating prediction models for top-N\nrecommendation tasks and compare them to existing top-N recommendation models\nfrom both performance and efficiency. We find that models utilizing only review\ninformation can not achieve better performances than vanilla implicit-feedback\nmatrix factorization method. When utilizing review information as a regularizer\nor auxiliary information, the performance of implicit-feedback matrix\nfactorization method can be further improved. However, the optimal model\nstructure to utilize textual reviews for E-commerce top-N recommendation is yet\nto be determined.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.03399,regular,pre_llm,2021,6,"{'ai_likelihood': 3.046459621853299e-06, 'text': 'Scientific Dataset Discovery via Topic-level Recommendation\n\n  Data intensive research requires the support of appropriate datasets.\nHowever, it is often time-consuming to discover usable datasets matching a\nspecific research topic. We formulate the dataset discovery problem on an\nattributed heterogeneous graph, which is composed of paper-paper citation,\npaper-dataset citation, and also paper content. We propose to characterize both\npaper and dataset nodes by their commonly shared latent topics, rather than\nlearning user and item representations via canonical graph embedding models,\nbecause the usage of datasets and the themes of research projects can be\nunderstood on the common base of research topics. The relevant datasets to a\ngiven research project can then be inferred in the shared topic space. The\nexperimental results show that our model can generate reasonable profiles for\ndatasets, and recommend proper datasets for a query, which represents a\nresearch project linked with several papers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.07347,regular,pre_llm,2021,6,"{'ai_likelihood': 3.6093923780653213e-06, 'text': ""Zipf Matrix Factorization : Matrix Factorization with Matthew Effect\n  Reduction\n\n  Recommender system recommends interesting items to users based on users' past\ninformation history. Researchers have been paying attention to improvement of\nalgorithmic performance such as MAE and precision@K. Major techniques such as\nmatrix factorization and learning to rank are optimized based on such\nevaluation metrics. However, the intrinsic Matthew Effect problem poses great\nthreat to the fairness of the recommender system, and the unfairness problem\ncannot be resolved by optimization of traditional metrics. In this paper, we\npropose a novel algorithm that incorporates Matthew Effect reduction with the\nmatrix factorization framework. We demonstrate that our approach can boost the\nfairness of the algorithm and enhances performance evaluated by traditional\nmetrics.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.04408,regular,pre_llm,2021,6,"{'ai_likelihood': 1.58283445570204e-05, 'text': 'HieRec: Hierarchical User Interest Modeling for Personalized News\n  Recommendation\n\n  User interest modeling is critical for personalized news recommendation.\nExisting news recommendation methods usually learn a single user embedding for\neach user from their previous behaviors to represent their overall interest.\nHowever, user interest is usually diverse and multi-grained, which is difficult\nto be accurately modeled by a single user embedding. In this paper, we propose\na news recommendation method with hierarchical user interest modeling, named\nHieRec. Instead of a single user embedding, in our method each user is\nrepresented in a hierarchical interest tree to better capture their diverse and\nmulti-grained interest in news. We use a three-level hierarchy to represent 1)\noverall user interest; 2) user interest in coarse-grained topics like sports;\nand 3) user interest in fine-grained topics like football. Moreover, we propose\na hierarchical user interest matching framework to match candidate news with\ndifferent levels of user interest for more accurate user interest targeting.\nExtensive experiments on two real-world datasets validate our method can\neffectively improve the performance of user modeling for personalized news\nrecommendation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.02831,regular,pre_llm,2021,6,"{'ai_likelihood': 4.539887110392253e-05, 'text': ""A novel method for recommendation systems using invasive weed\n  optimization\n\n  One of the popular approaches in recommendation systems is Collaborative\nFiltering (CF). The most significant step in CF is choosing the appropriate set\nof users. For this purpose, similarity measures are usually used for computing\nthe similarity between a specific user and the other users. This paper proposes\na new invasive weed optimization (IWO) based CF approach that uses users'\ncontext to identify important and effective users set. By using a newly defined\nsimilarity measure based on both rating values and a measure values called\nconfidence, the proposed approach calculates the similarity between users and\nthus identifies and filters the most similar users to a specific user. It then\nuses IWO to calculate the importance degree of users and finally, by using the\nidentified important users and their importance degrees it predicts unknown\nratings. To evaluate the proposed method, several experiments have been\nperformed on two known real world datasets and the results show that the\nproposed method improves the state of the art results up to 15% in terms of\nRoot Mean Square Error (RMSE) and Mean Absolute Error (MAE).\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.07864,regular,pre_llm,2021,6,"{'ai_likelihood': 3.4769376118977866e-06, 'text': 'User-specific Adaptive Fine-tuning for Cross-domain Recommendations\n\n  Making accurate recommendations for cold-start users has been a longstanding\nand critical challenge for recommender systems (RS). Cross-domain\nrecommendations (CDR) offer a solution to tackle such a cold-start problem when\nthere is no sufficient data for the users who have rarely used the system. An\neffective approach in CDR is to leverage the knowledge (e.g., user\nrepresentations) learned from a related but different domain and transfer it to\nthe target domain. Fine-tuning works as an effective transfer learning\ntechnique for this objective, which adapts the parameters of a pre-trained\nmodel from the source domain to the target domain. However, current methods are\nmainly based on the global fine-tuning strategy: the decision of which layers\nof the pre-trained model to freeze or fine-tune is taken for all users in the\ntarget domain. In this paper, we argue that users in RS are personalized and\nshould have their own fine-tuning policies for better preference transfer\nlearning. As such, we propose a novel User-specific Adaptive Fine-tuning method\n(UAF), selecting which layers of the pre-trained network to fine-tune, on a\nper-user basis. Specifically, we devise a policy network with three alternative\nstrategies to automatically decide which layers to be fine-tuned and which\nlayers to have their parameters frozen for each user. Extensive experiments\nshow that the proposed UAF exhibits significantly better and more robust\nperformance for user cold-start recommendation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.05729,regular,pre_llm,2021,6,"{'ai_likelihood': 2.086162567138672e-06, 'text': 'GRASP: Graph Alignment through Spectral Signatures\n\n  What is the best way to match the nodes of two graphs? This graph alignment\nproblem generalizes graph isomorphism and arises in applications from social\nnetwork analysis to bioinformatics. Some solutions assume that auxiliary\ninformation on known matches or node or edge attributes is available, or\nutilize arbitrary graph features. Such methods fare poorly in the pure form of\nthe problem, in which only graph structures are given. Other proposals\ntranslate the problem to one of aligning node embeddings, yet, by doing so,\nprovide only a single-scale view of the graph. In this paper, we transfer the\nshape-analysis concept of functional maps from the continuous to the discrete\ncase, and treat the graph alignment problem as a special case of the problem of\nfinding a mapping between functions on graphs. We present GRASP, a method that\nfirst establishes a correspondence between functions derived from Laplacian\nmatrix eigenvectors, which capture multiscale structural characteristics, and\nthen exploits this correspondence to align nodes. Our experimental study,\nfeaturing noise levels higher than anything used in previous studies, shows\nthat GRASP outperforms state-of-the-art methods for graph alignment across\nnoise levels and graph types.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.0239,regular,pre_llm,2021,7,"{'ai_likelihood': 1.357661353217231e-06, 'text': ""CausalRec: Causal Inference for Visual Debiasing in Visually-Aware\n  Recommendation\n\n  Visually-aware recommendation on E-commerce platforms aims to leverage visual\ninformation of items to predict a user's preference. It is commonly observed\nthat user's attention to visual features does not always reflect the real\npreference. Although a user may click and view an item in light of a visual\nsatisfaction of their expectations, a real purchase does not always occur due\nto the unsatisfaction of other essential features (e.g., brand, material,\nprice). We refer to the reason for such a visually related interaction\ndeviating from the real preference as a visual bias. Existing visually-aware\nmodels make use of the visual features as a separate collaborative signal\nsimilarly to other features to directly predict the user's preference without\nconsidering a potential bias, which gives rise to a visually biased\nrecommendation. In this paper, we derive a causal graph to identify and analyze\nthe visual bias of these existing methods. In this causal graph, the visual\nfeature of an item acts as a mediator, which could introduce a spurious\nrelationship between the user and the item. To eliminate this spurious\nrelationship that misleads the prediction of the user's real preference, an\nintervention and a counterfactual inference are developed over the mediator.\nParticularly, the Total Indirect Effect is applied for a debiased prediction\nduring the testing phase of the model. This causal inference framework is model\nagnostic such that it can be integrated into the existing methods. Furthermore,\nwe propose a debiased visually-aware recommender system, denoted as CausalRec\nto effectively retain the supportive significance of the visual information and\nremove the visual bias. Extensive experiments are conducted on eight benchmark\ndatasets, which shows the state-of-the-art performance of CausalRec and the\nefficacy of debiasing.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.13052,regular,pre_llm,2021,7,"{'ai_likelihood': 2.5166405571831597e-06, 'text': 'Understanding and Generalizing Monotonic Proximity Graphs for\n  Approximate Nearest Neighbor Search\n\n  Graph-based algorithms have shown great empirical potential for the\napproximate nearest neighbor (ANN) search problem. Currently, graph-based ANN\nsearch algorithms are designed mainly using heuristics, whereas theoretical\nanalysis of such algorithms is quite lacking. In this paper, we study a\nfundamental model of proximity graphs used in graph-based ANN search, called\nMonotonic Relative Neighborhood Graph (MRNG), from a theoretical perspective.\nWe use mathematical proofs to explain why proximity graphs that are built based\non MRNG tend to have good searching performance. We also run experiments on\nMRNG and graphs generalizing MRNG to obtain a deeper understanding of the\nmodel. Our experiments give guidance on how to approximate and generalize MRNG\nto build proximity graphs on a large scale. In addition, we discover and study\na hidden structure of MRNG called conflicting nodes, and we give theoretical\nevidence how conflicting nodes could be used to improve ANN search methods that\nare based on MRNG.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.08345,regular,pre_llm,2021,7,"{'ai_likelihood': 6.02669186062283e-06, 'text': 'A Discriminative Semantic Ranker for Question Retrieval\n\n  Similar question retrieval is a core task in community-based question\nanswering (CQA) services. To balance the effectiveness and efficiency, the\nquestion retrieval system is typically implemented as multi-stage rankers: The\nfirst-stage ranker aims to recall potentially relevant questions from a large\nrepository, and the latter stages attempt to re-rank the retrieved results.\nMost existing works on question retrieval mainly focused on the re-ranking\nstages, leaving the first-stage ranker to some traditional term-based methods.\nHowever, term-based methods often suffer from the vocabulary mismatch problem,\nespecially on short texts, which may block the re-rankers from relevant\nquestions at the very beginning. An alternative is to employ embedding-based\nmethods for the first-stage ranker, which compress texts into dense vectors to\nenhance the semantic matching. However, these methods often lose the\ndiscriminative power as term-based methods, thus introduce noise during\nretrieval and hurt the recall performance. In this work, we aim to tackle the\ndilemma of the first-stage ranker, and propose a discriminative semantic\nranker, namely DenseTrans, for high-recall retrieval. Specifically, DenseTrans\nis a densely connected Transformer, which learns semantic embeddings for texts\nbased on Transformer layers. Meanwhile, DenseTrans promotes low-level features\nthrough dense connections to keep the discriminative power of the learned\nrepresentations. DenseTrans is inspired by DenseNet in computer vision (CV),\nbut poses a new way to use the dense connectivity which is totally different\nfrom its original design purpose. Experimental results over two question\nretrieval benchmark datasets show that our model can obtain significant gain on\nrecall against strong term-based methods as well as state-of-the-art\nembedding-based methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.07453,regular,pre_llm,2021,7,"{'ai_likelihood': 3.8080745273166235e-06, 'text': 'Next-item Recommendations in Short Sessions\n\n  The changing preferences of users towards items trigger the emergence of\nsession-based recommender systems (SBRSs), which aim to model the dynamic\npreferences of users for next-item recommendations. However, most of the\nexisting studies on SBRSs are based on long sessions only for recommendations,\nignoring short sessions, though short sessions, in fact, account for a large\nproportion in most of the real-world datasets. As a result, the applicability\nof existing SBRSs solutions is greatly reduced. In a short session, quite\nlimited contextual information is available, making the next-item\nrecommendation very challenging. To this end, in this paper, inspired by the\nsuccess of few-shot learning (FSL) in effectively learning a model with limited\ninstances, we formulate the next-item recommendation as an FSL problem.\nAccordingly, following the basic idea of a representative approach for FSL,\ni.e., meta-learning, we devise an effective SBRS called INter-SEssion\ncollaborative Recommender netTwork (INSERT) for next-item recommendations in\nshort sessions. With the carefully devised local module and global module,\nINSERT is able to learn an optimal preference representation of the current\nuser in a given short session. In particular, in the global module, a similar\nsession retrieval network (SSRN) is designed to find out the sessions similar\nto the current short session from the historical sessions of both the current\nuser and other users, respectively. The obtained similar sessions are then\nutilized to complement and optimize the preference representation learned from\nthe current short session by the local module for more accurate next-item\nrecommendations in this short session. Extensive experiments conducted on two\nreal-world datasets demonstrate the superiority of our proposed INSERT over the\nstate-of-the-art SBRSs when making next-item recommendations in short sessions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.13752,regular,pre_llm,2021,7,"{'ai_likelihood': 1.0927518208821616e-06, 'text': 'ExpertRank: A Multi-level Coarse-grained Expert-based Listwise Ranking\n  Loss\n\n  The goal of information retrieval is to recommend a list of document\ncandidates that are most relevant to a given query. Listwise learning trains\nneural retrieval models by comparing various candidates simultaneously on a\nlarge scale, offering much more competitive performance than pairwise and\npointwise schemes. Existing listwise ranking losses treat the candidate\ndocument list as a whole unit without further inspection. Some candidates with\nmoderate semantic prominence may be ignored by the noisy similarity signals or\novershadowed by a few especially pronounced candidates. As a result, existing\nranking losses fail to exploit the full potential of neural retrieval models.\nTo address these concerns, we apply the classic pooling technique to conduct\nmulti-level coarse graining and propose ExpertRank, a novel expert-based\nlistwise ranking loss. The proposed scheme has three major advantages: (1)\nExpertRank introduces the profound physics concept of coarse graining to\ninformation retrieval by selecting prominent candidates at various local levels\nbased on model prediction and inter-document comparison. (2) ExpertRank applies\nthe mixture of experts (MoE) technique to combine different experts effectively\nby extending the traditional ListNet. (3) Compared to other existing listwise\nlearning approaches, ExpertRank produces much more reliable and competitive\nperformance for various neural retrieval models with different complexities,\nfrom traditional models, such as KNRM, ConvKNRM, MatchPyramid, to sophisticated\nBERT/ALBERT-based retrieval models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.06423,regular,pre_llm,2021,7,"{'ai_likelihood': 6.258487701416016e-06, 'text': 'Learning to Recommend Items to Wikidata Editors\n\n  Wikidata is an open knowledge graph built by a global community of\nvolunteers. As it advances in scale, it faces substantial challenges around\neditor engagement. These challenges are in terms of both attracting new editors\nto keep up with the sheer amount of work and retaining existing editors.\nExperience from other online communities and peer-production systems, including\nWikipedia, suggests that personalised recommendations could help, especially\nnewcomers, who are sometimes unsure about how to contribute best to an ongoing\neffort. For this reason, we propose a recommender system WikidataRec for\nWikidata items. The system uses a hybrid of content-based and collaborative\nfiltering techniques to rank items for editors relying on both item features\nand item-editor previous interaction. A neural network, named a neural mixture\nof representations, is designed to learn fine weights for the combination of\nitem-based representations and optimize them with editor-based representation\nby item-editor interaction. To facilitate further research in this space, we\nalso create two benchmark datasets, a general-purpose one with 220,000 editors\nresponsible for 14 million interactions with 4 million items and a second one\nfocusing on the contributions of more than 8,000 more active editors. We\nperform an offline evaluation of the system on both datasets with promising\nresults. Our code and datasets are available at\nhttps://github.com/WikidataRec-developer/Wikidata_Recommender.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.00525,regular,pre_llm,2021,7,"{'ai_likelihood': 7.5499216715494794e-06, 'text': ""SearchGCN: Powering Embedding Retrieval by Graph Convolution Networks\n  for E-Commerce Search\n\n  Graph convolution networks (GCN), which recently becomes new state-of-the-art\nmethod for graph node classification, recommendation and other applications,\nhas not been successfully applied to industrial-scale search engine yet. In\nthis proposal, we introduce our approach, namely SearchGCN, for embedding-based\ncandidate retrieval in one of the largest e-commerce search engine in the\nworld. Empirical studies demonstrate that SearchGCN learns better embedding\nrepresentations than existing methods, especially for long tail queries and\nitems. Thus, SearchGCN has been deployed into JD.com's search production since\nJuly 2020.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.04984,regular,pre_llm,2021,7,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'SVP-CF: Selection via Proxy for Collaborative Filtering Data\n\n  We study the practical consequences of dataset sampling strategies on the\nperformance of recommendation algorithms. Recommender systems are generally\ntrained and evaluated on samples of larger datasets. Samples are often taken in\na naive or ad-hoc fashion: e.g. by sampling a dataset randomly or by selecting\nusers or items with many interactions. As we demonstrate, commonly-used data\nsampling schemes can have significant consequences on algorithm performance --\nmasking performance deficiencies in algorithms or altering the relative\nperformance of algorithms, as compared to models trained on the complete\ndataset. Following this observation, this paper makes the following main\ncontributions: (1) characterizing the effect of sampling on algorithm\nperformance, in terms of algorithm and dataset characteristics (e.g. sparsity\ncharacteristics, sequential dynamics, etc.); and (2) designing SVP-CF, which is\na data-specific sampling strategy, that aims to preserve the relative\nperformance of models after sampling, and is especially suited to long-tail\ninteraction data. Detailed experiments show that SVP-CF is more accurate than\ncommonly used sampling schemes in retaining the relative ranking of different\nrecommendation algorithms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.06416,review,pre_llm,2021,7,"{'ai_likelihood': 1.8874804178873699e-06, 'text': 'Multi-Step Critiquing User Interface for Recommender Systems\n\n  Recommendations with personalized explanations have been shown to increase\nuser trust and perceived quality and help users make better decisions.\nMoreover, such explanations allow users to provide feedback by critiquing them.\nSeveral algorithms for recommender systems with multi-step critiquing have\ntherefore been developed. However, providing a user-friendly interface based on\npersonalized explanations and critiquing has not been addressed in the last\ndecade. In this paper, we introduce four different web interfaces (available\nunder https://lia.epfl.ch/critiquing/) helping users making decisions and\nfinding their ideal item. We have chosen the hotel recommendation domain as a\nuse case even though our approach is trivially adaptable for other domains.\nMoreover, our system is model-agnostic (for both recommender systems and\ncritiquing models) allowing a great flexibility and further extensions. Our\ninterfaces are above all a useful tool to help research in recommendation with\ncritiquing. They allow to test such systems on a real use case and also to\nhighlight some limitations of these approaches to find solutions to overcome\nthem.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.1429,regular,pre_llm,2021,7,"{'ai_likelihood': 2.4835268656412763e-06, 'text': ""Sparse Feature Factorization for Recommender Systems with Knowledge\n  Graphs\n\n  Deep Learning and factorization-based collaborative filtering recommendation\nmodels have undoubtedly dominated the scene of recommender systems in recent\nyears. However, despite their outstanding performance, these methods require a\ntraining time proportional to the size of the embeddings and it further\nincreases when also side information is considered for the computation of the\nrecommendation list. In fact, in these cases we have that with a large number\nof high-quality features, the resulting models are more complex and difficult\nto train. This paper addresses this problem by presenting KGFlex: a sparse\nfactorization approach that grants an even greater degree of expressiveness. To\nachieve this result, KGFlex analyzes the historical data to understand the\ndimensions the user decisions depend on (e.g., movie direction, musical genre,\nnationality of book writer). KGFlex represents each item feature as an\nembedding and it models user-item interactions as a factorized entropy-driven\ncombination of the item attributes relevant to the user. KGFlex facilitates the\ntraining process by letting users update only those relevant features on which\nthey base their decisions. In other words, the user-item prediction is mediated\nby the user's personal view that considers only relevant features. An extensive\nexperimental evaluation shows the approach's effectiveness, considering the\nrecommendation results' accuracy, diversity, and induced bias. The public\nimplementation of KGFlex is available at https://split.to/kgflex.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.05722,regular,pre_llm,2021,7,"{'ai_likelihood': 6.688965691460504e-06, 'text': ""COPER: a Query-adaptable Semantics-based Search Engine for Persian\n  COVID-19 Articles\n\n  With the surge of pretrained language models, a new pathway has been opened\nto incorporate Persian text contextual information. Meanwhile, as many other\ncountries, including Iran, are fighting against COVID-19, a plethora of\nCOVID-19 related articles has been published in Iranian Healthcare magazines to\nbetter inform the public of the situation. However, finding answers in this\nsheer volume of information is an extremely difficult task. In this paper, we\ncollected a large dataset of these articles, leveraged different BERT\nvariations as well as other keyword models such as BM25 and TF-IDF, and created\na search engine to sift through these documents and rank them, given a user's\nquery. Our final search engine consists of a ranker and a re-ranker, which\nadapts itself to the query. We fine-tune our models using Semantic Textual\nSimilarity and evaluate them with standard task metrics. Our final method\noutperforms the rest by a considerable margin.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.0576,regular,pre_llm,2021,7,"{'ai_likelihood': 5.0001674228244356e-06, 'text': 'Asking Clarifying Questions Based on Negative Feedback in Conversational\n  Search\n\n  Users often need to look through multiple search result pages or reformulate\nqueries when they have complex information-seeking needs. Conversational search\nsystems make it possible to improve user satisfaction by asking questions to\nclarify users\' search intents. This, however, can take significant effort to\nanswer a series of questions starting with ""what/why/how"". To quickly identify\nuser intent and reduce effort during interactions, we propose an intent\nclarification task based on yes/no questions where the system needs to ask the\ncorrect question about intents within the fewest conversation turns. In this\ntask, it is essential to use negative feedback about the previous questions in\nthe conversation history. To this end, we propose a Maximum-Marginal-Relevance\n(MMR) based BERT model (MMR-BERT) to leverage negative feedback based on the\nMMR principle for the next clarifying question selection. Experiments on the\nQulac dataset show that MMR-BERT outperforms state-of-the-art baselines\nsignificantly on the intent identification task and the selected questions also\nachieve significantly better performance in the associated document retrieval\ntasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.01454,regular,pre_llm,2021,7,"{'ai_likelihood': 6.291601392957899e-07, 'text': 'Inscriptis -- A Python-based HTML to text conversion library optimized\n  for knowledge extraction from the Web\n\n  Inscriptis provides a library, command line client and Web service for\nconverting HTML to plain text. Its development has been triggered by the need\nto obtain accurate text representations for knowledge extraction tasks that\npreserve the spatial alignment of text without drawing upon heavyweight,\nbrowser-based solutions such as Selenium. In contrast to related software\npackages, Inscriptis (i) provides a layout-aware conversion of HTML that more\nclosely resembles the rendering obtained from standard Web browsers; and (ii)\nsupports annotation rules, i.e., user-provided mappings that allow for\nannotating the extracted text based on structural and semantic information\nencoded in HTML tags and attributes. These unique features ensure that\ndownstream knowledge extraction components can operate on accurate text\nrepresentations, and may even use information on the semantics and structure of\nthe original HTML document.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.05247,regular,pre_llm,2021,7,"{'ai_likelihood': 5.629327562120226e-06, 'text': 'INMO: A Model-Agnostic and Scalable Module for Inductive Collaborative\n  Filtering\n\n  Collaborative filtering is one of the most common scenarios and popular\nresearch topics in recommender systems. Among existing methods, latent factor\nmodels, i.e., learning a specific embedding for each user/item by\nreconstructing the observed interaction matrix, have shown excellent\nperformances. However, such user-specific and item-specific embeddings are\nintrinsically transductive, making it difficult to deal with new users and new\nitems unseen during training. Besides, the number of model parameters heavily\ndepends on the number of all users and items, restricting its scalability to\nreal-world applications. To solve the above challenges, in this paper, we\npropose a novel model-agnostic and scalable Inductive Embedding Module for\ncollaborative filtering, namely INMO. INMO generates the inductive embeddings\nfor users (items) by characterizing their interactions with some template items\n(template users), instead of employing an embedding lookup table. Under the\ntheoretical analysis, we further propose an effective indicator for the\nselection of template users/items. Our proposed INMO can be attached to\nexisting latent factor models as a pre-module, inheriting the expressiveness of\nbackbone models, while bringing the inductive ability and reducing model\nparameters. We validate the generality of INMO by attaching it to both Matrix\nFactorization (MF) and LightGCN, which are two representative latent factor\nmodels for collaborative filtering. Extensive experiments on three public\nbenchmarks demonstrate the effectiveness and efficiency of INMO in both\ntransductive and inductive recommendation scenarios.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.00852,regular,pre_llm,2021,7,"{'ai_likelihood': 2.3510720994737412e-06, 'text': ""Exploiting Cross-Session Information for Session-based Recommendation\n  with Graph Neural Networks\n\n  Different from the traditional recommender system, the session-based\nrecommender system introduces the concept of the session, i.e., a sequence of\ninteractions between a user and multiple items within a period, to preserve the\nuser's recent interest. The existing work on the session-based recommender\nsystem mainly relies on mining sequential patterns within individual sessions,\nwhich are not expressive enough to capture more complicated dependency\nrelationships among items. In addition, it does not consider the cross-session\ninformation due to the anonymity of the session data, where the linkage between\ndifferent sessions is prevented. In this paper, we solve these problems with\nthe graph neural networks technique. First, each session is represented as a\ngraph rather than a linear sequence structure, based on which a novel Full\nGraph Neural Network (FGNN) is proposed to learn complicated item dependency.\nTo exploit and incorporate cross-session information in the individual\nsession's representation learning, we further construct a Broadly Connected\nSession (BCS) graph to link different sessions and a novel Mask-Readout\nfunction to improve session embedding based on the BCS graph. Extensive\nexperiments have been conducted on two e-commerce benchmark datasets, i.e.,\nYoochoose and Diginetica, and the experimental results demonstrate the\nsuperiority of our proposal through comparisons with state-of-the-art\nsession-based recommender models.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.07773,regular,pre_llm,2021,7,"{'ai_likelihood': 2.5828679402669272e-06, 'text': 'More Robust Dense Retrieval with Contrastive Dual Learning\n\n  Dense retrieval conducts text retrieval in the embedding space and has shown\nmany advantages compared to sparse retrieval. Existing dense retrievers\noptimize representations of queries and documents with contrastive training and\nmap them to the embedding space. The embedding space is optimized by aligning\nthe matched query-document pairs and pushing the negative documents away from\nthe query. However, in such training paradigm, the queries are only optimized\nto align to the documents and are coarsely positioned, leading to an\nanisotropic query embedding space. In this paper, we analyze the embedding\nspace distributions and propose an effective training paradigm, Contrastive\nDual Learning for Approximate Nearest Neighbor (DANCE) to learn fine-grained\nquery representations for dense retrieval. DANCE incorporates an additional\ndual training object of query retrieval, inspired by the classic information\nretrieval training axiom, query likelihood. With contrastive learning, the dual\ntraining object of DANCE learns more tailored representations for queries and\ndocuments to keep the embedding space smooth and uniform, thriving on the\nranking performance of DANCE on the MS MARCO document retrieval task. Different\nfrom ANCE that only optimized with the document retrieval task, DANCE\nconcentrates the query embeddings closer to document representations while\nmaking the document distribution more discriminative. Such concentrated query\nembedding distribution assigns more uniform negative sampling probabilities to\nqueries and helps to sufficiently optimize query representations in the query\nretrieval task. Our codes are released at https://github.com/thunlp/DANCE.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.0572,regular,pre_llm,2021,7,"{'ai_likelihood': 6.291601392957899e-06, 'text': 'SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking\n\n  In neural Information Retrieval, ongoing research is directed towards\nimproving the first retriever in ranking pipelines. Learning dense embeddings\nto conduct retrieval using efficient approximate nearest neighbors methods has\nproven to work well. Meanwhile, there has been a growing interest in learning\nsparse representations for documents and queries, that could inherit from the\ndesirable properties of bag-of-words models such as the exact matching of terms\nand the efficiency of inverted indexes. In this work, we present a new\nfirst-stage ranker based on explicit sparsity regularization and a\nlog-saturation effect on term weights, leading to highly sparse representations\nand competitive results with respect to state-of-the-art dense and sparse\nmethods. Our approach is simple, trained end-to-end in a single stage. We also\nexplore the trade-off between effectiveness and efficiency, by controlling the\ncontribution of the sparsity regularization.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.13751,regular,pre_llm,2021,7,"{'ai_likelihood': 4.13921144273546e-06, 'text': 'The Cross-Lingual Arabic Information REtrieval (CLAIRE) System\n\n  Despite advances in neural machine translation, cross-lingual retrieval tasks\nin which queries and documents live in different natural language spaces remain\nchallenging. Although neural translation models may provide an intuitive\napproach to tackle the cross-lingual problem, their resource-consuming training\nand advanced model structures may complicate the overall retrieval pipeline and\nreduce users engagement. In this paper, we build our end-to-end Cross-Lingual\nArabic Information REtrieval (CLAIRE) system based on the cross-lingual word\nembedding where searchers are assumed to have a passable passive understanding\nof Arabic and various supporting information in English is provided to aid\nretrieval experience. The proposed system has three major advantages: (1) The\nusage of English-Arabic word embedding simplifies the overall pipeline and\navoids the potential mistakes caused by machine translation. (2) Our CLAIRE\nsystem can incorporate arbitrary word embedding-based neural retrieval models\nwithout structural modification. (3) Early empirical results on an Arabic news\ncollection show promising performance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.00221,regular,pre_llm,2021,7,"{'ai_likelihood': 1.8874804178873699e-06, 'text': 'Embedding-based Recommender System for Job to Candidate Matching on\n  Scale\n\n  The online recruitment matching system has been the core technology and\nservice platform in CareerBuilder. One of the major challenges in an online\nrecruitment scenario is to provide good matches between job posts and\ncandidates using a recommender system on the scale. In this paper, we discussed\nthe techniques for applying an embedding-based recommender system for the large\nscale of job to candidates matching. To learn the comprehensive and effective\nembedding for job posts and candidates, we have constructed a fused-embedding\nvia different levels of representation learning from raw text, semantic\nentities and location information. The clusters of fused-embedding of job and\ncandidates are then used to build and train the Faiss index that supports\nruntime approximate nearest neighbor search for candidate retrieval. After the\nfirst stage of candidate retrieval, a second stage reranking model that\nutilizes other contextual information was used to generate the final matching\nresult. Both offline and online evaluation results indicate a significant\nimprovement of our proposed two-staged embedding-based system in terms of\nclick-through rate (CTR), quality and normalized discounted accumulated gain\n(nDCG), compared to those obtained from our baseline system. We further\ndescribed the deployment of the system that supports the million-scale job and\ncandidate matching process at CareerBuilder. The overall improvement of our job\nto candidate matching system has demonstrated its feasibility and scalability\nat a major online recruitment site.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.14681,review,pre_llm,2021,7,"{'ai_likelihood': 1.5894571940104167e-06, 'text': ""Differences in Chinese and Western tourists faced with Japanese\n  hospitality: A natural language processing approach\n\n  Since culture influences expectations, perceptions, and satisfaction, a\ncross-culture study is necessary to understand the differences between Japan's\nbiggest tourist populations, Chinese and Western tourists. However, with\never-increasing customer populations, this is hard to accomplish without\nextensive customer base studies. There is a need for an automated method for\nidentifying these expectations at a large scale. For this, we used a\ndata-driven approach to our analysis. Our study analyzed their satisfaction\nfactors comparing soft attributes, such as service, with hard attributes, such\nas location and facilities, and studied different price ranges. We collected\nhotel reviews and extracted keywords to classify the sentiment of sentences\nwith an SVC. We then used dependency parsing and part-of-speech tagging to\nextract nouns tied to positive adjectives. We found that Chinese tourists\nconsider room quality more than hospitality, whereas Westerners are delighted\nmore by staff behavior. Furthermore, the lack of a Chinese-friendly environment\nfor Chinese customers and cigarette smell for Western ones can be disappointing\nfactors of their stay. As one of the first studies in the tourism field to use\nthe high-standard Japanese hospitality environment for this analysis, our\ncross-cultural study contributes to both the theoretical understanding of\nsatisfaction and suggests practical applications and strategies for hotel\nmanagers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.01222,review,pre_llm,2021,8,"{'ai_likelihood': 2.7484363979763457e-06, 'text': 'The Many Dimensions of Truthfulness: Crowdsourcing Misinformation\n  Assessments on a Multidimensional Scale\n\n  Recent work has demonstrated the viability of using crowdsourcing as a tool\nfor evaluating the truthfulness of public statements. Under certain conditions\nsuch as: (1) having a balanced set of workers with different backgrounds and\ncognitive abilities; (2) using an adequate set of mechanisms to control the\nquality of the collected data; and (3) using a coarse grained assessment scale,\nthe crowd can provide reliable identification of fake news. However, fake news\nare a subtle matter: statements can be just biased (""cherrypicked""), imprecise,\nwrong, etc. and the unidimensional truth scale used in existing work cannot\naccount for such differences. In this paper we propose a multidimensional\nnotion of truthfulness and we ask the crowd workers to assess seven different\ndimensions of truthfulness selected based on existing literature: Correctness,\nNeutrality, Comprehensibility, Precision, Completeness, Speaker\'s\nTrustworthiness, and Informativeness. We deploy a set of quality control\nmechanisms to ensure that the thousands of assessments collected on 180\npublicly available fact-checked statements distributed over two datasets are of\nadequate quality, including a custom search engine used by the crowd workers to\nfind web pages supporting their truthfulness assessments. A comprehensive\nanalysis of crowdsourced judgments shows that: (1) the crowdsourced assessments\nare reliable when compared to an expert-provided gold standard; (2) the\nproposed dimensions of truthfulness capture independent pieces of information;\n(3) the crowdsourcing task can be easily learned by the workers; and (4) the\nresulting assessments provide a useful basis for a more complete estimation of\nstatement truthfulness.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.09346,regular,pre_llm,2021,8,"{'ai_likelihood': 1.556343502468533e-06, 'text': 'Pre-training for Ad-hoc Retrieval: Hyperlink is Also You Need\n\n  Designing pre-training objectives that more closely resemble the downstream\ntasks for pre-trained language models can lead to better performance at the\nfine-tuning stage, especially in the ad-hoc retrieval area. Existing\npre-training approaches tailored for IR tried to incorporate weak supervised\nsignals, such as query-likelihood based sampling, to construct pseudo\nquery-document pairs from the raw textual corpus. However, these signals rely\nheavily on the sampling method. For example, the query likelihood model may\nlead to much noise in the constructed pre-training data. \\blfootnote{$\\dagger$\nThis work was done during an internship at Huawei.} In this paper, we propose\nto leverage the large-scale hyperlinks and anchor texts to pre-train the\nlanguage model for ad-hoc retrieval. Since the anchor texts are created by\nwebmasters and can usually summarize the target document, it can help to build\nmore accurate and reliable pre-training samples than a specific algorithm.\nConsidering different views of the downstream ad-hoc retrieval, we devise four\npre-training tasks based on the hyperlinks. We then pre-train the Transformer\nmodel to predict the pair-wise preference, jointly with the Masked Language\nModel objective. Experimental results on two large-scale ad-hoc retrieval\ndatasets show the significant improvement of our model compared with the\nexisting methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.01542,regular,pre_llm,2021,8,"{'ai_likelihood': 1.158979203965929e-06, 'text': 'iART: A Search Engine for Art-Historical Images to Support Research in\n  the Humanities\n\n  In this paper, we introduce iART: an open Web platform for art-historical\nresearch that facilitates the process of comparative vision. The system\nintegrates various machine learning techniques for keyword- and content-based\nimage retrieval as well as category formation via clustering. An intuitive GUI\nsupports users to define queries and explore results. By using a\nstate-of-the-art cross-modal deep learning approach, it is possible to search\nfor concepts that were not previously detected by trained classification\nmodels. Art-historical objects from large, openly licensed collections such as\nAmsterdam Rijksmuseum and Wikidata are made available to users.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.06468,regular,pre_llm,2021,8,"{'ai_likelihood': 3.245141771104601e-06, 'text': 'Modeling Scale-free Graphs with Hyperbolic Geometry for Knowledge-aware\n  Recommendation\n\n  Aiming to alleviate data sparsity and cold-start problems of traditional\nrecommender systems, incorporating knowledge graphs (KGs) to supplement\nauxiliary information has recently gained considerable attention. Via unifying\nthe KG with user-item interactions into a tripartite graph, recent works\nexplore the graph topologies to learn the low-dimensional representations of\nusers and items with rich semantics. However, these real-world tripartite\ngraphs are usually scale-free, the intrinsic hierarchical graph structures of\nwhich are underemphasized in existing works, consequently, leading to\nsuboptimal recommendation performance.\n  To address this issue and provide more accurate recommendation, we propose a\nknowledge-aware recommendation method with the hyperbolic geometry, namely\nLorentzian Knowledge-enhanced Graph convolutional networks for Recommendation\n(LKGR). LKGR facilitates better modeling of scale-free tripartite graphs after\nthe data unification. Specifically, we employ different information propagation\nstrategies in the hyperbolic space to explicitly encode heterogeneous\ninformation from historical interactions and KGs. Our proposed knowledge-aware\nattention mechanism enables the model to automatically measure the information\ncontribution, producing the coherent information aggregation in the hyperbolic\nspace. Extensive experiments on three real-world benchmarks demonstrate that\nLKGR outperforms state-of-the-art methods by 3.6-15.3% of Recall@20 on Top-K\nrecommendation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.02138,regular,pre_llm,2021,8,"{'ai_likelihood': 9.006924099392361e-06, 'text': ""Predicting Music Relistening Behavior Using the ACT-R Framework\n\n  Providing suitable recommendations is of vital importance to improve the user\nsatisfaction of music recommender systems. Here, users often listen to the same\ntrack repeatedly and appreciate recommendations of the same song multiple\ntimes. Thus, accounting for users' relistening behavior is critical for music\nrecommender systems. In this paper, we describe a psychology-informed approach\nto model and predict music relistening behavior that is inspired by studies in\nmusic psychology, which relate music preferences to human memory. We adopt a\nwell-established psychological theory of human cognition that models the\noperations of human memory, i.e., Adaptive Control of Thought-Rational (ACT-R).\nIn contrast to prior work, which uses only the base-level component of ACT-R,\nwe utilize five components of ACT-R, i.e., base-level, spreading, partial\nmatching, valuation, and noise, to investigate the effect of five factors on\nmusic relistening behavior: (i) recency and frequency of prior exposure to\ntracks, (ii) co-occurrence of tracks, (iii) the similarity between tracks, (iv)\nfamiliarity with tracks, and (v) randomness in behavior. On a dataset of 1.7\nmillion listening events from Last.fm, we evaluate the performance of our\napproach by sequentially predicting the next track(s) in user sessions. We find\nthat recency and frequency of prior exposure to tracks is an effective\npredictor of relistening behavior. Besides, considering the co-occurrence of\ntracks and familiarity with tracks further improves performance in terms of\nR-precision. We hope that our work inspires future research on the merits of\nconsidering cognitive aspects of memory retrieval to model and predict complex\nuser behavior.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.05252,regular,pre_llm,2021,8,"{'ai_likelihood': 1.953707800971137e-06, 'text': 'Retrieval & Interaction Machine for Tabular Data Prediction\n\n  Prediction over tabular data is an essential task in many data science\napplications such as recommender systems, online advertising, medical\ntreatment, etc. Tabular data is structured into rows and columns, with each row\nas a data sample and each column as a feature attribute. Both the columns and\nrows of the tabular data carry useful patterns that could improve the model\nprediction performance. However, most existing models focus on the cross-column\npatterns yet overlook the cross-row patterns as they deal with single samples\nindependently. In this work, we propose a general learning framework named\nRetrieval & Interaction Machine (RIM) that fully exploits both cross-row and\ncross-column patterns among tabular data. Specifically, RIM first leverages\nsearch engine techniques to efficiently retrieve useful rows of the table to\nassist the label prediction of the target row, then uses feature interaction\nnetworks to capture the cross-column patterns among the target row and the\nretrieved rows so as to make the final label prediction. We conduct extensive\nexperiments on 11 datasets of three important tasks, i.e., CTR prediction\n(classification), top-n recommendation (ranking) and rating prediction\n(regression). Experimental results show that RIM achieves significant\nimprovements over the state-of-the-art and various baselines, demonstrating the\nsuperiority and efficacy of RIM.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.06835,regular,pre_llm,2021,8,"{'ai_likelihood': 6.622738308376736e-07, 'text': ""Deployment of a Free-Text Analytics Platform at a UK National Health\n  Service Research Hospital: CogStack at University College London Hospitals\n\n  As more healthcare organisations transition to using electronic health record\n(EHR) systems it is important for these organisations to maximise the secondary\nuse of their data to support service improvement and clinical research. These\norganisations will find it challenging to have systems which can mine\ninformation from the unstructured data fields in the record (clinical notes,\nletters etc) and more practically have such systems interact with all of the\nhospitals data systems (legacy and current). To tackle this problem at\nUniversity College London Hospitals, we have deployed an enhanced version of\nthe CogStack platform; an information retrieval platform with natural language\nprocessing capabilities which we have configured to process the hospital's\nexisting and legacy records. The platform has improved data ingestion\ncapabilities as well as better tools for natural language processing. To date\nwe have processed over 18 million records and the insights produced from\nCogStack have informed a number of clinical research use cases at the\nhospitals.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.03937,regular,pre_llm,2021,8,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'DoSSIER@COLIEE 2021: Leveraging dense retrieval and summarization-based\n  re-ranking for case law retrieval\n\n  In this paper, we present our approaches for the case law retrieval and the\nlegal case entailment task in the Competition on Legal Information\nExtraction/Entailment (COLIEE) 2021. As first stage retrieval methods combined\nwith neural re-ranking methods using contextualized language models like BERT\nachieved great performance improvements for information retrieval in the web\nand news domain, we evaluate these methods for the legal domain. A distinct\ncharacteristic of legal case retrieval is that the query case and case\ndescription in the corpus tend to be long documents and therefore exceed the\ninput length of BERT. We address this challenge by combining lexical and dense\nretrieval methods on the paragraph-level of the cases for the first stage\nretrieval. Here we demonstrate that the retrieval on the paragraph-level\noutperforms the retrieval on the document-level. Furthermore the experiments\nsuggest that dense retrieval methods outperform lexical retrieval. For\nre-ranking we address the problem of long documents by summarizing the cases\nand fine-tuning a BERT-based re-ranker with the summaries. Overall, our best\nresults were obtained with a combination of BM25 and dense passage retrieval\nusing domain-specific embeddings.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.05069,regular,pre_llm,2021,8,"{'ai_likelihood': 3.543164994981554e-06, 'text': 'FedMatch: Federated Learning Over Heterogeneous Question Answering Data\n\n  Question Answering (QA), a popular and promising technique for intelligent\ninformation access, faces a dilemma about data as most other AI techniques. On\none hand, modern QA methods rely on deep learning models which are typically\ndata-hungry. Therefore, it is expected to collect and fuse all the available QA\ndatasets together in a common site for developing a powerful QA model. On the\nother hand, real-world QA datasets are typically distributed in the form of\nisolated islands belonging to different parties. Due to the increasing\nawareness of privacy security, it is almost impossible to integrate the data\nscattered around, or the cost is prohibited. A possible solution to this\ndilemma is a new approach known as federated learning, which is a\nprivacy-preserving machine learning technique over distributed datasets. In\nthis work, we propose to adopt federated learning for QA with the special\nconcern on the statistical heterogeneity of the QA data. Here the heterogeneity\nrefers to the fact that annotated QA data are typically with non-identical and\nindependent distribution (non-IID) and unbalanced sizes in practice.\nTraditional federated learning methods may sacrifice the accuracy of individual\nmodels under the heterogeneous situation. To tackle this problem, we propose a\nnovel Federated Matching framework for QA, named FedMatch, with a\nbackbone-patch architecture. The shared backbone is to distill the common\nknowledge of all the participants while the private patch is a compact and\nefficient module to retain the domain information for each participant. To\nfacilitate the evaluation, we build a benchmark collection based on several QA\ndatasets from different domains to simulate the heterogeneous situation in\npractice. Empirical studies demonstrate that our model can achieve significant\nimprovements against the baselines over all the datasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.1051,regular,pre_llm,2021,8,"{'ai_likelihood': 2.1921263800726996e-05, 'text': ""Contrastive Learning of User Behavior Sequence for Context-Aware\n  Document Ranking\n\n  Context information in search sessions has proven to be useful for capturing\nuser search intent. Existing studies explored user behavior sequences in\nsessions in different ways to enhance query suggestion or document ranking.\nHowever, a user behavior sequence has often been viewed as a definite and exact\nsignal reflecting a user's behavior. In reality, it is highly variable: user's\nqueries for the same intent can vary, and different documents can be clicked.\nTo learn a more robust representation of the user behavior sequence, we propose\na method based on contrastive learning, which takes into account the possible\nvariations in user's behavior sequences. Specifically, we propose three data\naugmentation strategies to generate similar variants of user behavior sequences\nand contrast them with other sequences. In so doing, the model is forced to be\nmore robust regarding the possible variations. The optimized sequence\nrepresentation is incorporated into document ranking. Experiments on two real\nquery log datasets show that our proposed model outperforms the\nstate-of-the-art methods significantly, which demonstrates the effectiveness of\nour method for context-aware document ranking.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.0041,regular,pre_llm,2021,8,"{'ai_likelihood': 6.688965691460504e-06, 'text': 'Relevance ranking for proximity full-text search based on additional\n  indexes with multi-component keys\n\n  The problem of proximity full-text search is considered. If a search query\ncontains high-frequently occurring words, then multi-component key indexes\ndeliver an improvement in the search speed compared with ordinary inverted\nindexes. It was shown that we can increase the search speed by up to 130 times\nin cases when queries consist of high-frequently occurring words. In this\npaper, we investigate how the multi-component key index architecture affects\nthe quality of the search. We consider several well-known methods of relevance\nranking, where these methods are of different authors. Using these methods, we\nperform the search in the ordinary inverted index and then in an index enhanced\nwith multi-component key indexes. The results show that with multi-component\nkey indexes we obtain search results that are very close, in terms of relevance\nranking, to the search results that are obtained by means of ordinary inverted\nindexes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.01632,regular,pre_llm,2021,8,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'An Interpretable Music Similarity Measure Based on Path Interestingness\n\n  We introduce a novel and interpretable path-based music similarity measure.\nOur similarity measure assumes that items, such as songs and artists, and\ninformation about those items are represented in a knowledge graph. We find\npaths in the graph between a seed and a target item; we score those paths based\non their interestingness; and we aggregate those scores to determine the\nsimilarity between the seed and the target. A distinguishing feature of our\nsimilarity measure is its interpretability. In particular, we can translate the\nmost interesting paths into natural language, so that the causes of the\nsimilarity judgements can be readily understood by humans. We compare the\naccuracy of our similarity measure with other competitive path-based similarity\nbaselines in two experimental settings and with four datasets. The results\nhighlight the validity of our approach to music similarity, and demonstrate\nthat path interestingness scores can be the basis of an accurate and\ninterpretable similarity measure.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.06952,review,pre_llm,2021,8,"{'ai_likelihood': 2.615981631808811e-06, 'text': 'DGCN: Diversified Recommendation with Graph Convolutional Networks\n\n  These years much effort has been devoted to improving the accuracy or\nrelevance of the recommendation system. Diversity, a crucial factor which\nmeasures the dissimilarity among the recommended items, received rather little\nscrutiny. Directly related to user satisfaction, diversification is usually\ntaken into consideration after generating the candidate items. However, this\ndecoupled design of diversification and candidate generation makes the whole\nsystem suboptimal. In this paper, we aim at pushing the diversification to the\nupstream candidate generation stage, with the help of Graph Convolutional\nNetworks (GCN). Although GCN based recommendation algorithms have shown great\npower in modeling complex collaborative filtering effect to improve the\naccuracy of recommendation, how diversity changes is ignored in those advanced\nworks. We propose to perform rebalanced neighbor discovering, category-boosted\nnegative sampling and adversarial learning on top of GCN. We conduct extensive\nexperiments on real-world datasets. Experimental results verify the\neffectiveness of our proposed method on diversification. Further ablation\nstudies validate that our proposed method significantly alleviates the\naccuracy-diversity dilemma.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.0344,regular,pre_llm,2021,8,"{'ai_likelihood': 5.132622188991971e-06, 'text': 'Unbiased Cascade Bandits: Mitigating Exposure Bias in Online Learning to\n  Rank Recommendation\n\n  Exposure bias is a well-known issue in recommender systems where items and\nsuppliers are not equally represented in the recommendation results. This is\nespecially problematic when bias is amplified over time as a few popular items\nare repeatedly over-represented in recommendation lists. This phenomenon can be\nviewed as a recommendation feedback loop: the system repeatedly recommends\ncertain items at different time points and interactions of users with those\nitems will amplify bias towards those items over time. This issue has been\nextensively studied in the literature on model-based or neighborhood-based\nrecommendation algorithms, but less work has been done on online recommendation\nmodels such as those based on multi-armed Bandit algorithms. In this paper, we\nstudy exposure bias in a class of well-known bandit algorithms known as Linear\nCascade Bandits. We analyze these algorithms on their ability to handle\nexposure bias and provide a fair representation for items and suppliers in the\nrecommendation results. Our analysis reveals that these algorithms fail to\ntreat items and suppliers fairly and do not sufficiently explore the item space\nfor each user. To mitigate this bias, we propose a discounting factor and\nincorporate it into these algorithms that controls the exposure of items at\neach time step. To show the effectiveness of the proposed discounting factor on\nmitigating exposure bias, we perform experiments on two datasets using three\ncascading bandit algorithms and our experimental results show that the proposed\nmethod improves the exposure fairness for items and suppliers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.04026,regular,pre_llm,2021,8,"{'ai_likelihood': 1.7219119601779514e-06, 'text': 'IntenT5: Search Result Diversification using Causal Language Models\n\n  Search result diversification is a beneficial approach to overcome\nunder-specified queries, such as those that are ambiguous or multi-faceted.\nExisting approaches often rely on massive query logs and interaction data to\ngenerate a variety of possible query intents, which then can be used to re-rank\ndocuments. However, relying on user interaction data is problematic because one\nfirst needs a massive user base to build a sufficient log; public query logs\nare insufficient on their own. Given the recent success of causal language\nmodels (such as the Text-To-Text Transformer (T5) model) at text generation\ntasks, we explore the capacity of these models to generate potential query\nintents. We find that to encourage diversity in the generated queries, it is\nbeneficial to adapt the model by including a new Distributional Causal Language\nModeling (DCLM) objective during fine-tuning and a representation replacement\nduring inference. Across six standard evaluation benchmarks, we find that our\nmethod (which we call IntenT5) improves search result diversity and attains\n(and sometimes exceeds) the diversity obtained when using query suggestions\nbased on a proprietary query log. Our analysis shows that our approach is most\neffective for multi-faceted queries and is able to generalize effectively to\nqueries that were unseen in training data.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.0919,regular,pre_llm,2021,8,"{'ai_likelihood': 4.76837158203125e-06, 'text': 'Supervised Contrastive Learning for Interpretable Long-Form Document\n  Matching\n\n  Recent advancements in deep learning techniques have transformed the area of\nsemantic text matching. However, most state-of-the-art models are designed to\noperate with short documents such as tweets, user reviews, comments, etc. These\nmodels have fundamental limitations when applied to long-form documents such as\nscientific papers, legal documents, and patents. When handling such long\ndocuments, there are three primary challenges: (i) the presence of different\ncontexts for the same word throughout the document, (ii) small sections of\ncontextually similar text between two documents, but dissimilar text in the\nremaining parts (this defies the basic understanding of ""similarity""), and\n(iii) the coarse nature of a single global similarity measure which fails to\ncapture the heterogeneity of the document content. In this paper, we describe\nCoLDE: Contrastive Long Document Encoder - a transformer-based framework that\naddresses these challenges and allows for interpretable comparisons of long\ndocuments. CoLDE uses unique positional embeddings and a multi-headed chunkwise\nattention layer in conjunction with a supervised contrastive learning framework\nto capture similarity at three different levels: (i) high-level similarity\nscores between a pair of documents, (ii) similarity scores between different\nsections within and across documents, and (iii) similarity scores between\ndifferent chunks in the same document and across other documents. These\nfine-grained similarity scores aid in better interpretability. We evaluate\nCoLDE on three long document datasets namely, ACL Anthology publications,\nWikipedia articles, and USPTO patents. Besides outperforming the\nstate-of-the-art methods on the document matching task, CoLDE is also robust to\nchanges in document length and text perturbations and provides interpretable\nresults.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.08538,review,pre_llm,2021,8,"{'ai_likelihood': 2.384185791015625e-06, 'text': 'Mixture-Based Correction for Position and Trust Bias in Counterfactual\n  Learning to Rank\n\n  In counterfactual learning to rank (CLTR) user interactions are used as a\nsource of supervision. Since user interactions come with bias, an important\nfocus of research in this field lies in developing methods to correct for the\nbias of interactions. Inverse propensity scoring (IPS) is a popular method\nsuitable for correcting position bias. Affine correction (AC) is a\ngeneralization of IPS that corrects for position bias and trust bias. IPS and\nAC provably remove bias, conditioned on an accurate estimation of the bias\nparameters. Estimating the bias parameters, in turn, requires an accurate\nestimation of the relevance probabilities. This cyclic dependency introduces\npractical limitations in terms of sensitivity, convergence and efficiency.\n  We propose a new correction method for position and trust bias in CLTR in\nwhich, unlike the existing methods, the correction does not rely on relevance\nestimation. Our proposed method, mixture-based correction (MBC), is based on\nthe assumption that the distribution of the CTRs over the items being ranked is\na mixture of two distributions: the distribution of CTRs for relevant items and\nthe distribution of CTRs for non-relevant items. We prove that our method is\nunbiased. The validity of our proof is not conditioned on accurate bias\nparameter estimation. Our experiments show that MBC, when used in different\nbias settings and accompanied by different LTR algorithms, outperforms AC, the\nstate-of-the-art method for correcting position and trust bias, in some\nsettings, while performing on par in other settings. Furthermore, MBC is orders\nof magnitude more efficient than AC in terms of the training time.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.02343,regular,pre_llm,2021,8,"{'ai_likelihood': 1.8212530348036025e-06, 'text': ""Itinerary-aware Personalized Deep Matching at Fliggy\n\n  Matching items for a user from a travel item pool of large cardinality have\nbeen the most important technology for increasing the business at Fliggy, one\nof the most popular online travel platforms (OTPs) in China. There are three\nmajor challenges facing OTPs: sparsity, diversity, and implicitness. In this\npaper, we present a novel Fliggy ITinerary-aware deep matching NETwork (FitNET)\nto address these three challenges. FitNET is designed based on the popular deep\nmatching network, which has been successfully employed in many industrial\nrecommendation systems, due to its effectiveness. The concept itinerary is\nfirstly proposed under the context of recommendation systems for OTPs, which is\ndefined as the list of unconsumed orders of a user. All orders in a user\nitinerary are learned as a whole, based on which the implicit travel intention\nof each user can be more accurately inferred. To alleviate the sparsity\nproblem, users' profiles are incorporated into FitNET. Meanwhile, a series of\nitinerary-aware attention mechanisms that capture the vital interactions\nbetween user's itinerary and other input categories are carefully designed.\nThese mechanisms are very helpful in inferring a user's travel intention or\npreference, and handling the diversity in a user's need. Further, two training\nobjectives, i.e., prediction accuracy of user's travel intention and prediction\naccuracy of user's click behavior, are utilized by FitNET, so that these two\nobjectives can be optimized simultaneously. An offline experiment on Fliggy\nproduction dataset with over 0.27 million users and 1.55 million travel items,\nand an online A/B test both show that FitNET effectively learns users' travel\nintentions, preferences, and diverse needs, based on their itineraries and\ngains superior performance compared with state-of-the-art methods. FitNET now\nhas been successfully deployed at Fliggy, serving major online traffic.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.1148,regular,pre_llm,2021,8,"{'ai_likelihood': 5.828009711371528e-06, 'text': 'On Approximate Nearest Neighbour Selection for Multi-Stage Dense\n  Retrieval\n\n  Dense retrieval, which describes the use of contextualised language models\nsuch as BERT to identify documents from a collection by leveraging approximate\nnearest neighbour (ANN) techniques, has been increasing in popularity. Two\nfamilies of approaches have emerged, depending on whether documents and queries\nare represented by single or multiple embeddings. ColBERT, the exemplar of the\nlatter, uses an ANN index and approximate scores to identify a set of candidate\ndocuments for each query embedding, which are then re-ranked using accurate\ndocument representations. In this manner, a large number of documents can be\nretrieved for each query, hindering the efficiency of the approach. In this\nwork, we investigate the use of ANN scores for ranking the candidate documents,\nin order to decrease the number of candidate documents being fully scored.\nExperiments conducted on the MSMARCO passage ranking corpus demonstrate that,\nby cutting of the candidate set by using the approximate scores to only 200\ndocuments, we can still obtain an effective ranking without statistically\nsignificant differences in effectiveness, and resulting in a 2x speedup in\nefficiency.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.03586,regular,pre_llm,2021,8,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'PoolRank: Max/Min Pooling-based Ranking Loss for Listwise Learning &\n  Ranking Balance\n\n  Numerous neural retrieval models have been proposed in recent years. These\nmodels learn to compute a ranking score between the given query and document.\nThe majority of existing models are trained in pairwise fashion using\nhuman-judged labels directly without further calibration. The traditional\npairwise schemes can be time-consuming and require pre-defined\npositive-negative document pairs for training, potentially leading to learning\nbias due to document distribution mismatch between training and test\nconditions. Some popular existing listwise schemes rely on the strong\npre-defined probabilistic assumptions and stark difference between relevant and\nnon-relevant documents for the given query, which may limit the model potential\ndue to the low-quality or ambiguous relevance labels. To address these\nconcerns, we turn to a physics-inspired ranking balance scheme and propose\nPoolRank, a pooling-based listwise learning framework. The proposed scheme has\nfour major advantages: (1) PoolRank extracts training information from the best\ncandidates at the local level based on model performance and relative ranking\namong abundant document candidates. (2) By combining four pooling-based loss\ncomponents in a multi-task learning fashion, PoolRank calibrates the ranking\nbalance for the partially relevant and the highly non-relevant documents\nautomatically without costly human inspection. (3) PoolRank can be easily\ngeneralized to any neural retrieval model without requiring additional\nlearnable parameters or model structure modifications. (4) Compared to pairwise\nlearning and existing listwise learning schemes, PoolRank yields better ranking\nperformance for all studied retrieval models while retaining efficient\nconvergence rates.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.13063,regular,pre_llm,2021,9,"{'ai_likelihood': 2.7815500895182294e-06, 'text': 'An Automated Multi-Web Platform Voting Framework to Predict Misleading\n  Information Proliferated during COVID-19 Outbreak using Ensemble Method\n\n  Spreading of misleading information on social web platforms has fuelled huge\npanic and confusion among the public regarding the Corona disease, the\ndetection of which is of paramount importance. To address this issue, in this\npaper, we have developed an automated system that can collect and validate the\nfact from multi web-platform to decide the credibility of the content. To\nidentify the credibility of the posted claim, probable instances/clues(titles)\nof news information are first gathered from various web platforms. Later, the\ncrucial set of features is retrieved that further feeds into the ensemble-based\nmachine learning model to classify the news as misleading or real. The four\nsets of features based on the content, linguistics/semantic cues, similarity,\nand sentiments gathered from web-platforms and voting are applied to validate\nthe news. Finally, the combined voting decides the support given to a specific\nclaim. In addition to the validation part, a unique source platform is designed\nfor collecting data/facts from three web platforms (Twitter, Facebook, Google)\nbased on certain queries/words. This unique platform can also help researchers\nbuild datasets and gather useful/efficient clues from various web platforms. It\nhas been observed that our proposed intelligent strategy gives promising\nresults and quite effective in predicting misleading information. The proposed\nwork provides practical implications for the policy makers and health\npractitioners that could be useful in protecting the world from misleading\ninformation proliferation during this pandemic.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.00968,regular,pre_llm,2021,9,"{'ai_likelihood': 2.914004855685764e-06, 'text': ""Self-supervised Representation Learning for Trip Recommendation\n\n  Trip recommendation is a significant and engaging location-based service that\ncan help new tourists make more customized travel plans. It often attempts to\nsuggest a sequence of point of interests (POIs) for a user who requests a\npersonalized travel demand. Conventional methods either leverage the heuristic\nalgorithms (e.g., dynamic programming) or statistical analysis (e.g., Markov\nmodels) to search or rank a POI sequence. These procedures may fail to capture\nthe diversity of human needs and transitional regularities. They even provide\nrecommendations that deviate from tourists' real travel intention when the trip\ndata is sparse. Although recent deep recursive models (e.g., RNN) are capable\nof alleviating these concerns, existing solutions hardly recognize the\npractical reality, such as the diversity of tourist demands, uncertainties in\nthe trip generation, and the complex visiting preference. Inspired by the\nadvance in deep learning, we introduce a novel self-supervised representation\nlearning framework for trip recommendation -- SelfTrip, aiming at tackling the\naforementioned challenges. Specifically, we propose a two-step contrastive\nlearning mechanism concerning the POI representation, as well as trip\nrepresentation. Furthermore, we present four trip augmentation methods to\ncapture the visiting uncertainties in trip planning. We evaluate our SelfTrip\non four real-world datasets, and extensive results demonstrate the promising\ngain compared with several cutting-edge benchmarks, e.g., up to 4% and 12% on\nF1 and pair-F1, respectively.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.11903,regular,pre_llm,2021,9,"{'ai_likelihood': 2.384185791015625e-06, 'text': 'Multi-behavior Graph Contextual Aware Network for Session-based\n  Recommendation\n\n  Predicting the next interaction of a short-term sequence is a challenging\ntask in session-based recommendation (SBR).Multi-behavior session\nrecommendation considers session sequence with multiple interaction types, such\nas click and purchase, to capture more effective user intention representation\nsufficiently.Despite the superior performance of existing multi-behavior based\nmethods for SBR, there are still several severe limitations:(i) Almost all\nexisting works concentrate on single target type of next behavior and fail to\nmodel multiplex behavior sessions uniformly.(ii) Previous methods also ignore\nthe semantic relations between various next behavior and historical behavior\nsequence, which are significant signals to obtain current latent intention for\nSBR.(iii) The global cross-session item-item graph established by some existing\nmodels may incorporate semantics and context level noise for multi-behavior\nsession-based recommendation. To overcome the limitations (i) and (ii), we\npropose two novel tasks for SBR, which require the incorporation of both\nhistorical behaviors and next behaviors into unified multi-behavior\nrecommendation modeling. To this end, we design a Multi-behavior Graph\nContextual Aware Network (MGCNet) for multi-behavior session-based\nrecommendation for the two proposed tasks. Specifically, we build a\nmulti-behavior global item transition graph based on all sessions involving all\ninteraction types. Based on the global graph, MGCNet attaches the global\ninterest representation to final item representation based on local contextual\nintention to address the limitation (iii). In the end, we utilize the next\nbehavior information explicitly to guide the learning of general interest and\ncurrent intention for SBR. Experiments on three public benchmark datasets show\nthat MGCNet can outperform state-of-the-art models for multi-behavior\nsession-based recommendation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.0663,regular,pre_llm,2021,9,"{'ai_likelihood': 6.390942467583551e-06, 'text': 'Detecting Layout Templates in Complex Multiregion Files\n\n  Spreadsheets are among the most commonly used file formats for data\nmanagement, distribution, and analysis. Their widespread employment makes it\neasy to gather large collections of data, but their flexible canvas-based\nstructure makes automated analysis difficult without heavy preparation. One of\nthe common problems that practitioners face is the presence of multiple,\nindependent regions in a single spreadsheet, possibly separated by repeated\nempty cells. We define such files as ""multiregion"" files. In collections of\nvarious spreadsheets, we can observe that some share the same layout. We\npresent the Mondrian approach to automatically identify layout templates across\nmultiple files and systematically extract the corresponding regions. Our\napproach is composed of three phases: first, each file is rendered as an image\nand inspected for elements that could form regions; then, using a clustering\nalgorithm, the identified elements are grouped to form regions; finally, every\nfile layout is represented as a graph and compared with others to find layout\ntemplates. We compare our method to state-of-the-art table recognition\nalgorithms on two corpora of real-world enterprise spreadsheets. Our approach\nshows the best performances in detecting reliable region boundaries within each\nfile and can correctly identify recurring layouts across files.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.11059,regular,pre_llm,2021,9,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Exploring Heterogeneous Metadata for Video Recommendation with Two-tower\n  Model\n\n  Online video services acquire new content on a daily basis to increase\nengagement, and improve the user experience. Traditional recommender systems\nsolely rely on watch history, delaying the recommendation of newly added titles\nto the right customer. However, one can use the metadata information of a\ncold-start title to bootstrap the personalization. In this work, we propose to\nadopt a two-tower model, in which one tower is to learn the user representation\nbased on their watch history, and the other tower is to learn the effective\nrepresentations for titles using metadata. The contribution of this work can be\nsummarized as: (1) we show the feasibility of using two-tower model for\nrecommendations and conduct a series of offline experiments to show its\nperformance for cold-start titles; (2) we explore different types of metadata\n(categorical features, text description, cover-art image) and an attention\nlayer to fuse them; (3) with our Amazon proprietary data, we show that the\nattention layer can assign weights adaptively to different metadata with\nimproved recommendation for warm- and cold-start items.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.06906,regular,pre_llm,2021,9,"{'ai_likelihood': 4.304779900444879e-06, 'text': 'Recovering individual emotional states from sparse ratings using\n  collaborative filtering\n\n  A fundamental challenge in emotion research is measuring feeling states with\nhigh granularity and temporal precision without disrupting the emotion\ngeneration process. Here we introduce and validate a new approach in which\nresponses are sparsely sampled and the missing data are recovered using a\ncomputational technique known as collaborative filtering (CF). This approach\nleverages structured covariation across individual experiences and is available\nin Neighbors, an open-source Python toolbox. We validate our approach across\nthree different experimental contexts by recovering dense individual ratings\nusing only a small subset of the original data. In dataset 1, participants\n(n=316) separately rated 112 emotional images on 6 different discrete emotions.\nIn dataset 2, participants (n=203) watched 8 short emotionally engaging\nautobiographical stories while simultaneously providing moment-by-moment\nratings of the intensity of their affective experience. In dataset 3,\nparticipants (n=60) with distinct social preferences made 76 decisions about\nhow much money to return in a hidden multiplier trust game. Across all\nexperimental contexts, CF was able to accurately recover missing data and\nimportantly outperformed mean imputation, particularly in contexts with greater\nindividual variability. This approach will enable new avenues for affective\nscience research by allowing researchers to acquire high dimensional ratings\nfrom emotional experiences with minimal disruption to the emotion-generation\nprocess.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.10665,review,pre_llm,2021,9,"{'ai_likelihood': 1.3642840915256076e-05, 'text': 'A Survey on Reinforcement Learning for Recommender Systems\n\n  Recommender systems have been widely applied in different real-life scenarios\nto help us find useful information. In particular, Reinforcement Learning (RL)\nbased recommender systems have become an emerging research topic in recent\nyears, owing to the interactive nature and autonomous learning ability.\nEmpirical results show that RL-based recommendation methods often surpass most\nof supervised learning methods. Nevertheless, there are various challenges of\napplying RL in recommender systems. To understand the challenges and relevant\nsolutions, there should be a reference for researchers and practitioners\nworking on RL-based recommender systems. To this end, we firstly provide a\nthorough overview, comparisons, and summarization of RL approaches applied in\nfour typical recommendation scenarios, including interactive recommendation,\nconversational recommendatin, sequential recommendation, and explainable\nrecommendation. Furthermore, we systematically analyze the challenges and\nrelevant solutions on the basis of existing literature. Finally, under\ndiscussion for open issues of RL and its limitations of recommender systems, we\nhighlight some potential research directions in this field.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.12887,regular,pre_llm,2021,9,"{'ai_likelihood': 4.569689432779948e-06, 'text': ""ICPE: An Item Cluster-Wise Pareto-Efficient Framework for Recommendation\n  Debiasing\n\n  Recommender system based on historical user-item interactions is of vital\nimportance for web-based services. However, the observed data used to train the\nrecommender model suffers from severe bias issues. Practically, the item\nfrequency distribution of the dataset is a highly skewed power-law\ndistribution. Interactions of a small fraction of head items account for almost\nthe whole training data. The normal training paradigm from such biased data\ntends to repetitively generate recommendations from the head items, which\nfurther exacerbates the biases and affects the exploration of potentially\ninteresting items from the niche set. In this work, we innovatively explore the\ncentral theme of recommendation debiasing from an item cluster-wise\nmulti-objective optimization perspective. Aiming to balance the learning on\nvarious item clusters that differ in popularity during the training process, we\npropose a model-agnostic framework namely Item Cluster-Wise Pareto-Efficient\nRecommendation (ICPE). In detail, we define our item cluster-wise optimization\ntarget as the recommender model should balance all item clusters that differ in\npopularity, thus we set the model learning on each item cluster as a unique\noptimization objective. To achieve this goal, we first explore items'\npopularity levels from a novel causal reasoning perspective. Then, we devise\npopularity discrepancy-based bisecting clustering to separate the item\nclusters. Next, we adaptively find the overall harmonious gradient direction\nfor cluster-wise optimization objectives from a Pareto-efficient solver.\nFinally, in the prediction stage, we perform counterfactual inference to\nfurther eliminate the impact of global propensity. Extensive experimental\nresults verify the superiorities of ICPE on overall recommendation performance\nand biases elimination.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.06473,review,pre_llm,2021,9,"{'ai_likelihood': 3.973642985026042e-07, 'text': ""Position Paper on Simulating Privacy Dynamics in Recommender Systems\n\n  In this position paper, we discuss the merits of simulating privacy dynamics\nin recommender systems. We study this issue at hand from two perspectives:\nFirstly, we present a conceptual approach to integrate privacy into recommender\nsystem simulations, whose key elements are privacy agents. These agents can\nenhance users' profiles with different privacy preferences, e.g., their\ninclination to disclose data to the recommender system. Plus, they can protect\nusers' privacy by guarding all actions that could be a threat to privacy. For\nexample, agents can prohibit a user's privacy-threatening actions or apply\nprivacy-enhancing techniques, e.g., Differential Privacy, to make actions less\nthreatening. Secondly, we identify three critical topics for future research in\nprivacy-aware recommender system simulations: (i) How could we model users'\nprivacy preferences and protect users from performing any privacy-threatening\nactions? (ii) To what extent do privacy agents modify the users' document\npreferences? (iii) How do privacy preferences and privacy protections impact\nrecommendations and privacy of others? Our conceptual privacy-aware simulation\napproach makes it possible to investigate the impact of privacy preferences and\nprivacy protection on the micro-level, i.e., a single user, but also on the\nmacro-level, i.e., all recommender system users. With this work, we hope to\npresent perspectives on how privacy-aware simulations could be realized, such\nthat they enable researchers to study the dynamics of privacy within a\nrecommender system.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.00954,regular,pre_llm,2021,9,"{'ai_likelihood': 1.5232298109266494e-06, 'text': 'Towards Explaining STEM Document Classification using Mathematical\n  Entity Linking\n\n  Document subject classification is essential for structuring (digital)\nlibraries and allowing readers to search within a specific field. Currently,\nthe classification is typically made by human domain experts. Semi-supervised\nMachine Learning algorithms can support them by exploiting the labeled data to\npredict subject classes for unclassified new documents. However, while humans\npartly do, machines mostly do not explain the reasons for their decisions.\nRecently, explainable AI research to address the problem of Machine Learning\ndecisions being a black box has increasingly gained interest. Explainer models\nhave already been applied to the classification of natural language texts, such\nas legal or medical documents. Documents from Science, Technology, Engineering,\nand Mathematics (STEM) disciplines are more difficult to analyze, since they\ncontain both textual and mathematical formula content. In this paper, we\npresent first advances towards STEM document classification explainability\nusing classical and mathematical Entity Linking. We examine relationships\nbetween textual and mathematical subject classes and entities, mining a\ncollection of documents from the arXiv preprint repository (NTCIR and zbMATH\ndataset). The results indicate that mathematical entities have the potential to\nprovide high explainability as they are a crucial part of a STEM document.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.11231,regular,pre_llm,2021,9,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Dynamic inference of user context through social tag embedding for music\n  recommendation\n\n  Music listening preferences at a given time depend on a wide range of\ncontextual factors, such as user emotional state, location and activity at\nlistening time, the day of the week, the time of the day, etc. It is therefore\nof great importance to take them into account when recommending music. However,\nit is very difficult to develop context-aware recommender systems that consider\nthese factors, both because of the difficulty of detecting some of them, such\nas emotional state, and because of the drawbacks derived from the inclusion of\nmany factors, such as sparsity problems in contextual pre-filtering. This work\ninvolves the proposal of a method for the detection of the user contextual\nstate when listening to music based on the social tags of music items. The\nintrinsic characteristics of social tagging that allow for the description of\nitems in multiple dimensions can be exploited to capture many contextual\ndimensions in the user listening sessions. The embeddings of the tags of the\nfirst items played in each session are used to represent the context of that\nsession. Recommendations are then generated based on both user preferences and\nthe similarity of the items computed from tag embeddings. Social tags have been\nused extensively in many recommender systems, however, to our knowledge, they\nhave been hardly used to dynamically infer contextual states.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.05516,regular,pre_llm,2021,9,"{'ai_likelihood': 7.980399661593967e-06, 'text': ""An Improved Hybrid Recommender System: Integrating Document\n  Context-Based and Behavior-Based Methods\n\n  One of the main challenges in recommender systems is data sparsity which\nleads to high variance. Several attempts have been made to improve the\nbias-variance trade-off using auxiliary information. In particular, document\nmodeling-based methods have improved the model's accuracy by using textual data\nsuch as reviews, abstracts, and storylines when the user-to-item rating matrix\nis sparse. However, such models are insufficient to learn optimal\nrepresentation for users and items. User-based and item-based collaborative\nfiltering, owing to their efficiency and interpretability, have been long used\nfor building recommender systems. They create a profile for each user and item\nrespectively as their historically interacted items and the users who\ninteracted with the target item.\n  This work combines these two approaches with document context-aware\nrecommender systems by considering users' opinions on these items. Another\nadvantage of our model is that it supports online personalization. If a user\nhas new interactions, it needs to refresh the user and item history\nrepresentation vectors instead of updating model parameters. The proposed\nalgorithm is implemented and tested on three real-world datasets that\ndemonstrate our model's effectiveness over the baseline methods.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.04713,regular,pre_llm,2021,9,"{'ai_likelihood': 5.265076955159506e-06, 'text': 'Personalized Entity Search by Sparse and Scrutable User Profiles\n\n  Prior work on personalizing web search results has focused on considering\nquery-and-click logs to capture users individual interests. For product search,\nextensive user histories about purchases and ratings have been exploited.\nHowever, for general entity search, such as for books on specific topics or\ntravel destinations with certain features, personalization is largely\nunderexplored. In this paper, we address personalization of book search, as an\nexemplary case of entity search, by exploiting sparse user profiles obtained\nthrough online questionnaires. We devise and compare a variety of re-ranking\nmethods based on language models or neural learning. Our experiments show that\neven very sparse information about individuals can enhance the effectiveness of\nthe search results.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.01274,regular,pre_llm,2021,9,"{'ai_likelihood': 1.3907750447591146e-05, 'text': 'UserBERT: Contrastive User Model Pre-training\n\n  User modeling is critical for personalized web applications. Existing user\nmodeling methods usually train user models from user behaviors with\ntask-specific labeled data. However, labeled data in a target task may be\ninsufficient for training accurate user models. Fortunately, there are usually\nrich unlabeled user behavior data which encode rich information of user\ncharacteristics and interests. Thus, pre-training user models on unlabeled user\nbehavior data has the potential to improve user modeling for many downstream\ntasks. In this paper, we propose a contrastive user model pre-training method\nnamed UserBERT. Two self-supervision tasks are incorporated in UserBERT for\nuser model pre-training on unlabeled user behavior data to empower user\nmodeling. The first one is masked behavior prediction, which aims to model the\nrelatedness between user behaviors. The second one is behavior sequence\nmatching, which aims to capture the inherent user interests that are consistent\nin different periods. In addition, we propose a medium-hard negative sampling\nframework to select informative negative samples for better contrastive\npre-training. We maintain a synchronously updated candidate behavior pool and\nan asynchronously updated candidate behavior sequence pool to select the\nlocally hardest negative behaviors and behavior sequences in an efficient way.\nExtensive experiments on two real-world datasets in different tasks show that\nUserBERT can effectively improve various user models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.05789,regular,pre_llm,2021,9,"{'ai_likelihood': 3.675619761149089e-06, 'text': ""ARGO: Modeling Heterogeneity in E-commerce Recommendation\n\n  Nowadays, E-commerce is increasingly integrated into our daily lives.\nMeanwhile, shopping process has also changed incrementally from one behavior\n(purchase) to multiple behaviors (such as view, carting and purchase).\nTherefore, utilizing interaction data of auxiliary behavior data draws a lot of\nattention in the E-commerce recommender systems. However, all existing models\nignore two kinds of intrinsic heterogeneity which are helpful to capture the\ndifference of user preferences and the difference of item attributes. First\n(intra-heterogeneity), each user has multiple social identities with otherness,\nand these different identities can result in quite different interaction\npreferences. Second (inter-heterogeneity), each item can transfer an\nitem-specific percentage of score from low-level behavior to high-level\nbehavior for the gradual relationship among multiple behaviors. Thus, the lack\nof consideration of these heterogeneities damages recommendation rank\nperformance. To model the above heterogeneities, we propose a novel method\nnamed intra- and inter-heterogeneity recommendation model (ARGO). Specifically,\nwe embed each user into multiple vectors representing the user's identities,\nand the maximum of identity scores indicates the interaction preference.\nBesides, we regard the item-specific transition percentage as trainable\ntransition probability between different behaviors. Extensive experiments on\ntwo real-world datasets show that ARGO performs much better than the\nstate-of-the-art in multi-behavior scenarios.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.05236,regular,pre_llm,2021,9,"{'ai_likelihood': 2.7749273512098527e-05, 'text': 'Uni-FedRec: A Unified Privacy-Preserving News Recommendation Framework\n  for Model Training and Online Serving\n\n  News recommendation is important for personalized online news services. Most\nexisting news recommendation methods rely on centrally stored user behavior\ndata to both train models offline and provide online recommendation services.\nHowever, user data is usually highly privacy-sensitive, and centrally storing\nthem may raise privacy concerns and risks. In this paper, we propose a unified\nnews recommendation framework, which can utilize user data locally stored in\nuser clients to train models and serve users in a privacy-preserving way.\nFollowing a widely used paradigm in real-world recommender systems, our\nframework contains two stages. The first one is for candidate news generation\n(i.e., recall) and the second one is for candidate news ranking (i.e.,\nranking). At the recall stage, each client locally learns multiple interest\nrepresentations from clicked news to comprehensively model user interests.\nThese representations are uploaded to the server to recall candidate news from\na large news pool, which are further distributed to the user client at the\nranking stage for personalized news display. In addition, we propose an\ninterest decomposer-aggregator method with perturbation noise to better protect\nprivate user information encoded in user interest representations. Besides, we\ncollaboratively train both recall and ranking models on the data decentralized\nin a large number of user clients in a privacy-preserving way. Experiments on\ntwo real-world news datasets show that our method can outperform baseline\nmethods and effectively protect user privacy.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.10036,regular,pre_llm,2021,9,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'WorldKG: A World-Scale Geographic Knowledge Graph\n\n  OpenStreetMap is a rich source of openly available geographic information.\nHowever, the representation of geographic entities, e.g., buildings, mountains,\nand cities, within OpenStreetMap is highly heterogeneous, diverse, and\nincomplete. As a result, this rich data source is hardly usable for real-world\napplications. This paper presents WorldKG -- a new geographic knowledge graph\naiming to provide a comprehensive semantic representation of geographic\nentities in OpenStreetMap. We describe the WorldKG knowledge graph, including\nits ontology that builds the semantic dataset backbone, the extraction\nprocedure of the ontology and geographic entities from OpenStreetMap, and the\nmethods to enhance entity annotation. We perform statistical and qualitative\ndataset assessment, demonstrating the large scale and high precision of the\nsemantic geographic information in WorldKG.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.15285,regular,pre_llm,2021,9,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'Improving Neural Ranking via Lossless Knowledge Distillation\n\n  We explore a novel perspective of knowledge distillation (KD) for learning to\nrank (LTR), and introduce Self-Distilled neural Rankers (SDR), where student\nrankers are parameterized identically to their teachers. Unlike the existing\nranking distillation work which pursues a good trade-off between performance\nand efficiency, SDR is able to significantly improve ranking performance of\nstudents over the teacher rankers without increasing model capacity. The key\nsuccess factors of SDR, which differs from common distillation techniques for\nclassification are: (1) an appropriate teacher score transformation function,\nand (2) a novel listwise distillation framework. Both techniques are\nspecifically designed for ranking problems and are rarely studied in the\nexisting knowledge distillation literature. Building upon the state-of-the-art\nneural ranking structure, SDR is able to push the limits of neural ranking\nperformance above a recent rigorous benchmark study and significantly\noutperforms traditionally strong gradient boosted decision tree based models on\n7 out of 9 key metrics, the first time in the literature. In addition to the\nstrong empirical results, we give theoretical explanations on why listwise\ndistillation is effective for neural rankers, and provide ablation studies to\nverify the necessity of the key factors in the SDR framework.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.04716,regular,pre_llm,2021,9,"{'ai_likelihood': 2.8477774726019966e-06, 'text': 'You Get What You Chat: Using Conversations to Personalize Search-based\n  Recommendations\n\n  Prior work on personalized recommendations has focused on exploiting explicit\nsignals from user-specific queries, clicks, likes, and ratings. This paper\ninvestigates tapping into a different source of implicit signals of interests\nand tastes: online chats between users. The paper develops an expressive model\nand effective methods for personalizing search-based entity recommendations.\nUser models derived from chats augment different methods for re-ranking entity\nanswers for medium-grained queries. The paper presents specific techniques to\nenhance the user models by capturing domain-specific vocabularies and by\nentity-based expansion. Experiments are based on a collection of online chats\nfrom a controlled user study covering three domains: books, travel, food. We\nevaluate different configurations and compare chat-based user models against\nconcise user profiles from questionnaires. Overall, these two variants perform\non par in terms of NCDG@20, but each has advantages in certain domains.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.00368,regular,pre_llm,2021,9,"{'ai_likelihood': 1.986821492513021e-06, 'text': 'Memory Augmented Multi-Instance Contrastive Predictive Coding for\n  Sequential Recommendation\n\n  The sequential recommendation aims to recommend items, such as products,\nsongs and places, to users based on the sequential patterns of their historical\nrecords. Most existing sequential recommender models consider the next item\nprediction task as the training signal. Unfortunately, there are two essential\nchallenges for these methods: (1) the long-term preference is difficult to\ncapture, and (2) the supervision signal is too sparse to effectively train a\nmodel. In this paper, we propose a novel sequential recommendation framework to\novercome these challenges based on a memory augmented multi-instance\ncontrastive predictive coding scheme, denoted as MMInfoRec. The basic\ncontrastive predictive coding (CPC) serves as encoders of sequences and items.\nThe memory module is designed to augment the auto-regressive prediction in CPC\nto enable a flexible and general representation of the encoded preference,\nwhich can improve the ability to capture the long-term preference. For\neffective training of the MMInfoRec model, a novel multi-instance noise\ncontrastive estimation (MINCE) loss is proposed, using multiple positive\nsamples, which offers effective exploitation of samples inside a mini-batch.\nThe proposed MMInfoRec framework falls into the contrastive learning style,\nwithin which, however, a further finetuning step is not required given that its\ncontrastive training task is well aligned with the target recommendation task.\nWith extensive experiments on four benchmark datasets, MMInfoRec can outperform\nthe state-of-the-art baselines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.12594,regular,pre_llm,2021,10,"{'ai_likelihood': 7.119443681504991e-06, 'text': ""Developing a Meta-suggestion Engine for Search Queries\n\n  Typically, search engines provide query suggestions to assist users in the\nsearch process. Query suggestions are very important for improving users search\nexperience. However, most query suggestions are based on the user's search\nlogs, and they can be influenced by infrequently searched queries. Depending on\nthe user's query, query suggestions can be ineffective in global search engines\nbut effective in a domestic search engine. Conversely, it can be effective in\nglobal engines and weak in domestic engines. In addition, log-based query\nsuggestions require many search logs, which makes them difficult to construct\noutside of a large search engine. Some search engines do not provide query\nsuggestions, making searches difficult for users. These query suggestion\nvulnerabilities degrade the user's search experience. In this study, we develop\na meta-suggestion, a new query suggestion scheme. Similar to meta-searches,\nmeta-suggestions retrieves candidate queries of suggestions from other search\nengines. Meta-suggestions generate suggestions by reranking the aggregated\ncandidate queries. We develop a meta-suggestion engine (MSE) browser extension\nthat generates meta-suggestions. It can provide query suggestions for any\nwebpage and does not require a search log. Comparing our meta-suggestions to\nmajor search engines such as Google, showed a 17% performance improvement on\nnormalized discounted cumulative gain (NDCG) and a 31% improvement on\nprecision. If more detailed factors, such as user preferences are discovered\nthrough continued research, it is expected that user searches will greatly\nimprove. An enhanced user search experience is possible if factors, such as\nuser preference, are examined in future work.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.00759,regular,pre_llm,2021,10,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Person Entity Profiling Framework: Identifying, Integrating and\n  Visualizing Online Freely Available Entity-Related Information\n\n  When we consider our CV, it is full of entities that we are or were\nassociated with and that define us in some way(s). Such entities include where\nwe studied, where we worked, who we collaborated with on a project or on a\npaper etc. Entities we are linked to are part of who we are and may reveal\nabout what we are interested in. Hence, we can view any CV as a graph of\ninterlinked entities, where nodes are entities and edges are relations between\nthem. This study proposes a novel entity search framework that in response to a\nreal-time query about an entity, searches, crawls, analyzes and consolidates\nrelevant information that is freely available on the Web about the entity of\ninterest, culminating in the generation a profile of the searched entity.\nUnlike typical entity search settings, in which a ranked list of entities\nrelated to the target entity over a pre-specified relation is processed, we\npresent and visualize rich information about the entity of interest as a typed\nentity-relation graph without an apriori definition of the types of related\nentities and relations. This view is structured and compact, making it easy to\nunderstand as well as interpret. It enables the user to learn not only about\nthe entity in question, but also about related entities, thereby obtaining a\nbetter understanding of the entity in question. We evaluated each of the\nframeworks components separately and then performed an overall evaluation of\nthe framework, its visualization and the interest of users in the results. The\nresults show that the proposed framework performs entity searches, related\nentity identification and relation identification very well and that it\nsatisfies users needs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.09905,regular,pre_llm,2021,10,"{'ai_likelihood': 2.572933832804362e-05, 'text': ""Show Me the Whole World: Towards Entire Item Space Exploration for\n  Interactive Personalized Recommendations\n\n  User interest exploration is an important and challenging topic in\nrecommender systems, which alleviates the closed-loop effects between\nrecommendation models and user-item interactions. Contextual bandit (CB)\nalgorithms strive to make a good trade-off between exploration and exploitation\nso that users' potential interests have chances to expose. However, classical\nCB algorithms can only be applied to a small, sampled item set (usually\nhundreds), which forces the typical applications in recommender systems limited\nto candidate post-ranking, homepage top item ranking, ad creative selection, or\nonline model selection (A/B test).\n  In this paper, we introduce two simple but effective hierarchical CB\nalgorithms to make a classical CB model (such as LinUCB and Thompson Sampling)\ncapable to explore users' interest in the entire item space without limiting it\nto a small item set. We first construct a hierarchy item tree via a bottom-up\nclustering algorithm to organize items in a coarse-to-fine manner. Then we\npropose a hierarchical CB (HCB) algorithm to explore users' interest in the\nhierarchy tree. HCB takes the exploration problem as a series of\ndecision-making processes, where the goal is to find a path from the root to a\nleaf node, and the feedback will be back-propagated to all the nodes in the\npath. We further propose a progressive hierarchical CB (pHCB) algorithm, which\nprogressively extends visible nodes which reach a confidence level for\nexploration, to avoid misleading actions on upper-level nodes in the sequential\ndecision-making process. Extensive experiments on two public recommendation\ndatasets demonstrate the effectiveness and flexibility of our methods.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.03933,regular,pre_llm,2021,10,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Towards Creating a Standardized Collection of Simple and Targeted\n  Experiments to Analyze Core Aspects of the Recommender Systems Problem\n\n  Imagine you are a teacher attempting to assess a student\'s level in a\nparticular subject. If you design a test with only hard questions, and the\nstudent fails, this mostly proves that the student does not understand the more\nadvanced material. A more insightful exam would include different types of\nquestions varying in difficulty to truly understand the student\'s weaknesses\nand strengths from different perspectives. In the field of Recommender Systems\n(RS), more often than not, we design evaluations to measure an algorithm\'s\nability to optimize goals in complex scenarios, representative of the\nreal-world challenges the system would most probably face. Nevertheless, this\npaper posits that testing an algorithm\'s ability to address both simple and\ncomplex tasks/problems would offer a more detailed view of performance to help\nidentify, at a more granular level, the weaknesses and strengths of solutions\nwhen facing different scenarios/domains. We believe the RS community would\ngreatly benefit from creating a collection of standardized, simple, and\ntargeted experiments, which, much like a suite of ""unit tests"", would\nindividually assess an algorithm\'s ability to tackle core challenges that make\nup complex RS tasks. What\'s more, these experiments go beyond traditional\npass/fail ""unit tests"". Running an algorithm against the collection of\nexperiments allows a researcher to empirically analyze in which type of\nsettings an algorithm performs best and to what degree under different metrics.\nNot only do we defend this position, in this paper, we also offer a proposal of\nhow these simple and targeted experiments could be defined and shared and\nsuggest potential next steps to make this project a reality.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.03902,regular,pre_llm,2021,10,"{'ai_likelihood': 6.788306766086155e-06, 'text': ""Multi-trends Enhanced Dynamic Micro-video Recommendation\n\n  The explosively generated micro-videos on content sharing platforms call for\nrecommender systems to permit personalized micro-video discovery with ease.\nRecent advances in micro-video recommendation have achieved remarkable\nperformance in mining users' current preference based on historical behaviors.\nHowever, most of them neglect the dynamic and time-evolving nature of users'\npreference, and the prediction on future micro-videos with historically mined\npreference may deteriorate the effectiveness of recommender systems. In this\npaper, we propose the DMR framework to explicitly model dynamic multi-trends of\nusers' current preference and make predictions based on both the history and\nfuture potential trends. We devise the DMR framework, which comprises: 1) the\nimplicit user network module which identifies sequence fragments from other\nusers with similar interests and extracts the sequence fragments that are\nchronologically behind the identified fragments; 2) the multi-trend routing\nmodule which assigns each extracted sequence fragment into a trend group and\nupdate the corresponding trend vector; 3) the history-future trend prediction\nmodule jointly uses the history preference vectors and future trend vectors to\nyield the final click-through-rate. We validate the effectiveness of DMR over\nmultiple state-of-the-art micro-video recommenders on two publicly available\nreal-world datasets. Relatively extensive analysis further demonstrate the\nsuperiority of modeling dynamic multi-trend for micro-video recommendation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.1085,regular,pre_llm,2021,10,"{'ai_likelihood': 3.5100513034396704e-06, 'text': ""Locality-Sensitive Experience Replay for Online Recommendation\n\n  Online recommendation requires handling rapidly changing user preferences.\nDeep reinforcement learning (DRL) is gaining interest as an effective means of\ncapturing users' dynamic interest during interactions with recommender systems.\nHowever, it is challenging to train a DRL agent, due to large state space\n(e.g., user-item rating matrix and user profiles), action space (e.g.,\ncandidate items), and sparse rewards. Existing studies encourage the agent to\nlearn from past experience via experience replay (ER). They adapt poorly to the\ncomplex environment of online recommender systems and are inefficient in\ndetermining an optimal strategy from past experience. To address these issues,\nwe design a novel state-aware experience replay model, which uses\nlocality-sensitive hashing to map high dimensional data into low-dimensional\nrepresentations and a prioritized reward-driven strategy to replay more\nvaluable experience at a higher chance. Our model can selectively pick the most\nrelevant and salient experiences and recommend the agent with the optimal\npolicy. Experiments on three online simulation platforms demonstrate our model'\nfeasibility and superiority toseveral existing experience replay methods.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.1157,regular,pre_llm,2021,10,"{'ai_likelihood': 6.622738308376736e-06, 'text': ""MIC: Model-agnostic Integrated Cross-channel Recommenders\n\n  Semantically connecting users and items is a fundamental problem for the\nmatching stage of an industrial recommender system. Recent advances in this\ntopic are based on multi-channel retrieval to efficiently measure users'\ninterest on items from the massive candidate pool. However, existing work are\nprimarily built upon pre-defined retrieval channels, including User-CF (U2U),\nItem-CF (I2I), and Embedding-based Retrieval (U2I), thus access to the limited\ncorrelation between users and items which solely entail from partial\ninformation of latent interactions. In this paper, we propose a model-agnostic\nintegrated cross-channel (MIC) approach for the large-scale recommendation,\nwhich maximally leverages the inherent multi-channel mutual information to\nenhance the matching performance. Specifically, MIC robustly models correlation\nwithin user-item, user-user, and item-item from latent interactions in a\nuniversal schema. For each channel, MIC naturally aligns pairs with semantic\nsimilarity and distinguishes them otherwise with more uniform anisotropic\nrepresentation space. While state-of-the-art methods require specific\narchitectural design, MIC intuitively considers them as a whole by enabling the\ncomplete information flow among users and items. Thus MIC can be easily plugged\ninto other retrieval recommender systems. Extensive experiments show that our\nMIC helps several state-of-the-art models boost their performance on two\nreal-world benchmarks. The satisfactory deployment of the proposed MIC on\nindustrial online services empirically proves its scalability and flexibility.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.07039,regular,pre_llm,2021,10,"{'ai_likelihood': 2.4835268656412763e-06, 'text': ""Presenting a Larger Up-to-date Movie Dataset and Investigating the\n  Effects of Pre-released Attributes on Gross Revenue\n\n  Movie-making has become one of the most costly and risky endeavors in the\nentertainment industry. Continuous change in the preference of the audience\nmakes it harder to predict what kind of movie will be financially successful at\nthe box office. So, it is no wonder that cautious, intelligent stakeholders and\nlarge production houses will always want to know the probable revenue that will\nbe generated by a movie before making an investment. Researchers have been\nworking on finding an optimal strategy to help investors in making the right\ndecisions. But the lack of a large, up-to-date dataset makes their work harder.\nIn this work, we introduce an up-to-date, richer, and larger dataset that we\nhave prepared by scraping IMDb for researchers and data analysts to work with.\nThe compiled dataset contains the summery data of 7.5 million titles and detail\ninformation of more than 200K movies. Additionally, we perform different\nstatistical analysis approaches on our dataset to find out how a movie's\nrevenue is affected by different pre-released attributes such as budget,\nruntime, release month, content rating, genre etc. In our analysis, we have\nfound that having a star cast/director has a positive impact on generated\nrevenue. We introduce a novel approach for calculating the star power of a\nmovie. Based on our analysis we select a set of attributes as features and\ntrain different machine learning algorithms to predict a movie's expected\nrevenue. Based on generated revenue, we classified the movies in 10 categories\nand achieved a one-class-away accuracy rate of almost 60% (bingo accuracy of\n30%). All the generated datasets and analysis codes are available online. We\nalso made the source codes of our scraper bots public, so that researchers\ninterested in extending this work can easily modify these bots as they need and\nprepare their own up-to-date datasets.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.15114,regular,pre_llm,2021,10,"{'ai_likelihood': 1.9570191701253257e-05, 'text': 'UltraGCN: Ultra Simplification of Graph Convolutional Networks for\n  Recommendation\n\n  With the recent success of graph convolutional networks (GCNs), they have\nbeen widely applied for recommendation, and achieved impressive performance\ngains. The core of GCNs lies in its message passing mechanism to aggregate\nneighborhood information. However, we observed that message passing largely\nslows down the convergence of GCNs during training, especially for large-scale\nrecommender systems, which hinders their wide adoption. LightGCN makes an early\nattempt to simplify GCNs for collaborative filtering by omitting feature\ntransformations and nonlinear activations. In this paper, we take one step\nfurther to propose an ultra-simplified formulation of GCNs (dubbed UltraGCN),\nwhich skips infinite layers of message passing for efficient recommendation.\nInstead of explicit message passing, UltraGCN resorts to directly approximate\nthe limit of infinite-layer graph convolutions via a constraint loss.\nMeanwhile, UltraGCN allows for more appropriate edge weight assignments and\nflexible adjustment of the relative importances among different types of\nrelationships. This finally yields a simple yet effective UltraGCN model, which\nis easy to implement and efficient to train. Experimental results on four\nbenchmark datasets show that UltraGCN not only outperforms the state-of-the-art\nGCN models but also achieves more than 10x speedup over LightGCN. Our source\ncode will be available at https://reczoo.github.io/UltraGCN.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.09248,regular,pre_llm,2021,10,"{'ai_likelihood': 1.4437569512261285e-05, 'text': 'Demographic Biases of Crowd Workers in Key Opinion Leaders Finding\n\n  Key Opinion Leaders (KOLs) are people that have a strong influence and their\nopinions are listened to by people when making important decisions.\nCrowdsourcing provides an efficient and cost-effective means to gather data for\nthe KOL finding task. However, data collected through crowdsourcing is affected\nby the inherent demographic biases of crowd workers. To avoid such demographic\nbiases, we need to measure how biased each crowd worker is. In this paper, we\npropose a simple yet effective approach based on demographic information of\ncandidate KOLs and their counterfactual value. We argue that it is\neffectiveness because of the extra information that we can consider together\nwith labeled data to curate a less biased dataset.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.04037,review,pre_llm,2021,10,"{'ai_likelihood': 3.675619761149089e-06, 'text': 'Simulations for novel problems in recommendation: analyzing\n  misinformation and data characteristics\n\n  In this position paper, we discuss recent applications of simulation\napproaches for recommender systems tasks. In particular, we describe how they\nwere used to analyze the problem of misinformation spreading and understand\nwhich data characteristics affect the performance of recommendation algorithms\nmore significantly. We also present potential lines of future work where\nsimulation methods could advance the work in the recommendation community.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.0464,regular,pre_llm,2021,10,"{'ai_likelihood': 5.993578169080946e-06, 'text': 'Lookup or Exploratory: What is Your Search Intent?\n\n  Search query specificity is broadly divided into two categories - Exploratory\nor Lookup. If a query specificity can be identified at the run time, it can be\nused to significantly improve the search results as well as quality of\nsuggestions to alter the query. However, with millions of queries coming every\nday on a commercial search engine, it is non-trivial to develop a horizontal\ntechnique to determine query specificity at run time. Existing techniques\nsuffer either from lack of enough training data or are dependent on information\nsuch as query length or session information. In this paper, we show that such\nmethodologies are inadequate or at times misleading. We propose a novel\nmethodology, to overcome these limitations. First, we demonstrate a\nheuristic-based method to identify Exploratory or Lookup intent queries at\nscale, classifying millions of queries into the two classes with a high\naccuracy, as shown in our experiments. Our methodology is not dependent on\nsession data or on query length. Next, we train a transformer-based deep neural\nnetwork to classify the queries into one of the two classes at run time. Our\nmethod uses a bidirectional GRU initialized with pretrained BERT-base-uncased\nembeddings and an augmented triplet loss to classify the intent of queries\nwithout using any session data. We also introduce a novel Semi-Greedy Iterative\nTraining approach to fine-tune our model. Our model is deployable for real time\nquery specificity identification with response time of less than one\nmillisecond. Our technique is generic, and the results have valuable\nimplications for improving the quality of search results and suggestions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.11166,regular,pre_llm,2021,10,"{'ai_likelihood': 2.4835268656412763e-06, 'text': ""Driving the Herd: Search Engines as Content Influencers\n\n  In competitive search settings such as the Web, many documents' authors\n(publishers) opt to have their documents highly ranked for some queries. To\nthis end, they modify the documents - specifically, their content - in response\nto induced rankings. Thus, the search engine affects the content in the corpus\nvia its ranking decisions. We present a first study of the ability of search\nengines to drive pre-defined, targeted, content effects in the corpus using\nsimple techniques. The first is based on the herding phenomenon - a celebrated\nresult from the economics literature - and the second is based on biasing the\nrelevance ranking function. The types of content effects we study are either\ntopical or touch on specific document properties - length and inclusion of\nquery terms. Analysis of ranking competitions we organized between incentivized\npublishers shows that the types of content effects we target can indeed be\nattained by applying our suggested techniques. These findings have important\nimplications with regard to the role of search engines in shaping the corpus.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.02158,regular,pre_llm,2021,10,"{'ai_likelihood': 8.410877651638455e-06, 'text': 'Exploring usability of Reddit in data science and knowledge processing\n\n  This contribution argues that Reddit, as a massive, categorized, open-access\ndataset, is a useful data source, for ""almost any topic"". Hence, it can be used\nin data science, e.g. for knowledge exploration. This statement is backed-up\nwith presented analysis, based on 180 manually annotated papers, related to\nReddit itself, and data acquired from popular databases of scientific papers.\nFinally, an open source tool is introduced, which provides an easy access to\nReddit resources, and an exploratory data analysis of how Reddit covers\nselected topics. These functions can be used as a prelude analysis to a broader\nexploration of Reddit\'s applicability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.00123,regular,pre_llm,2021,10,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'Learning Representations for Zero-Shot Retrieval over Structured Data\n\n  Large Scale Question-Answering systems today are widely used in downstream\napplications such as chatbots and conversational dialogue agents. Typically,\nsuch systems consist of an Answer Passage retrieval layer coupled with Machine\nComprehension models trained on natural language query-passage pairs. Recent\nstudies have explored Question Answering over structured data sources such as\nweb-tables and relational databases. However, architectures such as Seq2SQL\nassume the correct table a priori which is input to the model along with the\nfree text question. Our proposed method, analogues to a passage retrieval model\nin traditional Question-Answering systems, describes an architecture to discern\nthe correct table pertaining to a given query from amongst a large pool of\ncandidate tables.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.00429,regular,pre_llm,2021,10,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Enhancing Top-N Item Recommendations by Peer Collaboration\n\n  Deep neural networks (DNN) have achieved great success in the recommender\nsystems (RS) domain. However, to achieve remarkable performance, DNN-based\nrecommender models often require numerous parameters, which inevitably bring\nredundant neurons and weights, a phenomenon referred to as\nover-parameterization. In this paper, we plan to exploit such redundancy\nphenomena to improve the performance of RS. Specifically, we propose PCRec, a\ntop-N item \\underline{rec}ommendation framework that leverages collaborative\ntraining of two DNN-based recommender models with the same network structure,\ntermed \\underline{p}eer \\underline{c}ollaboration. PCRec can reactivate and\nstrengthen the unimportant (redundant) weights during training, which achieves\nhigher prediction accuracy but maintains its original inference efficiency. To\nrealize this, we first introduce two criteria to identify the importance of\nweights of a given recommender model. Then, we rejuvenate the unimportant\nweights by transplanting outside information (i.e., weights) from its peer\nnetwork. After such an operation and retraining, the original recommender model\nis endowed with more representation capacity by possessing more functional\nmodel parameters. To show its generality, we instantiate PCRec by using three\nwell-known recommender models. We conduct extensive experiments on three\nreal-world datasets, and show that PCRec yields significantly better\nrecommendations than its counterpart with the same model (parameter) size.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.01788,regular,pre_llm,2021,10,"{'ai_likelihood': 8.013513353135851e-06, 'text': 'Voice Information Retrieval In Collaborative Information Seeking\n\n  Voice information retrieval is a technique that provides Information\nRetrieval System with the capacity to transcribe spoken queries and use the\ntext output for information search. CIS is a field of research that involves\nstudying the situation, motivations, and methods for people working in a\ncollaborative group for information seeking projects, as well as building a\nsystem for supporting such activities. Humans find it easier to communicate and\nexpress ideas via speech. Existing voice search like Google and other\nmainstream voice search does not support collaborative search. The spoken\nspeeches passed through the ASR for feature extraction using MFCC and HMM,\nViterbi algorithm precisely for pattern matching. The result of the ASR is then\npassed as input into CIS System, results is then filtered to have an aggregate\nresult. The result from the simulation shows that our model was able to achieve\n81.25% transcription accuracy.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.00821,regular,pre_llm,2021,10,"{'ai_likelihood': 3.642506069607205e-07, 'text': ""Relation Analysis between Hotel Review Rating Scores and Sentiment\n  Analysis of Reviews by Chinese Tourists Visiting Japan\n\n  In current times, the importance of online hotel review sites has become more\nand more apparent. Users of these sites reference of reviews strongly\ninfluences their purchase behavior and as such, reviews are important to\ncompanies and researchers alike. The majority of review sites offer both text\nreviews and numerical hotel ratings, and both information sources are widely\nused by researchers as a representation of a customer's sentiment and opinion.\nHowever, an opinion is a difficult concept to measure, and as such, depending\non the relation these two sources have, it would be apparent whether or not it\nis safe to consider them equally in research. In this study we utilize an\nentropy-based Support Vector Machine to classify positive and negative\nsentiments in hotel reviews from the site Ctrip, then calculating the ratio of\npositive and negative sentiment in each review and examine their correlation\nwith said review's rating score using Spearman and Kendall Correlation\ncoefficients and Maximal Information Coefficient (MIC).\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.09059,regular,pre_llm,2021,10,"{'ai_likelihood': 1.708666483561198e-05, 'text': ""Context-aware Reranking with Utility Maximization for Recommendation\n\n  As a critical task for large-scale commercial recommender systems, reranking\nhas shown the potential of improving recommendation results by uncovering\nmutual influence among items. Reranking rearranges items in the initial ranking\nlists from the previous ranking stage to better meet users' demands. However,\nrather than considering the context of initial lists as most existing methods\ndo, an ideal reranking algorithm should consider the counterfactual context --\nthe position and the alignment of the items in the reranked lists. In this\nwork, we propose a novel pairwise reranking framework, Context-aware Reranking\nwith Utility Maximization for recommendation (CRUM), which maximizes the\noverall utility after reranking efficiently. Specifically, we first design a\nutility-oriented evaluator, which applies Bi-LSTM and graph attention mechanism\nto estimate the listwise utility via the counterfactual context modeling. Then,\nunder the guidance of the evaluator, we propose a pairwise reranker model to\nfind the most suitable position for each item by swapping misplaced item pairs.\nExtensive experiments on two benchmark datasets and a proprietary real-world\ndataset demonstrate that CRUM significantly outperforms the state-of-the-art\nmodels in terms of both relevance-based metrics and utility-based metrics.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.15154,regular,pre_llm,2021,10,"{'ai_likelihood': 5.099508497450087e-06, 'text': 'Cross-Batch Negative Sampling for Training Two-Tower Recommenders\n\n  The two-tower architecture has been widely applied for learning item and user\nrepresentations, which is important for large-scale recommender systems. Many\ntwo-tower models are trained using various in-batch negative sampling\nstrategies, where the effects of such strategies inherently rely on the size of\nmini-batches. However, training two-tower models with a large batch size is\ninefficient, as it demands a large volume of memory for item and user contents\nand consumes a lot of time for feature encoding. Interestingly, we find that\nneural encoders can output relatively stable features for the same input after\nwarming up in the training process. Based on such facts, we propose a simple\nyet effective sampling strategy called Cross-Batch Negative Sampling (CBNS),\nwhich takes advantage of the encoded item embeddings from recent mini-batches\nto boost the model training. Both theoretical analysis and empirical\nevaluations demonstrate the effectiveness and the efficiency of CBNS.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.14036,regular,pre_llm,2021,11,"{'ai_likelihood': 9.304947323269314e-06, 'text': 'Pre-training Recommender Systems via Reinforced Attentive\n  Multi-relational Graph Neural Network\n\n  Recently, Graph Neural Networks (GNNs) have proven their effectiveness for\nrecommender systems. Existing studies have applied GNNs to capture\ncollaborative relations in the data. However, in real-world scenarios, the\nrelations in a recommendation graph can be of various kinds. For example, two\nmovies may be associated either by the same genre or by the same\ndirector/actor. If we use a single graph to elaborate all these relations, the\ngraph can be too complex to process. To address this issue, we bring the idea\nof pre-training to process the complex graph step by step. Based on the idea of\ndivide-and-conquer, we separate the large graph into three sub-graphs: user\ngraph, item graph, and user-item interaction graph. Then the user and item\nembeddings are pre-trained from user and item graphs, respectively. To conduct\npre-training, we construct the multi-relational user graph and item graph,\nrespectively, based on their attributes. In this paper, we propose a novel\nReinforced Attentive Multi-relational Graph Neural Network (RAM-GNN) to the\npre-train user and item embeddings on the user and item graph prior to the\nrecommendation step. Specifically, we design a relation-level attention layer\nto learn the importance of different relations. Next, a Reinforced Neighbor\nSampler (RNS) is applied to search the optimal filtering threshold for sampling\ntop-k similar neighbors in the graph, which avoids the over-smoothing issue. We\ninitialize the recommendation model with the pre-trained user/item embeddings.\nFinally, an aggregation-based GNN model is utilized to learn from the\ncollaborative relations in the user-item interaction graph and provide\nrecommendations. Our experiments demonstrate that RAM-GNN outperforms other\nstate-of-the-art graph-based recommendation models and multi-relational graph\nneural networks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.10864,regular,pre_llm,2021,11,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'The Impact of Main Content Extraction on Near-Duplicate Detection\n\n  Commercial web search engines employ near-duplicate detection to ensure that\nusers see each relevant result only once, albeit the underlying web crawls\ntypically include (near-)duplicates of many web pages. We revisit the risks and\npotential of near-duplicates with an information retrieval focus, motivating\nthat current efforts toward an open and independent European web search\ninfrastructure should maintain metadata on duplicate and near-duplicate\ndocuments in its index.\n  Near-duplicate detection implemented in an open web search infrastructure\nshould provide a suitable similarity threshold, a difficult choice since\nidentical pages may substantially differ in parts of a page that are irrelevant\nto searchers (templates, advertisements, etc.). We study this problem by\ncomparing the similarity of pages for five (main) content extraction methods in\ntwo studies on the ClueWeb crawls. We find that the full content of pages\nserves precision-oriented near-duplicate-detection, while main content\nextraction is more recall-oriented.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.03579,regular,pre_llm,2021,11,"{'ai_likelihood': 1.158979203965929e-06, 'text': 'A Semi-automatic Data Extraction System for Heterogeneous Data Sources:\n  A Case Study from Cotton Industry\n\n  With the recent developments in digitisation, there are increasing number of\ndocuments available online. There are several information extraction tools that\nare available to extract information from digitised documents. However,\nidentifying precise answers to a given query is often a challenging task\nespecially if the data source where the relevant information resides is\nunknown. This situation becomes more complex when the data source is available\nin multiple formats such as PDF, table and html. In this paper, we propose a\nnovel data extraction system to discover relevant and focused information from\ndiverse unstructured data sources based on text mining approaches. We perform a\nqualitative analysis to evaluate the proposed system and its suitability and\nadaptability using cotton industry.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.13853,review,pre_llm,2021,11,"{'ai_likelihood': 2.2517310248480902e-05, 'text': ""Pre-training Methods in Information Retrieval\n\n  The core of information retrieval (IR) is to identify relevant information\nfrom large-scale resources and return it as a ranked list to respond to the\nuser's information need. In recent years, the resurgence of deep learning has\ngreatly advanced this field and leads to a hot topic named NeuIR (i.e., neural\ninformation retrieval), especially the paradigm of pre-training methods (PTMs).\nOwing to sophisticated pre-training objectives and huge model size, pre-trained\nmodels can learn universal language representations from massive textual data,\nwhich are beneficial to the ranking task of IR. Recently, a large number of\nworks, which are dedicated to the application of PTMs in IR, have been\nintroduced to promote the retrieval performance. Considering the rapid progress\nof this direction, this survey aims to provide a systematic review of\npre-training methods in IR. To be specific, we present an overview of PTMs\napplied in different components of an IR system, including the retrieval\ncomponent, the re-ranking component, and other components. In addition, we also\nintroduce PTMs specifically designed for IR, and summarize available datasets\nas well as benchmark leaderboards. Moreover, we discuss some open challenges\nand highlight several promising directions, with the hope of inspiring and\nfacilitating more works on these topics for future research.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.03318,regular,pre_llm,2021,11,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'AIM: Automatic Interaction Machine for Click-Through Rate Prediction\n\n  Feature embedding learning and feature interaction modeling are two crucial\ncomponents of deep models for Click-Through Rate (CTR) prediction. Most\nexisting deep CTR models suffer from the following three problems. First,\nfeature interactions are either manually designed or simply enumerated. Second,\nall the feature interactions are modeled with an identical interaction\nfunction. Third, in most existing models, different features share the same\nembedding size which leads to memory inefficiency. To address these three\nissues mentioned above, we propose Automatic Interaction Machine (AIM) with\nthree core components, namely, Feature Interaction Search (FIS), Interaction\nFunction Search (IFS) and Embedding Dimension Search (EDS), to select\nsignificant feature interactions, appropriate interaction functions and\nnecessary embedding dimensions automatically in a unified framework.\nSpecifically, FIS component automatically identifies different orders of\nessential feature interactions with useless ones pruned; IFS component selects\nappropriate interaction functions for each individual feature interaction in a\nlearnable way; EDS component automatically searches proper embedding size for\neach feature. Offline experiments on three large-scale datasets validate the\nsuperior performance of AIM. A three-week online A/B test in the recommendation\nservice of a mainstream app market shows that AIM improves DeepFM model by 4.4%\nin terms of CTR.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.13576,review,pre_llm,2021,11,"{'ai_likelihood': 9.934107462565105e-07, 'text': ""Job Recommender Systems: A Review\n\n  This paper provides a review of the job recommender system (JRS) literature\npublished in the past decade (2011-2021). Compared to previous literature\nreviews, we put more emphasis on contributions that incorporate the temporal\nand reciprocal nature of job recommendations. Previous studies on JRS suggest\nthat taking such views into account in the design of the JRS can lead to\nimproved model performance. Also, it may lead to a more uniform distribution of\ncandidates over a set of similar jobs. We also consider the literature from the\nperspective of algorithm fairness. Here we find that this is rarely discussed\nin the literature, and if it is discussed, many authors wrongly assume that\nremoving the discriminatory feature would be sufficient. With respect to the\ntype of models used in JRS, authors frequently label their method as `hybrid'.\nUnfortunately, they thereby obscure what these methods entail. Using existing\nrecommender taxonomies, we split this large class of hybrids into subcategories\nthat are easier to analyse. We further find that data availability, and in\nparticular the availability of click data, has a large impact on the choice of\nmethod and validation. Last, although the generalizability of JRS across\ndifferent datasets is infrequently considered, results suggest that error\nscores may vary across these datasets.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.08999,regular,pre_llm,2021,11,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'NLP based grievance redressal system for Indian Railways\n\n  The current grievance redressal system has a dedicated 24X7 Twitter Cell,\nwherein the human experts take actions and respond to the tweets of customers\naddressed to Ministry of Railways. It is done quite promptly by the human\nexperts. It is understood that the software plugin to process the tweets\naddressed towards Ministry of Railways can not match the human expertise.\nStill, efforts can be done to build a software plugin which can ease the human\neffort. This project aims at building a software plug-in to minimize the human\neffort involved in analysis of tweets addressed to Indian Railways and aid in\nexisting complaints redressal system by identifying the complaints from the\ntweets. It is understood that it is not possible to match human promptness in\nterms of handling the tweets, still we can try to reduce the human efforts by\nworking on the following objectives: 1.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.02239,regular,pre_llm,2021,11,"{'ai_likelihood': 2.814663781060113e-06, 'text': 'Order Matters: Matching Multiple Knowledge Graphs\n\n  Knowledge graphs (KGs) provide information in machine interpretable form. In\ncases where multiple KGs are used in the same system, that information needs to\nbe integrated. This is usually done by automated matching systems. Most of\nthose systems consider only 1:1 (binary) matching tasks. Thus, matching a\nlarger number of knowledge graphs with such systems would lead to quadratic\nefforts. In this paper, we empirically analyze different approaches to reduce\nthe task of multi-source matching to a linear number of executions of binary\nmatching systems. We show that the matching order of KGs and the multi-source\nstrategy actually matter and that near-optimal results can be achieved with\nlinear efforts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.04282,regular,pre_llm,2021,11,"{'ai_likelihood': 7.847944895426433e-06, 'text': ""Learning an Adaptive Meta Model-Generator for Incrementally Updating\n  Recommender Systems\n\n  Recommender Systems (RSs) in real-world applications often deal with billions\nof user interactions daily. To capture the most recent trends effectively, it\nis common to update the model incrementally using only the newly arrived data.\nHowever, this may impede the model's ability to retain long-term information\ndue to the potential overfitting and forgetting issues. To address this\nproblem, we propose a novel Adaptive Sequential Model Generation (ASMG)\nframework, which generates a better serving model from a sequence of historical\nmodels via a meta generator. For the design of the meta generator, we propose\nto employ Gated Recurrent Units (GRUs) to leverage its ability to capture the\nlong-term dependencies. We further introduce some novel strategies to apply\ntogether with the GRU meta generator, which not only improve its computational\nefficiency but also enable more accurate sequential modeling. By instantiating\nthe model-agnostic framework on a general deep learning-based RS model, we\ndemonstrate that our method achieves state-of-the-art performance on three\npublic datasets and one industrial dataset.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.12481,regular,pre_llm,2021,11,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'It Is Different When Items Are Older: Debiasing Recommendations When\n  Selection Bias and User Preferences Are Dynamic\n\n  User interactions with recommender systems (RSs) are affected by user\nselection bias, e.g., users are more likely to rate popular items (popularity\nbias) or items that they expect to enjoy beforehand (positivity bias). Methods\nexist for mitigating the effects of selection bias in user ratings on the\nevaluation and optimization of RSs. However, these methods treat selection bias\nas static, despite the fact that the popularity of an item may change\ndrastically over time and the fact that user preferences may also change over\ntime. We focus on the age of an item and its effect on selection bias and user\npreferences. Our experimental analysis reveals that the rating behavior of\nusers on the MovieLens dataset is better captured by methods that consider\neffects from the age of item on bias and preferences. We theoretically show\nthat in a dynamic scenario in which both the selection bias and user\npreferences are dynamic, existing debiasing methods are no longer unbiased. To\naddress this limitation, we introduce DebiAsing in the dyNamiC scEnaRio\n(DANCER), a novel debiasing method that extends the inverse propensity scoring\ndebiasing method to account for dynamic selection bias and user preferences.\nOur experimental results indicate that DANCER improves rating prediction\nperformance compared to debiasing methods that incorrectly assume that\nselection bias is static in a dynamic scenario. To the best of our knowledge,\nDANCER is the first debiasing method that accounts for dynamic selection bias\nand user preferences in RSs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.05587,regular,pre_llm,2021,11,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'Design Theory to improve health evidence retrieval\n\n  Objective: Our study objective is to design a feasible technology solution\nfor health organizations to remove barriers to evidence-based clinical\ninformation retrieval, and improve Evidence-Based Practice. Methods: Literature\nfrom 2010 to 2020 was reviewed to define problems in evidence-based clinical\ninformation retrieval with recommendations from literature used to define\nsolution objectives. Design Science Research is used to complete three projects\nin a research stream using cloud services such as Web-Scale Discovery, Content\nManagement System, Federated Access, Global Knowledgebase, and Document\nDelivery. Design thinking, systems thinking, and user-oriented theory of\ninformation need are adopted to construct a design theory. Results: The\nresearch stream produced three novel and innovative artefacts: a contextual\nmodel, a unified architecture, and a context-aware unified architecture which\nwe evaluate as part of academic reviews, scholarly publications, and conference\nproceedings in various research stream stages. A fourth artefact or design\ntheory is presented to generalize results as mature knowledge.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.14467,regular,pre_llm,2021,11,"{'ai_likelihood': 3.2782554626464844e-06, 'text': ""What Drives Readership? An Online Study on User Interface Types and\n  Popularity Bias Mitigation in News Article Recommendations\n\n  Personalized news recommender systems support readers in finding the right\nand relevant articles in online news platforms. In this paper, we discuss the\nintroduction of personalized, content-based news recommendations on DiePresse,\na popular Austrian online news platform, focusing on two specific aspects: (i)\nuser interface type, and (ii) popularity bias mitigation. Therefore, we\nconducted a two-weeks online study that started in October 2020, in which we\nanalyzed the impact of recommendations on two user groups, i.e., anonymous and\nsubscribed users, and three user interface types, i.e., on a desktop, mobile\nand tablet device. With respect to user interface types, we find that the\nprobability of a recommendation to be seen is the highest for desktop devices,\nwhile the probability of interacting with recommendations is the highest for\nmobile devices. With respect to popularity bias mitigation, we find that\npersonalized, content-based news recommendations can lead to a more balanced\ndistribution of news articles' readership popularity in the case of anonymous\nusers. Apart from that, we find that significant events (e.g., the COVID-19\nlockdown announcement in Austria and the Vienna terror attack) influence the\ngeneral consumption behavior of popular articles for both, anonymous and\nsubscribed users.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.00926,regular,pre_llm,2021,11,"{'ai_likelihood': 1.0563267601860894e-05, 'text': 'Heterogeneous Graph Neural Networks for Large-Scale Bid Keyword Matching\n\n  Digital advertising is a critical part of many e-commerce platforms such as\nTaobao and Amazon. While in recent years a lot of attention has been drawn to\nthe consumer side including canonical problems like ctr/cvr prediction, the\nadvertiser side, which directly serves advertisers by providing them with\nmarketing tools, is now playing a more and more important role. When speaking\nof sponsored search, bid keyword recommendation is the fundamental service.\nThis paper addresses the problem of keyword matching, the primary step of\nkeyword recommendation. Existing methods for keyword matching merely consider\nmodeling relevance based on a single type of relation among ads and keywords,\nsuch as query clicks or text similarity, which neglects rich heterogeneous\ninteractions hidden behind them. To fill this gap, the keyword matching problem\nfaces several challenges including: 1) how to learn enriched and robust\nembeddings from complex interactions among various types of objects; 2) how to\nconduct high-quality matching for new ads that usually lack sufficient data.\n  To address these challenges, we develop a\nheterogeneous-graph-neural-network-based model for keyword matching named\nHetMatch, which has been deployed both online and offline at the core sponsored\nsearch platform of Alibaba Group. To extract enriched and robust embeddings\namong rich relations, we design a hierarchical structure to fuse and enhance\nthe relevant neighborhood patterns both on the micro and the macro level.\nMoreover, by proposing a multi-view framework, the model is able to involve\nmore positive samples for cold-start ads. Experimental results on a large-scale\nindustrial dataset as well as online AB tests exhibit the effectiveness of\nHetMatch.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.13468,regular,pre_llm,2021,11,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'Emotion Embedding Spaces for Matching Music to Stories\n\n  Content creators often use music to enhance their stories, as it can be a\npowerful tool to convey emotion. In this paper, our goal is to help creators\nfind music to match the emotion of their story. We focus on text-based stories\nthat can be auralized (e.g., books), use multiple sentences as input queries,\nand automatically retrieve matching music. We formalize this task as a\ncross-modal text-to-music retrieval problem. Both the music and text domains\nhave existing datasets with emotion labels, but mismatched emotion vocabularies\nprevent us from using mood or emotion annotations directly for matching. To\naddress this challenge, we propose and investigate several emotion embedding\nspaces, both manually defined (e.g., valence/arousal) and data-driven (e.g.,\nWord2Vec and metric learning) to bridge this gap. Our experiments show that by\nleveraging these embedding spaces, we are able to successfully bridge the gap\nbetween modalities to facilitate cross modal retrieval. We show that our method\ncan leverage the well established valence-arousal space, but that it can also\nachieve our goal via data-driven embedding spaces. By leveraging data-driven\nembeddings, our approach has the potential of being generalized to other\nretrieval tasks that require broader or completely different vocabularies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.07068,regular,pre_llm,2021,11,"{'ai_likelihood': 8.841355641682943e-06, 'text': 'Study of keyword extraction techniques for Electric Double Layer\n  Capacitor domain using text similarity indexes: An experimental analysis\n\n  Keywords perform a significant role in selecting various topic-related\ndocuments quite easily. Topics or keywords assigned by humans or experts\nprovide accurate information. However, this practice is quite expensive in\nterms of resources and time management. Hence, it is more satisfying to utilize\nautomated keyword extraction techniques. Nevertheless, before beginning the\nautomated process, it is necessary to check and confirm how similar\nexpert-provided and algorithm-generated keywords are. This paper presents an\nexperimental analysis of similarity scores of keywords generated by different\nsupervised and unsupervised automated keyword extraction algorithms with expert\nprovided keywords from the Electric Double Layer Capacitor (EDLC) domain. The\npaper also analyses which texts provide better keywords like positive sentences\nor all sentences of the document. From the unsupervised algorithms, YAKE,\nTopicRank, MultipartiteRank, and KPMiner are employed for keyword extraction.\nFrom the supervised algorithms, KEA and WINGNUS are employed for keyword\nextraction. To assess the similarity of the extracted keywords with\nexpert-provided keywords, Jaccard, Cosine, and Cosine with word vector\nsimilarity indexes are employed in this study. The experiment shows that the\nMultipartiteRank keyword extraction technique measured with cosine with word\nvector similarity index produces the best result with 92% similarity with\nexpert provided keywords. This study can help the NLP researchers working with\nthe EDLC domain or recommender systems to select more suitable keyword\nextraction and similarity index calculation techniques.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.12618,regular,pre_llm,2021,11,"{'ai_likelihood': 1.8775463104248047e-05, 'text': ""Group based Personalized Search by Integrating Search Behaviour and\n  Friend Network\n\n  The key to personalized search is to build the user profile based on\nhistorical behaviour. To deal with the users who lack historical data, group\nbased personalized models were proposed to incorporate the profiles of similar\nusers when re-ranking the results. However, similar users are mostly found\nbased on simple lexical or topical similarity in search behaviours. In this\npaper, we propose a neural network enhanced method to highlight similar users\nin semantic space. Furthermore, we argue that the behaviour-based similar users\nare still insufficient to understand a new query when user's historical\nactivities are limited. To tackle this issue, we introduce the friend network\ninto personalized search to determine the closeness between users in another\nway. Since the friendship is often formed based on similar background or\ninterest, there are plenty of personalized signals hidden in the friend network\nnaturally. Specifically, we propose a friend network enhanced personalized\nsearch model, which groups the user into multiple friend circles based on\nsearch behaviours and friend relations respectively. These two types of friend\ncircles are complementary to construct a more comprehensive group profile for\nrefining the personalization. Experimental results show the significant\nimprovement of our model over existing personalized search models.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.12614,regular,pre_llm,2021,11,"{'ai_likelihood': 3.516674041748047e-05, 'text': 'PSSL: Self-supervised Learning for Personalized Search with Contrastive\n  Sampling\n\n  Personalized search plays a crucial role in improving user search experience\nowing to its ability to build user profiles based on historical behaviors.\nPrevious studies have made great progress in extracting personal signals from\nthe query log and learning user representations. However, neural personalized\nsearch is extremely dependent on sufficient data to train the user model. Data\nsparsity is an inevitable challenge for existing methods to learn high-quality\nuser representations. Moreover, the overemphasis on final ranking quality leads\nto rough data representations and impairs the generalizability of the model. To\ntackle these issues, we propose a Personalized Search framework with\nSelf-supervised Learning (PSSL) to enhance data representations. Specifically,\nwe adopt a contrastive sampling method to extract paired self-supervised\ninformation from sequences of user behaviors in query logs. Four auxiliary\ntasks are designed to pre-train the sentence encoder and the sequence encoder\nused in the ranking model. They are optimized by contrastive loss which aims to\nclose the distance between similar user sequences, queries, and documents.\nExperimental results on two datasets demonstrate that our proposed model PSSL\nachieves state-of-the-art performance compared with existing baselines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.13057,regular,pre_llm,2021,11,"{'ai_likelihood': 2.317958407931858e-07, 'text': ""Evaluating the Robustness of Retrieval Pipelines with Query Variation\n  Generators\n\n  Heavily pre-trained transformers for language modelling, such as BERT, have\nshown to be remarkably effective for Information Retrieval (IR) tasks,\ntypically applied to re-rank the results of a first-stage retrieval model. IR\nbenchmarks evaluate the effectiveness of retrieval pipelines based on the\npremise that a single query is used to instantiate the underlying information\nneed. However, previous research has shown that (I) queries generated by users\nfor a fixed information need are extremely variable and, in particular, (II)\nneural models are brittle and often make mistakes when tested with modified\ninputs. Motivated by those observations we aim to answer the following\nquestion: how robust are retrieval pipelines with respect to different\nvariations in queries that do not change the queries' semantics? In order to\nobtain queries that are representative of users' querying variability, we first\ncreated a taxonomy based on the manual annotation of transformations occurring\nin a dataset (UQV100) of user-created query variations. For each\nsyntax-changing category of our taxonomy, we employed different automatic\nmethods that when applied to a query generate a query variation. Our\nexperimental results across two datasets for two IR tasks reveal that retrieval\npipelines are not robust to these query variations, with effectiveness drops of\n$\\approx20\\%$ on average. The code and datasets are available at\nhttps://github.com/Guzpenha/query_variation_generators.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.11886,regular,pre_llm,2021,11,"{'ai_likelihood': 2.0530488755967883e-05, 'text': 'Learning Dynamic Preference Structure Embedding From Temporal Networks\n\n  The dynamics of temporal networks lie in the continuous interactions between\nnodes, which exhibit the dynamic node preferences with time elapsing. The\nchallenges of mining temporal networks are thus two-fold: the dynamic structure\nof networks and the dynamic node preferences. In this paper, we investigate the\ndynamic graph sampling problem, aiming to capture the preference structure of\nnodes dynamically in cooperation with GNNs. Our proposed Dynamic Preference\nStructure (DPS) framework consists of two stages: structure sampling and graph\nfusion. In the first stage, two parameterized samplers are designed to learn\nthe preference structure adaptively with network reconstruction tasks. In the\nsecond stage, an additional attention layer is designed to fuse two sampled\ntemporal subgraphs of a node, generating temporal node embeddings for\ndownstream tasks. Experimental results on many real-life temporal networks show\nthat our DPS outperforms several state-of-the-art methods substantially owing\nto learning an adaptive preference structure. The code will be released soon at\nhttps://github.com/doujiang-zheng/Dynamic-Preference-Structure.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.08229,regular,pre_llm,2021,11,"{'ai_likelihood': 4.165702395968967e-05, 'text': ""QA4PRF: A Question Answering based Framework for Pseudo Relevance\n  Feedback\n\n  Pseudo relevance feedback (PRF) automatically performs query expansion based\non top-retrieved documents to better represent the user's information need so\nas to improve the search results. Previous PRF methods mainly select expansion\nterms with high occurrence frequency in top-retrieved documents or with high\nsemantic similarity with the original query. However, existing PRF methods\nhardly try to understand the content of documents, which is very important in\nperforming effective query expansion to reveal the user's information need. In\nthis paper, we propose a QA-based framework for PRF called QA4PRF to utilize\ncontextual information in documents. In such a framework, we formulate PRF as a\nQA task, where the query and each top-retrieved document play the roles of\nquestion and context in the corresponding QA system, while the objective is to\nfind some proper terms to expand the original query by utilizing contextual\ninformation, which are similar answers in QA task. Besides, an attention-based\npointer network is built on understanding the content of top-retrieved\ndocuments and selecting the terms to represent the original query better. We\nalso show that incorporating the traditional supervised learning methods, such\nas LambdaRank, to integrate PRF information will further improve the\nperformance of QA4PRF. Extensive experiments on three real-world datasets\ndemonstrate that QA4PRF significantly outperforms the state-of-the-art methods.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.03396,regular,pre_llm,2021,12,"{'ai_likelihood': 2.317958407931858e-06, 'text': 'A Sensitivity Analysis of the MSMARCO Passage Collection\n\n  The recent MSMARCO passage retrieval collection has allowed researchers to\ndevelop highly tuned retrieval systems. One aspect of this data set that makes\nit distinctive compared to traditional corpora is that most of the topics only\nhave a single answer passage marked relevant. Here we carry out a ""what if""\nsensitivity study, asking whether a set of systems would still have the same\nrelative performance if more passages per topic were deemed to be ""relevant"",\nexploring several mechanisms for identifying sets of passages to be so\ncategorized. Our results show that, in general, while run scores can vary\nmarkedly if additional plausible passages are presumed to be relevant, the\nderived system ordering is relatively insensitive to additional relevance,\nproviding support for the methodology that was used at the time the MSMARCO\npassage collection was created.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.0256,review,pre_llm,2021,12,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'A comment-driven evidence appraisal approach for decision-making when\n  only uncertain evidence available\n\n  Purpose: To explore whether comments could be used as an assistant tool for\nheuristic decision-making, especially in cases where missing, incomplete,\nuncertain, or even incorrect evidence is acquired. Methods: Six COVID-19 drug\ncandidates were selected from WHO clinical guidelines. Evidence-comment\nnetworks (ECNs) were completed of these six drug candidates based on\nevidence-comment pairs from all PubMed indexed COVID-19 publications with\nformal published comments. WHO guidelines were utilized to validate the\nfeasibility of comment-derived evidence assertions as a fast decision\nsupporting tool. Results: Out of 6 drug candidates, comment-derived evidence\nassertions of leading subgraphs of 5 drugs were consistent with WHO guidelines,\nand the overall comment sentiment of 6 drugs was aligned with WHO clinical\nguidelines. Additionally, comment topics were in accordance with the concerns\nof guidelines and evidence appraisal criteria. Furthermore, half of the\ncritical comments emerged 4.5 months earlier than the date guidelines were\npublished. Conclusions: Comment-derived evidence assertions have the potential\nas an evidence appraisal tool for heuristic decisions based on the accuracy,\nsensitivity, and efficiency of evidence-comment networks. In essence, comments\nreflect that academic communities do have a self-screening evaluation and\nself-purification (argumentation) mechanism, thus providing a tool for decision\nmakers to filter evidence.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.09435,regular,pre_llm,2021,12,"{'ai_likelihood': 1.1424223581949871e-05, 'text': 'Product Information Browsing Support System Using Analytic Hierarchy\n  Process\n\n  Large-scale e-commerce sites can collect and analyze a large number of user\npreferences and behaviors, and thus can recommend highly trusted products to\nusers. However, it is very difficult for individuals or non-corporate groups to\nobtain large-scale user data. Therefore, we consider whether knowledge of the\ndecision-making domain can be used to obtain user preferences and combine it\nwith content-based filtering to design an information retrieval system. This\nstudy describes the process of building a product information browsing support\nsystem with high satisfaction based on product similarity and multiple other\nperspectives about products on the Internet. We present the architecture of the\nproposed system and explain the working principle of its constituent modules.\nFinally, we demonstrate the effectiveness of the proposed system through an\nevaluation experiment and a questionnaire.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.01944,regular,pre_llm,2021,12,"{'ai_likelihood': 3.2782554626464844e-06, 'text': 'Towards Low-loss 1-bit Quantization of User-item Representations for\n  Top-K Recommendation\n\n  Due to the promising advantages in space compression and inference\nacceleration, quantized representation learning for recommender systems has\nbecome an emerging research direction recently. As the target is to embed\nlatent features in the discrete embedding space, developing quantization for\nuser-item representations with a few low-precision integers confronts the\nchallenge of high information loss, thus leading to unsatisfactory performance\nin Top-K recommendation.\n  In this work, we study the problem of representation learning for\nrecommendation with 1-bit quantization. We propose a model named Low-loss\nQuantized Graph Convolutional Network (L^2Q-GCN). Different from previous work\nthat plugs quantization as the final encoder of user-item embeddings, L^2Q-GCN\nlearns the quantized representations whilst capturing the structural\ninformation of user-item interaction graphs at different semantic levels. This\nachieves the substantial retention of intermediate interactive information,\nalleviating the feature smoothing issue for ranking caused by numerical\nquantization. To further improve the model performance, we also present an\nadvanced solution named L^2Q-GCN-anl with quantization approximation and\nannealing training strategy. We conduct extensive experiments on four\nbenchmarks over Top-K recommendation task. The experimental results show that,\nwith nearly 9x representation storage compression, L^2Q-GCN-anl attains about\n90~99% performance recovery compared to the state-of-the-art model.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.00999,regular,pre_llm,2021,12,"{'ai_likelihood': 4.106097751193576e-06, 'text': 'Contrastive Cross-domain Recommendation in Matching\n\n  Cross-domain recommendation (CDR) aims to provide better recommendation\nresults in the target domain with the help of the source domain, which is\nwidely used and explored in real-world systems. However, CDR in the matching\n(i.e., candidate generation) module struggles with the data sparsity and\npopularity bias issues in both representation learning and knowledge transfer.\nIn this work, we propose a novel Contrastive Cross-Domain Recommendation (CCDR)\nframework for CDR in matching. Specifically, we build a huge diversified\npreference network to capture multiple information reflecting user diverse\ninterests, and design an intra-domain contrastive learning (intra-CL) and three\ninter-domain contrastive learning (inter-CL) tasks for better representation\nlearning and knowledge transfer. The intra-CL enables more effective and\nbalanced training inside the target domain via a graph augmentation, while the\ninter-CL builds different types of cross-domain interactions from user,\ntaxonomy, and neighbor aspects. In experiments, CCDR achieves significant\nimprovements on both offline and online evaluations in a real-world system.\nCurrently, we have deployed our CCDR on WeChat Top Stories, affecting plenty of\nusers. The source code is in https://github.com/lqfarmer/CCDR.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.15352,regular,pre_llm,2021,12,"{'ai_likelihood': 1.6391277313232422e-05, 'text': 'Intention Adaptive Graph Neural Network for Category-aware Session-based\n  Recommendation\n\n  Session-based recommendation (SBR) is proposed to recommend items within\nshort sessions given that user profiles are invisible in various scenarios\nnowadays, such as e-commerce and short video recommendation. There is a common\nscenario that user specifies a target category of items as a global filter,\nhowever previous SBR settings mainly consider the item sequence and overlook\nthe rich target category information. Therefore, we define a new task called\nCategory-aware Session-Based Recommendation (CSBR), focusing on the above\nscenario, in which the user-specified category can be efficiently utilized by\nthe recommendation system. To address the challenges of the proposed task, we\ndevelop a novel method called Intention Adaptive Graph Neural Network (IAGNN),\nwhich takes advantage of relationship between items and their categories to\nachieve an accurate recommendation result. Specifically, we construct a\ncategory-aware graph with both item and category nodes to represent the complex\ntransition information in the session. An intention-adaptive graph neural\nnetwork on the category-aware graph is utilized to capture user intention by\ntransferring the historical interaction information to the user-specified\ncategory domain. Extensive experiments on three real-world datasets are\nconducted to show our IAGNN outperforms the state-of-the-art baselines in the\nnew task.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.05798,regular,pre_llm,2021,12,"{'ai_likelihood': 1.8874804178873699e-06, 'text': 'MTLTS: A Multi-Task Framework To Obtain Trustworthy Summaries From\n  Crisis-Related Microblogs\n\n  Occurrences of catastrophes such as natural or man-made disasters trigger the\nspread of rumours over social media at a rapid pace. Presenting a trustworthy\nand summarized account of the unfolding event in near real-time to the\nconsumers of such potentially unreliable information thus becomes an important\ntask. In this work, we propose MTLTS, the first end-to-end solution for the\ntask that jointly determines the credibility and summary-worthiness of tweets.\nOur credibility verifier is designed to recursively learn the structural\nproperties of a Twitter conversation cascade, along with the stances of replies\ntowards the source tweet. We then take a hierarchical multi-task learning\napproach, where the verifier is trained at a lower layer, and the summarizer is\ntrained at a deeper layer where it utilizes the verifier predictions to\ndetermine the salience of a tweet. Different from existing disaster-specific\nsummarizers, we model tweet summarization as a supervised task. Such an\napproach can automatically learn summary-worthy features, and can therefore\ngeneralize well across domains. When trained on the PHEME dataset [29], not\nonly do we outperform the strongest baselines for the auxiliary task of\nverification/rumour detection, we also achieve 21 - 35% gains in the verified\nratio of summary tweets, and 16 - 20% gains in ROUGE1-F1 scores over the\nexisting state-of-the-art solutions for the primary task of trustworthy\nsummarization.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.03084,regular,pre_llm,2021,12,"{'ai_likelihood': 1.3874636756049263e-05, 'text': 'ZeroMat: Solving Cold-start Problem of Recommender System with No Input Data\n\nRecommender system is an applicable technique in most E-commerce commercial product technical designs. However, nearly all recommender system faces a challenge called the cold-start problem. The problem is so notorious that almost every industrial practitioner needs to resolve this issue when building recommender systems. Most cold-start problem solvers need some kind of data input as the starter of the system. On the other hand, many real-world applications place popular items or random items as recommendation results. In this paper, we propose a new technique called ZeroMat that requries no input data at all and predicts the user item rating data that is competitive in Mean Absolute Error and fairness metric compared with the classic matrix factorization with affluent data, and much better performance than random placement.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.12773,regular,pre_llm,2021,12,"{'ai_likelihood': 1.2252065870496963e-06, 'text': 'Customising Ranking Models for Enterprise Search on Bilingual\n  Click-Through Dataset\n\n  In this work, we provide the details about the process of establishing an\nend-to-end system for enterprise search on bilingual click-through dataset. The\nfirst part of the paper will be about the high-level workflow of the system.\nThen, in the second part we will elaborately mention about the ranking models\nto improve the search results in the vertical search of the technical documents\nin enterprise domain. Throughout the paper, we will mention the way which we\ncombine the methods in IR literature. Finally, in the last part of the paper we\nwill report our results using different ranking algorithms with $NDCG@k$ where\nk is the cut-off value.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.04666,regular,pre_llm,2021,12,"{'ai_likelihood': 3.586212793986003e-05, 'text': 'Densifying Sparse Representations for Passage Retrieval by\n  Representational Slicing\n\n  Learned sparse and dense representations capture different successful\napproaches to text retrieval and the fusion of their results has proven to be\nmore effective and robust. Prior work combines dense and sparse retrievers by\nfusing their model scores. As an alternative, this paper presents a simple\napproach to densifying sparse representations for text retrieval that does not\ninvolve any training. Our densified sparse representations (DSRs) are\ninterpretable and can be easily combined with dense representations for\nend-to-end retrieval. We demonstrate that our approach can jointly learn sparse\nand dense representations within a single model and then combine them for dense\nretrieval. Experimental results suggest that combining our DSRs and dense\nrepresentations yields a balanced tradeoff between effectiveness and\nefficiency.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.06668,regular,pre_llm,2021,12,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'CT4Rec: Simple yet Effective Consistency Training for Sequential\n  Recommendation\n\n  Sequential recommendation methods are increasingly important in cutting-edge\nrecommender systems. Through leveraging historical records, the systems can\ncapture user interests and perform recommendations accordingly.\nState-of-the-art sequential recommendation models proposed very recently\ncombine contrastive learning techniques for obtaining high-quality user\nrepresentations. Though effective and performing well, the models based on\ncontrastive learning require careful selection of data augmentation methods and\npretext tasks, efficient negative sampling strategies, and massive\nhyper-parameters validation. In this paper, we propose an ultra-simple\nalternative for obtaining better user representations and improving sequential\nrecommendation performance. Specifically, we present a simple yet effective\n\\textbf{C}onsistency \\textbf{T}raining method for sequential\n\\textbf{Rec}ommendation (CT4Rec) in which only two extra training objectives\nare utilized without any structural modifications and data augmentation.\nExperiments on three benchmark datasets and one large newly crawled industrial\ncorpus demonstrate that our proposed method outperforms SOTA models by a large\nmargin and with much less training time than these based on contrastive\nlearning. Online evaluation on real-world content recommendation system also\nachieves 2.717\\% improvement on the click-through rate and 3.679\\% increase on\nthe average click number per capita. Further exploration reveals that such a\nsimple method has great potential for CTR prediction. Our code is available at\n\\url{https://github.com/ct4rec/CT4Rec.git}.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.14444,review,pre_llm,2021,12,"{'ai_likelihood': 5.298190646701389e-07, 'text': 'Literature Review of the Pioneering Approaches in Cloud-based Search\n  Engines Powered by LETOR Techniques\n\n  Search engines play an essential role in our daily lives. Nonetheless, they\nare also very crucial in enterprise domain to access documents from various\ninformation sources. Since traditional search systems index the documents\nmainly by looking at the frequency of the occurring words in these documents,\nthey are barely able to support natural language search, but rather keyword\nsearch. It seems that keyword based search will not be sufficient for\nenterprise data which is growing extremely fast. Thus, enterprise search\nbecomes increasingly critical in corporate domain. In this report, we present\nan overview of the state-of-the-art technologies in literature for three main\npurposes: i) to increase the retrieval performance of a search engine, ii) to\ndeploy a search platform to a cloud environment, and iii) to select the best\nterms in expanding queries for achieving even a higher retrieval performance as\nwell as to provide good query suggestions to its users for a better user\nexperience.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.14958,regular,pre_llm,2021,12,"{'ai_likelihood': 0.00010384453667534723, 'text': 'A Benchmark Dataset for Micro-video Thumbnail Selection\n\n  The thumbnail, as the first sight of a micro-video, plays a pivotal role in\nattracting users to click and watch. Although several pioneer efforts have been\ndedicated to jointly considering the quality and representativeness for\nselecting the thumbnail, they are limited in exploring the influence of users`\ninterests. While in the real scenario, the more the thumbnails satisfy the\nusers, the more likely the micro-videos will be clicked. In this paper, we aim\nto select the thumbnail of a given micro-video that meets most users`\ninterests. Towards this end, we construct a large-scale dataset for the\nmicro-video thumbnails. Ultimately, we conduct several baselines on the dataset\nand demonstrate the effectiveness of our dataset.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.02242,regular,pre_llm,2021,12,"{'ai_likelihood': 2.1523899502224394e-06, 'text': ""Recommender systems: when memory matters\n\n  In this paper, we study the effect of long memory in the learnability of a\nsequential recommender system including users' implicit feedback. We propose an\nonline algorithm, where model parameters are updated user per user over blocks\nof items constituted by a sequence of unclicked items followed by a clicked\none. We illustrate through thorough empirical evaluations that filtering users\nwith respect to the degree of long memory contained in their interactions with\nthe system allows to substantially gain in performance with respect to MAP and\nNDCG, especially in the context of training large-scale Recommender Systems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.11775,regular,pre_llm,2021,12,"{'ai_likelihood': 1.953707800971137e-06, 'text': 'Multiple Choice Questions based Multi-Interest Policy Learning for\n  Conversational Recommendation\n\n  Conversational recommendation system (CRS) is able to obtain fine-grained and\ndynamic user preferences based on interactive dialogue. Previous CRS assumes\nthat the user has a clear target item. However, for many users who resort to\nCRS, they might not have a clear idea about what they really like.\nSpecifically, the user may have a clear single preference for some attribute\ntypes (e.g. color) of items, while for other attribute types, the user may have\nmultiple preferences or even no clear preferences, which leads to multiple\nacceptable attribute instances (e.g. black and red) of one attribute type.\nTherefore, the users could show their preferences over items under multiple\ncombinations of attribute instances rather than a single item with unique\ncombination of all attribute instances. As a result, we first propose a more\nrealistic CRS learning setting, namely Multi-Interest Multi-round\nConversational Recommendation, where users may have multiple interests in\nattribute instance combinations and accept multiple items with partially\noverlapped combinations of attribute instances. To effectively cope with the\nnew CRS learning setting, in this paper, we propose a novel learning framework\nnamely, Multi-Choice questions based Multi-Interest Policy Learning . In order\nto obtain user preferences more efficiently, the agent generates multi-choice\nquestions rather than binary yes/no ones on specific attribute instance.\nBesides, we propose a union set strategy to select candidate items instead of\nexisting intersection set strategy in order to overcome over-filtering items\nduring the conversation. Finally, we design a Multi-Interest Policy Learning\nmodule, which utilizes captured multiple interests of the user to decide next\naction, either asking attribute instances or recommending items. Extensive\nexperimental results on four datasets verify the superiority of our method for\nthe proposed setting.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.12802,regular,pre_llm,2021,12,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Biased or Not?: The Story of Two Search Engines\n\n  Search engines can be considered as a gate to the world of WEB, and they also\ndecide what we see for a given search query. Since many people are exposed to\ninformation through search engines, it is fair to expect that search engines\nshould be neutral; i.e. the returned results must cover all the elements or\naspects of the search topic, and they should be impartial where the results are\nreturned based on relevance. However, the search engine results are based on\nmany features and sophisticated algorithms where search neutrality is not\nnecessarily the focal point. In this work we performed an empirical study on\ntwo popular search engines and analysed the search engine result pages for\ncontroversial topics such as abortion, medical marijuana, and gay marriage. Our\nanalysis is based on the sentiment in search results to identify their\nviewpoint as conservative or liberal. We also propose three sentiment-based\nmetrics to show the existence of bias as well as to compare viewpoints of the\ntwo search engines. Extensive experiments performed on controversial topics\nshow that both search engines are biased, moreover they have the same kind of\nbias towards a given controversial topic.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.08606,regular,pre_llm,2021,12,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'An Empirical Study on Transfer Learning for Privilege Review\n\n  Protecting privileged communications and data from inadvertent disclosure is\na paramount task in the US legal practice. Traditionally counsels rely on\nkeyword searching and manual review to identify privileged documents in cases.\nAs data volumes increase, this approach becomes less and less defensible in\ncosts. Machine learning methods have been used in identifying privilege\ndocuments. Given the generalizable nature of privilege in legal cases, we\nhypothesize that transfer learning can capitalize knowledge learned from\nexisting labeled data to identify privilege documents without requiring\nlabeling new training data. In this paper, we study both traditional machine\nlearning models and deep learning models based on BERT for privilege document\nclassification tasks in legal document review, and we examine the effectiveness\nof transfer learning in privilege model on three real world datasets with\nprivilege labels. Our results show that BERT model outperforms the industry\nstandard logistic regression algorithm and transfer learning models can achieve\ndecent performance on datasets in same or close domains.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.02601,regular,pre_llm,2021,12,"{'ai_likelihood': 5.529986487494575e-06, 'text': ""Variational Autoencoder with CCA for Audio-Visual Cross-Modal Retrieval\n\n  Cross-modal retrieval is to utilize one modality as a query to retrieve data\nfrom another modality, which has become a popular topic in information\nretrieval, machine learning, and database. How to effectively measure the\nsimilarity between different modality data is the major challenge of\ncross-modal retrieval. Although several reasearch works have calculated the\ncorrelation between different modality data via learning a common subspace\nrepresentation, the encoder's ability to extract features from multi-modal\ninformation is not satisfactory. In this paper, we present a novel variational\nautoencoder (VAE) architecture for audio-visual cross-modal retrieval, by\nlearning paired audio-visual correlation embedding and category correlation\nembedding as constraints to reinforce the mutuality of audio-visual\ninformation. On the one hand, audio encoder and visual encoder separately\nencode audio data and visual data into two different latent spaces. Further,\ntwo mutual latent spaces are respectively constructed by canonical correlation\nanalysis (CCA). On the other hand, probabilistic modeling methods is used to\ndeal with possible noise and missing information in the data. Additionally, in\nthis way, the cross-modal discrepancy from intra-modal and inter-modal\ninformation are simultaneously eliminated in the joint embedding subspace. We\nconduct extensive experiments over two benchmark datasets. The experimental\noutcomes exhibit that the proposed architecture is effective in learning\naudio-visual correlation and is appreciably better than the existing\ncross-modal retrieval methods.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.0646,regular,pre_llm,2021,12,"{'ai_likelihood': 0.04367404513888889, 'text': ""Improving Sequential Recommendations via Bidirectional Temporal Data\n  Augmentation with Pre-training\n\n  Sequential recommendation systems are integral to discerning temporal user\npreferences. Yet, the task of learning from abbreviated user interaction\nsequences poses a notable challenge. Data augmentation has been identified as a\npotent strategy to enhance the informational richness of these sequences.\nTraditional augmentation techniques, such as item randomization, may disrupt\nthe inherent temporal dynamics. Although recent advancements in reverse\nchronological pseudo-item generation have shown promise, they can introduce\ntemporal discrepancies when assessed in a natural chronological context. In\nresponse, we introduce a sophisticated approach, Bidirectional temporal data\nAugmentation with pre-training (BARec). Our approach leverages bidirectional\ntemporal augmentation and knowledge-enhanced fine-tuning to synthesize\nauthentic pseudo-prior items that retain user preferences and capture deeper\nitem semantic correlations, thus boosting the model's expressive power. Our\ncomprehensive experimental analysis on five benchmark datasets confirms the\nsuperiority of BARec across both short and elongated sequence contexts.\nMoreover, theoretical examination and case study offer further insight into the\nmodel's logical processes and interpretability. The source code for our study\nis publicly available at https://github.com/juyongjiang/BARec.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.02787,regular,pre_llm,2021,12,"{'ai_likelihood': 3.311369154188368e-06, 'text': ""Gumble Softmax For User Behavior Modeling\n\n  Recently, sequential recommendation systems are important in solving the\ninformation overload in many online services. Current methods in sequential\nrecommendation focus on learning a fixed number of representations for each\nuser at any time, with a single representation or multi representations for the\nuser. However, when a user is exploring items on an e-commerce recommendation\nsystem, the number of this user's hobbies may change overtime (e.g.\nincrease/reduce one more interest), affected by the user's evolving self needs.\nMoreover, different users may have various number of interests. In this paper,\nwe argue that it is meaningful to explore a personalized dynamic number of user\ninterests, and learn a dynamic group of user interest representations\naccordingly. We propose a sequential model with dynamic number of\nrepresentations for recommendation systems (RDRSR). Specifically, RDRSR is\ncomposed of a dynamic interest discriminator (DID) module and a dynamic\ninterest allocator (DIA) module. The DID module explores the number of a user's\ninterests by learning the overall sequential characteristics with\nbi-directional self-attention and Gumbel-Softmax. The DIA module make the\nhistorical clicked items into a group of item groups and constructs user's\ndynamic interest representation. Additionally, experiments on the real-world\ndatasets demonstrates our model's effectiveness.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.08024,regular,pre_llm,2022,1,"{'ai_likelihood': 2.1788809034559464e-05, 'text': 'UKD: Debiasing Conversion Rate Estimation via Uncertainty-regularized\n  Knowledge Distillation\n\n  In online advertising, conventional post-click conversion rate (CVR)\nestimation models are trained using clicked samples. However, during online\nserving the models need to estimate for all impression ads, leading to the\nsample selection bias (SSB) issue. Intuitively, providing reliable supervision\nsignals for unclicked ads is a feasible way to alleviate the SSB issue. This\npaper proposes an uncertainty-regularized knowledge distillation (UKD)\nframework to debias CVR estimation via distilling knowledge from unclicked ads.\nA teacher model learns click-adaptive representations and produces\npseudo-conversion labels on unclicked ads as supervision signals. Then a\nstudent model is trained on both clicked and unclicked ads with knowledge\ndistillation, performing uncertainty modeling to alleviate the inherent noise\nin pseudo-labels. Experiments on billion-scale datasets show that UKD\noutperforms previous debiasing methods. Online results verify that UKD achieves\nsignificant improvements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.07599,regular,pre_llm,2022,1,"{'ai_likelihood': 2.8808911641438803e-06, 'text': ""repro_eval: A Python Interface to Reproducibility Measures of\n  System-oriented IR Experiments\n\n  In this work we introduce repro_eval - a tool for reactive reproducibility\nstudies of system-oriented information retrieval (IR) experiments. The\ncorresponding Python package provides IR researchers with measures for\ndifferent levels of reproduction when evaluating their systems' outputs. By\noffering an easily extensible interface, we hope to stimulate common practices\nwhen conducting a reproducibility study of system-oriented IR experiments.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.06716,review,pre_llm,2022,1,"{'ai_likelihood': 1.4901161193847656e-06, 'text': 'On the Opportunity of Causal Learning in Recommendation Systems:\n  Foundation, Estimation, Prediction and Challenges\n\n  Recently, recommender system (RS) based on causal inference has gained much\nattention in the industrial community, as well as the states of the art\nperformance in many prediction and debiasing tasks. Nevertheless, a unified\ncausal analysis framework has not been established yet. Many causal-based\nprediction and debiasing studies rarely discuss the causal interpretation of\nvarious biases and the rationality of the corresponding causal assumptions. In\nthis paper, we first provide a formal causal analysis framework to survey and\nunify the existing causal-inspired recommendation methods, which can\naccommodate different scenarios in RS. Then we propose a new taxonomy and give\nformal causal definitions of various biases in RS from the perspective of\nviolating the assumptions adopted in causal analysis. Finally, we formalize\nmany debiasing and prediction tasks in RS, and summarize the statistical and\nmachine learning-based causal estimation methods, expecting to provide new\nresearch opportunities and perspectives to the causal RS community.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.08622,regular,pre_llm,2022,1,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Reproducing Personalised Session Search over the AOL Query Log\n\n  Despite its troubled past, the AOL Query Log continues to be an important\nresource to the research community -- particularly for tasks like search\npersonalisation. When using the query log these ranking experiments, little\nattention is usually paid to the document corpus. Recent work typically uses a\ncorpus containing versions of the documents collected long after the log was\nproduced. Given that web documents are prone to change over time, we study the\ndifferences present between a version of the corpus containing documents as\nthey appeared in 2017 (which has been used by several recent works) and a new\nversion we construct that includes documents close to as they appeared at the\ntime the query log was produced (2006). We demonstrate that this new version of\nthe corpus has a far higher coverage of documents present in the original log\n(93%) than the 2017 version (55%). Among the overlapping documents, the content\noften differs substantially. Given these differences, we re-conduct session\nsearch experiments that originally used the 2017 corpus and find that when\nusing our corpus for training or evaluation, system performance improves. We\nplace the results in context by introducing recent adhoc ranking baselines. We\nalso confirm the navigational nature of the queries in the AOL corpus by\nshowing that including the URL substantially improves performance across a\nvariety of models. Our version of the corpus can be easily reconstructed by\nother researchers and is included in the ir-datasets package.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.05333,regular,pre_llm,2022,1,"{'ai_likelihood': 7.715490129258898e-06, 'text': 'Attention over Self-attention:Intention-aware Re-ranking with Dynamic\n  Transformer Encoders for Recommendation\n\n  Re-ranking models refine item recommendation lists generated by the prior\nglobal ranking model, which have demonstrated their effectiveness in improving\nthe recommendation quality. However, most existing re-ranking solutions only\nlearn from implicit feedback with a shared prediction model, which regrettably\nignore inter-item relationships under diverse user intentions. In this paper,\nwe propose a novel Intention-aware Re-ranking Model with Dynamic Transformer\nEncoder (RAISE), aiming to perform user-specific prediction for each individual\nuser based on her intentions. Specifically, we first propose to mine latent\nuser intentions from text reviews with an intention discovering module (IDM).\nBy differentiating the importance of review information with a co-attention\nnetwork, the latent user intention can be explicitly modeled for each user-item\npair. We then introduce a dynamic transformer encoder (DTE) to capture\nuser-specific inter-item relationships among item candidates by seamlessly\naccommodating the learned latent user intentions via IDM. As such, one can not\nonly achieve more personalized recommendations but also obtain corresponding\nexplanations by constructing RAISE upon existing recommendation engines.\nEmpirical study on four public datasets shows the superiority of our proposed\nRAISE, with up to 13.95%, 9.60%, and 13.03% relative improvements evaluated by\nPrecision@5, MAP@5, and NDCG@5 respectively.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.07754,regular,pre_llm,2022,1,"{'ai_likelihood': 6.5896246168348525e-06, 'text': 'Grep-BiasIR: A Dataset for Investigating Gender Representation-Bias in\n  Information Retrieval Results\n\n  The provided contents by information retrieval (IR) systems can reflect the\nexisting societal biases and stereotypes. Such biases in retrieval results can\nlead to further establishing and strengthening stereotypes in society and also\nin the systems. To facilitate the studies of gender bias in the retrieval\nresults of IR systems, we introduce Gender Representation-Bias for Information\nRetrieval (Grep-BiasIR), a novel thoroughly-audited dataset consisting of 118\nbias-sensitive neutral search queries. The set of queries covers a wide range\nof gender-related topics, for which a biased representation of genders in the\nsearch result can be considered as socially problematic. Each query is\naccompanied with one relevant and one non-relevant document, where the document\nis also provided in three variations of female, male, and neutral. The dataset\nis available at https://github.com/KlaraKrieg/GrepBiasIR.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.10782,regular,pre_llm,2022,1,"{'ai_likelihood': 6.225374009874132e-06, 'text': 'Causality and Correlation Graph Modeling for Effective and Explainable\n  Session-based Recommendation\n\n  Session-based recommendation which has been witnessed a booming interest\nrecently, focuses on predicting a user\'s next interested item(s) based on an\nanonymous session. Most existing studies adopt complex deep learning techniques\n(e.g., graph neural networks) for effective session-based recommendation.\nHowever, they merely address co-occurrence between items, but fail to well\ndistinguish causality and correlation relationship. Considering the varied\ninterpretations and characteristics of causality and correlation relationship\nbetween items, in this study, we propose a novel method denoted as CGSR by\njointly modeling causality and correlation relationship between items. In\nparticular, we construct cause, effect and correlation graphs from sessions by\nsimultaneously considering the false causality problem. We further design a\ngraph neural network-based method for session-based recommendation. To\nconclude, we strive to explore the relationship between items from specific\n``causality"" (directed) and ``correlation"" (undirected) perspectives. Extensive\nexperiments on three datasets show that our model outperforms other\nstate-of-the-art methods in terms of recommendation accuracy. Moreover, we\nfurther propose an explainable framework on CGSR, and demonstrate the\nexplainability of our model via case studies on Amazon dataset.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.06056,regular,pre_llm,2022,1,"{'ai_likelihood': 2.4172994825575088e-06, 'text': 'Debiased Recommendation with User Feature Balancing\n\n  Debiased recommendation has recently attracted increasing attention from both\nindustry and academic communities. Traditional models mostly rely on the\ninverse propensity score (IPS), which can be hard to estimate and may suffer\nfrom the high variance issue. To alleviate these problems, in this paper, we\npropose a novel debiased recommendation framework based on user feature\nbalancing. The general idea is to introduce a projection function to adjust\nuser feature distributions, such that the ideal unbiased learning objective can\nbe upper bounded by a solvable objective purely based on the offline dataset.\nIn the upper bound, the projected user distributions are expected to be equal\ngiven different items. From the causal inference perspective, this requirement\naims to remove the causal relation from the user to the item, which enables us\nto achieve unbiased recommendation, bypassing the computation of IPS. In order\nto efficiently balance the user distributions upon each item pair, we propose\nthree strategies, including clipping, sampling and adversarial learning to\nimprove the training process. For more robust optimization, we deploy an\nexplicit model to capture the potential latent confounders in recommendation\nsystems. To the best of our knowledge, this paper is the first work on debiased\nrecommendation based on confounder balancing. In the experiments, we compare\nour framework with many state-of-the-art methods based on synthetic,\nsemi-synthetic and real-world datasets. Extensive experiments demonstrate that\nour model is effective in promoting the recommendation performance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.0762,regular,pre_llm,2022,1,"{'ai_likelihood': 4.635916815863716e-07, 'text': 'Validating Simulations of User Query Variants\n\n  System-oriented IR evaluations are limited to rather abstract understandings\nof real user behavior. As a solution, simulating user interactions provides a\ncost-efficient way to support system-oriented experiments with more realistic\ndirectives when no interaction logs are available. While there are several user\nmodels for simulated clicks or result list interactions, very few attempts have\nbeen made towards query simulations, and it has not been investigated if these\ncan reproduce properties of real queries. In this work, we validate simulated\nuser query variants with the help of TREC test collections in reference to real\nuser queries that were made for the corresponding topics. Besides, we introduce\na simple yet effective method that gives better reproductions of real queries\nthan the established methods. Our evaluation framework validates the\nsimulations regarding the retrieval performance, reproducibility of topic score\ndistributions, shared task utility, effort and effect, and query term\nsimilarity when compared with real user query variants. While the retrieval\neffectiveness and statistical properties of the topic score distributions as\nwell as economic aspects are close to that of real queries, it is still\nchallenging to simulate exact term matches and later query reformulations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.04716,regular,pre_llm,2022,1,"{'ai_likelihood': 5.298190646701389e-06, 'text': ""Event-centric Query Suggestion for Online News\n\n  Query suggestion refers to the task of suggesting relevant and related\nqueries to a search engine user to help in query formulation process and to\nexpedite information retrieval with minimum amount of effort. It is highly\nuseful in situations where the search requirements are not well understood and\nhence it has been widely adopted by search engines to guide users' search\nactivity. For news websites, user queries have a time sensitive nature inherent\nin them. When some new event happens, there is a sudden burst in queries\nrelated to that event and such queries are sustained over a period of time\nbefore fading away with that event. In addition to this temporal aspect of\nsearch queries fired at news websites, they have an addition distinct quality,\ni.e., they are intended to get event related information majority of the times.\nExisting work on generating query suggestions involves analyzing query logs to\nsuggest queries which are relevant and related to the search intent of the\nuser. But in case of news websites, when there is a sudden burst in information\nrelated to a particular event, there are not enough search queries fired by\nother users which leads to lack of click data, and hence giving query\nsuggestions related to some old event or even some irrelevant suggestions\naltogether. Another problem with query logs in the context of online news is\nthat, they mostly contain queries related to popular events and hence fail to\ncapture less popular events or events which got overshadowed by some other more\nsensational event. We propose a novel approach to generate event-centric query\nsuggestions using metadata of news articles published by news media. We\ncompared our proposed framework with existing state of the art query suggestion\nmechanisms provided by Google News, Bing News, Google Search and Bing Search on\nvarious parameters.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.00235,regular,pre_llm,2022,1,"{'ai_likelihood': 1.2252065870496963e-06, 'text': ""Simulating and Modeling the Risk of Conversational Search\n\n  In conversational search, agents can interact with users by asking clarifying\nquestions to increase their chance to find better results. Many recent works\nand shared tasks in both NLP and IR communities have focused on identifying the\nneed of asking clarifying questions and methodologies of generating them. These\nworks assume asking clarifying questions is a safe alternative to retrieving\nresults. As existing conversational search models are far from perfect, it's\npossible and common that they could retrieve or generate bad clarifying\nquestions. Asking too many clarifying questions can also drain user's patience\nwhen the user prefers searching efficiency over correctness. Hence, these\nmodels can get backfired and harm user's search experience because of these\nrisks by asking clarifying questions.\n  In this work, we propose a simulation framework to simulate the risk of\nasking questions in conversational search and further revise a risk-aware\nconversational search model to control the risk. We show the model's robustness\nand effectiveness through extensive experiments on three conversations\ndatasets, including MSDialog, Ubuntu Dialog Corpus, and Opendialkg in which we\ncompare it with multiple baselines. We show that the risk-control module can\nwork with two different re-ranker models and outperform all the baselines in\nmost of our experiments.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.0929,regular,pre_llm,2022,1,"{'ai_likelihood': 1.3245476616753473e-07, 'text': ""Reinforcement Routing on Proximity Graph for Efficient Recommendation\n\n  We focus on Maximum Inner Product Search (MIPS), which is an essential\nproblem in many machine learning communities. Given a query, MIPS finds the\nmost similar items with the maximum inner products. Methods for Nearest\nNeighbor Search (NNS) which is usually defined on metric space don't exhibit\nthe satisfactory performance for MIPS problem since inner product is a\nnon-metric function. However, inner products exhibit many good properties\ncompared with metric functions, such as avoiding vanishing and exploding\ngradients. As a result, inner product is widely used in many recommendation\nsystems, which makes efficient Maximum Inner Product Search a key for speeding\nup many recommendation systems.\n  Graph based methods for NNS problem show the superiorities compared with\nother class methods. Each data point of the database is mapped to a node of the\nproximity graph. Nearest neighbor search in the database can be converted to\nroute on the proximity graph to find the nearest neighbor for the query. This\ntechnique can be used to solve MIPS problem. Instead of searching the nearest\nneighbor for the query, we search the item with maximum inner product with\nquery on the proximity graph. In this paper, we propose a reinforcement model\nto train an agent to search on the proximity graph automatically for MIPS\nproblem if we lack the ground truths of training queries. If we know the ground\ntruths of some training queries, our model can also utilize these ground truths\nby imitation learning to improve the agent's search ability. By experiments, we\ncan see that our proposed mode which combines reinforcement learning with\nimitation learning shows the superiorities over the state-of-the-art methods\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.02327,regular,pre_llm,2022,1,"{'ai_likelihood': 1.4238887363009983e-06, 'text': ""On the Effectiveness of Sampled Softmax Loss for Item Recommendation\n\n  The learning objective plays a fundamental role to build a recommender\nsystem. Most methods routinely adopt either pointwise or pairwise loss to train\nthe model parameters, while rarely pay attention to softmax loss due to its\ncomputational complexity when scaling up to large datasets or intractability\nfor streaming data. The sampled softmax (SSM) loss emerges as an efficient\nsubstitute for softmax loss. Its special case, InfoNCE loss, has been widely\nused in self-supervised learning and exhibited remarkable performance for\ncontrastive learning. Nonetheless, limited recommendation work uses the SSM\nloss as the learning objective. Worse still, none of them explores its\nproperties thoroughly and answers ``Does SSM loss suit for item\nrecommendation?'' and ``What are the conceptual advantages of SSM loss, as\ncompared with the prevalent losses?'', to the best of our knowledge.\n  In this work, we aim to offer a better understanding of SSM for item\nrecommendation. Specifically, we first theoretically reveal three\nmodel-agnostic advantages: (1) mitigating popularity bias; (2) mining hard\nnegative samples; and (3) maximizing the ranking metric. However, based on our\nempirical studies, we recognize that the default choice of cosine similarity\nfunction in SSM limits its ability in learning the magnitudes of representation\nvectors. As such, the combinations of SSM with the models that also fall short\nin adjusting magnitudes may result in poor representations. One step further,\nwe provide mathematical proof that message passing schemes in graph convolution\nnetworks can adjust representation magnitude according to node degree, which\nnaturally compensates for the shortcoming of SSM. Extensive experiments on four\nbenchmark datasets justify our analyses, demonstrating the superiority of SSM\nfor item recommendation. Our implementations are available in both TensorFlow\nand PyTorch.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.05973,regular,pre_llm,2022,1,"{'ai_likelihood': 6.324715084499783e-06, 'text': ""Multi-Sparse-Domain Collaborative Recommendation via Enhanced\n  Comprehensive Aspect Preference Learning\n\n  Cross-domain recommendation (CDR) has been attracting increasing attention of\nresearchers for its ability to alleviate the data sparsity problem in\nrecommender systems. However, the existing single-target or dual-target CDR\nmethods often suffer from two drawbacks, the assumption of at least one rich\ndomain and the heavy dependence on domain-invariant preference, which are\nimpractical in real world where sparsity is ubiquitous and might degrade the\nuser preference learning. To overcome these issues, we propose a\nMulti-Sparse-Domain Collaborative Recommendation (MSDCR) model for multi-target\ncross-domain recommendation. Unlike traditional CDR methods, MSDCR treats the\nmultiple relevant domains as all sparse and can simultaneously improve the\nrecommendation performance in each domain. We propose a Multi-Domain Separation\nNetwork (MDSN) and a Gated Aspect Preference Enhancement (GAPE) module for\nMSDCR to enhance a user's domain-specific aspect preferences in a domain by\ntransferring the complementary aspect preferences in other domains, during\nwhich the uniqueness of the domain-specific preference can be preserved through\nthe adversarial training offered by MDSN and the complementarity can be\nadaptively determined by GAPE. Meanwhile, we propose a Multi-Domain Adaptation\nNetwork (MDAN) for MSDCR to capture a user's domain-invariant aspect\npreference. With the integration of the enhanced domain-specific aspect\npreference and the domain-invariant aspect preference, MSDCR can reach a\ncomprehensive understanding of a user's preference in each sparse domain. At\nlast, the extensive experiments conducted on real datasets demonstrate the\nremarkable superiority of MSDCR over the state-of-the-art single-domain\nrecommendation models and CDR models.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.08614,review,pre_llm,2022,1,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'Consumer Fairness in Recommender Systems: Contextualizing Definitions\n  and Mitigations\n\n  Enabling non-discrimination for end-users of recommender systems by\nintroducing consumer fairness is a key problem, widely studied in both academia\nand industry. Current research has led to a variety of notions, metrics, and\nunfairness mitigation procedures. The evaluation of each procedure has been\nheterogeneous and limited to a mere comparison with models not accounting for\nfairness. It is hence hard to contextualize the impact of each mitigation\nprocedure w.r.t. the others. In this paper, we conduct a systematic analysis of\nmitigation procedures against consumer unfairness in rating prediction and\ntop-n recommendation tasks. To this end, we collected 15 procedures proposed in\nrecent top-tier conferences and journals. Only 8 of them could be reproduced.\nUnder a common evaluation protocol, based on two public data sets, we then\nstudied the extent to which recommendation utility and consumer fairness are\nimpacted by these procedures, the interplay between two primary fairness\nnotions based on equity and independence, and the demographic groups harmed by\nthe disparate impact. Our study finally highlights open challenges and future\ndirections in this field. The source code is available at\nhttps://github.com/jackmedda/C-Fairness-RecSys.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.0014,regular,pre_llm,2022,1,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Toward Pareto Efficient Fairness-Utility Trade-off inRecommendation\n  through Reinforcement Learning\n\n  The issue of fairness in recommendation is becoming increasingly essential as\nRecommender Systems touch and influence more and more people in their daily\nlives. In fairness-aware recommendation, most of the existing algorithmic\napproaches mainly aim at solving a constrained optimization problem by imposing\na constraint on the level of fairness while optimizing the main recommendation\nobjective, e.g., CTR. While this alleviates the impact of unfair\nrecommendations, the expected return of an approach may significantly\ncompromise the recommendation accuracy due to the inherent trade-off between\nfairness and utility. This motivates us to deal with these conflicting\nobjectives and explore the optimal trade-off between them in recommendation.\nOne conspicuous approach is to seek a Pareto efficient solution to guarantee\noptimal compromises between utility and fairness. Moreover, considering the\nneeds of real-world e-commerce platforms, it would be more desirable if we can\ngeneralize the whole Pareto Frontier, so that the decision-makers can specify\nany preference of one objective over another based on their current business\nneeds. Therefore, in this work, we propose a fairness-aware recommendation\nframework using multi-objective reinforcement learning, called MoFIR, which is\nable to learn a single parametric representation for optimal recommendation\npolicies over the space of all possible preferences. Specially, we modify\ntraditional DDPG by introducing conditioned network into it, which conditions\nthe networks directly on these preferences and outputs Q-value-vectors.\nExperiments on several real-world recommendation datasets verify the\nsuperiority of our framework on both fairness metrics and recommendation\nmeasures when compared with all other baselines. We also extract the\napproximate Pareto Frontier on real-world datasets generated by MoFIR and\ncompare to state-of-the-art fairness methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.04883,regular,pre_llm,2022,1,"{'ai_likelihood': 1.986821492513021e-06, 'text': ""Ontological model identification based on data from heterogeneous\n  sources\n\n  The development of a company often entails the emergence of autonomous data\nsources with different structural and technological organization. This can lead\nto the inability of data analysis at a high level and a violation of the\nintegrity and reliability of data within the organization, hindering the\nadoption of high-quality decisions and further development of the company. This\nproblem can be solved by implementing a higher abstraction, representing\nheterogeneous organization data in a single space by combining them into a\nsingle knowledge graph. We propose a framework capable of autonomous\nconstruction of an organization's knowledge graph based on semi-structured data\nfrom various sources by finding links between sources based on data with an\narbitrary structure, and combining document collections into single entities.\nThe results of tests show the applicability of the developed approach for\nconstructing a knowledge graph based on partially-structured data from various\nsources and the high efficiency of the approach based on the metrics of\ncompleteness of data storage subsystems coverage (11 out of 11) and filtering\nfalse connections (there are only 2.5 connections of collections with neighbors\non average in the final graph).\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.06592,regular,pre_llm,2022,1,"{'ai_likelihood': 8.874469333224826e-06, 'text': 'Proactive Query Expansion for Streaming Data Using External Source\n\n  Query expansion is the process of reformulating the original query by adding\nrelevant words. Choosing which terms to add in order to improve the performance\nof the query expansion methods or to enhance the quality of the retrieved\nresults is an important aspect of any information retrieval system. Adding\nwords that can positively impact the quality of the search query or are\ninformative enough play an important role in returning or gathering relevant\ndocuments that cover a certain topic can result in improving the efficiency of\nthe information retrieval system. Typically, query expansion techniques are\nused to add or substitute words to a given search query to collect relevant\ndata. In this paper, we design and implement a pipeline of automated query\nexpansion. We outline several tools using different methods to expand the\nquery. Our methods depend on targeting emergent events in streaming data over\ntime and finding the hidden topics from targeted documents using probabilistic\ntopic models. We employ Dynamic Eigenvector Centrality to trigger the emergent\nevents, and the Latent Dirichlet Allocation to discover the topics. Also, we\nuse an external data source as a secondary stream to supplement the primary\nstream with relevant words and expand the query using the words from both\nprimary and secondary streams. An experimental study is performed on Twitter\ndata (primary stream) related to the events that happened during protests in\nBaltimore in 2015. The quality of the retrieved results was measured using a\nquality indicator of the streaming data: tweets count, hashtag count, and\nhashtag clustering.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.10582,regular,pre_llm,2022,1,"{'ai_likelihood': 7.152557373046875e-06, 'text': 'Out-of-Domain Semantics to the Rescue! Zero-Shot Hybrid Retrieval Models\n\n  The pre-trained language model (eg, BERT) based deep retrieval models\nachieved superior performance over lexical retrieval models (eg, BM25) in many\npassage retrieval tasks. However, limited work has been done to generalize a\ndeep retrieval model to other tasks and domains. In this work, we carefully\nselect five datasets, including two in-domain datasets and three out-of-domain\ndatasets with different levels of domain shift, and study the generalization of\na deep model in a zero-shot setting. Our findings show that the performance of\na deep retrieval model is significantly deteriorated when the target domain is\nvery different from the source domain that the model was trained on. On the\ncontrary, lexical models are more robust across domains. We thus propose a\nsimple yet effective framework to integrate lexical and deep retrieval models.\nOur experiments demonstrate that these two models are complementary, even when\nthe deep model is weaker in the out-of-domain setting. The hybrid model obtains\nan average of 20.4% relative gain over the deep retrieval model, and an average\nof 9.54% over the lexical model in three out-of-domain datasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.03482,regular,pre_llm,2022,1,"{'ai_likelihood': 4.735257890489366e-06, 'text': ""Disentangled Graph Neural Networks for Session-based Recommendation\n\n  Session-based recommendation (SBR) has drawn increasingly research attention\nin recent years, due to its great practical value by only exploiting the\nlimited user behavior history in the current session. Existing methods\ntypically learn the session embedding at the item level, namely, aggregating\nthe embeddings of items with or without the attention weights assigned to\nitems. However, they ignore the fact that a user's intent on adopting an item\nis driven by certain factors of the item (e.g., the leading actors of an\nmovie). In other words, they have not explored finer-granularity interests of\nusers at the factor level to generate the session embedding, leading to\nsub-optimal performance. To address the problem, we propose a novel method\ncalled Disentangled Graph Neural Network (Disen-GNN) to capture the session\npurpose with the consideration of factor-level attention on each item.\nSpecifically, we first employ the disentangled learning technique to cast item\nembeddings into the embedding of multiple factors, and then use the gated graph\nneural network (GGNN) to learn the embedding factor-wisely based on the item\nadjacent similarity matrix computed for each factor. Moreover, the distance\ncorrelation is adopted to enhance the independence between each pair of\nfactors. After representing each item with independent factors, an attention\nmechanism is designed to learn user intent to different factors of each item in\nthe session. The session embedding is then generated by aggregating the item\nembeddings with attention weights of each item's factors. To this end, our\nmodel takes user intents at the factor level into account to infer the user\npurpose in a session. Extensive experiments on three benchmark datasets\ndemonstrate the superiority of our method over existing methods.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.06771,regular,pre_llm,2022,2,"{'ai_likelihood': 0.00016874737209743925, 'text': 'DS4DH at TREC Health Misinformation 2021: Multi-Dimensional Ranking\n  Models with Transfer Learning and Rank Fusion\n\n  This paper describes the work of the Data Science for Digital Health (DS4DH)\ngroup at the TREC Health Misinformation Track 2021. The TREC Health\nMisinformation track focused on the development of retrieval methods that\nprovide relevant, correct and credible information for health related searches\non the Web. In our methodology, we used a two-step ranking approach that\nincludes i) a standard retrieval phase, based on BM25 model, and ii) a\nre-ranking phase, with a pipeline of models focused on the usefulness,\nsupportiveness and credibility dimensions of the retrieved documents. To\nestimate the usefulness, we classified the initial rank list using pre-trained\nlanguage models based on the transformers architecture fine-tuned on the MS\nMARCO corpus. To assess the supportiveness, we utilized BERT-based models\nfine-tuned on scientific and Wikipedia corpora. Finally, to evaluate the\ncredibility of the documents, we employed a random forest model trained on the\nMicrosoft Credibility dataset combined with a list of credible sites. The\nresulting ranked lists were then combined using the Reciprocal Rank Fusion\nalgorithm to obtain the final list of useful, supporting and credible\ndocuments. Our approach achieved competitive results, being top-2 in the\ncompatibility measurement for the automatic runs. Our findings suggest that\nintegrating automatic ranking models created for each information quality\ndimension with transfer learning can increase the effectiveness of\nhealth-related information retrieval.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.03097,regular,pre_llm,2022,2,"{'ai_likelihood': 7.516807980007596e-06, 'text': ""Learn over Past, Evolve for Future: Search-based Time-aware\n  Recommendation with Sequential Behavior Data\n\n  The personalized recommendation is an essential part of modern e-commerce,\nwhere user's demands are not only conditioned by their profile but also by\ntheir recent browsing behaviors as well as periodical purchases made some time\nago. In this paper, we propose a novel framework named Search-based Time-Aware\nRecommendation (STARec), which captures the evolving demands of users over time\nthrough a unified search-based time-aware model. More concretely, we first\ndesign a search-based module to retrieve a user's relevant historical\nbehaviors, which are then mixed up with her recent records to be fed into a\ntime-aware sequential network for capturing her time-sensitive demands. Besides\nretrieving relevant information from her personal history, we also propose to\nsearch and retrieve similar user's records as an additional reference. All\nthese sequential records are further fused to make the final recommendation.\nBeyond this framework, we also develop a novel label trick that uses the\nprevious labels (i.e., user's feedbacks) as the input to better capture the\nuser's browsing pattern. We conduct extensive experiments on three real-world\ncommercial datasets on click-through-rate prediction tasks against\nstate-of-the-art methods. Experimental results demonstrate the superiority and\nefficiency of our proposed framework and techniques. Furthermore, results of\nonline experiments on a daily item recommendation platform of Company X show\nthat STARec gains average performance improvement of around 6% and 1.5% in its\ntwo main item recommendation scenarios on CTR metric respectively.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.07296,regular,pre_llm,2022,2,"{'ai_likelihood': 1.6225708855523005e-06, 'text': ""Roomsemble: Progressive web application for intuitive property search\n\n  A successful real estate search process involves locating a property that\nmeets a user's search criteria subject to an allocated budget and time\nconstraints. Many studies have investigated modeling housing prices over time.\nHowever, little is known about how a user's tastes influence their real estate\nsearch and purchase decisions. It is unknown what house a user would choose\ntaking into account an individual's personal tastes, behaviors, and\nconstraints, and, therefore, creating an algorithm that finds the perfect\nmatch. In this paper, we investigate the first step in understanding a user's\ntastes by building a system to capture personal preferences. We concentrated\nour research on real estate photos, being inspired by house aesthetics, which\noften motivates prospective buyers into considering a property as a candidate\nfor purchase. We designed a system that takes a user-provided photo\nrepresenting that person's personal taste and recommends properties similar to\nthe photo available on the market. The user can additionally filter the\nrecommendations by budget and location when conducting a property search. The\npaper describes the application's overall layout including frontend design and\nbackend processes for locating a desired property. The proposed model, which\nserves as the application's core, was tested with 25 users, and the study's\nfindings, as well as some key conclusions, are detailed in this paper.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.00373,regular,pre_llm,2022,2,"{'ai_likelihood': 1.357661353217231e-06, 'text': 'Improving BERT-based Query-by-Document Retrieval with Multi-Task\n  Optimization\n\n  Query-by-document (QBD) retrieval is an Information Retrieval task in which a\nseed document acts as the query and the goal is to retrieve related documents\n-- it is particular common in professional search tasks. In this work we\nimprove the retrieval effectiveness of the BERT re-ranker, proposing an\nextension to its fine-tuning step to better exploit the context of queries. To\nthis end, we use an additional document-level representation learning objective\nbesides the ranking objective when fine-tuning the BERT re-ranker. Our\nexperiments on two QBD retrieval benchmarks show that the proposed multi-task\noptimization significantly improves the ranking effectiveness without changing\nthe BERT re-ranker or using additional training samples. In future work, the\ngeneralizability of our approach to other retrieval tasks should be further\ninvestigated.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.02757,review,pre_llm,2022,2,"{'ai_likelihood': 1.3907750447591147e-06, 'text': ""A Review of Modern Fashion Recommender Systems\n\n  The textile and apparel industries have grown tremendously over the last few\nyears. Customers no longer have to visit many stores, stand in long queues, or\ntry on garments in dressing rooms as millions of products are now available in\nonline catalogs. However, given the plethora of options available, an effective\nrecommendation system is necessary to properly sort, order, and communicate\nrelevant product material or information to users. Effective fashion RS can\nhave a noticeable impact on billions of customers' shopping experiences and\nincrease sales and revenues on the provider side. The goal of this survey is to\nprovide a review of recommender systems that operate in the specific vertical\ndomain of garment and fashion products. We have identified the most pressing\nchallenges in fashion RS research and created a taxonomy that categorizes the\nliterature according to the objective they are trying to accomplish (e.g., item\nor outfit recommendation, size recommendation, explainability, among others)\nand type of side-information (users, items, context). We have also identified\nthe most important evaluation goals and perspectives (outfit generation, outfit\nrecommendation, pairing recommendation, and fill-in-the-blank outfit\ncompatibility prediction) and the most commonly used datasets and evaluation\nmetrics.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.1087,regular,pre_llm,2022,2,"{'ai_likelihood': 1.0960631900363498e-05, 'text': 'Socialformer: Social Network Inspired Long Document Modeling for\n  Document Ranking\n\n  Utilizing pre-trained language models has achieved great success for neural\ndocument ranking. Limited by the computational and memory requirements, long\ndocument modeling becomes a critical issue. Recent works propose to modify the\nfull attention matrix in Transformer by designing sparse attention patterns.\nHowever, most of them only focus on local connections of terms within a\nfixed-size window. How to build suitable remote connections between terms to\nbetter model document representation remains underexplored. In this paper, we\npropose the model Socialformer, which introduces the characteristics of social\nnetworks into designing sparse attention patterns for long document modeling in\ndocument ranking. Specifically, we consider several attention patterns to\nconstruct a graph like social networks. Endowed with the characteristic of\nsocial networks, most pairs of nodes in such a graph can reach with a short\npath while ensuring the sparsity. To facilitate efficient calculation, we\nsegment the graph into multiple subgraphs to simulate friend circles in social\nscenarios. Experimental results confirm the effectiveness of our model on long\ndocument modeling.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.1309,regular,pre_llm,2022,2,"{'ai_likelihood': 1.0265244377983941e-05, 'text': ""Disentangling Long and Short-Term Interests for Recommendation\n\n  Modeling user's long-term and short-term interests is crucial for accurate\nrecommendation. However, since there is no manually annotated label for user\ninterests, existing approaches always follow the paradigm of entangling these\ntwo aspects, which may lead to inferior recommendation accuracy and\ninterpretability. In this paper, to address it, we propose a Contrastive\nlearning framework to disentangle Long and Short-term interests for\nRecommendation (CLSR) with self-supervision. Specifically, we first propose two\nseparate encoders to independently capture user interests of different time\nscales. We then extract long-term and short-term interests proxies from the\ninteraction sequences, which serve as pseudo labels for user interests. Then\npairwise contrastive tasks are designed to supervise the similarity between\ninterest representations and their corresponding interest proxies. Finally,\nsince the importance of long-term and short-term interests is dynamically\nchanging, we propose to adaptively aggregate them through an attention-based\nnetwork for prediction. We conduct experiments on two large-scale real-world\ndatasets for e-commerce and short-video recommendation. Empirical results show\nthat our CLSR consistently outperforms all state-of-the-art models with\nsignificant improvements: GAUC is improved by over 0.01, and NDCG is improved\nby over 4%. Further counterfactual evaluations demonstrate that stronger\ndisentanglement of long and short-term interests is successfully achieved by\nCLSR. The code and data are available at\nhttps://github.com/tsinghua-fib-lab/CLSR.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.06602,review,pre_llm,2022,2,"{'ai_likelihood': 1.5232298109266494e-06, 'text': 'Neural Re-ranking in Multi-stage Recommender Systems: A Review\n\n  As the final stage of the multi-stage recommender system (MRS), re-ranking\ndirectly affects user experience and satisfaction by rearranging the input\nranking lists, and thereby plays a critical role in MRS. With the advances in\ndeep learning, neural re-ranking has become a trending topic and been widely\napplied in industrial applications. This review aims at integrating re-ranking\nalgorithms into a broader picture, and paving ways for more comprehensive\nsolutions for future research. For this purpose, we first present a taxonomy of\ncurrent methods on neural re-ranking. Then we give a description of these\nmethods along with the historic development according to their objectives. The\nnetwork structure, personalization, and complexity are also discussed and\ncompared. Next, we provide benchmarks of the major neural re-ranking models and\nquantitatively analyze their re-ranking performance. Finally, the review\nconcludes with a discussion on future prospects of this field. A list of papers\ndiscussed in this review, the benchmark datasets, our re-ranking library\nLibRerank, and detailed parameter settings are publicly available at\nhttps://github.com/LibRerank-Community/LibRerank.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.11827,regular,pre_llm,2022,2,"{'ai_likelihood': 3.344482845730252e-06, 'text': 'TARexp: A Python Framework for Technology-Assisted Review Experiments\n\n  Technology-assisted review (TAR) is an important industrial application of\ninformation retrieval (IR) and machine learning (ML). While a small TAR\nresearch community exists, the complexity of TAR software and workflows is a\nmajor barrier to entry. Drawing on past open source TAR efforts, as well as\ndesign patterns from the IR and ML open source software, we present an open\nsource Python framework for conducting experiments on TAR algorithms. Key\ncharacteristics of this framework are declarative representations of workflows\nand experiment plans, the ability for components to play variable numbers of\nworkflow roles, and state maintenance and restart capabilities. Users can draw\non reference implementations of standard TAR algorithms while incorporating\nnovel components to explore their research interests. The framework is\navailable at https://github.com/eugene-yang/tarexp.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.05317,regular,pre_llm,2022,2,"{'ai_likelihood': 5.778339174058702e-05, 'text': 'A Multi-task Learning Framework for Product Ranking with BERT\n\n  Product ranking is a crucial component for many e-commerce services. One of\nthe major challenges in product search is the vocabulary mismatch between query\nand products, which may be a larger vocabulary gap problem compared to other\ninformation retrieval domains. While there is a growing collection of neural\nlearning to match methods aimed specifically at overcoming this issue, they do\nnot leverage the recent advances of large language models for product search.\nOn the other hand, product ranking often deals with multiple types of\nengagement signals such as clicks, add-to-cart, and purchases, while most of\nthe existing works are focused on optimizing one single metric such as\nclick-through rate, which may suffer from data sparsity. In this work, we\npropose a novel end-to-end multi-task learning framework for product ranking\nwith BERT to address the above challenges. The proposed model utilizes\ndomain-specific BERT with fine-tuning to bridge the vocabulary gap and employs\nmulti-task learning to optimize multiple objectives simultaneously, which\nyields a general end-to-end learning framework for product search. We conduct a\nset of comprehensive experiments on a real-world e-commerce dataset and\ndemonstrate significant improvement of the proposed approach over the\nstate-of-the-art baseline methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.10326,regular,pre_llm,2022,2,"{'ai_likelihood': 5.7948960198296445e-06, 'text': 'A Deep Learning Approach for Repairing Missing Activity Labels in Event\n  Logs for Process Mining\n\n  Process mining is a relatively new subject that builds a bridge between\ntraditional process modeling and data mining. Process discovery is one of the\nmost critical parts of process mining, which aims at discovering process models\nautomatically from event logs. The performance of existing process discovery\nalgorithms can be affected when there are missing activity labels in event\nlogs. Several methods have been proposed to repair missing activity labels, but\ntheir accuracy can drop when a large number of activity labels are missing. In\nthis paper, we propose an LSTM-based prediction model to predict the missing\nactivity labels in event logs. The proposed model takes both the prefix and\nsuffix sequences of the events with missing activity labels as input.\nAdditional attributes of event logs are also utilized to improve the\nperformance. Our evaluation of several publicly available datasets shows that\nthe proposed method performed consistently better than existing methods in\nterms of repairing missing activity labels in event logs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.04972,regular,pre_llm,2022,2,"{'ai_likelihood': 4.337893591986763e-06, 'text': 'IHGNN: Interactive Hypergraph Neural Network for Personalized Product\n  Search\n\n  A good personalized product search (PPS) system should not only focus on\nretrieving relevant products, but also consider user personalized preference.\nRecent work on PPS mainly adopts the representation learning paradigm, e.g.,\nlearning representations for each entity (including user, product and query)\nfrom historical user behaviors (aka. user-product-query interactions). However,\nwe argue that existing methods do not sufficiently exploit the crucial\ncollaborative signal, which is latent in historical interactions to reveal the\naffinity between the entities. Collaborative signal is quite helpful for\ngenerating high-quality representation, exploiting which would benefit the\nrepresentation learning of one node from its connected nodes.\n  To tackle this limitation, in this work, we propose a new model IHGNN for\npersonalized product search. IHGNN resorts to a hypergraph constructed from the\nhistorical user-product-query interactions, which could completely preserve\nternary relations and express collaborative signal based on the topological\nstructure. On this basis, we develop a specific interactive hypergraph neural\nnetwork to explicitly encode the structure information (i.e., collaborative\nsignal) into the embedding process. It collects the information from the\nhypergraph neighbors and explicitly models neighbor feature interaction to\nenhance the representation of the target entity. Extensive experiments on three\nreal-world datasets validate the superiority of our proposal over the\nstate-of-the-arts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.11351,regular,pre_llm,2022,2,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'A Semi-Synthetic Dataset Generation Framework for Causal Inference in\n  Recommender Systems\n\n  Accurate recommendation and reliable explanation are two key issues for\nmodern recommender systems. However, most recommendation benchmarks only\nconcern the prediction of user-item ratings while omitting the underlying\ncauses behind the ratings. For example, the widely-used Yahoo!R3 dataset\ncontains little information on the causes of the user-movie ratings. A solution\ncould be to conduct surveys and require the users to provide such information.\nIn practice, the user surveys can hardly avoid compliance issues and sparse\nuser responses, which greatly hinders the exploration of causality-based\nrecommendation. To better support the studies of causal inference and further\nexplanations in recommender systems, we propose a novel semi-synthetic data\ngeneration framework for recommender systems where causal graphical models with\nmissingness are employed to describe the causal mechanism of practical\nrecommendation scenarios. To illustrate the use of our framework, we construct\na semi-synthetic dataset with Causal Tags And Ratings (CTAR), based on the\nmovies as well as their descriptive tags and rating information collected from\na famous movie rating website. Using the collected data and the causal graph,\nthe user-item-ratings and their corresponding user-item-tags are automatically\ngenerated, which provides the reasons (selected tags) why the user rates the\nitems. Descriptive statistics and baseline results regarding the CTAR dataset\nare also reported. The proposed data generation framework is not limited to\nrecommendation, and the released APIs can be used to generate customized\ndatasets for other research tasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.13607,review,pre_llm,2022,2,"{'ai_likelihood': 2.3312038845486113e-05, 'text': 'Are Big Recommendation Models Fair to Cold Users?\n\n  Big models are widely used by online recommender systems to boost\nrecommendation performance. They are usually learned on historical user\nbehavior data to infer user interest and predict future user behaviors (e.g.,\nclicks). In fact, the behaviors of heavy users with more historical behaviors\ncan usually provide richer clues than cold users in interest modeling and\nfuture behavior prediction. Big models may favor heavy users by learning more\nfrom their behavior patterns and bring unfairness to cold users. In this paper,\nwe study whether big recommendation models are fair to cold users. We\nempirically demonstrate that optimizing the overall performance of big\nrecommendation models may lead to unfairness to cold users in terms of\nperformance degradation. To solve this problem, we propose a BigFair method\nbased on self-distillation, which uses the model predictions on original user\ndata as a teacher to regularize predictions on augmented data with randomly\ndropped user behaviors, which can encourage the model to fairly capture\ninterest distributions of heavy and cold users. Experiments on two datasets\nshow that BigFair can effectively improve the performance fairness of big\nrecommendation models on cold users without harming the performance on heavy\nusers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.06337,regular,pre_llm,2022,2,"{'ai_likelihood': 1.953707800971137e-06, 'text': 'Learning to Rank from Relevance Judgments Distributions\n\n  Learning to Rank (LETOR) algorithms are usually trained on annotated corpora\nwhere a single relevance label is assigned to each available document-topic\npair. Within the Cranfield framework, relevance labels result from merging\neither multiple expertly curated or crowdsourced human assessments. In this\npaper, we explore how to train LETOR models with relevance judgments\ndistributions (either real or synthetically generated) assigned to\ndocument-topic pairs instead of single-valued relevance labels. We propose five\nnew probabilistic loss functions to deal with the higher expressive power\nprovided by relevance judgments distributions and show how they can be applied\nboth to neural and GBM architectures. Moreover, we show how training a LETOR\nmodel on a sampled version of the relevance judgments from certain probability\ndistributions can improve its performance when relying either on traditional or\nprobabilistic loss functions. Finally, we validate our hypothesis on real-world\ncrowdsourced relevance judgments distributions. Overall, we observe that\nrelying on relevance judgments distributions to train different LETOR models\ncan boost their performance and even outperform strong baselines such as\nLambdaMART on several test collections.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.12524,regular,pre_llm,2022,2,"{'ai_likelihood': 3.029902776082357e-05, 'text': 'MAMDR: A Model Agnostic Learning Method for Multi-Domain Recommendation\n\n  Large-scale e-commercial platforms in the real-world usually contain various\nrecommendation scenarios (domains) to meet demands of diverse customer groups.\nMulti-Domain Recommendation (MDR), which aims to jointly improve\nrecommendations on all domains and easily scales to thousands of domains, has\nattracted increasing attention from practitioners and researchers. Existing MDR\nmethods usually employ a shared structure and several specific components to\nrespectively leverage reusable features and domain-specific information.\nHowever, data distribution differs across domains, making it challenging to\ndevelop a general model that can be applied to all circumstances. Additionally,\nduring training, shared parameters often suffer from the domain conflict while\nspecific parameters are inclined to overfitting on data sparsity domains. we\nfirst present a scalable MDR platform served in Taobao that enables to provide\nservices for thousands of domains without specialists involved. To address the\nproblems of MDR methods, we propose a novel model agnostic learning framework,\nnamely MAMDR, for the multi-domain recommendation. Specifically, we first\npropose a Domain Negotiation (DN) strategy to alleviate the conflict between\ndomains. Then, we develop a Domain Regularization (DR) to improve the\ngeneralizability of specific parameters by learning from other domains. We\nintegrate these components into a unified framework and present MAMDR, which\ncan be applied to any model structure to perform multi-domain recommendation.\nFinally, we present a large-scale implementation of MAMDR in the Taobao\napplication and construct various public MDR benchmark datasets which can be\nused for following studies. Extensive experiments on both benchmark datasets\nand industry datasets demonstrate the effectiveness and generalizability of\nMAMDR.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.05456,regular,pre_llm,2022,2,"{'ai_likelihood': 5.993578169080946e-06, 'text': 'NEAT: A Label Noise-resistant Complementary Item Recommender System with\n  Trustworthy Evaluation\n\n  The complementary item recommender system (CIRS) recommends the complementary\nitems for a given query item. Existing CIRS models consider the item\nco-purchase signal as a proxy of the complementary relationship due to the lack\nof human-curated labels from the huge transaction records. These methods\nrepresent items in a complementary embedding space and model the complementary\nrelationship as a point estimation of the similarity between items vectors.\nHowever, co-purchased items are not necessarily complementary to each other.\nFor example, customers may frequently purchase bananas and bottled water within\nthe same transaction, but these two items are not complementary. Hence, using\nco-purchase signals directly as labels will aggravate the model performance. On\nthe other hand, the model evaluation will not be trustworthy if the labels for\nevaluation are not reflecting the true complementary relatedness. To address\nthe above challenges from noisy labeling of the copurchase data, we model the\nco-purchases of two items as a Gaussian distribution, where the mean denotes\nthe co-purchases from the complementary relatedness, and covariance denotes the\nco-purchases from the noise. To do so, we represent each item as a Gaussian\nembedding and parameterize the Gaussian distribution of co-purchases by the\nmeans and covariances from item Gaussian embedding. To reduce the impact of the\nnoisy labels during evaluation, we propose an independence test-based method to\ngenerate a trustworthy label set with certain confidence. Our extensive\nexperiments on both the publicly available dataset and the large-scale\nreal-world dataset justify the effectiveness of our proposed model in\ncomplementary item recommendations compared with the state-of-the-art models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.04514,regular,pre_llm,2022,2,"{'ai_likelihood': 4.106097751193576e-06, 'text': ""A Model-Agnostic Causal Learning Framework for Recommendation using\n  Search Data\n\n  Machine-learning based recommender systems(RSs) has become an effective means\nto help people automatically discover their interests. Existing models often\nrepresent the rich information for recommendation, such as items, users, and\ncontexts, as embedding vectors and leverage them to predict users' feedback. In\nthe view of causal analysis, the associations between these embedding vectors\nand users' feedback are a mixture of the causal part that describes why an item\nis preferred by a user, and the non-causal part that merely reflects the\nstatistical dependencies between users and items, for example, the exposure\nmechanism, public opinions, display position, etc. However, existing RSs mostly\nignored the striking differences between the causal parts and non-causal parts\nwhen using these embedding vectors. In this paper, we propose a model-agnostic\nframework named IV4Rec that can effectively decompose the embedding vectors\ninto these two parts, hence enhancing recommendation results. Specifically, we\njointly consider users' behaviors in search scenarios and recommendation\nscenarios. Adopting the concepts in causal analysis, we embed users' search\nbehaviors as instrumental variables (IVs), to help decompose original embedding\nvectors in recommendation, i.e., treatments. IV4Rec then combines the two parts\nthrough deep neural networks and uses the combined results for recommendation.\nIV4Rec is model-agnostic and can be applied to a number of existing RSs such as\nDIN and NRHUB. Experimental results on both public and proprietary industrial\ndatasets demonstrate that IV4Rec consistently enhances RSs and outperforms a\nframework that jointly considers search and recommendation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.07376,regular,pre_llm,2022,2,"{'ai_likelihood': 1.556343502468533e-06, 'text': 'Deep-QPP: A Pairwise Interaction-based Deep Learning Model for\n  Supervised Query Performance Prediction\n\n  Motivated by the recent success of end-to-end deep neural models for ranking\ntasks, we present here a supervised end-to-end neural approach for query\nperformance prediction (QPP). In contrast to unsupervised approaches that rely\non various statistics of document score distributions, our approach is entirely\ndata-driven. Further, in contrast to weakly supervised approaches, our method\nalso does not rely on the outputs from different QPP estimators. In particular,\nour model leverages information from the semantic interactions between the\nterms of a query and those in the top-documents retrieved with it. The\narchitecture of the model comprises multiple layers of 2D convolution filters\nfollowed by a feed-forward layer of parameters. Experiments on standard test\ncollections demonstrate that our proposed supervised approach outperforms other\nstate-of-the-art supervised and unsupervised approaches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.06197,regular,pre_llm,2022,2,"{'ai_likelihood': 3.7087334526909726e-06, 'text': 'Web-Based File Clustering and Indexing for Mindoro State University\n\n  The Web Based File Clustering and Indexing for Mindoro State University aim\nto organize data circulated over the Web into groups or collections to\nfacilitate data availability and access and at the same time meet user\npreferences. The main benefits include increasing Web information\naccessibility, understanding users navigation behavior, improving information\nretrieval and content delivery on the Web. Web based file clustering could help\nin reaching the required documents that the user is searching for. In this\npaper a novel approach has been introduced for search results clustering that\nis based on the semantics of the retrieved documents rather than the syntax of\nthe terms in those documents. Data clustering was used to improve the\ninformation retrieval from the collection of documents. Data were processed and\nanalyzed using SPSS where the instrument was evaluated to test the reliability\nand validity of the measures used. Evaluation was based on a Likert scale of\nExcellent, Good, Fair, and Poor as described for the selected quality\ncharacteristics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.15876,review,pre_llm,2022,3,"{'ai_likelihood': 1.304679446750217e-05, 'text': 'Self-Supervised Learning for Recommender Systems: A Survey\n\n  In recent years, neural architecture-based recommender systems have achieved\ntremendous success, but they still fall short of expectation when dealing with\nhighly sparse data. Self-supervised learning (SSL), as an emerging technique\nfor learning from unlabeled data, has attracted considerable attention as a\npotential solution to this issue. This survey paper presents a systematic and\ntimely review of research efforts on self-supervised recommendation (SSR).\nSpecifically, we propose an exclusive definition of SSR, on top of which we\ndevelop a comprehensive taxonomy to divide existing SSR methods into four\ncategories: contrastive, generative, predictive, and hybrid. For each category,\nwe elucidate its concept and formulation, the involved methods, as well as its\npros and cons. Furthermore, to facilitate empirical comparison, we release an\nopen-source library SELFRec (https://github.com/Coder-Yu/SELFRec), which\nincorporates a wide range of SSR models and benchmark datasets. Through\nrigorous experiments using this library, we derive and report some significant\nfindings regarding the selection of self-supervised signals for enhancing\nrecommendation. Finally, we shed light on the limitations in the current\nresearch and outline the future research directions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.14232,regular,pre_llm,2022,3,"{'ai_likelihood': 1.5629662407769098e-05, 'text': 'Leveraging Search History for Improving Person-Job Fit\n\n  As the core technique of online recruitment platforms, person-job fit can\nimprove hiring efficiency by accurately matching job positions with qualified\ncandidates. However, existing studies mainly focus on the recommendation\nscenario, while neglecting another important channel for linking positions with\njob seekers, i.e. search. Intuitively, search history contains rich user\nbehavior in job seeking, reflecting important evidence for job intention of\nusers. In this paper, we present a novel Search History enhanced Person-Job Fit\nmodel, named as SHPJF. To utilize both text content from jobs/resumes and\nsearch histories from users, we propose two components with different purposes.\nFor text matching component, we design a BERT-based text encoder for capturing\nthe semantic interaction between resumes and job descriptions. For intention\nmodeling component, we design two kinds of intention modeling approaches based\non the Transformer architecture, either based on the click sequence or query\ntext sequence. To capture underlying job intentions, we further propose an\nintention clustering technique to identify and summarize the major intentions\nfrom search logs. Extensive experiments on a large real-world recruitment\ndataset have demonstrated the effectiveness of our approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.05824,review,pre_llm,2022,3,"{'ai_likelihood': 2.2186173333062066e-06, 'text': ""Towards Analyzing the Bias of News Recommender Systems Using Sentiment\n  and Stance Detection\n\n  News recommender systems are used by online news providers to alleviate\ninformation overload and to provide personalized content to users. However,\nalgorithmic news curation has been hypothesized to create filter bubbles and to\nintensify users' selective exposure, potentially increasing their vulnerability\nto polarized opinions and fake news. In this paper, we show how information on\nnews items' stance and sentiment can be utilized to analyze and quantify the\nextent to which recommender systems suffer from biases. To that end, we have\nannotated a German news corpus on the topic of migration using stance detection\nand sentiment analysis. In an experimental evaluation with four different\nrecommender systems, our results show a slight tendency of all four models for\nrecommending articles with negative sentiments and stances against the topic of\nrefugees and migration. Moreover, we observed a positive correlation between\nthe sentiment and stance bias of the text-based recommenders and the\npreexisting user bias, which indicates that these systems amplify users'\nopinions and decrease the diversity of recommended news. The knowledge-aware\nmodel appears to be the least prone to such biases, at the cost of predictive\naccuracy.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.00537,regular,pre_llm,2022,3,"{'ai_likelihood': 3.940529293484158e-06, 'text': 'DynamicRetriever: A Pre-training Model-based IR System with Neither\n  Sparse nor Dense Index\n\n  Web search provides a promising way for people to obtain information and has\nbeen extensively studied. With the surgence of deep learning and large-scale\npre-training techniques, various neural information retrieval models are\nproposed and they have demonstrated the power for improving search (especially,\nthe ranking) quality. All these existing search methods follow a common\nparadigm, i.e. index-retrieve-rerank, where they first build an index of all\ndocuments based on document terms (i.e., sparse inverted index) or\nrepresentation vectors (i.e., dense vector index), then retrieve and rerank\nretrieved documents based on similarity between the query and documents via\nranking models. In this paper, we explore a new paradigm of information\nretrieval with neither sparse nor dense index but only a model. Specifically,\nwe propose a pre-training model-based IR system called DynamicRetriever. As for\nthis system, the training stage embeds the token-level and document-level\ninformation (especially, document identifiers) of the corpus into the model\nparameters, then the inference stage directly generates document identifiers\nfor a given query. Compared with existing search methods, the model-based IR\nsystem has two advantages: i) it parameterizes the traditional static index\nwith a pre-training model, which converts the document semantic mapping into a\ndynamic and updatable process; ii) with separate document identifiers, it\ncaptures both the term-level and document-level information for each document.\nExtensive experiments conducted on the public search benchmark MS MARCO verify\nthe effectiveness and potential of our proposed new paradigm for information\nretrieval.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.14278,regular,pre_llm,2022,3,"{'ai_likelihood': 1.655684577094184e-06, 'text': 'StruBERT: Structure-aware BERT for Table Search and Matching\n\n  A large amount of information is stored in data tables. Users can search for\ndata tables using a keyword-based query. A table is composed primarily of data\nvalues that are organized in rows and columns providing implicit structural\ninformation. A table is usually accompanied by secondary information such as\nthe caption, page title, etc., that form the textual information. Understanding\nthe connection between the textual and structural information is an important\nyet neglected aspect in table retrieval as previous methods treat each source\nof information independently. In addition, users can search for data tables\nthat are similar to an existing table, and this setting can be seen as a\ncontent-based table retrieval. In this paper, we propose StruBERT, a\nstructure-aware BERT model that fuses the textual and structural information of\na data table to produce context-aware representations for both textual and\ntabular content of a data table. StruBERT features are integrated in a new\nend-to-end neural ranking model to solve three table-related downstream tasks:\nkeyword- and content-based table retrieval, and table similarity. We evaluate\nour approach using three datasets, and we demonstrate substantial improvements\nin terms of retrieval and classification metrics over state-of-the-art methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.15328,regular,pre_llm,2022,3,"{'ai_likelihood': 2.682209014892578e-06, 'text': 'Compact Token Representations with Contextual Quantization for Efficient\n  Document Re-ranking\n\n  Transformer based re-ranking models can achieve high search relevance through\ncontext-aware soft matching of query tokens with document tokens. To alleviate\nruntime complexity of such inference, previous work has adopted a late\ninteraction architecture with pre-computed contextual token representations at\nthe cost of a large online storage. This paper proposes contextual quantization\nof token embeddings by decoupling document-specific and document-independent\nranking contributions during codebook-based compression. This allows effective\nonline decompression and embedding composition for better search relevance.\nThis paper presents an evaluation of the above compact token representation\nmodel in terms of relevance and space efficiency.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.16942,regular,pre_llm,2022,3,"{'ai_likelihood': 2.086162567138672e-06, 'text': 'Sequential Recommendation with User Evolving Preference Decomposition\n\n  Modeling user sequential behaviors has recently attracted increasing\nattention in the recommendation domain. Existing methods mostly assume coherent\npreference in the same sequence. However, user personalities are volatile and\neasily changed, and there can be multiple mixed preferences underlying user\nbehaviors. To solve this problem, in this paper, we propose a novel sequential\nrecommender model via decomposing and modeling user independent preferences. To\nachieve this goal, we highlight three practical challenges considering the\ninconsistent, evolving and uneven nature of the user behavior, which are seldom\nnoticed by the previous work. For overcoming these challenges in a unified\nframework, we introduce a reinforcement learning module to simulate the\nevolution of user preference. More specifically, the action aims to allocate\neach item into a sub-sequence or create a new one according to how the previous\nitems are decomposed as well as the time interval between successive behaviors.\nThe reward is associated with the final loss of the learning objective, aiming\nto generate sub-sequences which can better fit the training data. We conduct\nextensive experiments based on six real-world datasets across different\ndomains. Compared with the state-of-the-art methods, empirical studies manifest\nthat our model can on average improve the performance by about 8.21%, 10.08%,\n10.32%, and 9.82% on the metrics of Precision, Recall, NDCG and MRR,\nrespectively.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.01256,regular,pre_llm,2022,3,"{'ai_likelihood': 2.2086832258436418e-05, 'text': ""Recommendations in a Multi-Domain Setting: Adapting for Customization,\n  Scalability and Real-Time Performance\n\n  In this industry talk at ECIR'2022, we illustrate how to build a modern\nrecommender system that can serve recommendations in real-time for a diverse\nset of application domains. Specifically, we present our system architecture\nthat utilizes popular recommendation algorithms from the literature such as\nCollaborative Filtering, Content-based Filtering as well as various neural\nembedding approaches (e.g., Doc2Vec, Autoencoders, etc.). We showcase the\napplicability of our system architecture using two real-world use-cases, namely\nproviding recommendations for the domains of (i) job marketplaces, and (ii)\nentrepreneurial start-up founding. We strongly believe that our experiences\nfrom both research- and industry-oriented settings should be of interest for\npractitioners in the field of real-time multi-domain recommender systems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.0543,review,pre_llm,2022,3,"{'ai_likelihood': 1.980198754204644e-05, 'text': 'Overview of LiLAS 2021 -- Living Labs for Academic Search\n\n  The Living Labs for Academic Search (LiLAS) lab aims to strengthen the\nconcept of user-centric living labs for academic search. The methodological gap\nbetween real-world and lab-based evaluation should be bridged by allowing lab\nparticipants to evaluate their retrieval approaches in two real-world academic\nsearch systems from life sciences and social sciences. This overview paper\noutlines the two academic search systems LIVIVO and GESIS Search, and their\ncorresponding tasks within LiLAS, which are ad-hoc retrieval and dataset\nrecommendation. The lab is based on a new evaluation infrastructure named\nSTELLA that allows participants to submit results corresponding to their\nexperimental systems in the form of pre-computed runs and Docker containers\nthat can be integrated into production systems and generate experimental\nresults in real-time. Both submission types are interleaved with the results\nprovided by the productive systems allowing for a seamless presentation and\nevaluation. The evaluation of results and a meta-analysis of the different\ntasks and submission types complement this overview.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.13922,review,pre_llm,2022,3,"{'ai_likelihood': 5.9604644775390625e-06, 'text': 'AutoML for Deep Recommender Systems: A Survey\n\n  Recommender systems play a significant role in information filtering and have\nbeen utilized in different scenarios, such as e-commerce and social media. With\nthe prosperity of deep learning, deep recommender systems show superior\nperformance by capturing non-linear information and item-user relationships.\nHowever, the design of deep recommender systems heavily relies on human\nexperiences and expert knowledge. To tackle this problem, Automated Machine\nLearning (AutoML) is introduced to automatically search for the proper\ncandidates for different parts of deep recommender systems. This survey\nperforms a comprehensive review of the literature in this field. Firstly, we\npropose an abstract concept for AutoML for deep recommender systems\n(AutoRecSys) that describes its building blocks and distinguishes it from\nconventional AutoML techniques and recommender systems. Secondly, we present a\ntaxonomy as a classification framework containing feature selection search,\nembedding dimension search, feature interaction search, model architecture\nsearch, and other components search. Furthermore, we put a particular emphasis\non the search space and search strategy, as they are the common thread to\nconnect all methods within each category and enable practitioners to analyze\nand compare various approaches. Finally, we propose four future promising\nresearch directions that will lead this line of research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.05954,regular,pre_llm,2022,3,"{'ai_likelihood': 6.357828776041667e-06, 'text': 'An Adaptive Hybrid Active Learning Strategy with Free Ratings in\n  Collaborative Filtering\n\n  Recommender systems are information retrieval methods that predict user\npreferences to personalize services. These systems use the feedback and the\nratings provided by users to model the behavior of users and to generate\nrecommendations. Typically, the ratings are quite sparse, i.e., only a small\nfraction of items are rated by each user. To address this issue and enhance the\nperformance, active learning strategies can be used to select the most\ninformative items to be rated. This rating elicitation procedure enriches the\ninteraction matrix with informative ratings and therefore assists the\nrecommender system to better model the preferences of the users. In this paper,\nwe evaluate various non-personalized and personalized rating elicitation\nstrategies. We also propose a hybrid strategy that adaptively combines a\nnon-personalized and a personalized strategy. Furthermore, we propose a new\nprocedure to obtain free ratings based on the side information of the items. We\nevaluate these ideas on the MovieLens dataset. The experiments reveal that our\nproposed hybrid strategy outperforms the strategies from the literature. We\nalso propose the extent to which free ratings are obtained, improving further\nthe performance and also the user experience.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.01731,review,pre_llm,2022,3,"{'ai_likelihood': 8.576446109347873e-06, 'text': 'Do Perceived Gender Biases in Retrieval Results Affect Relevance\n  Judgements?\n\n  This work investigates the effect of gender-stereotypical biases in the\ncontent of retrieved results on the relevance judgement of users/annotators. In\nparticular, since relevance in information retrieval (IR) is a\nmulti-dimensional concept, we study whether the value and quality of the\nretrieved documents for some bias-sensitive queries can be judged differently\nwhen the content of the documents represents different genders. To this aim, we\nconduct a set of experiments where the genders of the participants are known as\nwell as experiments where the participants genders are not specified. The set\nof experiments comprise of retrieval tasks, where participants perform a rated\nrelevance judgement for different search query and search result document\ncompilations. The shown documents contain different gender indications and are\neither relevant or non-relevant to the query. The results show the differences\nbetween the average judged relevance scores among documents with various gender\ncontents. Our work initiates further research on the connection of the\nperception of gender stereotypes in users with their judgements and effects on\nIR systems, and aim to raise awareness about the possible biases in this\ndomain.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.00897,regular,pre_llm,2022,3,"{'ai_likelihood': 3.344482845730252e-06, 'text': 'An Effective Way for Cross-Market Recommendation with Hybrid Pre-Ranking\n  and Ranking Models\n\n  The Cross-Market Recommendation task of WSDM CUP 2022 is about finding\nsolutions to improve individual recommendation systems in resource-scarce\ntarget markets by leveraging data from similar high-resource source markets.\nFinally, our team OPDAI won the first place with NDCG@10 score of 0.6773 on the\nleaderboard. Our solution to this task will be detailed in this paper. To\nbetter transform information from source markets to target markets, we adopt\ntwo stages of ranking. In pre-ranking stage, we adopt diverse pre-ranking\nmethods or models to do feature generation. After elaborate feature analysis\nand feature selection, we train LightGBM with 10-fold bagging to do the final\nranking.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.0542,regular,pre_llm,2022,3,"{'ai_likelihood': 2.086162567138672e-06, 'text': 'Evaluating Elements of Web-based Data Enrichment for Pseudo-Relevance\n  Feedback Retrieval\n\n  In this work, we analyze a pseudo-relevance retrieval method based on the\nresults of web search engines. By enriching topics with text data from web\nsearch engine result pages and linked contents, we train topic-specific and\ncost-efficient classifiers that can be used to search test collections for\nrelevant documents. Building upon attempts initially made at TREC Common Core\n2018 by Grossman and Cormack, we address questions of system performance over\ntime considering different search engines, queries, and test collections. Our\nexperimental results show how and to which extent the considered components\naffect the retrieval performance. Overall, the analyzed method is robust in\nterms of average retrieval performance and a promising way to use web content\nfor the data enrichment of relevance feedback methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.13956,regular,pre_llm,2022,3,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'From Intervention to Domain Transportation: A Novel Perspective to\n  Optimize Recommendation\n\n  The interventional nature of recommendation has attracted increasing\nattention in recent years. It particularly motivates researchers to formulate\nlearning and evaluating recommendation as causal inference and data\nmissing-not-at-random problems. However, few take seriously the consequence of\nviolating the critical assumption of overlapping, which we prove can\nsignificantly threaten the validity and interpretation of the outcome. We find\na critical piece missing in the current understanding of information retrieval\n(IR) systems: as interventions, recommendation not only affects the already\nobserved data, but it also interferes with the target domain (distribution) of\ninterest. We then rephrase optimizing recommendation as finding an intervention\nthat best transports the patterns it learns from the observed domain to its\nintervention domain. Towards this end, we use domain transportation to\ncharacterize the learning-intervention mechanism of recommendation. We design a\nprincipled transportation-constraint risk minimization objective and convert it\nto a two-player minimax game. We prove the consistency, generalization, and\nexcessive risk bounds for the proposed objective, and elaborate how they\ncompare to the current results. Finally, we carry out extensive real-data and\nsemi-synthetic experiments to demonstrate the advantage of our approach, and\nlaunch online testing with a real-world IR system.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.0035,regular,pre_llm,2022,3,"{'ai_likelihood': 2.076228459676107e-05, 'text': ""Results Merging in the Patent Domain\n\n  In this paper, we test machine learning methods for results merging in patent\ndocument retrieval. Specifically, we examine random forest, decision tree,\nsupport vector machine (SVR), linear regression, polynomial regression, and\ndeep neural networks (DNNs). We use two different methods for results merging,\nthe multiple models (MM) method and the global model method (GM). Furthermore,\nwe examine whether the ranking of the document's scores is linearly\nexplainable. The CLEF-IP 2011 standard test collection was used in our\nexperiments. The random forest produces the best results in comparison to all\nother models, and it fits the data better than linear and polynomial\napproaches.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.13962,regular,pre_llm,2022,3,"{'ai_likelihood': 7.616149054633247e-07, 'text': ""Tutorial: Modern Theoretical Tools for Understanding and Designing\n  Next-generation Information Retrieval System\n\n  In the relatively short history of machine learning, the subtle balance\nbetween engineering and theoretical progress has been proved critical at\nvarious stages. The most recent wave of AI has brought to the IR community\npowerful techniques, particularly for pattern recognition. While many benefits\nfrom the burst of ideas as numerous tasks become algorithmically feasible, the\nbalance is tilting toward the application side. The existing theoretical tools\nin IR can no longer explain, guide, and justify the newly-established\nmethodologies.\n  The consequences can be suffering: in stark contrast to how the IR industry\nhas envisioned modern AI making life easier, many are experiencing increased\nconfusion and costs in data manipulation, model selection, monitoring,\ncensoring, and decision making. This reality is not surprising: without handy\ntheoretical tools, we often lack principled knowledge of the pattern\nrecognition model's expressivity, optimization property, generalization\nguarantee, and our decision-making process has to rely on over-simplified\nassumptions and human judgments from time to time.\n  Time is now to bring the community a systematic tutorial on how we\nsuccessfully adapt those tools and make significant progress in understanding,\ndesigning, and eventually productionize impactful IR systems. We emphasize\nsystematicity because IR is a comprehensive discipline that touches upon\nparticular aspects of learning, causal inference analysis, interactive (online)\ndecision-making, etc. It thus requires systematic calibrations to render the\nactual usefulness of the imported theoretical tools to serve IR problems, as\nthey usually exhibit unique structures and definitions. Therefore, we plan this\ntutorial to systematically demonstrate our learning and successful experience\nof using advanced theoretical tools for understanding and designing IR systems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.07939,regular,pre_llm,2022,3,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Strategic Thinking for Sustainable Practice-Centered Computing\n\n  This book was a trigger for me to reflect on all the projects we have\nconducted at the local, regional, national, and European levels around\nhealthcare, addressing the social isolation of elderlies, the social support\namong caregivers, and the coordination issues of professionals willing to take\ncare for patients at their home. Since Ina Wagner interviewed me in Paris, the\nsituation evolved in France as a new healthcare law was voted that emphasizes\nthe importance of cooperation among healthcare practitioners, the need for IT\nsupport, and the key role of local territories (which is not obvious in France,\nthat is a very centralized country). The reflection that started with this book\ndefinitely helped me to make decisions in this evolving context, and I am\ngrateful to Claudia M{\\""u}ller who, through an interview, facilitated me to\nrealize what are, from my experience, the main issues that have to be dealt\nwith when aiming at ensuring a sustainable practice-based computing approach.\nIn this epilogue, I briefly list and illustrate these main issues, hoping they\ncould support other sustainable experiences.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.11163,regular,pre_llm,2022,3,"{'ai_likelihood': 3.3775965372721357e-06, 'text': 'Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math\n  Information Retrieval\n\n  With the recent success of dense retrieval methods based on bi-encoders,\nstudies have applied this approach to various interesting downstream retrieval\ntasks with good efficiency and in-domain effectiveness. Recently, we have also\nseen the presence of dense retrieval models in Math Information Retrieval (MIR)\ntasks, but the most effective systems remain classic retrieval methods that\nconsider hand-crafted structure features. In this work, we try to combine the\nbest of both worlds:\\ a well-defined structure search method for effective\nformula search and efficient bi-encoder dense retrieval models to capture\ncontextual similarities. Specifically, we have evaluated two representative\nbi-encoder models for token-level and passage-level dense retrieval on recent\nMIR tasks. Our results show that bi-encoder models are highly complementary to\nexisting structure search methods, and we are able to advance the\nstate-of-the-art on MIR datasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.01155,review,pre_llm,2022,3,"{'ai_likelihood': 5.927350785997179e-06, 'text': 'Top-N Recommendation Algorithms: A Quest for the State-of-the-Art\n\n  Research on recommender systems algorithms, like other areas of applied\nmachine learning, is largely dominated by efforts to improve the\nstate-of-the-art, typically in terms of accuracy measures. Several recent\nresearch works however indicate that the reported improvements over the years\nsometimes ""don\'t add up"", and that methods that were published several years\nago often outperform the latest models when evaluated independently. Different\nfactors contribute to this phenomenon, including that some researchers probably\noften only fine-tune their own models but not the baselines. In this paper, we\nreport the outcomes of an in-depth, systematic, and reproducible comparison of\nten collaborative filtering algorithms - covering both traditional and neural\nmodels - on several common performance measures on three datasets which are\nfrequently used for evaluation in the recent literature. Our results show that\nthere is no consistent winner across datasets and metrics for the examined\ntop-n recommendation task. Moreover, we find that for none of the accuracy\nmeasurements any of the considered neural models led to the best performance.\nRegarding the performance ranking of algorithms across the measurements, we\nfound that linear models, nearest-neighbor methods, and traditional matrix\nfactorization consistently perform well for the evaluated modest-sized, but\ncommonly-used datasets. Our work shall therefore serve as a guideline for\nresearchers regarding existing baselines to consider in future performance\ncomparisons. Moreover, by providing a set of fine-tuned baseline models for\ndifferent datasets, we hope that our work helps to establish a common\nunderstanding of the state-of-the-art for top-n recommendation tasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.13596,regular,pre_llm,2022,4,"{'ai_likelihood': 4.3047799004448785e-07, 'text': ""Generative Multi-hop Retrieval\n\n  A common practice for text retrieval is to use an encoder to map the\ndocuments and the query to a common vector space and perform a nearest neighbor\nsearch (NNS); multi-hop retrieval also often adopts the same paradigm, usually\nwith a modification of iteratively reformulating the query vector so that it\ncan retrieve different documents at each hop. However, such a bi-encoder\napproach has limitations in multi-hop settings; (1) the reformulated query gets\nlonger as the number of hops increases, which further tightens the embedding\nbottleneck of the query vector, and (2) it is prone to error propagation. In\nthis paper, we focus on alleviating these limitations in multi-hop settings by\nformulating the problem in a fully generative way. We propose an\nencoder-decoder model that performs multi-hop retrieval by simply generating\nthe entire text sequences of the retrieval targets, which means the query and\nthe documents interact in the language model's parametric space rather than L2\nor inner product space as in the bi-encoder approach. Our approach, Generative\nMulti-hop Retrieval(GMR), consistently achieves comparable or higher\nperformance than bi-encoder models in five datasets while demonstrating\nsuperior GPU memory and storage footprint.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.00824,regular,pre_llm,2022,4,"{'ai_likelihood': 2.3510720994737412e-06, 'text': ""Graph-based Approximate NN Search: A Revisit\n\n  Nearest neighbor search plays a fundamental role in many disciplines such as\nmultimedia information retrieval, data-mining, and machine learning. The\ngraph-based search approaches show superior performance over other types of\napproaches in recent studies. In this paper, the graph-based NN search is\nrevisited. We optimize two key components in the approach, namely the search\nprocedure and the graph that supports the search. For the graph construction, a\ntwo-stage graph diversification scheme is proposed, which makes a good\ntrade-off between the efficiency and reachability for the search procedure that\nbuilds upon it. Moreover, the proposed diversification scheme allows the search\nprocedure to decide dynamically how many nodes should be visited in one node's\nneighborhood. By this way, the computing power of the devices is fully utilized\nwhen the search is carried out under different circumstances. Furthermore, two\nNN search procedures are designed respectively for small and large batch\nqueries on the GPU. The optimized NN search, when being supported by the\ntwo-stage diversified graph, outperforms all the state-of-the-art approaches on\nboth the CPU and the GPU across all the considered large-scale datasets.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.13844,regular,pre_llm,2022,4,"{'ai_likelihood': 1.0629494984944662e-05, 'text': 'User-controllable Recommendation Against Filter Bubbles\n\n  Recommender systems usually face the issue of filter bubbles:\noverrecommending homogeneous items based on user features and historical\ninteractions. Filter bubbles will grow along the feedback loop and\ninadvertently narrow user interests. Existing work usually mitigates filter\nbubbles by incorporating objectives apart from accuracy such as diversity and\nfairness. However, they typically sacrifice accuracy, hurting model fidelity\nand user experience. Worse still, users have to passively accept the\nrecommendation strategy and influence the system in an inefficient manner with\nhigh latency, e.g., keeping providing feedback (e.g., like and dislike) until\nthe system recognizes the user intention.\n  This work proposes a new recommender prototype called UserControllable\nRecommender System (UCRS), which enables users to actively control the\nmitigation of filter bubbles. Functionally, 1) UCRS can alert users if they are\ndeeply stuck in filter bubbles. 2) UCRS supports four kinds of control commands\nfor users to mitigate the bubbles at different granularities. 3) UCRS can\nrespond to the controls and adjust the recommendations on the fly. The key to\nadjusting lies in blocking the effect of out-of-date user representations on\nrecommendations, which contains historical information inconsistent with the\ncontrol commands. As such, we develop a causality-enhanced User-Controllable\nInference (UCI) framework, which can quickly revise the recommendations based\non user controls in the inference stage and utilize counterfactual inference to\nmitigate the effect of out-of-date user representations. Experiments on three\ndatasets validate that the UCI framework can effectively recommend more desired\nitems based on user controls, showing promising performance w.r.t. both\naccuracy and diversity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.0937,regular,pre_llm,2022,4,"{'ai_likelihood': 1.079506344265408e-05, 'text': ""Multi-Level Interaction Reranking with User Behavior History\n\n  As the final stage of the multi-stage recommender system (MRS), reranking\ndirectly affects users' experience and satisfaction, thus playing a critical\nrole in MRS. Despite the improvement achieved in the existing work, three\nissues are yet to be solved. First, users' historical behaviors contain rich\npreference information, such as users' long and short-term interests, but are\nnot fully exploited in reranking. Previous work typically treats items in\nhistory equally important, neglecting the dynamic interaction between the\nhistory and candidate items. Second, existing reranking models focus on\nlearning interactions at the item level while ignoring the fine-grained\nfeature-level interactions. Lastly, estimating the reranking score on the\nordered initial list before reranking may lead to the early scoring problem,\nthereby yielding suboptimal reranking performance. To address the above issues,\nwe propose a framework named Multi-level Interaction Reranking (MIR). MIR\ncombines low-level cross-item interaction and high-level set-to-list\ninteraction, where we view the candidate items to be reranked as a set and the\nusers' behavior history in chronological order as a list. We design a novel\nSLAttention structure for modeling the set-to-list interactions with\npersonalized long-short term interests. Moreover, feature-level interactions\nare incorporated to capture the fine-grained influence among items. We design\nMIR in such a way that any permutation of the input items would not change the\noutput ranking, and we theoretically prove it. Extensive experiments on three\npublic and proprietary datasets show that MIR significantly outperforms the\nstate-of-the-art models using various ranking and utility metrics.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.08146,regular,pre_llm,2022,4,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'PrivateRec: Differentially Private Training and Serving for Federated\n  News Recommendation\n\n  Collecting and training over sensitive personal data raise severe privacy\nconcerns in personalized recommendation systems, and federated learning can\npotentially alleviate the problem by training models over decentralized user\ndata.However, a theoretically private solution in both the training and serving\nstages of federated recommendation is essential but still lacking.Furthermore,\nnaively applying differential privacy (DP) to the two stages in federated\nrecommendation would fail to achieve a satisfactory trade-off between privacy\nand utility due to the high-dimensional characteristics of model gradients and\nhidden representations.In this work, we propose a federated news recommendation\nmethod for achieving a better utility in model training and online serving\nunder a DP guarantee.We first clarify the DP definition over behavior data for\neach round in the life-circle of federated recommendation systems.Next, we\npropose a privacy-preserving online serving mechanism under this definition\nbased on the idea of decomposing user embeddings with public basic vectors and\nperturbing the lower-dimensional combination coefficients. We apply a random\nbehavior padding mechanism to reduce the required noise intensity for better\nutility. Besides, we design a federated recommendation model training method,\nwhich can generate effective and public basic vectors for serving while\nproviding DP for training participants. We avoid the dimension-dependent noise\nfor large models via label permutation and differentially private attention\nmodules. Experiments on real-world news recommendation datasets validate that\nour method achieves superior utility under a DP guarantee in both training and\nserving of federated news recommendations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.04959,regular,pre_llm,2022,4,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'HAKG: Hierarchy-Aware Knowledge Gated Network for Recommendation\n\n  Knowledge graph (KG) plays an increasingly important role to improve the\nrecommendation performance and interpretability. A recent technical trend is to\ndesign end-to-end models based on information propagation schemes. However,\nexisting propagation-based methods fail to (1) model the underlying\nhierarchical structures and relations, and (2) capture the high-order\ncollaborative signals of items for learning high-quality user and item\nrepresentations.\n  In this paper, we propose a new model, called Hierarchy-Aware Knowledge Gated\nNetwork (HAKG), to tackle the aforementioned problems. Technically, we model\nusers and items (that are captured by a user-item graph), as well as entities\nand relations (that are captured in a KG) in hyperbolic space, and design a\nhyperbolic aggregation scheme to gather relational contexts over KG. Meanwhile,\nwe introduce a novel angle constraint to preserve characteristics of items in\nthe embedding space. Furthermore, we propose a dual item embeddings design to\nrepresent and propagate collaborative signals and knowledge associations\nseparately, and leverage the gated aggregation to distill discriminative\ninformation for better capturing user behavior patterns. Experimental results\non three benchmark datasets show that, HAKG achieves significant improvement\nover the state-of-the-art methods like CKAN, Hyper-Know, and KGIN. Further\nanalyses on the learned hyperbolic embeddings confirm that HAKG offers\nmeaningful insights into the hierarchies of data.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.10362,review,pre_llm,2022,4,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'Human Preferences as Dueling Bandits\n\n  The dramatic improvements in core information retrieval tasks engendered by\nneural rankers create a need for novel evaluation methods. If every ranker\nreturns highly relevant items in the top ranks, it becomes difficult to\nrecognize meaningful differences between them and to build reusable test\ncollections. Several recent papers explore pairwise preference judgments as an\nalternative to traditional graded relevance assessments. Rather than viewing\nitems one at a time, assessors view items side-by-side and indicate the one\nthat provides the better response to a query, allowing fine-grained\ndistinctions. If we employ preference judgments to identify the probably best\nitems for each query, we can measure rankers by their ability to place these\nitems as high as possible. We frame the problem of finding best items as a\ndueling bandits problem. While many papers explore dueling bandits for online\nranker evaluation via interleaving, they have not been considered as a\nframework for offline evaluation via human preference judgments. We review the\nliterature for possible solutions. For human preference judgments, any usable\nalgorithm must tolerate ties, since two items may appear nearly equal to\nassessors, and it must minimize the number of judgments required for any\nspecific pair, since each such comparison requires an independent assessor.\nSince the theoretical guarantees provided by most algorithms depend on\nassumptions that are not satisfied by human preference judgments, we simulate\nselected algorithms on representative test cases to provide insight into their\npractical utility. Based on these simulations, one algorithm stands out for its\npotential. Our simulations suggest modifications to further improve its\nperformance. Using the modified algorithm, we collect over 10,000 preference\njudgments for submissions to the TREC 2021 Deep Learning Track, confirming its\nsuitability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.13016,regular,pre_llm,2022,4,"{'ai_likelihood': 8.212195502387152e-06, 'text': ""RankMat : Matrix Factorization with Calibrated Distributed Embedding and\n  Fairness Enhancement\n\n  Matrix Factorization is a widely adopted technique in the field of\nrecommender system. Matrix Factorization techniques range from SVD, LDA, pLSA,\nSVD++, MatRec, Zipf Matrix Factorization and Item2Vec. In recent years,\ndistributed word embeddings have inspired innovation in the area of recommender\nsystems. Word2vec and GloVe have been especially emphasized in many industrial\napplication scenario such as Xiaomi's recommender system. In this paper, we\npropose a new matrix factorization inspired by the theory of power law and\nGloVe. Instead of the exponential nature of GloVe model, we take advantage of\nPareto Distribution to model our loss function. Our method is explainable in\ntheory and easy-to-implement in practice. In the experiment section, we prove\nour approach is superior to vanilla matrix factorization technique and\ncomparable with GloVe-based model in both accuracy and fairness metrics.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.02659,regular,pre_llm,2022,4,"{'ai_likelihood': 2.8808911641438803e-06, 'text': 'Towards Better Understanding of User Satisfaction in Open-Domain\n  Conversational Search\n\n  With the increasing popularity of conversational search, how to evaluate the\nperformance of conversational search systems has become an important question\nin the IR community. Existing works on conversational search evaluation can\nmainly be categorized into two streams: (1) constructing metrics based on\nsemantic similarity (e.g. BLUE, METEOR and BERTScore), or (2) directly\nevaluating the response ranking performance of the system using traditional\nsearch methods (e.g. nDCG, RBP and nERR). However, these methods either ignore\nthe information need of the user or ignore the mixed-initiative property of\nconversational search. This raises the question of how to accurately model user\nsatisfaction in conversational search scenarios. Since explicitly asking users\nto provide satisfaction feedback is difficult, traditional IR studies often\nrely on the Cranfield paradigm (i.e., third-party annotation) and user behavior\nmodeling to estimate user satisfaction in search. However, the feasibility and\neffectiveness of these two approaches have not been fully explored in\nconversational search. In this paper, we dive into the evaluation of\nconversational search from the perspective of user satisfaction. We build a\nnovel conversational search experimental platform and construct a Chinese\nopen-domain conversational search behavior dataset containing rich annotations\nand search behavior data. We also collect third-party satisfaction annotation\nat the session-level and turn-level, to investigate the feasibility of the\nCranfield paradigm in the conversational search scenario. Experimental results\nshow both some consistency and considerable differences between the user\nsatisfaction annotations and third-party annotations. We also propose dialog\ncontinuation or ending behavior models (DCEBM) to capture session-level user\nsatisfaction based on turn-level information.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.11314,regular,pre_llm,2022,4,"{'ai_likelihood': 3.314680523342556e-05, 'text': 'Faster Learned Sparse Retrieval with Guided Traversal\n\n  Neural information retrieval architectures based on transformers such as BERT\nare able to significantly improve system effectiveness over traditional sparse\nmodels such as BM25. Though highly effective, these neural approaches are very\nexpensive to run, making them difficult to deploy under strict latency\nconstraints. To address this limitation, recent studies have proposed new\nfamilies of learned sparse models that try to match the effectiveness of\nlearned dense models, while leveraging the traditional inverted index data\nstructure for efficiency. Current learned sparse models learn the weights of\nterms in documents and, sometimes, queries; however, they exploit different\nvocabulary structures, document expansion techniques, and query expansion\nstrategies, which can make them slower than traditional sparse models such as\nBM25. In this work, we propose a novel indexing and query processing technique\nthat exploits a traditional sparse model\'s ""guidance"" to efficiently traverse\nthe index, allowing the more effective learned model to execute fewer scoring\noperations. Our experiments show that our guided processing heuristic is able\nto boost the efficiency of the underlying learned sparse model by a factor of\nfour without any measurable loss of effectiveness.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.04727,regular,pre_llm,2022,4,"{'ai_likelihood': 5.347861184014215e-05, 'text': ""FUM: Fine-grained and Fast User Modeling for News Recommendation\n\n  User modeling is important for news recommendation. Existing methods usually\nfirst encode user's clicked news into news embeddings independently and then\naggregate them into user embedding. However, the word-level interactions across\ndifferent clicked news from the same user, which contain rich detailed clues to\ninfer user interest, are ignored by these methods. In this paper, we propose a\nfine-grained and fast user modeling framework (FUM) to model user interest from\nfine-grained behavior interactions for news recommendation. The core idea of\nFUM is to concatenate the clicked news into a long document and transform user\nmodeling into a document modeling task with both intra-news and inter-news\nword-level interactions. Since vanilla transformer cannot efficiently handle\nlong document, we apply an efficient transformer named Fastformer to model\nfine-grained behavior interactions. Extensive experiments on two real-world\ndatasets verify that FUM can effectively and efficiently model user interest\nfor news recommendation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.06832,regular,pre_llm,2022,4,"{'ai_likelihood': 1.5497207641601562e-05, 'text': 'Self-Guided Learning to Denoise for Robust Recommendation\n\n  The ubiquity of implicit feedback makes them the default choice to build\nmodern recommender systems. Generally speaking, observed interactions are\nconsidered as positive samples, while unobserved interactions are considered as\nnegative ones. However, implicit feedback is inherently noisy because of the\nubiquitous presence of noisy-positive and noisy-negative interactions.\nRecently, some studies have noticed the importance of denoising implicit\nfeedback for recommendations, and enhanced the robustness of recommendation\nmodels to some extent. Nonetheless, they typically fail to (1) capture the hard\nyet clean interactions for learning comprehensive user preference, and (2)\nprovide a universal denoising solution that can be applied to various kinds of\nrecommendation models.\n  In this paper, we thoroughly investigate the memorization effect of\nrecommendation models, and propose a new denoising paradigm, i.e., Self-Guided\nDenoising Learning (SGDL), which is able to collect memorized interactions at\nthe early stage of the training (i.e., ""noise-resistant"" period), and leverage\nthose data as denoising signals to guide the following training (i.e.,\n""noise-sensitive"" period) of the model in a meta-learning manner. Besides, our\nmethod can automatically switch its learning phase at the memorization point\nfrom memorization to self-guided learning, and select clean and informative\nmemorized data via a novel adaptive denoising scheduler to improve the\nrobustness. We incorporate SGDL with four representative recommendation models\n(i.e., NeuMF, CDAE, NGCF and LightGCN) and different loss functions (i.e.,\nbinary cross-entropy and BPR loss). The experimental results on three benchmark\ndatasets demonstrate the effectiveness of SGDL over the state-of-the-art\ndenoising methods like T-CE, IR, DeCA, and even state-of-the-art robust\ngraph-based methods like SGCN and SGL.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.11447,regular,pre_llm,2022,4,"{'ai_likelihood': 3.642506069607205e-06, 'text': 'Evaluating Interpolation and Extrapolation Performance of Neural\n  Retrieval Models\n\n  A retrieval model should not only interpolate the training data but also\nextrapolate well to the queries that are different from the training data.\nWhile neural retrieval models have demonstrated impressive performance on\nad-hoc search benchmarks, we still know little about how they perform in terms\nof interpolation and extrapolation. In this paper, we demonstrate the\nimportance of separately evaluating the two capabilities of neural retrieval\nmodels. Firstly, we examine existing ad-hoc search benchmarks from the two\nperspectives. We investigate the distribution of training and test data and\nfind a considerable overlap in query entities, query intent, and relevance\nlabels. This finding implies that the evaluation on these test sets is biased\ntoward interpolation and cannot accurately reflect the extrapolation capacity.\nSecondly, we propose a novel evaluation protocol to separately evaluate the\ninterpolation and extrapolation performance on existing benchmark datasets. It\nresamples the training and test data based on query similarity and utilizes the\nresampled dataset for training and evaluation. Finally, we leverage the\nproposed evaluation protocol to comprehensively revisit a number of\nwidely-adopted neural retrieval models. Results show models perform differently\nwhen moving from interpolation to extrapolation. For example,\nrepresentation-based retrieval models perform almost as well as\ninteraction-based retrieval models in terms of interpolation but not\nextrapolation. Therefore, it is necessary to separately evaluate both\ninterpolation and extrapolation performance and the proposed resampling method\nserves as a simple yet effective evaluation tool for future IR studies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.12195,review,pre_llm,2022,4,"{'ai_likelihood': 6.192260318332249e-06, 'text': ""Understanding User Satisfaction with Task-oriented Dialogue Systems\n\n  $ $Dialogue systems are evaluated depending on their type and purpose. Two\ncategories are often distinguished: (1) task-oriented dialogue systems (TDS),\nwhich are typically evaluated on utility, i.e., their ability to complete a\nspecified task, and (2) open domain chatbots, which are evaluated on the user\nexperience, i.e., based on their ability to engage a person. What is the\ninfluence of user experience on the user satisfaction rating of TDS as opposed\nto, or in addition to, utility? We collect data by providing an additional\nannotation layer for dialogues sampled from the ReDial dataset, a widely used\nconversational recommendation dataset. Unlike prior work, we annotate the\nsampled dialogues at both the turn and dialogue level on six dialogue aspects:\nrelevance, interestingness, understanding, task completion, efficiency, and\ninterest arousal. The annotations allow us to study how different dialogue\naspects influence user satisfaction. We introduce a comprehensive set of user\nexperience aspects derived from the annotators' open comments that can\ninfluence users' overall impression. We find that the concept of satisfaction\nvaries across annotators and dialogues, and show that a relevant turn is\nsignificant for some annotators, while for others, an interesting turn is all\nthey need. Our analysis indicates that the proposed user experience aspects\nprovide a fine-grained analysis of user satisfaction that is not captured by a\nmonolithic overall human rating.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.12326,review,pre_llm,2022,4,"{'ai_likelihood': 1.7881393432617188e-06, 'text': 'Investigating Accuracy-Novelty Performance for Graph-based Collaborative\n  Filtering\n\n  Recent years have witnessed the great accuracy performance of graph-based\nCollaborative Filtering (CF) models for recommender systems. By taking the\nuser-item interaction behavior as a graph, these graph-based CF models borrow\nthe success of Graph Neural Networks (GNN), and iteratively perform\nneighborhood aggregation to propagate the collaborative signals. While\nconventional CF models are known for facing the challenges of the popularity\nbias that favors popular items, one may wonder ""Whether the existing\ngraph-based CF models alleviate or exacerbate popularity bias of recommender\nsystems?"" To answer this question, we first investigate the two-fold\nperformances w.r.t. accuracy and novelty for existing graph-based CF methods.\nThe empirical results show that symmetric neighborhood aggregation adopted by\nmost existing graph-based CF models exacerbate the popularity bias and this\nphenomenon becomes more serious as the depth of graph propagation increases.\nFurther, we theoretically analyze the cause of popularity bias for graph-based\nCF. Then, we propose a simple yet effective plugin, namely r-AdjNorm, to\nachieve an accuracy-novelty trade-off by controlling the normalization strength\nin the neighborhood aggregation process. Meanwhile, r-AdjNorm can be smoothly\napplied to the existing graph-based CF backbones without additional\ncomputation. Finally, experimental results on three benchmark datasets show\nthat our proposed method can improve novelty without sacrificing accuracy under\nvarious graph-based CF backbones.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.11159,review,pre_llm,2022,4,"{'ai_likelihood': 1.1788474188910592e-05, 'text': 'Explainable Fairness in Recommendation\n\n  Existing research on fairness-aware recommendation has mainly focused on the\nquantification of fairness and the development of fair recommendation models,\nneither of which studies a more substantial problem--identifying the underlying\nreason of model disparity in recommendation. This information is critical for\nrecommender system designers to understand the intrinsic recommendation\nmechanism and provides insights on how to improve model fairness to decision\nmakers. Fortunately, with the rapid development of Explainable AI, we can use\nmodel explainability to gain insights into model (un)fairness. In this paper,\nwe study the problem of explainable fairness, which helps to gain insights\nabout why a system is fair or unfair, and guides the design of fair recommender\nsystems with a more informed and unified methodology. Particularly, we focus on\na common setting with feature-aware recommendation and exposure unfairness, but\nthe proposed explainable fairness framework is general and can be applied to\nother recommendation settings and fairness definitions. We propose a\nCounterfactual Explainable Fairness framework, called CEF, which generates\nexplanations about model fairness that can improve the fairness without\nsignificantly hurting the performance.The CEF framework formulates an\noptimization problem to learn the ""minimal"" change of the input features that\nchanges the recommendation results to a certain level of fairness. Based on the\ncounterfactual recommendation result of each feature, we calculate an\nexplainability score in terms of the fairness-utility trade-off to rank all the\nfeature-based explanations, and select the top ones as fairness explanations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.11154,regular,pre_llm,2022,4,"{'ai_likelihood': 2.284844716389974e-06, 'text': 'Dual Skipping Guidance for Document Retrieval with Learned Sparse\n  Representations\n\n  This paper proposes a dual skipping guidance scheme with hybrid scoring to\naccelerate document retrieval that uses learned sparse representations while\nstill delivering a good relevance. This scheme uses both lexical BM25 and\nlearned neural term weights to bound and compose the rank score of a candidate\ndocument separately for skipping and final ranking, and maintains two top-k\nthresholds during inverted index traversal. This paper evaluates time\nefficiency and ranking relevance of the proposed scheme in searching MS MARCO\nTREC datasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.02592,regular,pre_llm,2022,4,"{'ai_likelihood': 3.245141771104601e-06, 'text': ""Thinking inside The Box: Learning Hypercube Representations for Group\n  Recommendation\n\n  As a step beyond traditional personalized recommendation, group\nrecommendation is the task of suggesting items that can satisfy a group of\nusers. In group recommendation, the core is to design preference aggregation\nfunctions to obtain a quality summary of all group members' preferences. Such\nuser and group preferences are commonly represented as points in the vector\nspace (i.e., embeddings), where multiple user embeddings are compressed into\none to facilitate ranking for group-item pairs. However, the resulted group\nrepresentations, as points, lack adequate flexibility and capacity to account\nfor the multi-faceted user preferences. Also, the point embedding-based\npreference aggregation is a less faithful reflection of a group's\ndecision-making process, where all users have to agree on a certain value in\neach embedding dimension instead of a negotiable interval. In this paper, we\npropose a novel representation of groups via the notion of hypercubes, which\nare subspaces containing innumerable points in the vector space. Specifically,\nwe design the hypercube recommender (CubeRec) to adaptively learn group\nhypercubes from user embeddings with minimal information loss during preference\naggregation, and to leverage a revamped distance metric to measure the affinity\nbetween group hypercubes and item points. Moreover, to counteract the\nlong-standing issue of data sparsity in group recommendation, we make full use\nof the geometric expressiveness of hypercubes and innovatively incorporate\nself-supervision by intersecting two groups. Experiments on four real-world\ndatasets have validated the superiority of CubeRec over state-of-the-art\nbaselines.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.11091,regular,pre_llm,2022,4,"{'ai_likelihood': 6.490283542209202e-06, 'text': 'On-Device Next-Item Recommendation with Self-Supervised Knowledge\n  Distillation\n\n  Modern recommender systems operate in a fully server-based fashion. To cater\nto millions of users, the frequent model maintaining and the high-speed\nprocessing for concurrent user requests are required, which comes at the cost\nof a huge carbon footprint. Meanwhile, users need to upload their behavior data\neven including the immediate environmental context to the server, raising the\npublic concern about privacy. On-device recommender systems circumvent these\ntwo issues with cost-conscious settings and local inference. However, due to\nthe limited memory and computing resources, on-device recommender systems are\nconfronted with two fundamental challenges: (1) how to reduce the size of\nregular models to fit edge devices? (2) how to retain the original capacity?\nPrevious research mostly adopts tensor decomposition techniques to compress the\nregular recommendation model with limited compression ratio so as to avoid\ndrastic performance degradation. In this paper, we explore ultra-compact models\nfor next-item recommendation, by loosing the constraint of dimensionality\nconsistency in tensor decomposition. Meanwhile, to compensate for the capacity\nloss caused by compression, we develop a self-supervised knowledge distillation\nframework which enables the compressed model (student) to distill the essential\ninformation lying in the raw data, and improves the long-tail item\nrecommendation through an embedding-recombination strategy with the original\nmodel (teacher). The extensive experiments on two benchmarks demonstrate that,\nwith 30x model size reduction, the compressed model almost comes with no\naccuracy loss, and even outperforms its uncompressed counterpart in most cases.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.04726,regular,pre_llm,2022,4,"{'ai_likelihood': 1.6424391004774305e-05, 'text': 'News Recommendation with Candidate-aware User Modeling\n\n  News recommendation aims to match news with personalized user interest.\nExisting methods for news recommendation usually model user interest from\nhistorical clicked news without the consideration of candidate news. However,\neach user usually has multiple interests, and it is difficult for these methods\nto accurately match a candidate news with a specific user interest. In this\npaper, we present a candidate-aware user modeling method for personalized news\nrecommendation, which can incorporate candidate news into user modeling for\nbetter matching between candidate news and user interest. We propose a\ncandidate-aware self-attention network that uses candidate news as clue to\nmodel candidate-aware global user interest. In addition, we propose a\ncandidate-aware CNN network to incorporate candidate news into local behavior\ncontext modeling and learn candidate-aware short-term user interest. Besides,\nwe use a candidate-aware attention network to aggregate previously clicked news\nweighted by their relevance with candidate news to build candidate-aware user\nrepresentation. Experiments on real-world datasets show the effectiveness of\nour method in improving news recommendation performance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.13351,regular,pre_llm,2022,5,"{'ai_likelihood': 1.483493381076389e-05, 'text': 'LeiBi@COLIEE 2022: Aggregating Tuned Lexical Models with a\n  Cluster-driven BERT-based Model for Case Law Retrieval\n\n  This paper summarizes our approaches submitted to the case law retrieval task\nin the Competition on Legal Information Extraction/Entailment (COLIEE) 2022.\nOur methodology consists of four steps; in detail, given a legal case as a\nquery, we reformulate it by extracting various meaningful sentences or n-grams.\nThen, we utilize the pre-processed query case to retrieve an initial set of\npossible relevant legal cases, which we further re-rank. Lastly, we aggregate\nthe relevance scores obtained by the first stage and the re-ranking models to\nimprove retrieval effectiveness. In each step of our methodology, we explore\nvarious well-known and novel methods. In particular, to reformulate the query\ncases aiming to make them shorter, we extract unigrams using three different\nstatistical methods: KLI, PLM, IDF-r, as well as models that leverage\nembeddings (e.g., KeyBERT). Moreover, we investigate if automatic summarization\nusing Longformer-Encoder-Decoder (LED) can produce an effective query\nrepresentation for this retrieval task. Furthermore, we propose a novel\nre-ranking cluster-driven approach, which leverages Sentence-BERT models that\nare pre-tuned on large amounts of data for embedding sentences from query and\ncandidate documents. Finally, we employ a linear aggregation method to combine\nthe relevance scores obtained by traditional IR models and neural-based models,\naiming to incorporate the semantic understanding of neural models and the\nstatistically measured topical relevance. We show that aggregating these\nrelevance scores can improve the overall retrieval effectiveness.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.0287,regular,pre_llm,2022,5,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'MS-Shift: An Analysis of MS MARCO Distribution Shifts on Neural\n  Retrieval\n\n  Pre-trained Language Models have recently emerged in Information Retrieval as\nproviding the backbone of a new generation of neural systems that outperform\ntraditional methods on a variety of tasks. However, it is still unclear to what\nextent such approaches generalize in zero-shot conditions. The recent BEIR\nbenchmark provides partial answers to this question by comparing models on\ndatasets and tasks that differ from the training conditions. We aim to address\nthe same question by comparing models under more explicit distribution shifts.\nTo this end, we build three query-based distribution shifts within MS MARCO\n(query-semantic, query-intent, query-length), which are used to evaluate the\nthree main families of neural retrievers based on BERT: sparse, dense, and\nlate-interaction -- as well as a monoBERT re-ranker. We further analyse the\nperformance drops between the train and test query distributions. In\nparticular, we experiment with two generalization indicators: the first one\nbased on train/test query vocabulary overlap, and the second based on\nrepresentations of a trained bi-encoder. Intuitively, those indicators verify\nthat the further away the test set is from the train one, the worse the drop in\nperformance. We also show that models respond differently to the shifts --\ndense approaches being the most impacted. Overall, our study demonstrates that\nit is possible to design more controllable distribution shifts as a tool to\nbetter understand generalization of IR models. Finally, we release the MS MARCO\nquery subsets, which provide an additional resource to benchmark zero-shot\ntransfer in Information Retrieval.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.00147,regular,pre_llm,2022,5,"{'ai_likelihood': 3.2318962944878476e-05, 'text': 'Unbiased Implicit Feedback via Bi-level Optimization\n\n  Implicit feedback is widely leveraged in recommender systems since it is easy\nto collect and provides weak supervision signals. Recent works reveal a huge\ngap between the implicit feedback and user-item relevance due to the fact that\nimplicit feedback is also closely related to the item exposure. To bridge this\ngap, existing approaches explicitly model the exposure and propose unbiased\nestimators to improve the relevance. Unfortunately, these unbiased estimators\nsuffer from the high gradient variance, especially for long-tail items, leading\nto inaccurate gradient updates and degraded model performance. To tackle this\nchallenge, we propose a low-variance unbiased estimator from a probabilistic\nperspective, which effectively bounds the variance of the gradient. Unlike\nprevious works which either estimate the exposure via heuristic-based\nstrategies or use a large biased training set, we propose to estimate the\nexposure via an unbiased small-scale validation set. Specifically, we first\nparameterize the user-item exposure by incorporating both user and item\ninformation, and then construct an unbiased validation set from the biased\ntraining set. By leveraging the unbiased validation set, we adopt bi-level\noptimization to automatically update exposure-related parameters along with\nrecommendation model parameters during the learning. Experiments on two\nreal-world datasets and two semi-synthetic datasets verify the effectiveness of\nour method.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.10588,regular,pre_llm,2022,5,"{'ai_likelihood': 0.00014172659979926216, 'text': ""Micro-video recommendation model based on graph neural network and\n  attention mechanism\n\n  With the rapid development of Internet technology and the comprehensive\npopularity of Internet applications, online activities have gradually become an\nindispensable part of people's daily life. The original recommendation learning\nalgorithm is mainly based on user-microvideo interaction for learning, modeling\nthe user-micro-video connection relationship, which is difficult to capture the\nmore complex relationships between nodes. To address the above problems, we\npropose a personalized recommendation model based on graph neural network,\nwhich utilizes the feature that graph neural network can tap deep information\nof graph data more effectively, and transforms the input user rating\ninformation and item side information into graph structure, for effective\nfeature extraction, based on the importance sampling strategy. The\nimportance-based sampling strategy measures the importance of neighbor nodes to\nthe central node by calculating the relationship tightness between the neighbor\nnodes and the central node, and selects the neighbor nodes for recommendation\ntasks based on the importance level, which can be more targeted to select the\nsampling neighbors with more influence on the target micro-video nodes. The\npooling aggregation strategy, on the other hand, trains the aggregation weights\nby inputting the neighborhood node features into the fully connected layer\nbefore aggregating the neighborhood features, and then introduces the pooling\nlayer for feature aggregation, and finally aggregates the obtained neighborhood\naggregation features with the target node itself, which directly introduces a\nsymmetric trainable function to fuse the neighborhood weight training into the\nmodel to better capture the different neighborhood nodes' differential features\nin a learnable manner to allow for a more accurate representation of the\ncurrent node features.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.08776,regular,pre_llm,2022,5,"{'ai_likelihood': 2.0199351840549047e-06, 'text': 'AdaMCT: Adaptive Mixture of CNN-Transformer for Sequential\n  Recommendation\n\n  Sequential recommendation (SR) aims to model users dynamic preferences from a\nseries of interactions. A pivotal challenge in user modeling for SR lies in the\ninherent variability of user preferences. An effective SR model is expected to\ncapture both the long-term and short-term preferences exhibited by users,\nwherein the former can offer a comprehensive understanding of stable interests\nthat impact the latter. To more effectively capture such information, we\nincorporate locality inductive bias into the Transformer by amalgamating its\nglobal attention mechanism with a local convolutional filter, and adaptively\nascertain the mixing importance on a personalized basis through layer-aware\nadaptive mixture units, termed as AdaMCT. Moreover, as users may repeatedly\nbrowse potential purchases, it is expected to consider multiple relevant items\nconcurrently in long-/short-term preferences modeling. Given that softmax-based\nattention may promote unimodal activation, we propose the Squeeze-Excitation\nAttention (with sigmoid activation) into SR models to capture multiple\npertinent items (keys) simultaneously. Extensive experiments on three widely\nemployed benchmarks substantiate the effectiveness and efficiency of our\nproposed approach. Source code is available at\nhttps://github.com/juyongjiang/AdaMCT.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.10137,regular,pre_llm,2022,5,"{'ai_likelihood': 2.317958407931858e-07, 'text': ""A Simple yet Effective Framework for Active Learning to Rank\n\n  While China has become the biggest online market in the world with around 1\nbillion internet users, Baidu runs the world largest Chinese search engine\nserving more than hundreds of millions of daily active users and responding\nbillions queries per day. To handle the diverse query requests from users at\nweb-scale, Baidu has done tremendous efforts in understanding users' queries,\nretrieve relevant contents from a pool of trillions of webpages, and rank the\nmost relevant webpages on the top of results. Among these components used in\nBaidu search, learning to rank (LTR) plays a critical role and we need to\ntimely label an extremely large number of queries together with relevant\nwebpages to train and update the online LTR models. To reduce the costs and\ntime consumption of queries/webpages labeling, we study the problem of Activ\nLearning to Rank (active LTR) that selects unlabeled queries for annotation and\ntraining in this work. Specifically, we first investigate the criterion --\nRanking Entropy (RE) characterizing the entropy of relevant webpages under a\nquery produced by a sequence of online LTR models updated by different\ncheckpoints, using a Query-By-Committee (QBC) method. Then, we explore a new\ncriterion namely Prediction Variances (PV) that measures the variance of\nprediction results for all relevant webpages under a query. Our empirical\nstudies find that RE may favor low-frequency queries from the pool for labeling\nwhile PV prioritizing high-frequency queries more. Finally, we combine these\ntwo complementary criteria as the sample selection strategies for active\nlearning. Extensive experiments with comparisons to baseline algorithms show\nthat the proposed approach could train LTR models achieving higher Discounted\nCumulative Gain (i.e., the relative improvement {\\Delta}DCG4=1.38%) with the\nsame budgeted labeling efforts.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.09593,regular,pre_llm,2022,5,"{'ai_likelihood': 1.0629494984944662e-05, 'text': ""Improving Micro-video Recommendation via Contrastive Multiple Interests\n\n  With the rapid increase of micro-video creators and viewers, how to make\npersonalized recommendations from a large number of candidates to viewers\nbegins to attract more and more attention. However, existing micro-video\nrecommendation models rely on expensive multi-modal information and learn an\noverall interest embedding that cannot reflect the user's multiple interests in\nmicro-videos. Recently, contrastive learning provides a new opportunity for\nrefining the existing recommendation techniques. Therefore, in this paper, we\npropose to extract contrastive multi-interests and devise a micro-video\nrecommendation model CMI. Specifically, CMI learns multiple interest embeddings\nfor each user from his/her historical interaction sequence, in which the\nimplicit orthogonal micro-video categories are used to decouple multiple user\ninterests. Moreover, it establishes the contrastive multi-interest loss to\nimprove the robustness of interest embeddings and the performance of\nrecommendations. The results of experiments on two micro-video datasets\ndemonstrate that CMI achieves state-of-the-art performance over existing\nbaselines.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.09666,regular,pre_llm,2022,5,"{'ai_likelihood': 1.4901161193847656e-06, 'text': ""Personalized Prompt for Sequential Recommendation\n\n  Pre-training models have shown their power in sequential recommendation.\nRecently, prompt has been widely explored and verified for tuning in NLP\npre-training, which could help to more effectively and efficiently extract\nuseful knowledge from pre-training models for downstream tasks, especially in\ncold-start scenarios. However, it is challenging to bring prompt-tuning from\nNLP to recommendation, since the tokens in recommendation (i.e., items) do not\nhave explicit explainable semantics, and the sequence modeling should be\npersonalized. In this work, we first introduces prompt to recommendation and\npropose a novel Personalized prompt-based recommendation (PPR) framework for\ncold-start recommendation. Specifically, we build the personalized soft prefix\nprompt via a prompt generator based on user profiles and enable a sufficient\ntraining of prompts via a prompt-oriented contrastive learning with both\nprompt- and behavior-based augmentations. We conduct extensive evaluations on\nvarious tasks. In both few-shot and zero-shot recommendation, PPR models\nachieve significant improvements over baselines on various metrics in three\nlarge-scale open datasets. We also conduct ablation tests and sparsity analysis\nfor a better understanding of PPR. Moreover, We further verify PPR's\nuniversality on different pre-training models, and conduct explorations on\nPPR's other promising downstream tasks including cross-domain recommendation\nand user profile prediction.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.04168,regular,pre_llm,2022,5,"{'ai_likelihood': 4.006756676567926e-06, 'text': 'Visual Encoding and Debiasing for CTR Prediction\n\n  Extracting expressive visual features is crucial for accurate\nClick-Through-Rate (CTR) prediction in visual search advertising systems.\nCurrent commercial systems use off-the-shelf visual encoders to facilitate fast\nonline service. However, the extracted visual features are coarse-grained\nand/or biased. In this paper, we present a visual encoding framework for CTR\nprediction to overcome these problems. The framework is based on contrastive\nlearning which pulls positive pairs closer and pushes negative pairs apart in\nthe visual feature space. To obtain fine-grained visual features,we present\ncontrastive learning supervised by click through data to fine-tune the visual\nencoder. To reduce sample selection bias, firstly we train the visual encoder\noffline by leveraging both unbiased self-supervision and click supervision\nsignals. Secondly, we incorporate a debiasing network in the online CTR\npredictor to adjust the visual features by contrasting high impression items\nwith selected items with lower impressions.We deploy the framework in the\nvisual sponsor search system at Alibaba. Offline experiments on billion-scale\ndatasets and online experiments demonstrate that the proposed framework can\nmake accurate and unbiased predictions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.11231,regular,pre_llm,2022,5,"{'ai_likelihood': 1.619259516398112e-05, 'text': ""SUGER: A Subgraph-based Graph Convolutional Network Method for Bundle\n  Recommendation\n\n  Bundle recommendation is an emerging research direction in the recommender\nsystem with the focus on recommending customized bundles of items for users.\nAlthough Graph Neural Networks (GNNs) have been applied in this problem and\nachieve superior performance, existing methods underexplore the graph-level GNN\nmethods, which exhibit great potential in traditional recommender system.\nFurthermore, they usually lack the transferability from one domain with\nsufficient supervision to another domain which might suffer from the label\nscarcity issue. In this work, we propose a subgraph-based Graph Neural Network\nmodel, SUGER, for bundle recommendation to handle these limitations. SUGER\ngenerates heterogeneous subgraphs around the user-bundle pairs, and then maps\nthose subgraphs to the users' preference predictions via neural relational\ngraph propagation. Experimental results show that SUGER significantly\noutperforms the state-of-the-art baselines in both the basic and the transfer\nbundle recommendation problems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.12901,review,pre_llm,2022,5,"{'ai_likelihood': 2.947118547227648e-06, 'text': ""Fairness of Exposure in Light of Incomplete Exposure Estimation\n\n  Fairness of exposure is a commonly used notion of fairness for ranking\nsystems. It is based on the idea that all items or item groups should get\nexposure proportional to the merit of the item or the collective merit of the\nitems in the group. Often, stochastic ranking policies are used to ensure\nfairness of exposure. Previous work unrealistically assumes that we can\nreliably estimate the expected exposure for all items in each ranking produced\nby the stochastic policy. In this work, we discuss how to approach fairness of\nexposure in cases where the policy contains rankings of which, due to\ninter-item dependencies, we cannot reliably estimate the exposure distribution.\nIn such cases, we cannot determine whether the policy can be considered fair.\nOur contributions in this paper are twofold. First, we define a method called\nFELIX for finding stochastic policies that avoid showing rankings with unknown\nexposure distribution to the user without having to compromise user utility or\nitem fairness. Second, we extend the study of fairness of exposure to the top-k\nsetting and also assess FELIX in this setting. We find that FELIX can\nsignificantly reduce the number of rankings with unknown exposure distribution\nwithout a drop in user utility or fairness compared to existing fair ranking\nmethods, both for full-length and top-k rankings. This is an important first\nstep in developing fair ranking methods for cases where we have incomplete\nknowledge about the user's behaviour.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.05209,regular,pre_llm,2022,5,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Bayesian Prior Learning via Neural Networks for Next-item Recommendation\n\n  Next-item prediction is a a popular problem in the recommender systems\ndomain. As the name suggests, the task is to recommend subsequent items that a\nuser would be interested in given contextual information and historical\ninteraction data. In our paper, we model a general notion of context via a\nsequence of item interactions. We model the next item prediction problem using\nthe Bayesian framework and capture the probability of appearance of a sequence\nthrough the posterior mean of the Beta distribution. We train two neural\nnetworks to accurately predict the alpha & beta parameter values of the Beta\ndistribution. Our novel approach of combining black-box style neural networks,\nknown to be suitable for function approximation with Bayesian estimation\nmethods have resulted in an innovative method that outperforms various\nstate-of-the-art baselines. We demonstrate the effectiveness of our method in\ntwo real world datasets. Our framework is an important step towards the goal of\nbuilding privacy preserving recommender systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.08084,regular,pre_llm,2022,5,"{'ai_likelihood': 3.311369154188368e-07, 'text': ""M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender\n  Systems\n\n  Industrial recommender systems have been growing increasingly complex, may\ninvolve \\emph{diverse domains} such as e-commerce products and user-generated\ncontents, and can comprise \\emph{a myriad of tasks} such as retrieval, ranking,\nexplanation generation, and even AI-assisted content production. The mainstream\napproach so far is to develop individual algorithms for each domain and each\ntask. In this paper, we explore the possibility of developing a unified\nfoundation model to support \\emph{open-ended domains and tasks} in an\nindustrial recommender system, which may reduce the demand on downstream\nsettings' data and can minimize the carbon footprint by avoiding training a\nseparate model from scratch for every task. Deriving a unified foundation is\nchallenging due to (i) the potentially unlimited set of downstream domains and\ntasks, and (ii) the real-world systems' emphasis on computational efficiency.\nWe thus build our foundation upon M6, an existing large-scale industrial\npretrained language model similar to GPT-3 and T5, and leverage M6's pretrained\nability for sample-efficient downstream adaptation, by representing user\nbehavior data as plain texts and converting the tasks to either language\nunderstanding or generation. To deal with a tight hardware budget, we propose\nan improved version of prompt tuning that outperforms fine-tuning with\nnegligible 1\\% task-specific parameters, and employ techniques such as late\ninteraction, early exiting, parameter sharing, and pruning to further reduce\nthe inference time and the model size. We demonstrate the foundation model's\nversatility on a wide range of tasks such as retrieval, ranking, zero-shot\nrecommendation, explanation generation, personalized content creation, and\nconversational recommendation, and manage to deploy it on both cloud servers\nand mobile devices.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.04819,regular,pre_llm,2022,5,"{'ai_likelihood': 8.940696716308594e-07, 'text': 'Massive Enhanced Extracted Email Features Tailored for Cosine Distance\n\n  In this paper, the process of converting the Enron email dataset (the version\ncited in the preprint) to thousands of features per email for a selected set of\n2400 labelled emails is explained and evaluated. The final features are\ntailored for Cosine distance so that the Cosine distance invertly reflect the\nnumber of top indicative words of each email that are common between the two\nemails in an explainable normalized fashion. The labelling is based on the leaf\nfolder name in the Enron email dataset (the version cited in the preprint)\nfolders tree and the 2400 emails selected consist 300 emails for each of the 8\nlabels. The evaluation is based on the accuracy of a k nearest neighbours\nmajority voting classification using Cosine distance. In addition to KNN\nmajority voting classification accuracy and confusion matrix, some statistics\nfor the process is reported. The KNN majority voting classification accuracy\nusing Cosine distance is 76.75% which shows at least some level of success\ngiven the 8 labels involved. The result of conversion is 48557 features per\nselected email out of which exactly 40 features per email are non-zero. The\nresult of conversion is a data set named MeeefTCD (Massive Enhanced Extracted\nEmail Features Tailored for Cosine Distance) available at\nhttps://web.cs.dal.ca/~barahimi/data-sets/meeeftcd/ and on a github repository\nmentioned in this paper.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.12408,regular,pre_llm,2022,5,"{'ai_likelihood': 5.496872795952691e-06, 'text': ""Using user's local context to support local news\n\n  American local newspapers have been experiencing a large loss of reader\nretention and business within the past 15 years due to the proliferation of\nonline news sources. Local media companies are starting to shift from an\nadvertising-supported business model to one based on subscriptions to mitigate\nthis problem. With this subscription model, there is a need to increase user\nengagement and personalization, and recommender systems are one way for these\nnews companies to accomplish this goal. However, using standard modeling\napproaches that focus on users' global preferences is not appropriate in this\ncontext because the local preferences of users exhibit some specific\ncharacteristics which do not necessarily match their long-term or global\npreferences in the news. Our research explores a localized session-based\nrecommendation approach, using recommendations based on local news articles and\narticles pertaining to the different local news categories. Experiments\nperformed on a news dataset from a local newspaper show that these local\nmodels, particularly certain categories of items, do indeed provide more\naccuracy and effectiveness for personalization which, in turn, may lead to more\nuser engagement with local news content.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.1497,regular,pre_llm,2022,5,"{'ai_likelihood': 1.4172659979926217e-05, 'text': 'Towards Personalized Bundle Creative Generation with Contrastive\n  Non-Autoregressive Decoding\n\n  Current bundle generation studies focus on generating a combination of items\nto improve user experience. In real-world applications, there is also a great\nneed to produce bundle creatives that consist of mixture types of objects\n(e.g., items, slogans and templates) for achieving better promotion effect. We\nstudy a new problem named bundle creative generation: for given users, the goal\nis to generate personalized bundle creatives that the users will be interested\nin. To take both quality and efficiency into account, we propose a contrastive\nnon-autoregressive model that captures user preferences with ingenious decoding\nobjective. Experiments on large-scale real-world datasets verify that our\nproposed model shows significant advantages in terms of creative quality and\ngeneration speed.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.05414,regular,pre_llm,2022,5,"{'ai_likelihood': 1.0927518208821616e-06, 'text': ""Recommending Research Papers to Chemists: A Specialized Interface for\n  Chemical Entity Exploration\n\n  Researchers and scientists increasingly rely on specialized information\nretrieval (IR) or recommendation systems (RS) to support them in their daily\nresearch tasks. Paper recommender systems are one such tool scientists use to\nstay on top of the ever-increasing number of academic publications in their\nfield. Improving research paper recommender systems is an active research\nfield. However, less research has focused on how the interfaces of research\npaper recommender systems can be tailored to suit the needs of different\nresearch domains. For example, in the field of biomedicine and chemistry,\nresearchers are not only interested in textual relevance but may also want to\ndiscover or compare the contained chemical entity information found in a\npaper's full text. Existing recommender systems for academic literature do not\nsupport the discovery of this non-textual, but semantically valuable, chemical\nentity data. We present the first implementation of a specialized chemistry\npaper recommender system capable of visualizing the contained chemical\nstructures, chemical formulae, and synonyms for chemical compounds within the\ndocument's full text. We review existing tools and related research in this\nfield before describing the implementation of our ChemVis system. With the help\nof chemists, we are expanding the functionality of ChemVis, and will perform an\nevaluation of recommendation performance and usability in future work.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.0967,regular,pre_llm,2022,5,"{'ai_likelihood': 2.8477774726019966e-06, 'text': 'A Unified Collaborative Representation Learning for Neural-Network based\n  Recommender Systems\n\n  Most NN-RSs focus on accuracy by building representations from the direct\nuser-item interactions (e.g., user-item rating matrix), while ignoring the\nunderlying relatedness between users and items (e.g., users who rate the same\nratings for the same items should be embedded into similar representations),\nwhich is an ideological disadvantage. On the other hand, ME models directly\nemploy inner products as a default loss function metric that cannot project\nusers and items into a proper latent space, which is a methodological\ndisadvantage. In this paper, we propose a supervised collaborative\nrepresentation learning model - Magnetic Metric Learning (MML) - to map users\nand items into a unified latent vector space, enhancing the representation\nlearning for NN-RSs. Firstly, MML utilizes dual triplets to model not only the\nobserved relationships between users and items, but also the underlying\nrelationships between users as well as items to overcome the ideological\ndisadvantage. Specifically, a modified metric-based dual loss function is\nproposed in MML to gather similar entities and disperse the dissimilar ones.\nWith MML, we can easily compare all the relationships (user to user, item to\nitem, user to item) according to the weighted metric, which overcomes the\nmethodological disadvantage. We conduct extensive experiments on four\nreal-world datasets with large item space. The results demonstrate that MML can\nlearn a proper unified latent space for representations from the user-item\nmatrix with high accuracy and effectiveness, and lead to a performance gain\nover the state-of-the-art RS models by an average of 17%.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.04366,regular,pre_llm,2022,5,"{'ai_likelihood': 1.4901161193847656e-06, 'text': ""Effectively Using Long and Short Sessions for Multi-Session-based\n  Recommendations\n\n  It is not accurate to make recommendations only based one single current\nsession. Therefore, multi-session-based recommendation(MSBR) is a solution for\nthe problem. Compared with the previous MSBR models, we have made three\nimprovements in this paper. First, the previous work choose to use all the\nhistory sessions of the user and/or of his similar users. When the user's\ncurrent interest changes greatly from the past, most of these sessions can only\nhave negative impacts. Therefore, we select a large number of randomly chosen\nsessions from the dataset as candidate sessions to avoid over depending on\nhistory data. Then we only choose to use the most similar sessions to get the\nmost useful information while reduce the noise caused by dissimilar sessions.\nSecond, in real-world datasets, short sessions account for a large proportion.\nThe RNN often used in previous work is not suitable to process short sessions,\nbecause RNN only focuses on the sequential relationship, which we find is not\nthe only relationship between items in short sessions. So, we designed a more\nsuitable method named GAFE based on attention to process short sessions. Third,\nAlthough there are few long sessions, they can not be ignored. Not like\nprevious models, which simply process long sessions in the same way as short\nsessions, we propose LSIS, which can split the interest of long sessions, to\nmake better use of long sessions. Finally, to help recommendations, we also\nhave considered users' long-term interests captured by a multi-layer GRU.\nConsidering the four points above, we built the model ENIREC. Experiments on\ntwo real-world datasets show that the comprehensive performance of ENIREC is\nbetter than other existing models.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.12682,regular,pre_llm,2022,5,"{'ai_likelihood': 3.311369154188368e-08, 'text': ""TaCube: Pre-computing Data Cubes for Answering Numerical-Reasoning\n  Questions over Tabular Data\n\n  Existing auto-regressive pre-trained language models (PLMs) like T5 and BART,\nhave been well applied to table question answering by UNIFIEDSKG and TAPEX,\nrespectively, and demonstrated state-of-the-art results on multiple benchmarks.\nHowever, auto-regressive PLMs are challenged by recent emerging numerical\nreasoning datasets, such as TAT-QA, due to the error-prone implicit\ncalculation. In this paper, we present TaCube, to pre-compute\naggregation/arithmetic results for the table in advance, so that they are handy\nand readily available for PLMs to answer numerical reasoning questions. TaCube\nsystematically and comprehensively covers a collection of computational\noperations over table segments. By simply concatenating TaCube to the input\nsequence of PLMs, it shows significant experimental effectiveness. TaCube\npromotes the F1 score from 49.6% to 66.2% on TAT-QA and achieves new\nstate-of-the-art results on WikiTQ (59.6% denotation accuracy). TaCube's\nimprovements on numerical reasoning cases are even more notable: on TAT-QA,\nTaCube promotes the exact match accuracy of BART-large by 39.6% on sum, 52.5%\non average, 36.6% on substraction, and 22.2% on division. We believe that\nTaCube is a general and portable pre-computation solution that can be\npotentially integrated to various numerical reasoning frameworks\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.09912,regular,pre_llm,2022,6,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'A Dense Representation Framework for Lexical and Semantic Matching\n\n  Lexical and semantic matching capture different successful approaches to text\nretrieval and the fusion of their results has proven to be more effective and\nrobust than either alone. Prior work performs hybrid retrieval by conducting\nlexical and semantic matching using different systems (e.g., Lucene and Faiss,\nrespectively) and then fusing their model outputs. In contrast, our work\nintegrates lexical representations with dense semantic representations by\ndensifying high-dimensional lexical representations into what we call\nlow-dimensional dense lexical representations (DLRs). Our experiments show that\nDLRs can effectively approximate the original lexical representations,\npreserving effectiveness while improving query latency. Furthermore, we can\ncombine dense lexical and semantic representations to generate dense hybrid\nrepresentations (DHRs) that are more flexible and yield faster retrieval\ncompared to existing hybrid techniques. In addition, we explore it jointly\ntraining lexical and semantic representations in a single model and empirically\nshow that the resulting DHRs are able to combine the advantages of the\nindividual components. Our best DHR model is competitive with state-of-the-art\nsingle-vector and multi-vector dense retrievers in both in-domain and zero-shot\nevaluation settings. Furthermore, our model is both faster and requires smaller\nindexes, making our dense representation framework an attractive approach to\ntext retrieval. Our code is available at https://github.com/castorini/dhr.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.10839,regular,pre_llm,2022,6,"{'ai_likelihood': 1.158979203965929e-06, 'text': ""Proximity Graph Maintenance for Fast Online Nearest Neighbor Search\n\n  Approximate Nearest Neighbor (ANN) search is a fundamental technique for\n(e.g.,) the deployment of recommender systems. Recent studies bring proximity\ngraph-based methods into practitioners' attention -- proximity graph-based\nmethods outperform other solutions such as quantization, hashing, and\ntree-based ANN algorithm families. In current recommendation systems, data\npoint insertions, deletions, and queries are streamed into the system in an\nonline fashion as users and items change dynamically. As proximity graphs are\nconstructed incrementally by inserting data points as new vertices into the\ngraph, online insertions and queries are well-supported in proximity graph.\nHowever, a data point deletion incurs removing a vertex from the proximity\ngraph index, while no proper graph index updating mechanisms are discussed in\nprevious studies. To tackle the challenge, we propose an incremental proximity\ngraph maintenance (IPGM) algorithm for online ANN. IPGM supports both vertex\ndeletion and insertion on proximity graphs. Given a vertex deletion request, we\nthoroughly investigate solutions to update the connections of the vertex. The\nproposed updating scheme eliminates the performance drop in online ANN methods\non proximity graphs, making the algorithm suitable for practical systems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.0051,regular,pre_llm,2022,6,"{'ai_likelihood': 4.304779900444879e-06, 'text': 'HIEN: Hierarchical Intention Embedding Network for Click-Through Rate\n  Prediction\n\n  Click-through rate (CTR) prediction plays an important role in online\nadvertising and recommendation systems, which aims at estimating the\nprobability of a user clicking on a specific item. Feature interaction modeling\nand user interest modeling methods are two popular domains in CTR prediction,\nand they have been studied extensively in recent years. However, these methods\nstill suffer from two limitations. First, traditional methods regard item\nattributes as ID features, while neglecting structure information and relation\ndependencies among attributes. Second, when mining user interests from\nuser-item interactions, current models ignore user intents and item intents for\ndifferent attributes, which lacks interpretability. Based on this observation,\nin this paper, we propose a novel approach Hierarchical Intention Embedding\nNetwork (HIEN), which considers dependencies of attributes based on bottom-up\ntree aggregation in the constructed attribute graph. HIEN also captures user\nintents for different item attributes as well as item intents based on our\nproposed hierarchical attention mechanism. Extensive experiments on both public\nand production datasets show that the proposed model significantly outperforms\nthe state-of-the-art methods. In addition, HIEN can be applied as an input\nmodule to state-of-the-art CTR prediction methods, bringing further performance\nlift for these existing models that might already be intensively used in real\nsystems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.03281,regular,pre_llm,2022,6,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Unsupervised Context Aware Sentence Representation Pretraining for\n  Multi-lingual Dense Retrieval\n\n  Recent research demonstrates the effectiveness of using pretrained language\nmodels (PLM) to improve dense retrieval and multilingual dense retrieval. In\nthis work, we present a simple but effective monolingual pretraining task\ncalled contrastive context prediction~(CCP) to learn sentence representation by\nmodeling sentence level contextual relation. By pushing the embedding of\nsentences in a local context closer and pushing random negative samples away,\ndifferent languages could form isomorphic structure, then sentence pairs in two\ndifferent languages will be automatically aligned. Our experiments show that\nmodel collapse and information leakage are very easy to happen during\ncontrastive training of language model, but language-specific memory bank and\nasymmetric batch normalization operation play an essential role in preventing\ncollapsing and information leakage, respectively. Besides, a post-processing\nfor sentence embedding is also very effective to achieve better retrieval\nperformance. On the multilingual sentence retrieval task Tatoeba, our model\nachieves new SOTA results among methods without using bilingual data. Our model\nalso shows larger gain on Tatoeba when transferring between non-English pairs.\nOn two multi-lingual query-passage retrieval tasks, XOR Retrieve and Mr.TYDI,\nour model even achieves two SOTA results in both zero-shot and supervised\nsetting among all pretraining models using bilingual data.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.07353,regular,pre_llm,2022,6,"{'ai_likelihood': 2.6490953233506946e-07, 'text': ""Rethinking Reinforcement Learning for Recommendation: A Prompt\n  Perspective\n\n  Modern recommender systems aim to improve user experience. As reinforcement\nlearning (RL) naturally fits this objective -- maximizing an user's reward per\nsession -- it has become an emerging topic in recommender systems. Developing\nRL-based recommendation methods, however, is not trivial due to the\n\\emph{offline training challenge}. Specifically, the keystone of traditional RL\nis to train an agent with large amounts of online exploration making lots of\n`errors' in the process. In the recommendation setting, though, we cannot\nafford the price of making `errors' online. As a result, the agent needs to be\ntrained through offline historical implicit feedback, collected under different\nrecommendation policies; traditional RL algorithms may lead to sub-optimal\npolicies under these offline training settings.\n  Here we propose a new learning paradigm -- namely Prompt-Based Reinforcement\nLearning (PRL) -- for the offline training of RL-based recommendation agents.\nWhile traditional RL algorithms attempt to map state-action input pairs to\ntheir expected rewards (e.g., Q-values), PRL directly infers actions (i.e.,\nrecommended items) from state-reward inputs. In short, the agents are trained\nto predict a recommended item given the prior interactions and an observed\nreward value -- with simple supervised learning. At deployment time, this\nhistorical (training) data acts as a knowledge base, while the state-reward\npairs are used as a prompt. The agents are thus used to answer the question:\n\\emph{ Which item should be recommended given the prior interactions \\& the\nprompted reward value}? We implement PRL with four notable recommendation\nmodels and conduct experiments on two real-world e-commerce datasets.\nExperimental results demonstrate the superior performance of our proposed\nmethods.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.03474,regular,pre_llm,2022,6,"{'ai_likelihood': 4.11934322781033e-05, 'text': 'A COVID-19 Search Engine (CO-SE) with Transformer-based Architecture\n\n  Coronavirus disease (COVID-19) is an infectious disease, which is caused by\nthe SARS-CoV-2 virus. Due to the growing literature on COVID-19, it is hard to\nget precise, up-to-date information about the virus. Practitioners, front-line\nworkers, and researchers require expert-specific methods to stay current on\nscientific knowledge and research findings. However, there are a lot of\nresearch papers being written on the subject, which makes it hard to keep up\nwith the most recent research. This problem motivates us to propose the design\nof the COVID-19 Search Engine (CO-SE), which is an algorithmic system that\nfinds relevant documents for each query (asked by a user) and answers complex\nquestions by searching a large corpus of publications. The CO-SE has a\nretriever component trained on the TF-IDF vectorizer that retrieves the\nrelevant documents from the system. It also consists of a reader component that\nconsists of a Transformer-based model, which is used to read the paragraphs and\nfind the answers related to the query from the retrieved documents. The\nproposed model has outperformed previous models, obtaining an exact match ratio\nscore of 71.45% and a semantic answer similarity score of 78.55%. It also\noutperforms other benchmark datasets, demonstrating the generalizability of the\nproposed approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.00242,regular,pre_llm,2022,6,"{'ai_likelihood': 3.7749608357747397e-06, 'text': ""CrossCBR: Cross-view Contrastive Learning for Bundle Recommendation\n\n  Bundle recommendation aims to recommend a bundle of related items to users,\nwhich can satisfy the users' various needs with one-stop convenience. Recent\nmethods usually take advantage of both user-bundle and user-item interactions\ninformation to obtain informative representations for users and bundles,\ncorresponding to bundle view and item view, respectively. However, they either\nuse a unified view without differentiation or loosely combine the predictions\nof two separate views, while the crucial cooperative association between the\ntwo views' representations is overlooked. In this work, we propose to model the\ncooperative association between the two different views through cross-view\ncontrastive learning. By encouraging the alignment of the two separately\nlearned views, each view can distill complementary information from the other\nview, achieving mutual enhancement. Moreover, by enlarging the dispersion of\ndifferent users/bundles, the self-discrimination of representations is\nenhanced. Extensive experiments on three public datasets demonstrate that our\nmethod outperforms SOTA baselines by a large margin. Meanwhile, our method\nrequires minimal parameters of three set of embeddings (user, bundle, and item)\nand the computational costs are largely reduced due to more concise graph\nstructure and graph learning module. In addition, various ablation and model\nstudies demystify the working mechanism and justify our hypothesis. Codes and\ndatasets are available at https://github.com/mysbupt/CrossCBR.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.04415,review,pre_llm,2022,6,"{'ai_likelihood': 1.7881393432617188e-06, 'text': 'Deep Meta-learning in Recommendation Systems: A Survey\n\n  Deep neural network based recommendation systems have achieved great success\nas information filtering techniques in recent years. However, since model\ntraining from scratch requires sufficient data, deep learning-based\nrecommendation methods still face the bottlenecks of insufficient data and\ncomputational inefficiency. Meta-learning, as an emerging paradigm that learns\nto improve the learning efficiency and generalization ability of algorithms,\nhas shown its strength in tackling the data sparsity issue. Recently, a growing\nnumber of studies on deep meta-learning based recommenddation systems have\nemerged for improving the performance under recommendation scenarios where\navailable data is limited, e.g. user cold-start and item cold-start. Therefore,\nthis survey provides a timely and comprehensive overview of current deep\nmeta-learning based recommendation methods. Specifically, we propose a taxonomy\nto discuss existing methods according to recommendation scenarios,\nmeta-learning techniques, and meta-knowledge representations, which could\nprovide the design space for meta-learning based recommendation methods. For\neach recommendation scenario, we further discuss technical details about how\nexisting methods apply meta-learning to improve the generalization ability of\nrecommendation models. Finally, we also point out several limitations in\ncurrent research and highlight some promising directions for future research in\nthis area.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.09672,regular,pre_llm,2022,6,"{'ai_likelihood': 1.3907750447591147e-06, 'text': ""Adaptive Domain Interest Network for Multi-domain Recommendation\n\n  Industrial recommender systems usually hold data from multiple business\nscenarios and are expected to provide recommendation services for these\nscenarios simultaneously. In the retrieval step, the topK high-quality items\nselected from a large number of corpus usually need to be various for multiple\nscenarios. Take Alibaba display advertising system for example, not only\nbecause the behavior patterns of Taobao users are diverse, but also\ndifferentiated scenarios' bid prices assigned by advertisers vary\nsignificantly. Traditional methods either train models for each scenario\nseparately, ignoring the cross-domain overlapping of user groups and items, or\nsimply mix all samples and maintain a shared model which makes it difficult to\ncapture significant diversities between scenarios. In this paper, we present\nAdaptive Domain Interest network that adaptively handles the commonalities and\ndiversities across scenarios, making full use of multi-scenarios data during\ntraining. Then the proposed method is able to improve the performance of each\nbusiness domain by giving various topK candidates for different scenarios\nduring online inference. Specifically, our proposed ADI models the\ncommonalities and diversities for different domains by shared networks and\ndomain-specific networks, respectively. In addition, we apply the\ndomain-specific batch normalization and design the domain interest adaptation\nlayer for feature-level domain adaptation. A self training strategy is also\nincorporated to capture label-level connections across domains.ADI has been\ndeployed in the display advertising system of Alibaba, and obtains 1.8%\nimprovement on advertising revenue.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.05954,regular,pre_llm,2022,6,"{'ai_likelihood': 2.086162567138672e-06, 'text': ""Scalable Exploration for Neural Online Learning to Rank with Perturbed\n  Feedback\n\n  Deep neural networks (DNNs) demonstrate significant advantages in improving\nranking performance in retrieval tasks. Driven by the recent technical\ndevelopments in optimization and generalization of DNNs, learning a neural\nranking model online from its interactions with users becomes possible.\nHowever, the required exploration for model learning has to be performed in the\nentire neural network parameter space, which is prohibitively expensive and\nlimits the application of such online solutions in practice.\n  In this work, we propose an efficient exploration strategy for online\ninteractive neural ranker learning based on the idea of bootstrapping. Our\nsolution employs an ensemble of ranking models trained with perturbed user\nclick feedback. The proposed method eliminates explicit confidence set\nconstruction and the associated computational overhead, which enables the\nonline neural rankers' training to be efficiently executed in practice with\ntheoretical guarantees. Extensive comparisons with an array of state-of-the-art\nOL2R algorithms on two public learning to rank benchmark datasets demonstrate\nthe effectiveness and computational efficiency of our proposed neural OL2R\nsolution.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.12016,regular,pre_llm,2022,6,"{'ai_likelihood': 3.642506069607205e-06, 'text': 'Unsupervised Learning Algorithms for Keyword Extraction in an\n  Undergraduate Thesis\n\n  The amount of data managed in many academic institutions has increased in\nrecent years, particularly in all the research work done by undergraduate\nstudents, who simply use empirical techniques for keyword selection, forgetting\nexisting technical methods to assist their students in this process.\nInformation and communication technologies, such as the platform for integrated\nresearch and academic work with responsibility (PILAR), which records\ninformation about research projects, such as titles, summaries, and keywords in\ntheir various modalities, have gained relevance and importance in the\nmanagement of these. We proved algorithms with these records of research\nprojects that have been analysed in this study, and predictions were made for\neach of the nine (09) models of unsupervised machine learning algorithms that\nwere implemented for each of the 7430 records from the dataset. The most\nefficient way of extracting keywords for this dataset was the TF-IDF method,\nobtaining 72% accuracy and [0.4786, SD 0.0501] in average extraction time for\neach thesis file processed by this model.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.06003,regular,pre_llm,2022,6,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'Deconfounding Duration Bias in Watch-time Prediction for Video\n  Recommendation\n\n  Watch-time prediction remains to be a key factor in reinforcing user\nengagement via video recommendations. It has become increasingly important\ngiven the ever-growing popularity of online videos. However, prediction of\nwatch time not only depends on the match between the user and the video but is\noften mislead by the duration of the video itself. With the goal of improving\nwatch time, recommendation is always biased towards videos with long duration.\nModels trained on this imbalanced data face the risk of bias amplification,\nwhich misguides platforms to over-recommend videos with long duration but\noverlook the underlying user interests.\n  This paper presents the first work to study duration bias in watch-time\nprediction for video recommendation. We employ a causal graph illuminating that\nduration is a confounding factor that concurrently affects video exposure and\nwatch-time prediction -- the first effect on video causes the bias issue and\nshould be eliminated, while the second effect on watch time originates from\nvideo intrinsic characteristics and should be preserved. To remove the\nundesired bias but leverage the natural effect, we propose a Duration\nDeconfounded Quantile-based (D2Q) watch-time prediction framework, which allows\nfor scalability to perform on industry production systems. Through extensive\noffline evaluation and live experiments, we showcase the effectiveness of this\nduration-deconfounding framework by significantly outperforming the\nstate-of-the-art baselines. We have fully launched our approach on Kuaishou\nApp, which has substantially improved real-time video consumption due to more\naccurate watch-time predictions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.02323,regular,pre_llm,2022,6,"{'ai_likelihood': 5.596213870578343e-06, 'text': 'ID-Agnostic User Behavior Pre-training for Sequential Recommendation\n\n  Recently, sequential recommendation has emerged as a widely studied topic.\nExisting researches mainly design effective neural architectures to model user\nbehavior sequences based on item IDs. However, this kind of approach highly\nrelies on user-item interaction data and neglects the attribute- or\ncharacteristic-level correlations among similar items preferred by a user. In\nlight of these issues, we propose IDA-SR, which stands for ID-Agnostic User\nBehavior Pre-training approach for Sequential Recommendation. Instead of\nexplicitly learning representations for item IDs, IDA-SR directly learns item\nrepresentations from rich text information. To bridge the gap between text\nsemantics and sequential user behaviors, we utilize the pre-trained language\nmodel as text encoder, and conduct a pre-training architecture on the\nsequential user behaviors. In this way, item text can be directly utilized for\nsequential recommendation without relying on item IDs. Extensive experiments\nshow that the proposed approach can achieve comparable results when only using\nID-agnostic item representations, and performs better than baselines by a large\nmargin when fine-tuned with ID information.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.11869,regular,pre_llm,2022,6,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'Case Study: The Impact of Location on Bias in Search Results\n\n  In this work, we aim to investigate the impact of location (different\ncountries) on bias in search results. For this, we use the search results of\nGoogle and Bing in the UK and US locations. The query set is composed of\ncontroversial queries obtained from ProCon.org that have specific ideological\nleanings as conservative or liberal. In a previous work, researchers analyse\nsearch results in terms of stance and ideological bias with rank and relevance\nbased measures. Yet, in the scope of this work, by using the query subset of\ncontroversial queries we examine the effect of location on the existence of\nbias as well as the magnitude of bias difference between Bing and Google. Note\nthat this study follows a similar evaluation procedure. Our preliminary results\nshow that location might affect the retrieval performance of search engines as\nwell as the bias in the search results returned by Bing and Google towards the\ncontroversial queries.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.15198,regular,pre_llm,2022,6,"{'ai_likelihood': 1.1920928955078125e-06, 'text': 'ListBERT: Learning to Rank E-commerce products with Listwise BERT\n\n  Efficient search is a critical component for an e-commerce platform with an\ninnumerable number of products. Every day millions of users search for products\npertaining to their needs. Thus, showing the relevant products on the top will\nenhance the user experience. In this work, we propose a novel approach of\nfusing a transformer-based model with various listwise loss functions for\nranking e-commerce products, given a user query. We pre-train a RoBERTa model\nover a fashion e-commerce corpus and fine-tune it using different listwise loss\nfunctions. Our experiments indicate that the RoBERTa model fine-tuned with an\nNDCG based surrogate loss function(approxNDCG) achieves an NDCG improvement of\n13.9% compared to other popular listwise loss functions like ListNET and\nListMLE. The approxNDCG based RoBERTa model also achieves an NDCG improvement\nof 20.6% compared to the pairwise RankNet based RoBERTa model. We call our\nmethodology of directly optimizing the RoBERTa model in an end-to-end manner\nwith a listwise surrogate loss function as ListBERT. Since there is a low\nlatency requirement in a real-time search setting, we show how these models can\nbe easily adopted by using a knowledge distillation technique to learn a\nrepresentation-focused student model that can be easily deployed and leads to\n~10 times lower ranking latency.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.05554,regular,pre_llm,2022,6,"{'ai_likelihood': 1.357661353217231e-06, 'text': 'Incremental Information Gain Mining Of Temporal Relational Streams\n\n  This paper studies the problem of mining for data values with high\ninformation gain in relational tables. High information gain can help data\nanalysts and secondary data mining algorithms gain insights into strong\nstatistical dependencies and causality relationship between key metrics. In\nthis paper, we will study the problem of high information gain identification\nfor scenarios involving temporal relations where new records are added\ncontinuously to the relations. We show that information gain can be efficiently\nmaintained in an incremental fashion, making it possible to monitor\ncontinuously high information gain values.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.14759,regular,pre_llm,2022,6,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'How Train-Test Leakage Affects Zero-shot Retrieval\n\n  Neural retrieval models are often trained on (subsets of) the millions of\nqueries of the MS MARCO / ORCAS datasets and then tested on the 250 Robust04\nqueries or other TREC benchmarks with often only 50 queries. In such setups,\nmany of the few test queries can be very similar to queries from the huge\ntraining data -- in fact, 69% of the Robust04 queries have near-duplicates in\nMS MARCO / ORCAS. We investigate the impact of this unintended train-test\nleakage by training neural retrieval models on combinations of a fixed number\nof MS MARCO / ORCAS queries that are highly similar to the actual test queries\nand an increasing number of other queries. We find that leakage can improve\neffectiveness and even change the ranking of systems. However, these effects\ndiminish as the amount of leakage among all training instances decreases and\nthus becomes more realistic.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.02115,regular,pre_llm,2022,6,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Learning Binarized Graph Representations with Multi-faceted Quantization\n  Reinforcement for Top-K Recommendation\n\n  Learning vectorized embeddings is at the core of various recommender systems\nfor user-item matching. To perform efficient online inference, representation\nquantization, aiming to embed the latent features by a compact sequence of\ndiscrete numbers, recently shows the promising potentiality in optimizing both\nmemory and computation overheads. However, existing work merely focuses on\nnumerical quantization whilst ignoring the concomitant information loss issue,\nwhich, consequently, leads to conspicuous performance degradation. In this\npaper, we propose a novel quantization framework to learn Binarized Graph\nRepresentations for Top-K Recommendation (BiGeaR). BiGeaR introduces\nmulti-faceted quantization reinforcement at the pre-, mid-, and post-stage of\nbinarized representation learning, which substantially retains the\nrepresentation informativeness against embedding binarization. In addition to\nsaving the memory footprint, BiGeaR further develops solid online inference\nacceleration with bitwise operations, providing alternative flexibility for the\nrealistic deployment. The empirical results over five large real-world\nbenchmarks show that BiGeaR achieves about 22%~40% performance improvement over\nthe state-of-the-art quantization-based recommender system, and recovers about\n95%~102% of the performance capability of the best full-precision counterpart\nwith over 8x time and space reduction.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.14468,regular,pre_llm,2022,6,"{'ai_likelihood': 1.0861290825737848e-05, 'text': ""Minimalist and High-performance Conversational Recommendation with\n  Uncertainty Estimation for User Preference\n\n  Conversational recommendation system (CRS) is emerging as a user-friendly way\nto capture users' dynamic preferences over candidate items and attributes.\nMulti-shot CRS is designed to make recommendations multiple times until the\nuser either accepts the recommendation or leaves at the end of their patience.\nExisting works are trained with reinforcement learning (RL), which may suffer\nfrom unstable learning and prohibitively high demands for computing. In this\nwork, we propose a simple and efficient CRS, MInimalist Non-reinforced\nInteractive COnversational Recommender Network (MINICORN). MINICORN models the\nepistemic uncertainty of the estimated user preference and queries the user for\nthe attribute with the highest uncertainty. The system employs a simple network\narchitecture and makes the query-vs-recommendation decision using a single\nrule. Somewhat surprisingly, this minimalist approach outperforms\nstate-of-the-art RL methods on three real-world datasets by large margins. We\nhope that MINICORN will serve as a valuable baseline for future research.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.12204,review,pre_llm,2022,6,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Reaching the End of Unbiasedness: Uncovering Implicit Limitations of\n  Click-Based Learning to Rank\n\n  Click-based learning to rank (LTR) tackles the mismatch between click\nfrequencies on items and their actual relevance. The approach of previous work\nhas been to assume a model of click behavior and to subsequently introduce a\nmethod for unbiasedly estimating preferences under that assumed model. The\nsuccess of this approach is evident in that unbiased methods have been found\nfor an increasing number of behavior models and types of bias. This work aims\nto uncover the implicit limitations of the high-level prevalent approach in the\ncounterfactual LTR field. Thus, in contrast with limitations that follow from\nexplicit assumptions, our aim is to recognize limitations that the field is\ncurrently unaware of. We do this by inverting the existing approach: we start\nby capturing existing methods in generic terms, and subsequently, from these\ngeneric descriptions we derive the click behavior for which these methods can\nbe unbiased. Our inverted approach reveals that there are indeed implicit\nlimitations to the counterfactual LTR approach: we find counterfactual\nestimation can only produce unbiased methods for click behavior based on affine\ntransformations. In addition, we also recognize previously undiscussed\nlimitations of click-modelling and pairwise approaches to click-based LTR. Our\nfindings reveal that it is impossible for existing approaches to provide\nunbiasedness guarantees for all plausible click behavior models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.01674,regular,pre_llm,2022,7,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'GazBy: Gaze-Based BERT Model to Incorporate Human Attention in Neural\n  Information Retrieval\n\n  This paper is interested in investigating whether human gaze signals can be\nleveraged to improve state-of-the-art search engine performance and how to\nincorporate this new input signal marked by human attention into existing\nneural retrieval models. In this paper, we propose GazBy ({\\bf Gaz}e-based {\\bf\nB}ert model for document relevanc{\\bf y}), a light-weight joint model that\nintegrates human gaze fixation estimation into transformer models to predict\ndocument relevance, incorporating more nuanced information about cognitive\nprocessing into information retrieval (IR). We evaluate our model on the Text\nRetrieval Conference (TREC) Deep Learning (DL) 2019 and 2020 Tracks. Our\nexperiments show encouraging results and illustrate the effective and\nineffective entry points for using human gaze to help with transformer-based\nneural retrievers. With the rise of virtual reality (VR) and augmented reality\n(AR), human gaze data will become more available. We hope this work serves as a\nfirst step exploring using gaze signals in modern neural search engines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.03588,regular,pre_llm,2022,7,"{'ai_likelihood': 1.0364585452609592e-05, 'text': 'On the Metric Properties of IR Evaluation Measures Based on Ranking\n  Axioms\n\n  The axiomatic analysis of IR evaluation metrics has contributed to a better\nunderstanding of their properties. Some works have modelled the effectiveness\nof retrieval measures with axioms that capture desirable properties on the set\nof ranked lists of documents. Recently, it has been shown that three of these\naxioms lead to some orderings. This work formally explores the metric\nproperties of the set of rankings, endowed with these orderings. Based on\nlattice theory, the possible metrics and pseudo-metrics, defined on these\nstructures, are determined. It is found that, when the relevant documents are\nprioritized, precision, recall, RBP and DCG are metrics on the set of rankings,\nhowever they are pseudo-metrics when the swapping of documents is considered.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.00511,regular,pre_llm,2022,7,"{'ai_likelihood': 3.311369154188368e-08, 'text': ""Aggretriever: A Simple Approach to Aggregate Textual Representations for\n  Robust Dense Passage Retrieval\n\n  Pre-trained language models have been successful in many knowledge-intensive\nNLP tasks. However, recent work has shown that models such as BERT are not\n``structurally ready'' to aggregate textual information into a [CLS] vector for\ndense passage retrieval (DPR). This ``lack of readiness'' results from the gap\nbetween language model pre-training and DPR fine-tuning. Previous solutions\ncall for computationally expensive techniques such as hard negative mining,\ncross-encoder distillation, and further pre-training to learn a robust DPR\nmodel. In this work, we instead propose to fully exploit knowledge in a\npre-trained language model for DPR by aggregating the contextualized token\nembeddings into a dense vector, which we call agg*. By concatenating vectors\nfrom the [CLS] token and agg*, our Aggretriever model substantially improves\nthe effectiveness of dense retrieval models on both in-domain and zero-shot\nevaluations without introducing substantial training overhead. Code is\navailable at https://github.com/castorini/dhr\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.03372,regular,pre_llm,2022,7,"{'ai_likelihood': 4.106097751193576e-06, 'text': 'Evolution of Popularity Bias: Empirical Study and Debiasing\n\n  Popularity bias is a long-standing challenge in recommender systems. Such a\nbias exerts detrimental impact on both users and item providers, and many\nefforts have been dedicated to studying and solving such a bias. However, most\nexisting works situate this problem in a static setting, where the bias is\nanalyzed only for a single round of recommendation with logged data. These\nworks fail to take account of the dynamic nature of real-world recommendation\nprocess, leaving several important research questions unanswered: how does the\npopularity bias evolve in a dynamic scenario? what are the impacts of unique\nfactors in a dynamic recommendation process on the bias? and how to debias in\nthis long-term dynamic process? In this work, we aim to tackle these research\ngaps. Concretely, we conduct an empirical study by simulation experiments to\nanalyze popularity bias in the dynamic scenario and propose a dynamic debiasing\nstrategy and a novel False Positive Correction method utilizing false positive\nsignals to debias, which show effective performance in extensive experiments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.03073,regular,pre_llm,2022,7,"{'ai_likelihood': 6.126032935248481e-06, 'text': ""Contrastive Information Transfer for Pre-Ranking Systems\n\n  Real-word search and recommender systems usually adopt a multi-stage ranking\narchitecture, including matching, pre-ranking, ranking, and re-ranking.\nPrevious works mainly focus on the ranking stage while very few focus on the\npre-ranking stage. In this paper, we focus on the information transfer from\nranking to pre-ranking stage. We propose a new Contrastive Information Transfer\n(CIT) framework to transfer useful information from ranking model to\npre-ranking model. We train the pre-ranking model to distinguish the positive\npair of representation from a set of positive and negative pairs with a\ncontrastive objective. As a consequence, the pre-ranking model can make full\nuse of rich information in ranking model's representations. The CIT framework\nalso has the advantage of alleviating selection bias and improving the\nperformance of recall metrics, which is crucial for pre-ranking models. We\nconduct extensive experiments including offline datasets and online A/B\ntesting. Experimental results show that CIT achieves superior results than\ncompetitive models. In addition, a strict online A/B testing at one of the\nworld's largest E-commercial platforms shows that the proposed model achieves\n0.63\\% improvements on CTR and 1.64\\% improvements on VBR. The proposed model\nnow has been deployed online and serves the main traffic of this system,\ncontributing a remarkable business growth.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.04862,regular,pre_llm,2022,7,"{'ai_likelihood': 9.602970547146267e-07, 'text': ""Slot Filling for Extracting Reskilling and Upskilling Options from the\n  Web\n\n  Disturbances in the job market such as advances in science and technology,\ncrisis and increased competition have triggered a surge in reskilling and\nupskilling programs. Information on suitable continuing education options is\ndistributed across many sites, rendering the search, comparison and selection\nof useful programs a cumbersome task. This paper, therefore, introduces a\nknowledge extraction system that integrates reskilling and upskilling options\ninto a single knowledge graph. The system collects educational programs from\n488 different providers and uses context extraction for identifying and\ncontextualizing relevant content. Afterwards, entity recognition and entity\nlinking methods draw upon a domain ontology to locate relevant entities such as\nskills, occupations and topics. Finally, slot filling integrates entities based\non their context into the corresponding slots of the continuous education\nknowledge graph. We also introduce a German gold standard that comprises 169\ndocuments and over 3800 annotations for benchmarking the necessary content\nextraction, entity linking, entity recognition and slot filling tasks, and\nprovide an overview of the system's performance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.0035,regular,pre_llm,2022,7,"{'ai_likelihood': 1.2252065870496963e-06, 'text': 'Modelling Users with Item Metadata for Explainable and Interactive\n  Recommendation\n\n  Recommender systems are used in many different applications and contexts,\nhowever their main goal can always be summarised as ""connecting relevant\ncontent to interested users"". Personalized recommendation algorithms achieve\nthis goal by first building a profile of the user, either implicitly or\nexplicitly, and then matching items with this profile to find relevant content.\nThe more interpretable the profile and this ""matching function"" are, the easier\nit is to provide users with accurate and intuitive explanations, and also to\nlet them interact with the system. Indeed, for a user to see what the system\nhas already learned about her interests is of key importance for her to provide\nfeedback to the system and to guide it towards better understanding her\npreferences.\n  To this end, we propose a linear collaborative filtering recommendation model\nthat builds user profiles within the domain of item metadata, which is arguably\nthe most interpretable domain for end users. Our method is hence inherently\ntransparent and explainable. Moreover, since recommendations are computed as a\nlinear function of item metadata and the interpretable user profile, our method\nseamlessly supports interactive recommendation. In other words, users can\ndirectly tweak the weights of the learned profile for more fine-grained\nbrowsing and discovery of content based on their current interests.\n  We demonstrate the interactive aspect of this model in an online application\nfor discovering cultural events in Belgium. Additionally, the performance of\nthe model is evaluated with offline experiments, both static and with simulated\nfeedback, and compared to several state-of-the-art and state-of-practice\nbaselines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.11088,regular,pre_llm,2022,7,"{'ai_likelihood': 7.980399661593967e-06, 'text': 'Layer-refined Graph Convolutional Networks for Recommendation\n\n  Recommendation models utilizing Graph Convolutional Networks (GCNs) have\nachieved state-of-the-art performance, as they can integrate both the node\ninformation and the topological structure of the user-item interaction graph.\nHowever, these GCN-based recommendation models not only suffer from\nover-smoothing when stacking too many layers but also bear performance\ndegeneration resulting from the existence of noise in user-item interactions.\nIn this paper, we first identify a recommendation dilemma of over-smoothing and\nsolution collapsing in current GCN-based models. Specifically, these models\nusually aggregate all layer embeddings for node updating and achieve their best\nrecommendation performance within a few layers because of over-smoothing.\nConversely, if we place learnable weights on layer embeddings for node\nupdating, the weight space will always collapse to a fixed point, at which the\nweighting of the ego layer almost holds all. We propose a layer-refined GCN\nmodel, dubbed LayerGCN, that refines layer representations during information\npropagation and node updating of GCN. Moreover, previous GCN-based\nrecommendation models aggregate all incoming information from neighbors without\ndistinguishing the noise nodes, which deteriorates the recommendation\nperformance. Our model further prunes the edges of the user-item interaction\ngraph following a degree-sensitive probability instead of the uniform\ndistribution. Experimental results show that the proposed model outperforms the\nstate-of-the-art models significantly on four public datasets with fast\ntraining convergence. The implementation code of the proposed method is\navailable at https://github.com/enoche/ImRec.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.08922,regular,pre_llm,2022,7,"{'ai_likelihood': 1.11593140496148e-05, 'text': ""ir_metadata: An Extensible Metadata Schema for IR Experiments\n\n  The information retrieval (IR) community has a strong tradition of making the\ncomputational artifacts and resources available for future reuse, allowing the\nvalidation of experimental results. Besides the actual test collections, the\nunderlying run files are often hosted in data archives as part of conferences\nlike TREC, CLEF, or NTCIR. Unfortunately, the run data itself does not provide\nmuch information about the underlying experiment. For instance, the single run\nfile is not of much use without the context of the shared task's website or the\nrun data archive. In other domains, like the social sciences, it is good\npractice to annotate research data with metadata. In this work, we introduce\nir_metadata - an extensible metadata schema for TREC run files based on the\nPRIMAD model. We propose to align the metadata annotations to PRIMAD, which\nconsiders components of computational experiments that can affect\nreproducibility. Furthermore, we outline important components and information\nthat should be reported in the metadata and give evidence from the literature.\nTo demonstrate the usefulness of these metadata annotations, we implement new\nfeatures in repro_eval that support the outlined metadata schema for the use\ncase of reproducibility studies. Additionally, we curate a dataset with run\nfiles derived from experiments with different instantiations of PRIMAD\ncomponents and annotate these with the corresponding metadata. In the\nexperiments, we cover reproducibility experiments that are identified by the\nmetadata and classified by PRIMAD. With this work, we enable IR researchers to\nannotate TREC run files and improve the reuse value of experimental artifacts\neven further.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.13443,review,pre_llm,2022,7,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Lecture Notes on Neural Information Retrieval\n\n  These lecture notes focus on the recent advancements in neural information\nretrieval, with particular emphasis on the systems and models exploiting\ntransformer networks. These networks, originally proposed by Google in 2017,\nhave seen a large success in many natural language processing and information\nretrieval tasks. While there are many fantastic textbook on information\nretrieval and natural language processing as well as specialised books for a\nmore advanced audience, these lecture notes target people aiming at developing\na basic understanding of the main information retrieval techniques and\napproaches based on deep learning. These notes have been prepared for a IR\ngraduate course of the MSc program in Artificial Intelligence and Data\nEngineering at the University of Pisa, Italy.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.0932,review,pre_llm,2022,7,"{'ai_likelihood': 1.0596381293402778e-06, 'text': ""Group Validation in Recommender Systems: Framework for Multi-layer\n  Performance Evaluation\n\n  Interpreting the performance results of models that attempt to realize user\nbehavior in platforms that employ recommenders is a big challenge that\nresearchers and practitioners continue to face. Although current evaluation\ntools possess the capacity to provide solid general overview of a system's\nperformance, they still lack consistency and effectiveness in their use as\nevident in most recent studies on the topic. Current traditional assessment\ntechniques tend to fail to detect variations that could occur on smaller\nsubsets of the data and lack the ability to explain how such variations affect\nthe overall performance. In this article, we focus on the concept of data\nclustering for evaluation in recommenders and apply a neighborhood assessment\nmethod for the datasets of recommender system applications. This new method,\nnamed neighborhood-based evaluation, aids in better understanding critical\nperformance variations in more compact subsets of the system to help spot\nweaknesses where such variations generally go unnoticed with conventional\nmetrics and are typically averaged out. This new modular evaluation layer\ncomplements the existing assessment mechanisms and provides the possibility of\nseveral applications to the recommender ecosystem such as model evolution\ntests, fraud/attack detection and a possibility for hosting a hybrid model\nsetup.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.05525,regular,pre_llm,2022,7,"{'ai_likelihood': 9.338061014811199e-06, 'text': 'FedHAP: Federated Hashing with Global Prototypes for Cross-silo\n  Retrieval\n\n  Deep hashing has been widely applied in large-scale data retrieval due to its\nsuperior retrieval efficiency and low storage cost. However, data are often\nscattered in data silos with privacy concerns, so performing centralized data\nstorage and retrieval is not always possible. Leveraging the concept of\nfederated learning (FL) to perform deep hashing is a recent research trend.\nHowever, existing frameworks mostly rely on the aggregation of the local deep\nhashing models, which are trained by performing similarity learning with local\nskewed data only. Therefore, they cannot work well for non-IID clients in a\nreal federated environment. To overcome these challenges, we propose a novel\nfederated hashing framework that enables participating clients to jointly train\nthe shared deep hashing model by leveraging the prototypical hash codes for\neach class. Globally, the transmission of global prototypes with only one\nprototypical hash code per class will minimize the impact of communication cost\nand privacy risk. Locally, the use of global prototypes are maximized by\njointly training a discriminator network and the local hashing network.\nExtensive experiments on benchmark datasets are conducted to demonstrate that\nour method can significantly improve the performance of the deep hashing model\nin the federated environments with non-IID data distributions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.05772,review,pre_llm,2022,7,"{'ai_likelihood': 2.086162567138672e-06, 'text': 'EvalRS: a Rounded Evaluation of Recommender Systems\n\n  Much of the complexity of Recommender Systems (RSs) comes from the fact that\nthey are used as part of more complex applications and affect user experience\nthrough a varied range of user interfaces. However, research focused almost\nexclusively on the ability of RSs to produce accurate item rankings while\ngiving little attention to the evaluation of RS behavior in real-world\nscenarios. Such narrow focus has limited the capacity of RSs to have a lasting\nimpact in the real world and makes them vulnerable to undesired behavior, such\nas reinforcing data biases. We propose EvalRS as a new type of challenge, in\norder to foster this discussion among practitioners and build in the open new\nmethodologies for testing RSs ""in the wild"".\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.04317,review,pre_llm,2022,7,"{'ai_likelihood': 5.761782328287761e-06, 'text': 'On the Relationship Between Counterfactual Explainer and Recommender\n\n  Recommender systems employ machine learning models to learn from historical\ndata to predict the preferences of users. Deep neural network (DNN) models such\nas neural collaborative filtering (NCF) are increasingly popular. However, the\ntangibility and trustworthiness of the recommendations are questionable due to\nthe complexity and lack of explainability of the models. To enable\nexplainability, recent techniques such as ACCENT and FIA are looking for\ncounterfactual explanations that are specific historical actions of a user, the\nremoval of which leads to a change to the recommendation result. In this work,\nwe present a general framework for both DNN and non-DNN models so that the\ncounterfactual explainers all belong to it with specific choices of components.\nThis framework first estimates the influence of a certain historical action\nafter its removal and then uses search algorithms to find the minimal set of\nsuch actions for the counterfactual explanation. With this framework, we are\nable to investigate the relationship between the explainers and recommenders.\nWe empirically study two recommender models (NCF and Factorization Machine) and\ntwo datasets (MovieLens and Yelp). We analyze the relationship between the\nperformance of the recommender and the quality of the explainer. We observe\nthat with standard evaluation metrics, the explainers deliver worse performance\nwhen the recommendations are more accurate. This indicates that having good\nexplanations to correct predictions is harder than having them to wrong\npredictions. The community needs more fine-grained evaluation metrics to\nmeasure the quality of counterfactual explanations to recommender systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.01201,review,pre_llm,2022,7,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'On the Effect of Ranking Axioms on IR Evaluation Metrics\n\n  The study of IR evaluation metrics through axiomatic analysis enables a\nbetter understanding of their numerical properties. Some works have modelled\nthe effectiveness of retrieval metrics with axioms that capture desirable\nproperties on the set of rankings of documents. This paper formally explores\nthe effect of these ranking axioms on the numerical values of some IR\nevaluation metrics. It focuses on the set of ranked lists of documents with\nmultigrade relevance. The possible orderings in this set are derived from three\ncommonly accepted ranking axioms on retrieval metrics; then, they are\nclassified by their latticial properties. When relevant documents are\nprioritised, a subset of document rankings are identified: the join-irreducible\nelements, which have some resemblance to the concept of basis in vector space.\nIt is possible to compute the precision, recall, RBP or DCG values of any\nranking from their values in the join-irreducible elements. However this is not\nthe case when the swapping of documents is considered.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.11497,regular,pre_llm,2022,7,"{'ai_likelihood': 4.106097751193576e-06, 'text': 'Patent Search Using Triplet Networks Based Fine-Tuned SciBERT\n\n  In this paper, we propose a novel method for the prior-art search task. We\nfine-tune SciBERT transformer model using Triplet Network approach, allowing us\nto represent each patent with a fixed-size vector. This also enables us to\nconduct efficient vector similarity computations to rank patents in query time.\nIn our experiments, we show that our proposed method outperforms baseline\nmethods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.03153,regular,pre_llm,2022,7,"{'ai_likelihood': 1.0927518208821616e-06, 'text': 'Supervised Contrastive Learning Approach for Contextual Ranking\n\n  Contextual ranking models have delivered impressive performance improvements\nover classical models in the document ranking task. However, these highly\nover-parameterized models tend to be data-hungry and require large amounts of\ndata even for fine tuning. This paper proposes a simple yet effective method to\nimprove ranking performance on smaller datasets using supervised contrastive\nlearning for the document ranking problem. We perform data augmentation by\ncreating training data using parts of the relevant documents in the\nquery-document pairs. We then use a supervised contrastive learning objective\nto learn an effective ranking model from the augmented dataset. Our experiments\non subsets of the TREC-DL dataset show that, although data augmentation leads\nto an increasing the training data sizes, it does not necessarily improve the\nperformance using existing pointwise or pairwise training objectives. However,\nour proposed supervised contrastive loss objective leads to performance\nimprovements over the standard non-augmented setting showcasing the utility of\ndata augmentation using contrastive losses. Finally, we show the real benefit\nof using supervised contrastive learning objectives by showing marked\nimprovements in smaller ranking datasets relating to news (Robust04), finance\n(FiQA), and scientific fact checking (SciFact).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.0733,review,pre_llm,2022,7,"{'ai_likelihood': 2.4206108517116972e-05, 'text': 'A Comparison of Source Distribution and Result Overlap in Web Search\n  Engines\n\n  When it comes to search engines, users generally prefer Google. Our study\naims to find the differences between the results found in Google compared to\nother search engines. We compared the top 10 results from Google, Bing,\nDuckDuckGo, and Metager, using 3,537 queries generated from Google Trends from\nGermany and the US. Google displays more unique domains in the top results than\nits competitors. Wikipedia and news websites are the most popular sources\noverall. With some top sources dominating search results, the distribution of\ndomains is also consistent across all search engines. The overlap between\nGoogle and Bing is always under 32%, while Metager has a higher overlap with\nBing than DuckDuckGo, going up to 78%. This study shows that the use of another\nsearch engine, especially in addition to Google, provides a wider variety in\nsources and might lead the user to find new perspectives.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.13262,regular,pre_llm,2022,7,"{'ai_likelihood': 9.106265174018012e-06, 'text': ""Factorial User Modeling with Hierarchical Graph Neural Network for\n  Enhanced Sequential Recommendation\n\n  Most sequential recommendation (SR) systems employing graph neural networks\n(GNNs) only model a user's interaction sequence as a flat graph without\nhierarchy, overlooking diverse factors in the user's preference. Moreover, the\ntimespan between interacted items is not sufficiently utilized by previous\nmodels, restricting SR performance gains. To address these problems, we propose\na novel SR system employing a hierarchical graph neural network (HGNN) to model\nfactorial user preferences. Specifically, a timespan-aware sequence graph (TSG)\nfor the target user is first constructed with the timespan among interacted\nitems. Next, all original nodes in TSG are softly clustered into factor nodes,\neach of which represents a certain factor of the user's preference. At last,\nall factor nodes' representations are used together to predict SR results. Our\nextensive experiments upon two datasets justify that our HGNN-based factorial\nuser modeling obtains better SR performance than the state-of-the-art SR\nmodels.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.03103,review,pre_llm,2022,7,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'Batch Evaluation Metrics in Information Retrieval: Measures, Scales, and\n  Meaning\n\n  A sequence of recent papers has considered the role of measurement scales in\ninformation retrieval (IR) experimentation, and presented the argument that\n(only) uniform-step interval scales should be used, and hence that well-known\nmetrics such as reciprocal rank, expected reciprocal rank, normalized\ndiscounted cumulative gain, and average precision, should be either discarded\nas measurement tools, or adapted so that their metric values lie at\nuniformly-spaced points on the number line. These papers paint a rather bleak\npicture of past decades of IR evaluation, at odds with the community\'s overall\nemphasis on practical experimentation and measurable improvement.\n  Our purpose in this work is to challenge that position. In particular, we\nargue that mappings from categorical and ordinal data to sets of points on the\nnumber line are valid provided there is an external reason for each target\npoint to have been selected. We first consider the general role of measurement\nscales, and of categorical, ordinal, interval, ratio, and absolute data\ncollections. In connection with the first two of those categories we also\nprovide examples of the knowledge that is captured and represented by numeric\nmappings to the real number line. Focusing then on information retrieval, we\nargue that document rankings are categorical data, and that the role of an\neffectiveness metric is to provide a single value that represents the\nusefulness to a user or population of users of any given ranking, with\nusefulness able to be represented as a continuous variable on a ratio scale.\nThat is, we argue that current IR metrics are well-founded, and, moreover, that\nthose metrics are more meaningful in their current form than in the proposed\n""intervalized"" versions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.06936,regular,pre_llm,2022,8,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'TripJudge: A Relevance Judgement Test Collection for TripClick Health\n  Retrieval\n\n  Robust test collections are crucial for Information Retrieval research.\nRecently there is a growing interest in evaluating retrieval systems for\ndomain-specific retrieval tasks, however these tasks often lack a reliable test\ncollection with human-annotated relevance assessments following the Cranfield\nparadigm. In the medical domain, the TripClick collection was recently\nproposed, which contains click log data from the Trip search engine and\nincludes two click-based test sets. However the clicks are biased to the\nretrieval model used, which remains unknown, and a previous study shows that\nthe test sets have a low judgement coverage for the Top-10 results of lexical\nand neural retrieval models. In this paper we present the novel, relevance\njudgement test collection TripJudge for TripClick health retrieval. We collect\nrelevance judgements in an annotation campaign and ensure the quality and\nreusability of TripJudge by a variety of ranking methods for pool creation, by\nmultiple judgements per query-document pair and by an at least moderate\ninter-annotator agreement. We compare system evaluation with TripJudge and\nTripClick and find that that click and judgement-based evaluation can lead to\nsubstantially different system rankings.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.04482,regular,pre_llm,2022,8,"{'ai_likelihood': 1.0629494984944662e-05, 'text': ""OptEmbed: Learning Optimal Embedding Table for Click-through Rate\n  Prediction\n\n  Learning embedding table plays a fundamental role in Click-through rate(CTR)\nprediction from the view of the model performance and memory usage. The\nembedding table is a two-dimensional tensor, with its axes indicating the\nnumber of feature values and the embedding dimension, respectively. To learn an\nefficient and effective embedding table, recent works either assign various\nembedding dimensions for feature fields and reduce the number of embeddings\nrespectively or mask the embedding table parameters. However, all these\nexisting works cannot get an optimal embedding table. On the one hand, various\nembedding dimensions still require a large amount of memory due to the vast\nnumber of features in the dataset. On the other hand, decreasing the number of\nembeddings usually suffers from performance degradation, which is intolerable\nin CTR prediction. Finally, pruning embedding parameters will lead to a sparse\nembedding table, which is hard to be deployed. To this end, we propose an\noptimal embedding table learning framework OptEmbed, which provides a practical\nand general method to find an optimal embedding table for various base CTR\nmodels. Specifically, we propose pruning the redundant embeddings regarding\ncorresponding features' importance by learnable pruning thresholds.\nFurthermore, we consider assigning various embedding dimensions as one single\ncandidate architecture. To efficiently search the optimal embedding dimensions,\nwe design a uniform embedding dimension sampling scheme to equally train all\ncandidate architectures, meaning architecture-related parameters and learnable\nthresholds are trained simultaneously in one supernet. We then propose an\nevolution search method based on the supernet to find the optimal embedding\ndimensions for each field. Experiments on public datasets show that OptEmbed\ncan learn a compact embedding table which can further improve the model\nperformance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.10192,regular,pre_llm,2022,8,"{'ai_likelihood': 2.947118547227648e-06, 'text': ""Towards Confidence-aware Calibrated Recommendation\n\n  Recommender systems utilize users' historical data to learn and predict their\nfuture interests, providing them with suggestions tailored to their tastes.\nCalibration ensures that the distribution of recommended item categories is\nconsistent with the user's historical data. Mitigating miscalibration brings\nvarious benefits to a recommender system. For example, it becomes less likely\nthat a system overlooks categories with less interaction on a user's profile by\nonly recommending popular categories. Despite the notable success, calibration\nmethods have several drawbacks, such as limiting the diversity of the\nrecommended items and not considering the calibration confidence. This work,\npresents a set of properties that address various aspects of a desired\ncalibrated recommender system. Considering these properties, we propose a\nconfidence-aware optimization-based re-ranking algorithm to find the balance\nbetween calibration, relevance, and item diversity, while simultaneously\naccounting for calibration confidence based on user profile size. Our model\noutperforms state-of-the-art methods in terms of various accuracy and\nbeyond-accuracy metrics for different user groups.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.05663,regular,pre_llm,2022,8,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'On the Value of Behavioral Representations for Dense Retrieval\n\n  We consider text retrieval within dense representational space in real-world\nsettings such as e-commerce search where (a) document popularity and (b)\ndiversity of queries associated with a document have a skewed distribution.\nMost of the contemporary dense retrieval literature presents two shortcomings\nin these settings. (1) They learn an almost equal number of representations per\ndocument, agnostic to the fact that a few head documents are disproportionately\nmore critical to achieving a good retrieval performance. (ii) They learn purely\nsemantic document representations inferred from intrinsic document\ncharacteristics which may not contain adequate information to determine the\nqueries for which the document is relevant--especially when the document is\nshort. We propose to overcome these limitations by augmenting semantic document\nrepresentations learned by bi-encoders with behavioral document representations\nlearned by our proposed approach MVG. To do so, MVG (1) determines how to\ndivide the total budget for behavioral representations by drawing a connection\nto the Pitman-Yor process, and (2) simply clusters the queries related to a\ngiven document (based on user behavior) within the representational space\nlearned by a base bi-encoder, and treats the cluster centers as its behavioral\nrepresentations. Our central contribution is the finding such a simple\nintuitive light-weight approach leads to substantial gains in key first-stage\nretrieval metrics by incurring only a marginal memory overhead. We establish\nthis via extensive experiments over three large public datasets comparing\nseveral single-vector and multi-vector bi-encoders, a proprietary e-commerce\nsearch dataset compared to production-quality bi-encoder, and an A/B test.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.14139,regular,pre_llm,2022,8,"{'ai_likelihood': 2.086162567138672e-06, 'text': ""Large-scale Multi-granular Concept Extraction Based on Machine Reading\n  Comprehension\n\n  The concepts in knowledge graphs (KGs) enable machines to understand natural\nlanguage, and thus play an indispensable role in many applications. However,\nexisting KGs have the poor coverage of concepts, especially fine-grained\nconcepts. In order to supply existing KGs with more fine-grained and new\nconcepts, we propose a novel concept extraction framework, namely MRC-CE, to\nextract large-scale multi-granular concepts from the descriptive texts of\nentities. Specifically, MRC-CE is built with a machine reading comprehension\nmodel based on BERT, which can extract more fine-grained concepts with a\npointer network. Furthermore, a random forest and rule-based pruning are also\nadopted to enhance MRC-CE's precision and recall simultaneously. Our\nexperiments evaluated upon multilingual KGs, i.e., English Probase and Chinese\nCN-DBpedia, justify MRC-CE's superiority over the state-of-the-art extraction\nmodels in KG completion. Particularly, after running MRC-CE for each entity in\nCN-DBpedia, more than 7,053,900 new concepts (instanceOf relations) are\nsupplied into the KG. The code and datasets have been released at\nhttps://github.com/fcihraeipnusnacwh/MRC-CE\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.08024,regular,pre_llm,2022,8,"{'ai_likelihood': 1.2914339701334636e-06, 'text': ""CCL4Rec: Contrast over Contrastive Learning for Micro-video\n  Recommendation\n\n  Micro-video recommender systems suffer from the ubiquitous noises in users'\nbehaviors, which might render the learned user representation indiscriminating,\nand lead to trivial recommendations (e.g., popular items) or even weird ones\nthat are far beyond users' interests. Contrastive learning is an emergent\ntechnique for learning discriminating representations with random data\naugmentations. However, due to neglecting the noises in user behaviors and\ntreating all augmented samples equally, the existing contrastive learning\nframework is insufficient for learning discriminating user representations in\nrecommendation. To bridge this research gap, we propose the Contrast over\nContrastive Learning framework for training recommender models, named CCL4Rec,\nwhich models the nuances of different augmented views by further contrasting\naugmented positives/negatives with adaptive pulling/pushing strengths, i.e.,\nthe contrast over (vanilla) contrastive learning. To accommodate these\ncontrasts, we devise the hardness-aware augmentations that track the importance\nof behaviors being replaced in the query user and the relatedness of\nsubstitutes, and thus determining the quality of augmented positives/negatives.\nThe hardness-aware augmentation also permits controllable contrastive learning,\nleading to performance gains and robust training. In this way, CCL4Rec captures\nthe nuances of historical behaviors for a given user, which explicitly shields\noff the learned user representation from the effects of noisy behaviors. We\nconduct extensive experiments on two micro-video recommendation benchmarks,\nwhich demonstrate that CCL4Rec with far less model parameters could achieve\ncomparable performance to existing state-of-the-art method, and improve the\ntraining/inference speed by several orders of magnitude.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.06264,regular,pre_llm,2022,8,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'A Boring-yet-effective Approach for the Product Ranking Task of the\n  Amazon KDD Cup 2022\n\n  In this work we describe our submission to the product ranking task of the\nAmazon KDD Cup 2022. We rely on a receipt that showed to be effective in\nprevious competitions: we focus our efforts towards efficiently training and\ndeploying large language odels, such as mT5, while reducing to a minimum the\nnumber of task-specific adaptations. Despite the simplicity of our approach,\nour best model was less than 0.004 nDCG@20 below the top submission. As the top\n20 teams achieved an nDCG@20 close to .90, we argue that we need more difficult\ne-Commerce evaluation datasets to discriminate retrieval methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.08612,regular,pre_llm,2022,8,"{'ai_likelihood': 2.3146470387776695e-05, 'text': 'Modeling Two-Way Selection Preference for Person-Job Fit\n\n  Person-job fit is the core technique of online recruitment platforms, which\ncan improve the efficiency of recruitment by accurately matching the job\npositions with the job seekers. Existing works mainly focus on modeling the\nunidirectional process or overall matching. However, recruitment is a two-way\nselection process, which means that both candidate and employer involved in the\ninteraction should meet the expectation of each other, instead of unilateral\nsatisfaction. In this paper, we propose a dual-perspective graph representation\nlearning approach to model directed interactions between candidates and jobs.\nTo model the two-way selection preference from the dual-perspective of job\nseekers and employers, we incorporate two different nodes for each candidate\n(or job) and characterize both successful matching and failed matching via a\nunified dual-perspective interaction graph. To learn dual-perspective node\nrepresentations effectively, we design an effective optimization algorithm,\nwhich involves a quadruple-based loss and a dual-perspective contrastive\nlearning loss. Extensive experiments on three large real-world recruitment\ndatasets have shown the effectiveness of our approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.01889,regular,pre_llm,2022,8,"{'ai_likelihood': 1.817941665649414e-05, 'text': ""Multi-Scale User Behavior Network for Entire Space Multi-Task Learning\n\n  Modelling the user's multiple behaviors is an essential part of modern\ne-commerce, whose widely adopted application is to jointly optimize\nclick-through rate (CTR) and conversion rate (CVR) predictions. Most of\nexisting methods overlook the effect of two key characteristics of the user's\nbehaviors: for each item list, (i) contextual dependence refers to that the\nuser's behaviors on any item are not purely determinated by the item itself but\nalso are influenced by the user's previous behaviors (e.g., clicks, purchases)\non other items in the same sequence; (ii) multiple time scales means that users\nare likely to click frequently but purchase periodically. To this end, we\ndevelop a new multi-scale user behavior network named Hierarchical rEcurrent\nRanking On the Entire Space (HEROES) which incorporates the contextual\ninformation to estimate the user multiple behaviors in a multi-scale fashion.\nConcretely, we introduce a hierarchical framework, where the lower layer models\nthe user's engagement behaviors while the upper layer estimates the user's\nsatisfaction behaviors. The proposed architecture can automatically learn a\nsuitable time scale for each layer to capture the dynamic user's behavioral\npatterns. Besides the architecture, we also introduce the Hawkes process to\nform a novel recurrent unit which can not only encode the items' features in\nthe context but also formulate the excitation or discouragement from the user's\nprevious behaviors. We further show that HEROES can be extended to build\nunbiased ranking systems through combinations with the survival analysis\ntechnique. Extensive experiments over three large-scale industrial datasets\ndemonstrate the superiority of our model compared with the state-of-the-art\nmethods.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.04116,regular,pre_llm,2022,8,"{'ai_likelihood': 1.2483861711290148e-05, 'text': 'UFNRec: Utilizing False Negative Samples for Sequential Recommendation\n\n  Sequential recommendation models are primarily optimized to distinguish\npositive samples from negative ones during training in which negative sampling\nserves as an essential component in learning the evolving user preferences\nthrough historical records. Except for randomly sampling negative samples from\na uniformly distributed subset, many delicate methods have been proposed to\nmine negative samples with high quality. However, due to the inherent\nrandomness of negative sampling, false negative samples are inevitably\ncollected in model training. Current strategies mainly focus on removing such\nfalse negative samples, which leads to overlooking potential user interests,\nlack of recommendation diversity, less model robustness, and suffering from\nexposure bias. To this end, we propose a novel method that can Utilize False\nNegative samples for sequential Recommendation (UFNRec) to improve model\nperformance. We first devise a simple strategy to extract false negative\nsamples and then transfer these samples to positive samples in the following\ntraining process. Furthermore, we construct a teacher model to provide soft\nlabels for false negative samples and design a consistency loss to regularize\nthe predictions of these samples from the student model and the teacher model.\nTo the best of our knowledge, this is the first work to utilize false negative\nsamples instead of simply removing them for the sequential recommendation.\nExperiments on three benchmark public datasets are conducted using three widely\napplied SOTA models. The experiment results demonstrate that our proposed\nUFNRec can effectively draw information from false negative samples and further\nimprove the performance of SOTA models. The code is available at\nhttps://github.com/UFNRec-code/UFNRec.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.05777,regular,pre_llm,2022,8,"{'ai_likelihood': 5.9604644775390625e-06, 'text': 'Dbias: Detecting biases and ensuring Fairness in news articles\n\n  Because of the increasing use of data-centric systems and algorithms in\nmachine learning, the topic of fairness is receiving a lot of attention in the\nacademic and broader literature. This paper introduces Dbias\n(https://pypi.org/project/Dbias/), an open-source Python package for ensuring\nfairness in news articles. Dbias can take any text to determine if it is\nbiased. Then, it detects biased words in the text, masks them, and suggests a\nset of sentences with new words that are bias-free or at least less biased. We\nconduct extensive experiments to assess the performance of Dbias. To see how\nwell our approach works, we compare it to the existing fairness models. We also\ntest the individual components of Dbias to see how effective they are. The\nexperimental results show that Dbias outperforms all the baselines in terms of\naccuracy and fairness. We make this package (Dbias) as publicly available for\nthe developers and practitioners to mitigate biases in textual data (such as\nnews articles), as well as to encourage extension of this work.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.0134,regular,pre_llm,2022,8,"{'ai_likelihood': 1.205338372124566e-05, 'text': ""Parameterizing Kterm Hashing\n\n  Kterm Hashing provides an innovative approach to novelty detection on massive\ndata streams. Previous research focused on maximizing the efficiency of Kterm\nHashing and succeeded in scaling First Story Detection to Twitter-size data\nstream without sacrificing detection accuracy. In this paper, we focus on\nimproving the effectiveness of Kterm Hashing. Traditionally, all kterms are\nconsidered as equally important when calculating a document's degree of novelty\nwith respect to the past. We believe that certain kterms are more important\nthan others and hypothesize that uniform kterm weights are sub-optimal for\ndetermining novelty in data streams. To validate our hypothesis, we\nparameterize Kterm Hashing by assigning weights to kterms based on their\ncharacteristics. Our experiments apply Kterm Hashing in a First Story Detection\nsetting and reveal that parameterized Kterm Hashing can surpass\nstate-of-the-art detection accuracy and significantly outperform the uniformly\nweighted approach.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.07807,review,pre_llm,2022,8,"{'ai_likelihood': 5.4637591044108076e-06, 'text': 'A User-Centered Investigation of Personal Music Tours\n\n  Streaming services use recommender systems to surface the right music to\nusers. Playlists are a popular way to present music in a list-like fashion, ie\nas a plain list of songs. An alternative are tours, where the songs alternate\nsegues, which explain the connections between consecutive songs. Tours address\nthe user need of seeking background information about songs, and are found to\nbe superior to playlists, given the right user context. In this work, we\nprovide, for the first time, a user-centered evaluation of two tour-generation\nalgorithms (Greedy and Optimal) using semi-structured interviews. We assess the\nalgorithms, we discuss attributes of the tours that the algorithms produce, we\nidentify which attributes are desirable and which are not, and we enumerate\nseveral possible improvements to the algorithms, along with practical\nsuggestions on how to implement the improvements. Our main findings are that\nGreedy generates more likeable tours than Optimal, and that three important\nattributes of tours are segue diversity, song arrangement and song familiarity.\nMore generally, we provide insights into how to present music to users, which\ncould inform the design of user-centered recommender systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.13453,review,pre_llm,2022,8,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Understanding Diversity in Session-Based Recommendation\n\n  Current session-based recommender systems (SBRSs) mainly focus on maximizing\nrecommendation accuracy, while few studies have been devoted to improve\ndiversity beyond accuracy. Meanwhile, it is unclear how the accuracy-oriented\nSBRSs perform in terms of diversity. Besides, the asserted ""trade-off""\nrelationship between accuracy and diversity has been increasingly questioned in\nthe literature. Towards the aforementioned issues, we conduct a holistic study\nto particularly examine the recommendation performance of representative SBRSs\nw.r.t. both accuracy and diversity, striving for better understanding the\ndiversity-related issues for SBRSs and providing guidance on designing\ndiversified SBRSs. Particularly, for a fair and thorough comparison, we\ndeliberately select state-of-the-art non-neural, deep neural, and diversified\nSBRSs, by covering more scenarios with appropriate experimental setups, e.g.,\nrepresentative datasets, evaluation metrics, and hyper-parameter optimization\ntechnique. Our empirical results unveil that: 1) non-diversified methods can\nalso obtain satisfying performance on diversity, which might even surpass\ndiversified ones; and 2) the relationship between accuracy and diversity is\nquite complex. Besides the ""trade-off"" relationship, they might be positively\ncorrelated with each other, that is, having a same-trend (win-win or lose-lose)\nrelationship, which varies across different methods and datasets. Additionally,\nwe further identify three possible influential factors on diversity in SBRSs\n(i.e., granularity of item categorization, session diversity of datasets, and\nlength of recommendation lists).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.07262,regular,pre_llm,2022,8,"{'ai_likelihood': 2.1523899502224394e-06, 'text': ""Retrieval-efficiency trade-off of Unsupervised Keyword Extraction\n\n  Efficiently identifying keyphrases that represent a given document is a\nchallenging task. In the last years, plethora of keyword detection approaches\nwere proposed. These approaches can be based on statistical (frequency-based)\nproperties of e.g., tokens, specialized neural language models, or a\ngraph-based structure derived from a given document. The graph-based methods\ncan be computationally amongst the most efficient ones, while maintaining the\nretrieval performance. One of the main properties, common to graph-based\nmethods, is their immediate conversion of token space into graphs, followed by\nsubsequent processing. In this paper, we explore a novel unsupervised approach\nwhich merges parts of a document in sequential form, prior to construction of\nthe token graph. Further, by leveraging personalized PageRank, which considers\nfrequencies of such sub-phrases alongside token lengths during node ranking, we\ndemonstrate state-of-the-art retrieval capabilities while being up to two\norders of magnitude faster than current state-of-the-art unsupervised detectors\nsuch as YAKE and MultiPartiteRank. The proposed method's scalability was also\ndemonstrated by computing keyphrases for a biomedical corpus comprised of 14\nmillion documents in less than a minute.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.02438,regular,pre_llm,2022,8,"{'ai_likelihood': 5.033281114366319e-06, 'text': ""Simplifying Sparse Expert Recommendation by Revisiting Graph Diffusion\n\n  Community Question Answering (CQA) websites have become valuable knowledge\nrepositories where individuals exchange information by asking and answering\nquestions. With an ever-increasing number of questions and high migration of\nusers in and out of communities, a key challenge is to design effective\nstrategies for recommending experts for new questions. In this paper, we\npropose a simple graph-diffusion expert recommendation model for CQA, that can\noutperform state-of-the art deep learning representatives and collaborative\nmodels. Our proposed method learns users' expertise in the context of both\nsemantic and temporal information to capture their changing interest and\nactivity levels with time. Experiments on five real-world datasets from the\nStack Exchange network demonstrate that our approach outperforms competitive\nbaseline methods. Further, experiments on cold-start users (users with a\nlimited historical record) show our model achieves an average of ~ 30%\nperformance gain compared to the best baseline method.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.12397,review,pre_llm,2022,8,"{'ai_likelihood': 2.8477774726019966e-06, 'text': 'Causal Inference in Recommender Systems: A Survey and Future Directions\n\n  Recommender systems have become crucial in information filtering nowadays.\nExisting recommender systems extract user preferences based on the correlation\nin data, such as behavioral correlation in collaborative filtering,\nfeature-feature, or feature-behavior correlation in click-through rate\nprediction. However, unfortunately, the real world is driven by causality, not\njust correlation, and correlation does not imply causation. For instance,\nrecommender systems might recommend a battery charger to a user after buying a\nphone, where the latter can serve as the cause of the former; such a causal\nrelation cannot be reversed. Recently, to address this, researchers in\nrecommender systems have begun utilizing causal inference to extract causality,\nthereby enhancing the recommender system. In this survey, we offer a\ncomprehensive review of the literature on causal inference-based\nrecommendation. Initially, we introduce the fundamental concepts of both\nrecommender system and causal inference as the foundation for subsequent\ncontent. We then highlight the typical issues faced by non-causality\nrecommender system. Following that, we thoroughly review the existing work on\ncausal inference-based recommender systems, based on a taxonomy of three-aspect\nchallenges that causal inference can address. Finally, we discuss the open\nproblems in this critical research area and suggest important potential future\nworks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.04057,regular,pre_llm,2022,8,"{'ai_likelihood': 2.6490953233506944e-06, 'text': 'Relevance Judgment Convergence Degree -- A Measure of Inconsistency\n  among Assessors for Information Retrieval\n\n  Relevance judgment of human assessors is inherently subjective and dynamic\nwhen evaluation datasets are created for Information Retrieval (IR) systems.\nHowever, a small group of experts\' relevance judgment results are usually taken\nas ground truth to ""objectively"" evaluate the performance of the IR systems.\nRecent trends intend to employ a group of judges, such as outsourcing, to\nalleviate the potentially biased judgment results stemmed from using only a\nsingle expert\'s judgment. Nevertheless, different judges may have different\nopinions and may not agree with each other, and the inconsistency in human\nrelevance judgment may affect the IR system evaluation results. In this\nresearch, we introduce a Relevance Judgment Convergence Degree (RJCD) to\nmeasure the quality of queries in the evaluation datasets. Experimental results\nreveal a strong correlation coefficient between the proposed RJCD score and the\nperformance differences between the two IR systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.03895,regular,pre_llm,2022,8,"{'ai_likelihood': 9.404288397894965e-06, 'text': 'Contrastive Learning with Bidirectional Transformers for Sequential\n  Recommendation\n\n  Contrastive learning with Transformer-based sequence encoder has gained\npredominance for sequential recommendation. It maximizes the agreements between\npaired sequence augmentations that share similar semantics. However, existing\ncontrastive learning approaches in sequential recommendation mainly center upon\nleft-to-right unidirectional Transformers as base encoders, which are\nsuboptimal for sequential recommendation because user behaviors may not be a\nrigid left-to-right sequence. To tackle that, we propose a novel framework\nnamed \\textbf{C}ontrastive learning with \\textbf{Bi}directional\n\\textbf{T}ransformers for sequential recommendation (\\textbf{CBiT}).\nSpecifically, we first apply the slide window technique for long user sequences\nin bidirectional Transformers, which allows for a more fine-grained division of\nuser sequences. Then we combine the cloze task mask and the dropout mask to\ngenerate high-quality positive samples and perform multi-pair contrastive\nlearning, which demonstrates better performance and adaptability compared with\nthe normal one-pair contrastive learning. Moreover, we introduce a novel\ndynamic loss reweighting strategy to balance between the cloze task loss and\nthe contrastive loss. Experiment results on three public benchmark datasets\nshow that our model outperforms state-of-the-art models for sequential\nrecommendation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.07201,regular,pre_llm,2022,8,"{'ai_likelihood': 1.5795230865478516e-05, 'text': ""AMinerGNN: Heterogeneous Graph Neural Network for Paper Click-through\n  Rate Prediction with Fusion Query\n\n  Paper recommendation with user-generated keyword is to suggest papers that\nsimultaneously meet user's interests and are relevant to the input keyword.\nThis is a recommendation task with two queries, a.k.a. user ID and keyword.\nHowever, existing methods focus on recommendation according to one query,\na.k.a. user ID, and are not applicable to solving this problem. In this paper,\nwe propose a novel click-through rate (CTR) prediction model with heterogeneous\ngraph neural network, called AMinerGNN, to recommend papers with two queries.\nSpecifically, AMinerGNN constructs a heterogeneous graph to project user,\npaper, and keyword into the same embedding space by graph representation\nlearning. To process two queries, a novel query attentive fusion layer is\ndesigned to recognize their importances dynamically and then fuse them as one\nquery to build a unified and end-to-end recommender system. Experimental\nresults on our proposed dataset and online A/B tests prove the superiority of\nAMinerGNN.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.13133,regular,pre_llm,2022,9,"{'ai_likelihood': 1.6159481472439238e-05, 'text': ""Privacy-Preserving Synthetic Data Generation for Recommendation Systems\n\n  Recommendation systems make predictions chiefly based on users' historical\ninteraction data (e.g., items previously clicked or purchased). There is a risk\nof privacy leakage when collecting the users' behavior data for building the\nrecommendation model. However, existing privacy-preserving solutions are\ndesigned for tackling the privacy issue only during the model training and\nresults collection phases. The problem of privacy leakage still exists when\ndirectly sharing the private user interaction data with organizations or\nreleasing them to the public. To address this problem, in this paper, we\npresent a User Privacy Controllable Synthetic Data Generation model (short for\nUPC-SDG), which generates synthetic interaction data for users based on their\nprivacy preferences. The generation model aims to provide certain privacy\nguarantees while maximizing the utility of the generated synthetic data at both\ndata level and item level. Specifically, at the data level, we design a\nselection module that selects those items that contribute less to a user's\npreferences from the user's interaction data. At the item level, a synthetic\ndata generation module is proposed to generate a synthetic item corresponding\nto the selected item based on the user's preferences. Furthermore, we also\npresent a privacy-utility trade-off strategy to balance the privacy and utility\nof the synthetic data. Extensive experiments and ablation studies have been\nconducted on three publicly accessible datasets to justify our method,\ndemonstrating its effectiveness in generating synthetic data under users'\nprivacy preferences.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.09427,regular,pre_llm,2022,9,"{'ai_likelihood': 2.715322706434462e-06, 'text': 'Spatiotemporal-Enhanced Network for Click-Through Rate Prediction in\n  Location-based Services\n\n  In Location-Based Services(LBS), user behavior naturally has a strong\ndependence on the spatiotemporal information, i.e., in different geographical\nlocations and at different times, user click behavior will change\nsignificantly. Appropriate spatiotemporal enhancement modeling of user click\nbehavior and large-scale sparse attributes is key to building an LBS model.\nAlthough most of existing methods have been proved to be effective, they are\ndifficult to apply to takeaway scenarios due to insufficient modeling of\nspatiotemporal information. In this paper, we address this challenge by seeking\nto explicitly model the timing and locations of interactions and proposing a\nSpatiotemporal-Enhanced Network, namely StEN. In particular, StEN applies a\nSpatiotemporal Profile Activation module to capture common spatiotemporal\npreference through attribute features. A Spatiotemporal Preference Activation\nis further applied to model the personalized spatiotemporal preference embodied\nby behaviors in detail. Moreover, a Spatiotemporal-aware Target Attention\nmechanism is adopted to generate different parameters for target attention at\ndifferent locations and times, thereby improving the personalized\nspatiotemporal awareness of the model.Comprehensive experiments are conducted\non three large-scale industrial datasets, and the results demonstrate the\nstate-of-the-art performance of our methods. In addition, we have also released\nan industrial dataset for takeaway industry to make up for the lack of public\ndatasets in this community.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.11461,regular,pre_llm,2022,9,"{'ai_likelihood': 4.2054388258192275e-06, 'text': ""Spatio-Temporal Contrastive Learning Enhanced GNNs for Session-based\n  Recommendation\n\n  Session-based recommendation (SBR) systems aim to utilize the user's\nshort-term behavior sequence to predict the next item without the detailed user\nprofile. Most recent works try to model the user preference by treating the\nsessions as between-item transition graphs and utilize various graph neural\nnetworks (GNNs) to encode the representations of pair-wise relations among\nitems and their neighbors. Some of the existing GNN-based models mainly focus\non aggregating information from the view of spatial graph structure, which\nignores the temporal relations within neighbors of an item during message\npassing and the information loss results in a sub-optimal problem. Other works\nembrace this challenge by incorporating additional temporal information but\nlack sufficient interaction between the spatial and temporal patterns. To\naddress this issue, inspired by the uniformity and alignment properties of\ncontrastive learning techniques, we propose a novel framework called\nSession-based Recommendation with Spatio-Temporal Contrastive Learning Enhanced\nGNNs (RESTC). The idea is to supplement the GNN-based main supervised\nrecommendation task with the temporal representation via an auxiliary\ncross-view contrastive learning mechanism. Furthermore, a novel global\ncollaborative filtering graph (CFG) embedding is leveraged to enhance the\nspatial view in the main task. Extensive experiments demonstrate the\nsignificant performance of RESTC compared with the state-of-the-art baselines\ne.g., with an improvement as much as 27.08% gain on HR@20 and 20.10% gain on\nMRR@20.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.13422,regular,pre_llm,2022,9,"{'ai_likelihood': 8.44399134318034e-06, 'text': 'Efficient On-Device Session-Based Recommendation\n\n  On-device session-based recommendation systems have been achieving increasing\nattention on account of the low energy/resource consumption and privacy\nprotection while providing promising recommendation performance. To fit the\npowerful neural session-based recommendation models in resource-constrained\nmobile devices, tensor-train decomposition and its variants have been widely\napplied to reduce memory footprint by decomposing the embedding table into\nsmaller tensors, showing great potential in compressing recommendation models.\nHowever, these model compression techniques significantly increase the local\ninference time due to the complex process of generating index lists and a\nseries of tensor multiplications to form item embeddings, and the resultant\non-device recommender fails to provide real-time response and recommendation.\nTo improve the online recommendation efficiency, we propose to learn\ncompositional encoding-based compact item representations. Specifically, each\nitem is represented by a compositional code that consists of several codewords,\nand we learn embedding vectors to represent each codeword instead of each item.\nThen the composition of the codeword embedding vectors from different embedding\nmatrices (i.e., codebooks) forms the item embedding. Since the size of\ncodebooks can be extremely small, the recommender model is thus able to fit in\nresource-constrained devices and meanwhile can save the codebooks for fast\nlocal inference.Besides, to prevent the loss of model capacity caused by\ncompression, we propose a bidirectional self-supervised knowledge distillation\nframework. Extensive experimental results on two benchmark datasets demonstrate\nthat compared with existing methods, the proposed on-device recommender not\nonly achieves an 8x inference speedup with a large compression ratio but also\nshows superior recommendation performance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.03913,review,pre_llm,2022,9,"{'ai_likelihood': 2.0199351840549047e-06, 'text': ""Data Management Challenges for Internet-scale 3D Search Engines\n\n  This paper describes the most significant data-related challenges involved in\nbuilding internet-scale 3D search engines. The discussion centers on the most\npressing data management issues in this domain, including model acquisition,\nsupport for multiple file formats, asset versioning, data integrity errors, the\ndata lifecycle, intellectual property, and the legality of web crawling. The\npaper also discusses numerous issues that fall under the rubric of trustworthy\ncomputing, including privacy, security, inappropriate content, and\ncopying/remixing of assets. The goal of the paper is to provide an overview of\nthese general issues, illustrated by empirical data drawn from the internet's\nlargest operational search engine. While numerous works have been published on\n3D information retrieval, this paper is the first to discuss the real-world\nchallenges that arise in building practical search engines at scale.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.08446,review,pre_llm,2022,9,"{'ai_likelihood': 1.5530321333143445e-05, 'text': ""Mutual Harmony: Sequential Recommendation with Dual Contrastive Network\n\n  With the outbreak of today's streaming data, the sequential recommendation is\na promising solution to achieve time-aware personalized modeling. It aims to\ninfer the next interacted item of a given user based on the historical item\nsequence. Some recent works tend to improve the sequential recommendation via\nrandom masking on the historical item so as to generate self-supervised\nsignals. But such approaches will indeed result in sparser item sequence and\nunreliable signals. Besides, the existing sequential recommendation models are\nonly user-centric, i.e., based on the historical items by chronological order\nto predict the probability of candidate items, which ignores whether the items\nfrom a provider can be successfully recommended. Such user-centric\nrecommendation will make it impossible for the provider to expose their new\nitems, failing to consider the accordant interactions between user and item\ndimensions. In this paper, we propose a novel Dual Contrastive Network (DCN) to\nachieve mutual harmony between user and item provider, generating ground-truth\nself-supervised signals for sequential recommendation by auxiliary\nuser-sequence from an item-centric dimension. Specifically, we propose dual\nrepresentation contrastive learning to refine the representation learning by\nminimizing the Euclidean distance between the representations of a given\nuser/item and historical items/users of them. Before the second contrastive\nlearning module, we perform the next user prediction to capture the trends of\nitems preferred by certain types of users and provide personalized exploration\nopportunities for item providers. Finally, we further propose dual interest\ncontrastive learning to self-supervise the dynamic interest from the next\nitem/user prediction and static interest of matching probability. Experiments\non four benchmark datasets verify the effectiveness of our proposed method.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.01347,regular,pre_llm,2022,9,"{'ai_likelihood': 2.682209014892578e-06, 'text': ""Explanation Guided Contrastive Learning for Sequential Recommendation\n\n  Recently, contrastive learning has been applied to the sequential\nrecommendation task to address data sparsity caused by users with few item\ninteractions and items with few user adoptions. Nevertheless, the existing\ncontrastive learning-based methods fail to ensure that the positive (or\nnegative) sequence obtained by some random augmentation (or sequence sampling)\non a given anchor user sequence remains to be semantically similar (or\ndifferent). When the positive and negative sequences turn out to be false\npositive and false negative respectively, it may lead to degraded\nrecommendation performance. In this work, we address the above problem by\nproposing Explanation Guided Augmentations (EGA) and Explanation Guided\nContrastive Learning for Sequential Recommendation (EC4SRec) model framework.\nThe key idea behind EGA is to utilize explanation method(s) to determine items'\nimportance in a user sequence and derive the positive and negative sequences\naccordingly. EC4SRec then combines both self-supervised and supervised\ncontrastive learning over the positive and negative sequences generated by EGA\noperations to improve sequence representation learning for more accurate\nrecommendation results. Extensive experiments on four real-world benchmark\ndatasets demonstrate that EC4SRec outperforms the state-of-the-art sequential\nrecommendation methods and two recent contrastive learning-based sequential\nrecommendation methods, CL4SRec and DuoRec. Our experiments also show that\nEC4SRec can be easily adapted for different sequence encoder backbones (e.g.,\nGRU4Rec and Caser), and improve their recommendation performance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.07997,regular,pre_llm,2022,9,"{'ai_likelihood': 3.112687004937066e-06, 'text': ""Recursive Attentive Methods with Reused Item Representations for\n  Sequential Recommendation\n\n  Sequential recommendation aims to recommend the next item of users' interest\nbased on their historical interactions. Recently, the self-attention mechanism\nhas been adapted for sequential recommendation, and demonstrated\nstate-of-the-art performance. However, in this manuscript, we show that the\nself-attention-based sequential recommendation methods could suffer from the\nlocalization-deficit issue. As a consequence, in these methods, over the first\nfew blocks, the item representations may quickly diverge from their original\nrepresentations, and thus, impairs the learning in the following blocks. To\nmitigate this issue, in this manuscript, we develop a recursive attentive\nmethod with reused item representations (RAM) for sequential recommendation. We\ncompare RAM with five state-of-the-art baseline methods on six public benchmark\ndatasets. Our experimental results demonstrate that RAM significantly\noutperforms the baseline methods on benchmark datasets, with an improvement of\nas much as 11.3%. Our stability analysis shows that RAM could enable deeper and\nwider models for better performance. Our run-time performance comparison\nsignifies that RAM could also be more efficient on benchmark datasets.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.00179,regular,pre_llm,2022,9,"{'ai_likelihood': 5.39753172132704e-06, 'text': 'Universal Vision-Language Dense Retrieval: Learning A Unified\n  Representation Space for Multi-Modal Retrieval\n\n  This paper presents Universal Vision-Language Dense Retrieval (UniVL-DR),\nwhich builds a unified model for multi-modal retrieval. UniVL-DR encodes\nqueries and multi-modality resources in an embedding space for searching\ncandidates from different modalities. To learn a unified embedding space for\nmulti-modal retrieval, UniVL-DR proposes two techniques: 1) Universal embedding\noptimization strategy, which contrastively optimizes the embedding space using\nthe modality-balanced hard negatives; 2) Image verbalization method, which\nbridges the modality gap between images and texts in the raw data space.\nUniVL-DR achieves the state-of-the-art on the multi-modal open-domain question\nanswering benchmark, WebQA, and outperforms all retrieval models on the two\nsubtasks, text-text retrieval and text-image retrieval. It demonstrates that\nuniversal multi-modal search is feasible to replace the divide-and-conquer\npipeline with a united model and also benefits single/cross modality tasks. All\nsource codes of this work are available at\nhttps://github.com/OpenMatch/UniVL-DR.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.1157,regular,pre_llm,2022,9,"{'ai_likelihood': 6.556510925292969e-06, 'text': 'A Unified Generative Framework based on Prompt Learning for Various\n  Information Extraction Tasks\n\n  Prompt learning is an effective paradigm that bridges gaps between the\npre-training tasks and the corresponding downstream applications. Approaches\nbased on this paradigm have achieved great transcendent results in various\napplications. However, it still needs to be answered how to design a unified\nframework based on the prompt learning paradigm for various information\nextraction tasks. In this paper, we propose a novel composable prompt-based\ngenerative framework, which could be applied to a wide range of tasks in the\nfield of Information Extraction. Specifically, we reformulate information\nextraction tasks into the form of filling slots in pre-designed type-specific\nprompts, which consist of one or multiple sub-prompts. A strategy of\nconstructing composable prompts is proposed to enhance the generalization\nability to extract events in data-scarce scenarios. Furthermore, to fit this\nframework, we transform Relation Extraction into the task of determining\nsemantic consistency in prompts. The experimental results demonstrate that our\napproach surpasses compared baselines on real-world datasets in data-abundant\nand data-scarce scenarios. Further analysis of the proposed framework is\npresented, as well as numerical experiments conducted to investigate impact\nfactors of performance on various tasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.00325,regular,pre_llm,2022,9,"{'ai_likelihood': 5.198849572075738e-06, 'text': 'MTS Kion Implicit Contextualised Sequential Dataset for Movie\n  Recommendation\n\n  We present a new movie and TV show recommendation dataset collected from the\nreal users of MTS Kion video-on-demand platform. In contrast to other popular\nmovie recommendation datasets, such as MovieLens or Netflix, our dataset is\nbased on the implicit interactions registered at the watching time, rather than\non explicit ratings. We also provide rich contextual and side information\nincluding interactions characteristics (such as temporal information, watch\nduration and watch percentage), user demographics and rich movies\nmeta-information. In addition, we describe the MTS Kion Challenge - an online\nrecommender systems challenge that was based on this dataset - and provide an\noverview of the best performing solutions of the winners. We keep the\ncompetition sandbox open, so the researchers are welcome to try their own\nrecommendation algorithms and measure the quality on the private part of the\ndataset.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.12474,regular,pre_llm,2022,9,"{'ai_likelihood': 5.298190646701389e-07, 'text': ""Legal Case Document Similarity: You Need Both Network and Text\n\n  Estimating the similarity between two legal case documents is an important\nand challenging problem, having various downstream applications such as\nprior-case retrieval and citation recommendation. There are two broad\napproaches for the task -- citation network-based and text-based. Prior\ncitation network-based approaches consider citations only to prior-cases (also\ncalled precedents) (PCNet). This approach misses important signals inherent in\nStatutes (written laws of a jurisdiction). In this work, we propose Hier-SPCNet\nthat augments PCNet with a heterogeneous network of Statutes. We incorporate\ndomain knowledge for legal document similarity into Hier-SPCNet, thereby\nobtaining state-of-the-art results for network-based legal document similarity.\nBoth textual and network similarity provide important signals for legal case\nsimilarity; but till now, only trivial attempts have been made to unify the two\nsignals. In this work, we apply several methods for combining textual and\nnetwork information for estimating legal case similarity. We perform extensive\nexperiments over legal case documents from the Indian judiciary, where the gold\nstandard similarity between document-pairs is judged by law experts from two\nreputed Law institutes in India. Our experiments establish that our proposed\nnetwork-based methods significantly improve the correlation with domain\nexperts' opinion when compared to the existing methods for network-based legal\ndocument similarity. Our best-performing combination method (that combines\nnetwork-based and text-based similarity) improves the correlation with domain\nexperts' opinion by 11.8% over the best text-based method and 20.6\\% over the\nbest network-based method. We also establish that our best-performing method\ncan be used to recommend / retrieve citable and similar cases for a source\n(query) case, which are well appreciated by legal experts.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.11471,regular,pre_llm,2022,9,"{'ai_likelihood': 6.755193074544271e-06, 'text': ""Modeling and Leveraging Prerequisite Context in Recommendation\n\n  Prerequisites can play a crucial role in users' decision-making yet\nrecommendation systems have not fully utilized such contextual background\nknowledge. Traditional recommendation systems (RS) mostly enrich user-item\ninteractions where the context consists of static user profiles and item\ndescriptions, ignoring the contextual logic and constraints that underlie them.\nFor example, an RS may recommend an item on the condition that the user has\ninteracted with another item as its prerequisite. Modeling prerequisite context\nfrom conceptual side information can overcome this weakness. We propose\nPrerequisite Driven Recommendation (PDR), a generic context-aware framework\nwhere prerequisite context is explicitly modeled to facilitate recommendation.\nWe first design a Prerequisite Knowledge Linking (PKL) algorithm, to curate\ndatasets facilitating PDR research. Employing it, we build a 75k+ high-quality\nprerequisite concept dataset which spans three domains. We then contribute\nPDRS, a neural instantiation of PDR. By jointly optimizing both the\nprerequisite learning and recommendation tasks through multi-layer perceptrons,\nwe find PDRS consistently outperforms baseline models in all three domains, by\nan average margin of 7.41%. Importantly, PDRS performs especially well in\ncold-start scenarios with improvements of up to 17.65%.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.05072,regular,pre_llm,2022,9,"{'ai_likelihood': 3.410710228814019e-06, 'text': 'Hard Negatives or False Negatives: Correcting Pooling Bias in Training\n  Neural Ranking Models\n\n  Neural ranking models (NRMs) have become one of the most important techniques\nin information retrieval (IR). Due to the limitation of relevance labels, the\ntraining of NRMs heavily relies on negative sampling over unlabeled data. In\ngeneral machine learning scenarios, it has shown that training with hard\nnegatives (i.e., samples that are close to positives) could lead to better\nperformance. Surprisingly, we find opposite results from our empirical studies\nin IR. When sampling top-ranked results (excluding the labeled positives) as\nnegatives from a stronger retriever, the performance of the learned NRM becomes\neven worse. Based on our investigation, the superficial reason is that there\nare more false negatives (i.e., unlabeled positives) in the top-ranked results\nwith a stronger retriever, which may hurt the training process; The root is the\nexistence of pooling bias in the dataset constructing process, where annotators\nonly judge and label very few samples selected by some basic retrievers.\nTherefore, in principle, we can formulate the false negative issue in training\nNRMs as learning from labeled datasets with pooling bias. To solve this\nproblem, we propose a novel Coupled Estimation Technique (CET) that learns both\na relevance model and a selection model simultaneously to correct the pooling\nbias for training NRMs. Empirical results on three retrieval benchmarks show\nthat NRMs trained with our technique can achieve significant gains on ranking\neffectiveness against other baseline strategies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.08228,regular,pre_llm,2022,9,"{'ai_likelihood': 9.602970547146267e-07, 'text': ""Intrinsically Motivated Reinforcement Learning based Recommendation with\n  Counterfactual Data Augmentation\n\n  Deep reinforcement learning (DRL) has been proven its efficiency in capturing\nusers' dynamic interests in recent literature. However, training a DRL agent is\nchallenging, because of the sparse environment in recommender systems (RS), DRL\nagents could spend times either exploring informative user-item interaction\ntrajectories or using existing trajectories for policy learning. It is also\nknown as the exploration and exploitation trade-off which affects the\nrecommendation performance significantly when the environment is sparse. It is\nmore challenging to balance the exploration and exploitation in DRL RS where RS\nagent need to deeply explore the informative trajectories and exploit them\nefficiently in the context of recommender systems. As a step to address this\nissue, We design a novel intrinsically ,otivated reinforcement learning method\nto increase the capability of exploring informative interaction trajectories in\nthe sparse environment, which are further enriched via a counterfactual\naugmentation strategy for more efficient exploitation. The extensive\nexperiments on six offline datasets and three online simulation platforms\ndemonstrate the superiority of our model to a set of existing state-of-the-art\nmethods.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.13973,regular,pre_llm,2022,9,"{'ai_likelihood': 3.410710228814019e-06, 'text': 'Knowledge-aware Neural Networks with Personalized Feature Referencing\n  for Cold-start Recommendation\n\n  Incorporating knowledge graphs (KGs) as side information in recommendation\nhas recently attracted considerable attention. Despite the success in general\nrecommendation scenarios, prior methods may fall short of performance\nsatisfaction for the cold-start problem in which users are associated with very\nlimited interactive information. Since the conventional methods rely on\nexploring the interaction topology, they may however fail to capture sufficient\ninformation in cold-start scenarios. To mitigate the problem, we propose a\nnovel Knowledge-aware Neural Networks with Personalized Feature Referencing\nMechanism, namely KPER. Different from most prior methods which simply enrich\nthe targets\' semantics from KGs, e.g., product attributes, KPER utilizes the\nKGs as a ""semantic bridge"" to extract feature references for cold-start users\nor items. Specifically, given cold-start targets, KPER first probes\nsemantically relevant but not necessarily structurally close users or items as\nadaptive seeds for referencing features. Then a Gated Information Aggregation\nmodule is introduced to learn the combinatorial latent features for cold-start\nusers and items. Our extensive experiments over four real-world datasets show\nthat, KPER consistently outperforms all competing methods in cold-start\nscenarios, whilst maintaining superiority in general scenarios without\ncompromising overall performance, e.g., by achieving 0.81%-16.08% and\n1.01%-14.49% performance improvement across all datasets in Top-10\nrecommendation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.02544,regular,pre_llm,2022,9,"{'ai_likelihood': 4.238552517361111e-06, 'text': 'XSimGCL: Towards Extremely Simple Graph Contrastive Learning for\n  Recommendation\n\n  Contrastive learning (CL) has recently been demonstrated critical in\nimproving recommendation performance. The underlying principle of CL-based\nrecommendation models is to ensure the consistency between representations\nderived from different graph augmentations of the user-item bipartite graph.\nThis self-supervised approach allows for the extraction of general features\nfrom raw data, thereby mitigating the issue of data sparsity. Despite the\neffectiveness of this paradigm, the factors contributing to its performance\ngains have yet to be fully understood. This paper provides novel insights into\nthe impact of CL on recommendation. Our findings indicate that CL enables the\nmodel to learn more evenly distributed user and item representations, which\nalleviates the prevalent popularity bias and promoting long-tail items. Our\nanalysis also suggests that the graph augmentations, previously considered\nessential, are relatively unreliable and of limited significance in CL-based\nrecommendation. Based on these findings, we put forward an eXtremely Simple\nGraph Contrastive Learning method (XSimGCL) for recommendation, which discards\nthe ineffective graph augmentations and instead employs a simple yet effective\nnoise-based embedding augmentation to generate views for CL. A comprehensive\nexperimental study on four large and highly sparse benchmark datasets\ndemonstrates that, though the proposed method is extremely simple, it can\nsmoothly adjust the uniformity of learned representations and outperforms its\ngraph augmentation-based counterparts by a large margin in both recommendation\naccuracy and training efficiency. The code and used datasets are released at\nhttps://github.com/Coder-Yu/SELFRec.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.09149,regular,pre_llm,2022,9,"{'ai_likelihood': 1.0761949751112197e-05, 'text': 'Duration modeling with semi-Markov Conditional Random Fields for\n  keyphrase extraction\n\n  Existing methods for keyphrase extraction need preprocessing to generate\ncandidate phrase or post-processing to transform keyword into keyphrase. In\nthis paper, we propose a novel approach called duration modeling with\nsemi-Markov Conditional Random Fields (DM-SMCRFs) for keyphrase extraction.\nFirst of all, based on the property of semi-Markov chain, DM-SMCRFs can encode\nsegment-level features and sequentially classify the phrase in the sentence as\nkeyphrase or non-keyphrase. Second, by assuming the independence between state\ntransition and state duration, DM-SMCRFs model the distribution of duration\n(length) of keyphrases to further explore state duration information, which can\nhelp identify the size of keyphrase. Based on the convexity of parametric\nduration feature derived from duration distribution, a constrained Viterbi\nalgorithm is derived to improve the performance of decoding in DM-SMCRFs. We\nthoroughly evaluate the performance of DM-SMCRFs on the datasets from various\ndomains. The experimental results demonstrate the effectiveness of proposed\nmodel.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.05917,regular,pre_llm,2022,9,"{'ai_likelihood': 5.993578169080946e-06, 'text': 'SpaDE: Improving Sparse Representations using a Dual Document Encoder\n  for First-stage Retrieval\n\n  Sparse document representations have been widely used to retrieve relevant\ndocuments via exact lexical matching. Owing to the pre-computed inverted index,\nit supports fast ad-hoc search but incurs the vocabulary mismatch problem.\nAlthough recent neural ranking models using pre-trained language models can\naddress this problem, they usually require expensive query inference costs,\nimplying the trade-off between effectiveness and efficiency. Tackling the\ntrade-off, we propose a novel uni-encoder ranking model, Sparse retriever using\na Dual document Encoder (SpaDE), learning document representation via the dual\nencoder. Each encoder plays a central role in (i) adjusting the importance of\nterms to improve lexical matching and (ii) expanding additional terms to\nsupport semantic matching. Furthermore, our co-training strategy trains the\ndual encoder effectively and avoids unnecessary intervention in training each\nother. Experimental results on several benchmarks show that SpaDE outperforms\nexisting uni-encoder ranking models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.05112,review,pre_llm,2022,9,"{'ai_likelihood': 6.291601392957899e-07, 'text': ""A challenge-based survey of e-recruitment recommendation systems\n\n  E-recruitment recommendation systems recommend jobs to job seekers and job\nseekers to recruiters. The recommendations are generated based on the\nsuitability of the job seekers for the positions as well as the job seekers'\nand the recruiters' preferences. Therefore, e-recruitment recommendation\nsystems could greatly impact job seekers' careers. Moreover, by affecting the\nhiring processes of the companies, e-recruitment recommendation systems play an\nimportant role in shaping the companies' competitive edge in the market. Hence,\nthe domain of e-recruitment recommendation deserves specific attention.\nExisting surveys on this topic tend to discuss past studies from the\nalgorithmic perspective, e.g., by categorizing them into collaborative\nfiltering, content based, and hybrid methods. This survey, instead, takes a\ncomplementary, challenge-based approach, which we believe might be more\npractical to developers facing a concrete e-recruitment design task with a\nspecific set of challenges, as well as to researchers looking for impactful\nresearch projects in this domain. We first identify the main challenges in the\ne-recruitment recommendation research. Next, we discuss how those challenges\nhave been studied in the literature. Finally, we provide future research\ndirections that we consider promising in the e-recruitment recommendation\ndomain.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.04513,regular,pre_llm,2022,10,"{'ai_likelihood': 1.6225708855523005e-06, 'text': ""Improving Continual Relation Extraction through Prototypical Contrastive\n  Learning\n\n  Continual relation extraction (CRE) aims to extract relations towards the\ncontinuous and iterative arrival of new data, of which the major challenge is\nthe catastrophic forgetting of old tasks. In order to alleviate this critical\nproblem for enhanced CRE performance, we propose a novel Continual Relation\nExtraction framework with Contrastive Learning, namely CRECL, which is built\nwith a classification network and a prototypical contrastive network to achieve\nthe incremental-class learning of CRE. Specifically, in the contrastive network\na given instance is contrasted with the prototype of each candidate relations\nstored in the memory module. Such contrastive learning scheme ensures the data\ndistributions of all tasks more distinguishable, so as to alleviate the\ncatastrophic forgetting further. Our experiment results not only demonstrate\nour CRECL's advantage over the state-of-the-art baselines on two public\ndatasets, but also verify the effectiveness of CRECL's contrastive learning on\nimproving CRE performance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.0329,regular,pre_llm,2022,10,"{'ai_likelihood': 1.6391277313232422e-05, 'text': 'Embedding Representation of Academic Heterogeneous Information Networks\n  Based on Federated Learning\n\n  Academic networks in the real world can usually be portrayed as heterogeneous\ninformation networks (HINs) with multi-type, universally connected nodes and\nmulti-relationships. Some existing studies for the representation learning of\nhomogeneous information networks cannot be applicable to heterogeneous\ninformation networks because of the lack of ability to issue heterogeneity. At\nthe same time, data has become a factor of production, playing an increasingly\nimportant role. Due to the closeness and blocking of businesses among different\nenterprises, there is a serious phenomenon of data islands. To solve the above\nchallenges, aiming at the data information of scientific research teams closely\nrelated to science and technology, we proposed an academic heterogeneous\ninformation network embedding representation learning method based on federated\nlearning (FedAHE), which utilizes node attention and meta path attention\nmechanism to learn low-dimensional, dense and real-valued vector\nrepresentations while preserving the rich topological information and\nmeta-path-based semantic information of nodes in network. Moreover, we combined\nfederated learning with the representation learning of HINs composed of\nscientific research teams and put forward a federal training mechanism based on\ndynamic weighted aggregation of parameters (FedDWA) to optimize the node\nembeddings of HINs. Through sufficient experiments, the efficiency, accuracy\nand feasibility of our proposed framework are demonstrated.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.13202,regular,pre_llm,2022,10,"{'ai_likelihood': 2.4835268656412763e-06, 'text': 'Online Information Retrieval Evaluation using the STELLA Framework\n\n  Involving users in early phases of software development has become a common\nstrategy as it enables developers to consider user needs from the beginning.\nOnce a system is in production, new opportunities to observe, evaluate and\nlearn from users emerge as more information becomes available. Gathering\ninformation from users to continuously evaluate their behavior is a common\npractice for commercial software, while the Cranfield paradigm remains the\npreferred option for Information Retrieval (IR) and recommendation systems in\nthe academic world. Here we introduce the Infrastructures for Living Labs\nSTELLA project which aims to create an evaluation infrastructure allowing\nexperimental systems to run along production web-based academic search systems\nwith real users. STELLA combines user interactions and log files analyses to\nenable large-scale A/B experiments for academic search.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.14577,regular,pre_llm,2022,10,"{'ai_likelihood': 1.953707800971137e-06, 'text': ""Disentangling Past-Future Modeling in Sequential Recommendation via Dual\n  Networks\n\n  Sequential recommendation (SR) plays an important role in personalized\nrecommender systems because it captures dynamic and diverse preferences from\nusers' real-time increasing behaviors. Unlike the standard autoregressive\ntraining strategy, future data (also available during training) has been used\nto facilitate model training as it provides richer signals about user's current\ninterests and can be used to improve the recommendation quality. However, these\nmethods suffer from a severe training-inference gap, i.e., both past and future\ncontexts are modeled by the same encoder when training, while only historical\nbehaviors are available during inference. This discrepancy leads to potential\nperformance degradation. To alleviate the training-inference gap, we propose a\nnew framework DualRec, which achieves past-future disentanglement and\npast-future mutual enhancement by a novel dual network. Specifically, a dual\nnetwork structure is exploited to model the past and future context separately.\nAnd a bi-directional knowledge transferring mechanism enhances the knowledge\nlearnt by the dual network. Extensive experiments on four real-world datasets\ndemonstrate the superiority of our approach over baseline methods. Besides, we\ndemonstrate the compatibility of DualRec by instantiating using RNN,\nTransformer, and filter-MLP as backbones. Further empirical analysis verifies\nthe high utility of modeling future contexts under our DualRec framework. The\ncode of DualRec is publicly available at https://github.com/zhy99426/DualRec.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.11934,regular,pre_llm,2022,10,"{'ai_likelihood': 1.7219119601779514e-06, 'text': 'An Analysis of Fusion Functions for Hybrid Retrieval\n\n  We study hybrid search in text retrieval where lexical and semantic search\nare fused together with the intuition that the two are complementary in how\nthey model relevance. In particular, we examine fusion by a convex combination\n(CC) of lexical and semantic scores, as well as the Reciprocal Rank Fusion\n(RRF) method, and identify their advantages and potential pitfalls. Contrary to\nexisting studies, we find RRF to be sensitive to its parameters; that the\nlearning of a CC fusion is generally agnostic to the choice of score\nnormalization; that CC outperforms RRF in in-domain and out-of-domain settings;\nand finally, that CC is sample efficient, requiring only a small set of\ntraining examples to tune its only parameter to a target domain.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.03291,regular,pre_llm,2022,10,"{'ai_likelihood': 7.112820943196615e-05, 'text': 'A Relational Triple Extraction Method Based on Feature Reasoning for\n  Technological Patents\n\n  The relation triples extraction method based on table filling can address the\nissues of relation overlap and bias propagation. However, most of them only\nestablish separate table features for each relationship, which ignores the\nimplicit relationship between different entity pairs and different relationship\nfeatures. Therefore, a feature reasoning relational triple extraction method\nbased on table filling for technological patents is proposed to explore the\nintegration of entity recognition and entity relationship, and to extract\nentity relationship triples from multi-source scientific and technological\npatents data. Compared with the previous methods, the method we proposed for\nrelational triple extraction has the following advantages: 1) The table filling\nmethod that saves more running space enhances the speed and efficiency of the\nmodel. 2) Based on the features of existing token pairs and table relations,\nreasoning the implicit relationship features, and improve the accuracy of\ntriple extraction. On five benchmark datasets, we evaluated the model we\nsuggested. The result suggest that our model is advanced and effective, and it\nperformed well on most of these datasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.10517,regular,pre_llm,2022,10,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'Evaluation Metrics for Measuring Bias in Search Engine Results\n\n  Search engines decide what we see for a given search query. Since many people\nare exposed to information through search engines, it is fair to expect that\nsearch engines are neutral. However, search engine results do not necessarily\ncover all the viewpoints of a search query topic, and they can be biased\ntowards a specific view since search engine results are returned based on\nrelevance, which is calculated using many features and sophisticated algorithms\nwhere search neutrality is not necessarily the focal point. Therefore, it is\nimportant to evaluate the search engine results with respect to bias. In this\nwork we propose novel web search bias evaluation measures which take into\naccount the rank and relevance. We also propose a framework to evaluate web\nsearch bias using the proposed measures and test our framework on two popular\nsearch engines based on 57 controversial query topics such as abortion, medical\nmarijuana, and gay marriage. We measure the stance bias (in support or\nagainst), as well as the ideological bias (conservative or liberal). We observe\nthat the stance does not necessarily correlate with the ideological leaning,\ne.g. a positive stance on abortion indicates a liberal leaning but a positive\nstance on Cuba embargo indicates a conservative leaning. Our experiments show\nthat neither of the search engines suffers from stance bias. However, both\nsearch engines suffer from ideological bias, both favouring one ideological\nleaning to the other, which is more significant from the perspective of\npolarisation in our society.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.05512,regular,pre_llm,2022,10,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'On the Interpolation of Contextualized Term-based Ranking with BM25 for\n  Query-by-Example Retrieval\n\n  Term-based ranking with pre-trained transformer-based language models has\nrecently gained attention as they bring the contextualization power of\ntransformer models into the highly efficient term-based retrieval. In this\nwork, we examine the generalizability of two of these deep contextualized\nterm-based models in the context of query-by-example (QBE) retrieval in which a\nseed document acts as the query to find relevant documents. In this setting --\nwhere queries are much longer than common keyword queries -- BERT inference at\nquery time is problematic as it involves quadratic complexity. We investigate\nTILDE and TILDEv2, both of which leverage BERT tokenizer as their query\nencoder. With this approach, there is no need for BERT inference at query time,\nand also the query can be of any length. Our extensive evaluation on the four\nQBE tasks of SciDocs benchmark shows that in a query-by-example retrieval\nsetting TILDE and TILDEv2 are still less effective than a cross-encoder BERT\nranker. However, we observe that BM25 could show a competitive ranking quality\ncompared to TILDE and TILDEv2 which is in contrast to the findings about the\nrelative performance of these three models on retrieval for short queries\nreported in prior work. This result raises the question about the use of\ncontextualized term-based ranking models being beneficial in QBE setting. We\nfollow-up on our findings by studying the score interpolation between the\nrelevance score from TILDE (TILDEv2) and BM25. We conclude that these two\ncontextualized term-based ranking models capture different relevance signals\nthan BM25 and combining the different term-based rankers results in\nstatistically significant improvements in QBE retrieval. Our work sheds light\non the challenges of retrieval settings different from the common evaluation\nbenchmarks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.14309,regular,pre_llm,2022,10,"{'ai_likelihood': 8.808241950141059e-06, 'text': ""Empowering Long-tail Item Recommendation through Cross Decoupling\n  Network (CDN)\n\n  Industry recommender systems usually suffer from highly-skewed long-tail item\ndistributions where a small fraction of the items receives most of the user\nfeedback. This skew hurts recommender quality especially for the item slices\nwithout much user feedback. While there have been many research advances made\nin academia, deploying these methods in production is very difficult and very\nfew improvements have been made in industry. One challenge is that these\nmethods often hurt overall performance; additionally, they could be complex and\nexpensive to train and serve. In this work, we aim to improve tail item\nrecommendations while maintaining the overall performance with less training\nand serving cost. We first find that the predictions of user preferences are\nbiased under long-tail distributions. The bias comes from the differences\nbetween training and serving data in two perspectives: 1) the item\ndistributions, and 2) user's preference given an item. Most existing methods\nmainly attempt to reduce the bias from the item distribution perspective,\nignoring the discrepancy from user preference given an item. This leads to a\nsevere forgetting issue and results in sub-optimal performance.\n  To address the problem, we design a novel Cross Decoupling Network (CDN) (i)\ndecouples the learning process of memorization and generalization on the item\nside through a mixture-of-expert architecture; (ii) decouples the user samples\nfrom different distributions through a regularized bilateral branch network.\nFinally, a new adapter is introduced to aggregate the decoupled vectors, and\nsoftly shift the training attention to tail items. Extensive experimental\nresults show that CDN significantly outperforms state-of-the-art approaches on\nbenchmark datasets. We also demonstrate its effectiveness by a case study of\nCDN in a large-scale recommendation system at Google.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.04614,review,pre_llm,2022,10,"{'ai_likelihood': 1.3146135542127822e-05, 'text': ""Joint Multi-grained Popularity-aware Graph Convolution Collaborative\n  Filtering for Recommendation\n\n  Graph Convolution Networks (GCNs), with their efficient ability to capture\nhigh-order connectivity in graphs, have been widely applied in recommender\nsystems. Stacking multiple neighbor aggregation is the major operation in GCNs.\nIt implicitly captures popularity features because the number of neighbor nodes\nreflects the popularity of a node. However, existing GCN-based methods ignore a\nuniversal problem: users' sensitivity to item popularity is differentiated, but\nthe neighbor aggregations in GCNs actually fix this sensitivity through Graph\nLaplacian Normalization, leading to suboptimal personalization.\n  In this work, we propose to model multi-grained popularity features and\njointly learn them together with high-order connectivity, to match the\ndifferentiation of user preferences exhibited in popularity features.\nSpecifically, we develop a Joint Multi-grained Popularity-aware Graph\nConvolution Collaborative Filtering model, short for JMP-GCF, which uses a\npopularity-aware embedding generation to construct multi-grained popularity\nfeatures, and uses the idea of joint learning to capture the signals within and\nbetween different granularities of popularity features that are relevant for\nmodeling user preferences. Additionally, we propose a multistage stacked\ntraining strategy to speed up model convergence. We conduct extensive\nexperiments on three public datasets to show the state-of-the-art performance\nof JMP-GCF.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.01154,regular,pre_llm,2022,10,"{'ai_likelihood': 1.4139546288384332e-05, 'text': 'Mitigating Popularity Bias in Recommendation with Unbalanced\n  Interactions: A Gradient Perspective\n\n  Recommender systems learn from historical user-item interactions to identify\npreferred items for target users. These observed interactions are usually\nunbalanced following a long-tailed distribution. Such long-tailed data lead to\npopularity bias to recommend popular but not personalized items to users. We\npresent a gradient perspective to understand two negative impacts of popularity\nbias in recommendation model optimization: (i) the gradient direction of\npopular item embeddings is closer to that of positive interactions, and (ii)\nthe magnitude of positive gradient for popular items are much greater than that\nof unpopular items. To address these issues, we propose a simple yet efficient\nframework to mitigate popularity bias from a gradient perspective.\nSpecifically, we first normalize each user embedding and record accumulated\ngradients of users and items via popularity bias measures in model training. To\naddress the popularity bias issues, we develop a gradient-based embedding\nadjustment approach used in model testing. This strategy is generic,\nmodel-agnostic, and can be seamlessly integrated into most existing recommender\nsystems. Our extensive experiments on two classic recommendation models and\nfour real-world datasets demonstrate the effectiveness of our method over\nstate-of-the-art debiasing baselines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.01701,regular,pre_llm,2022,10,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'Knowledge Distillation based Contextual Relevance Matching for\n  E-commerce Product Search\n\n  Online relevance matching is an essential task of e-commerce product search\nto boost the utility of search engines and ensure a smooth user experience.\nPrevious work adopts either classical relevance matching models or\nTransformer-style models to address it. However, they ignore the inherent\nbipartite graph structures that are ubiquitous in e-commerce product search\nlogs and are too inefficient to deploy online. In this paper, we design an\nefficient knowledge distillation framework for e-commerce relevance matching to\nintegrate the respective advantages of Transformer-style models and classical\nrelevance matching models. Especially for the core student model of the\nframework, we propose a novel method using $k$-order relevance modeling. The\nexperimental results on large-scale real-world data (the size is 6$\\sim$174\nmillion) show that the proposed method significantly improves the prediction\naccuracy in terms of human relevance judgment. We deploy our method to the\nanonymous online search platform. The A/B testing results show that our method\nsignificantly improves 5.7% of UV-value under price sort mode.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.0766,regular,pre_llm,2022,10,"{'ai_likelihood': 5.4637591044108076e-06, 'text': 'MV-HAN: A Hybrid Attentive Networks based Multi-View Learning Model for\n  Large-scale Contents Recommendation\n\n  Industrial recommender systems usually employ multi-source data to improve\nthe recommendation quality, while effectively sharing information between\ndifferent data sources remain a challenge. In this paper, we introduce a novel\nMulti-View Approach with Hybrid Attentive Networks (MV-HAN) for contents\nretrieval at the matching stage of recommender systems. The proposed model\nenables high-order feature interaction from various input features while\neffectively transferring knowledge between different types. By employing a\nwell-placed parameters sharing strategy, the MV-HAN substantially improves the\nretrieval performance in sparse types. The designed MV-HAN inherits the\nefficiency advantages in the online service from the two-tower model, by\nmapping users and contents of different types into the same features space.\nThis enables fast retrieval of similar contents with an approximate nearest\nneighbor algorithm. We conduct offline experiments on several industrial\ndatasets, demonstrating that the proposed MV-HAN significantly outperforms\nbaselines on the content retrieval tasks. Importantly, the MV-HAN is deployed\nin a real-world matching system. Online A/B test results show that the proposed\nmethod can significantly improve the quality of recommendations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.10555,regular,pre_llm,2022,10,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Data-Augmented Counterfactual Learning for Bundle Recommendation\n\n  Bundle Recommendation (BR) aims at recommending bundled items on online\ncontent or e-commerce platform, such as song lists on a music platform or book\nlists on a reading website. Several graph based models have achieved\nstate-of-the-art performance on BR task. But their performance is still\nsub-optimal, since the data sparsity problem tends to be more severe in real\nbundle recommendation scenarios, which limits graph-based models from more\nsufficient learning. In this paper, we propose a novel graph learning paradigm\ncalled Counterfactual Learning for Bundle Recommendation (CLBR) to mitigate the\nimpact of data sparsity problem and improve bundle recommendation. Our paradigm\nconsists of two main parts: counterfactual data augmentation and counterfactual\nconstraint. The main idea of our paradigm lies in answering the counterfactual\nquestions: ""What would a user interact with if his/her interaction history\nchanges?"" ""What would a user interact with if the bundle-item affiliation\nrelations change?"" In counterfactual data augmentation, we design a heuristic\nsampler to generate counterfactual graph views for graph-based models, which\nhas better noise controlling than the stochastic sampler. We further propose\ncounterfactual loss to constrain model learning for mitigating the effects of\nresidual noise in augmented data and achieving more sufficient model\noptimization. Further theoretical analysis demonstrates the rationality of our\ndesign. Extensive experiments of BR models applied with our paradigm on two\nreal-world datasets are conducted to verify the effectiveness of the paradigm.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.11828,regular,pre_llm,2022,10,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Towards Employing Recommender Systems for Supporting Data and Algorithm\n  Sharing\n\n  Data and algorithm sharing is an imperative part of data and AI-driven\neconomies. The efficient sharing of data and algorithms relies on the active\ninterplay between users, data providers, and algorithm providers. Although\nrecommender systems are known to effectively interconnect users and items in\ne-commerce settings, there is a lack of research on the applicability of\nrecommender systems for data and algorithm sharing. To fill this gap, we\nidentify six recommendation scenarios for supporting data and algorithm\nsharing, where four of these scenarios substantially differ from the\ntraditional recommendation scenarios in e-commerce applications. We evaluate\nthese recommendation scenarios using a novel dataset based on interaction data\nof the OpenML data and algorithm sharing platform, which we also provide for\nthe scientific community. Specifically, we investigate three types of\nrecommendation approaches, namely popularity-, collaboration-, and\ncontent-based recommendations. We find that collaboration-based recommendations\nprovide the most accurate recommendations in all scenarios. Plus, the\nrecommendation accuracy strongly depends on the specific scenario, e.g.,\nalgorithm recommendations for users are a more difficult problem than algorithm\nrecommendations for datasets. Finally, the content-based approach generates the\nleast popularity-biased recommendations that cover the most datasets and\nalgorithms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.03292,regular,pre_llm,2022,10,"{'ai_likelihood': 2.798106935289171e-05, 'text': 'Unsupervised Semantic Representation Learning of Scientific Literature\n  Based on Graph Attention Mechanism and Maximum Mutual Information\n\n  Since most scientific literature data are unlabeled, this makes unsupervised\ngraph-based semantic representation learning crucial. Therefore, an\nunsupervised semantic representation learning method of scientific literature\nbased on graph attention mechanism and maximum mutual information (GAMMI) is\nproposed. By introducing a graph attention mechanism, the weighted summation of\nnearby node features make the weights of adjacent node features entirely depend\non the node features. Depending on the features of the nearby nodes, different\nweights can be applied to each node in the graph. Therefore, the correlations\nbetween vertex features can be better integrated into the model. In addition,\nan unsupervised graph contrastive learning strategy is proposed to solve the\nproblem of being unlabeled and scalable on large-scale graphs. By comparing the\nmutual information between the positive and negative local node representations\non the latent space and the global graph representation, the graph neural\nnetwork can capture both local and global information. Experimental results\ndemonstrate competitive performance on various node classification benchmarks,\nachieving good results and sometimes even surpassing the performance of\nsupervised learning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.01332,regular,pre_llm,2022,10,"{'ai_likelihood': 3.341171476576063e-05, 'text': 'A Health Focused Text Classification Tool (HFTCT)\n\n  Due to the high number of users on social media and the massive amounts of\nqueries requested every second to share a new video, picture, or message,\nsocial platforms struggle to manage this humungous amount of data that is\nendlessly coming in. HFTCT relies on wordlists to classify opinions. It can\ncarry out its tasks reasonably well; however, sometimes, the wordlists\nthemselves fail to be reliable as they are a limited source of positive and\nnegative words.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.14958,regular,pre_llm,2022,10,"{'ai_likelihood': 1.6887982686360678e-06, 'text': ""Constrained Approximate Similarity Search on Proximity Graph\n\n  Search engines and recommendation systems are built to efficiently display\nrelevant information from those massive amounts of candidates. Typically a\nthree-stage mechanism is employed in those systems: (i) a small collection of\nitems are first retrieved by (e.g.,) approximate near neighbor search\nalgorithms; (ii) then a collection of constraints are applied on the retrieved\nitems; (iii) a fine-grained ranking neural network is employed to determine the\nfinal recommendation. We observe a major defect of the original three-stage\npipeline: Although we only target to retrieve $k$ vectors in the final\nrecommendation, we have to preset a sufficiently large $s$ ($s > k$) for each\nquery, and ``hope'' the number of survived vectors after the filtering is not\nsmaller than $k$. That is, at least $k$ vectors in the $s$ similar candidates\nsatisfy the query constraints.\n  In this paper, we investigate this constrained similarity search problem and\nattempt to merge the similarity search stage and the filtering stage into one\nsingle search operation. We introduce AIRSHIP, a system that integrates a\nuser-defined function filtering into the similarity search framework. The\nproposed system does not need to build extra indices nor require prior\nknowledge of the query constraints. We propose three optimization strategies:\n(1) starting point selection, (2) multi-direction search, and (3) biased\npriority queue selection. Experimental evaluations on both synthetic and real\ndata confirm the effectiveness of the proposed AIRSHIP algorithm. We focus on\nconstrained graph-based approximate near neighbor (ANN) search in this study,\nin part because graph-based ANN is known to achieve excellent performance. We\nbelieve it is also possible to develop constrained hashing-based ANN or\nconstrained quantization-based ANN.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.0264,regular,pre_llm,2022,10,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'ForestQB: An Adaptive Query Builder to Support Wildlife Research\n\n  This paper presents ForestQB, a SPARQL query builder, to assist Bioscience\nand Wildlife Researchers in accessing Linked-Data. As they are unfamiliar with\nthe Semantic Web and the data ontologies, ForestQB aims to empower them to\nbenefit from using Linked-Data to extract valuable information without having\nto grasp the nature of the data and its underlying technologies. ForestQB is\nintegrating Form-Based Query builders with Natural Language to simplify query\nconstruction to match the user requirements. Demo available at\nhttps://iotgarage.net/demo/forestQB\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.03288,regular,pre_llm,2022,10,"{'ai_likelihood': 7.086329989963108e-06, 'text': 'Scientific Paper Classification Based on Graph Neural Network with\n  Hypergraph Self-attention Mechanism\n\n  The number of scientific papers has increased rapidly in recent years. How to\nmake good use of scientific papers for research is very important. Through the\nhigh-quality classification of scientific papers, researchers can quickly find\nthe resource content they need from the massive scientific resources. The\nclassification of scientific papers will effectively help researchers filter\nredundant information, obtain search results quickly and accurately, and\nimprove the search quality, which is necessary for scientific resource\nmanagement. This paper proposed a science-technique paper classification method\nbased on hypergraph neural network(SPHNN). In the heterogeneous information\nnetwork of scientific papers, the repeated high-order subgraphs are modeled as\nhyperedges composed of multiple related nodes. Then the whole heterogeneous\ninformation network is transformed into a hypergraph composed of different\nhyperedges. The graph convolution operation is carried out on the hypergraph\nstructure, and the hyperedges self-attention mechanism is introduced to\naggregate different types of nodes in the hypergraph, so that the final node\nrepresentation can effectively maintain high-order nearest neighbor\nrelationships and complex semantic information. Finally, by comparing with\nother methods, we proved that the model proposed in this paper has improved its\nperformance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.11662,regular,pre_llm,2022,11,"{'ai_likelihood': 4.13921144273546e-06, 'text': 'Mutually-Regularized Dual Collaborative Variational Auto-encoder for\n  Recommendation Systems\n\n  Recently, user-oriented auto-encoders (UAEs) have been widely used in\nrecommender systems to learn semantic representations of users based on their\nhistorical ratings. However, since latent item variables are not modeled in\nUAE, it is difficult to utilize the widely available item content information\nwhen ratings are sparse. In addition, whenever new items arrive, we need to\nwait for collecting rating data for these items and retrain the UAE from\nscratch, which is inefficient in practice. Aiming to address the above two\nproblems simultaneously, we propose a mutually-regularized dual collaborative\nvariational auto-encoder (MD-CVAE) for recommendation. First, by replacing\nrandomly initialized last layer weights of the vanilla UAE with stacked latent\nitem embeddings, MD-CVAE integrates two heterogeneous information sources,\ni.e., item content and user ratings, into the same principled variational\nframework where the weights of UAE are regularized by item content such that\nconvergence to a non-optima due to data sparsity can be avoided. In addition,\nthe regularization is mutual in that user ratings can also help the dual item\ncontent module learn more recommendation-oriented item content embeddings.\nFinally, we propose a symmetric inference strategy for MD-CVAE where the first\nlayer weights of the UAE encoder are tied to the latent item embeddings of the\nUAE decoder. Through this strategy, no retraining is required to recommend\nnewly introduced items. Empirical studies show the effectiveness of MD-CVAE in\nboth normal and cold-start scenarios. Codes are available at\nhttps://github.com/yaochenzhu/MD-CVAE.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.0761,regular,pre_llm,2022,11,"{'ai_likelihood': 2.122587627834744e-05, 'text': 'Pied Piper: Meta Search for Music\n\n  Internet search engines have become an integral part of life, but for pop\nmusic, people still rely on textual search engines like Google. We propose Pied\npiper, a meta search engine for music. It can search for music lyrics, song\nmetadata and song audio or a combination of any of these as the input query and\nefficiently return the relevant results.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.10486,regular,pre_llm,2022,11,"{'ai_likelihood': 7.682376437717014e-06, 'text': 'DGRec: Graph Neural Network for Recommendation with Diversified\n  Embedding Generation\n\n  Graph Neural Network (GNN) based recommender systems have been attracting\nmore and more attention in recent years due to their excellent performance in\naccuracy. Representing user-item interactions as a bipartite graph, a GNN model\ngenerates user and item representations by aggregating embeddings of their\nneighbors. However, such an aggregation procedure often accumulates information\npurely based on the graph structure, overlooking the redundancy of the\naggregated neighbors and resulting in poor diversity of the recommended list.\nIn this paper, we propose diversifying GNN-based recommender systems by\ndirectly improving the embedding generation procedure. Particularly, we utilize\nthe following three modules: submodular neighbor selection to find a subset of\ndiverse neighbors to aggregate for each GNN node, layer attention to assign\nattention weights for each layer, and loss reweighting to focus on the learning\nof items belonging to long-tail categories. Blending the three modules into\nGNN, we present DGRec(Diversified GNN-based Recommender System) for diversified\nrecommendation. Experiments on real-world datasets demonstrate that the\nproposed method can achieve the best diversity while keeping the accuracy\ncomparable to state-of-the-art GNN-based recommender systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.07297,regular,pre_llm,2022,11,"{'ai_likelihood': 3.3775965372721357e-06, 'text': 'Talent Recommendation on LinkedIn User Profiles\n\n  With the increasing amount of information on the Internet, recommender\nsystems are becoming increasingly crucial in supporting people to find and\nexplore relevant content. This is also true in the online recruitment space,\nwith websites such as LinkedIn, Indeed.com, and Monster.com all using\nrecommender systems. In online recruitment, it can often be challenging for\ncompanies to find suitable candidates with appropriate skills because of the\nhuge volume of user profiles available. Identifying users which satisfy a range\nof different employer needs is also a difficult task. Thus, effective matching\nof user-profiles and jobs is becoming crucial for companies. This research\nproject applies a wide range of recommendation techniques to the task of user\nprofile recommendation. Extensive experiments are conducted on a large-scale\nreal-world LinkedIn dataset to evaluate their performance, with the aim of\nidentifying the most suitable approach in this particular recommendation\nscenario.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.02767,regular,pre_llm,2022,11,"{'ai_likelihood': 4.967053731282552e-07, 'text': ""Fuzzy Substring Matching: On-device Fuzzy Friend Search at Snapchat\n\n  About 50% of all queries on Snapchat app are targeted at finding the right\nfriend to interact with. Since everyone has a unique list of friends and that\nlist is not very large (maximum a few thousand), it makes sense to perform this\nsearch locally, on users' devices. In addition, the friend list is already\navailable for other purposes, such as showing the chat feed, and the latency\nsavings can be significant by avoiding a server round-trip call. Historically,\nwe resorted to substring matching, ranking prefix matches at the top of the\nresult list. Introducing the ability to perform fuzzy search on a\nresource-constrained device and in the environment where typo's are prevalent\nis both prudent and challenging. In this paper, we describe our efficient and\naccurate two-step approach to fuzzy search, characterized by a skip-bigram\nretrieval layer and a novel local Levenshtein distance computation used for\nfinal ranking.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.07837,regular,pre_llm,2022,11,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'An Ontology for the Social Determinants of Health Domain\n\n  Social determinants of health are societal factors, such as where a person\nwas born, grew up, works, lives, etc, along with socioeconomic and community\nfactors that affect individual health. Social Determinants of Health are\ncorrelated with many clinical outcomes, hence it is desirable to record SDOH\ndata in Electronic Health Records (EHRs). Besides storing images, text, etc.,\nEHRs rely on coded terms available in standard ontologies and terminologies to\nrecord observations and analyses. There is a substantial amount of research on\nunderstanding the clinical impact of SDOH, ranging from screening tools to\npractice based interventions. However, there is no comprehensive collection of\nterms for recording SDOH observations in EHRs. Our research goal is to develop\nan ontology that covers the terms describing SDOH. We present a prototype\nontology called Social Determinant of Health Ontology (SOHO) that covers\nrelevant concepts and IS--A relationships describing impacts and associations\nof social determinants. We describe the evaluation techniques that we applied\nto SOHO, including human experts review and algorithmic evaluation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.09303,review,pre_llm,2022,11,"{'ai_likelihood': 3.245141771104601e-06, 'text': ""A Bird's-eye View of Reranking: from List Level to Page Level\n\n  Reranking, as the final stage of multi-stage recommender systems, refines the\ninitial lists to maximize the total utility. With the development of multimedia\nand user interface design, the recommendation page has evolved to a multi-list\nstyle. Separately employing traditional list-level reranking methods for\ndifferent lists overlooks the inter-list interactions and the effect of\ndifferent page formats, thus yielding suboptimal reranking performance.\nMoreover, simply applying a shared network for all the lists fails to capture\nthe commonalities and distinctions in user behaviors on different lists. To\nthis end, we propose to draw a bird's-eye view of \\textbf{page-level reranking}\nand design a novel Page-level Attentional Reranking (PAR) model. We introduce a\nhierarchical dual-side attention module to extract personalized intra- and\ninter-list interactions. A spatial-scaled attention network is devised to\nintegrate the spatial relationship into pairwise item influences, which\nexplicitly models the page format. The multi-gated mixture-of-experts module is\nfurther applied to capture the commonalities and differences of user behaviors\nbetween different lists. Extensive experiments on a public dataset and a\nproprietary dataset show that PAR significantly outperforms existing baseline\nmodels.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.11964,regular,pre_llm,2022,11,"{'ai_likelihood': 1.1722246805826824e-05, 'text': 'One for All, All for One: Learning and Transferring User Embeddings for\n  Cross-Domain Recommendation\n\n  Cross-domain recommendation is an important method to improve recommender\nsystem performance, especially when observations in target domains are sparse.\nHowever, most existing techniques focus on single-target or dual-target\ncross-domain recommendation (CDR) and are hard to be generalized to CDR with\nmultiple target domains. In addition, the negative transfer problem is\nprevalent in CDR, where the recommendation performance in a target domain may\nnot always be enhanced by knowledge learned from a source domain, especially\nwhen the source domain has sparse data. In this study, we propose CAT-ART, a\nmulti-target CDR method that learns to improve recommendations in all\nparticipating domains through representation learning and embedding transfer.\nOur method consists of two parts: a self-supervised Contrastive AuToencoder\n(CAT) framework to generate global user embeddings based on information from\nall participating domains, and an Attention-based Representation Transfer (ART)\nframework which transfers domain-specific user embeddings from other domains to\nassist with target domain recommendation. CAT-ART boosts the recommendation\nperformance in any target domain through the combined use of the learned global\nuser representation and knowledge transferred from other domains, in addition\nto the original user embedding in the target domain. We conducted extensive\nexperiments on a collected real-world CDR dataset spanning 5 domains and\ninvolving a million users. Experimental results demonstrate the superiority of\nthe proposed method over a range of prior arts. We further conducted ablation\nstudies to verify the effectiveness of the proposed components. Our collected\ndataset will be open-sourced to facilitate future research in the field of\nmulti-domain recommender systems and user modeling.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.11535,regular,pre_llm,2022,11,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Preliminary Bias Results in Search Engines\n\n  This report aims to report my thesis progress so far. My work attempts to\nshow the differences in the perspectives of two search engines, Bing and Google\non several selected controversial topics. In this work, we try to make a\ndistinction on the viewpoints of Bing \\& Google by using sentiment as well as\nthe ranking of the document returned from these two search engines on the same\nqueries, these queries are related mainly to controversial topics. You can find\nthe methods we used with experimental results below.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.01494,regular,pre_llm,2022,11,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'Regression Compatible Listwise Objectives for Calibrated Ranking with\n  Binary Relevance\n\n  As Learning-to-Rank (LTR) approaches primarily seek to improve ranking\nquality, their output scores are not scale-calibrated by design. This\nfundamentally limits LTR usage in score-sensitive applications. Though a simple\nmulti-objective approach that combines a regression and a ranking objective can\neffectively learn scale-calibrated scores, we argue that the two objectives are\nnot necessarily compatible, which makes the trade-off less ideal for either of\nthem. In this paper, we propose a practical regression compatible ranking (RCR)\napproach that achieves a better trade-off, where the two ranking and regression\ncomponents are proved to be mutually aligned. Although the same idea applies to\nranking with both binary and graded relevance, we mainly focus on binary labels\nin this paper. We evaluate the proposed approach on several public LTR\nbenchmarks and show that it consistently achieves either best or competitive\nresult in terms of both regression and ranking metrics, and significantly\nimproves the Pareto frontiers in the context of multi-objective optimization.\nFurthermore, we evaluated the proposed approach on YouTube Search and found\nthat it not only improved the ranking quality of the production pCTR model, but\nalso brought gains to the click prediction accuracy. The proposed approach has\nbeen successfully deployed in the YouTube production system.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.02894,regular,pre_llm,2022,11,"{'ai_likelihood': 4.5365757412380644e-06, 'text': 'Deep Factorization Model for Robust Recommendation\n\n  Recently, malevolent user hacking has become a huge problem for real-world\ncompanies. In order to learn predictive models for recommender systems,\nfactorization techniques have been developed to deal with user-item ratings. In\nthis paper, we suggest a broad architecture of a factorization model with\nadversarial training to get over these issues. The effectiveness of our systems\nis demonstrated by experimental findings on real-world datasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.09681,regular,pre_llm,2022,11,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Did They Really Tweet That? Querying Fact-Checking Sites and Politwoops\n  to Determine Tweet Misattribution\n\n  Screenshots of social media posts have become common place on social media\nsites. While screenshots definitely serve a purpose, their ubiquity enables the\nspread of fabricated screenshots of posts that were never actually made,\nthereby proliferating misattribution disinformation. With the motivation of\ndetecting this type of disinformation, we researched developing methods of\nquerying the Web for evidence of a tweet\'s existence. We developed software\nthat automatically makes search queries utilizing the body of alleged tweets to\na variety of services (Google, Snopes built-in search, and Reuters built-in\nsearch) in an effort to find fact-check articles and other evidence of\nsupposedly made tweets. We also developed tools to automatically search the\nsite Politwoops for a particular tweet that may have been made and deleted by\nan elected official. In addition, we developed software to scrape fact-check\narticles from the sites Reuters.com and Snopes.com in order to derive a ``truth\nrating"" from any given article from these sites. For evaluation, we began the\nconstruction of a ground truth dataset of tweets with known evidence (currently\nonly Snopes fact-check articles) on the live web, and we gathered MRR and P@1\nvalues based on queries made using only the bodies of those tweets. These\nqueries showed that the Snopes built-in search was effective at finding\nappropriate articles about half of the time with MRR=0.5500 and P@1=0.5333,\nwhile Google when used with the site:snopes.com operator was generally\neffective at finding the articles in question, with MRR=0.8667 and P@1=0.8667.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.0529,regular,pre_llm,2022,11,"{'ai_likelihood': 1.0397699144151476e-05, 'text': 'Equivariant Contrastive Learning for Sequential Recommendation\n\n  Contrastive learning (CL) benefits the training of sequential recommendation\nmodels with informative self-supervision signals. Existing solutions apply\ngeneral sequential data augmentation strategies to generate positive pairs and\nencourage their representations to be invariant. However, due to the inherent\nproperties of user behavior sequences, some augmentation strategies, such as\nitem substitution, can lead to changes in user intent. Learning\nindiscriminately invariant representations for all augmentation strategies\nmight be suboptimal. Therefore, we propose Equivariant Contrastive Learning for\nSequential Recommendation (ECL-SR), which endows SR models with great\ndiscriminative power, making the learned user behavior representations\nsensitive to invasive augmentations (e.g., item substitution) and insensitive\nto mild augmentations (e.g., featurelevel dropout masking). In detail, we use\nthe conditional discriminator to capture differences in behavior due to item\nsubstitution, which encourages the user behavior encoder to be equivariant to\ninvasive augmentations. Comprehensive experiments on four benchmark datasets\nshow that the proposed ECL-SR framework achieves competitive performance\ncompared to state-of-the-art SR models. The source code is available at\nhttps://github.com/Tokkiu/ECL.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.02405,review,pre_llm,2022,11,"{'ai_likelihood': 1.9106600019666883e-05, 'text': 'Explainable Information Retrieval: A Survey\n\n  Explainable information retrieval is an emerging research area aiming to make\ntransparent and trustworthy information retrieval systems. Given the increasing\nuse of complex machine learning models in search systems, explainability is\nessential in building and auditing responsible information retrieval models.\nThis survey fills a vital gap in the otherwise topically diverse literature of\nexplainable information retrieval. It categorizes and discusses recent\nexplainability methods developed for different application domains in\ninformation retrieval, providing a common framework and unifying perspectives.\nIn addition, it reflects on the common concern of evaluating explanations and\nhighlights open challenges and opportunities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.17171,regular,pre_llm,2022,11,"{'ai_likelihood': 2.0199351840549047e-06, 'text': 'CDSM: Cascaded Deep Semantic Matching on Textual Graphs Leveraging\n  Ad-hoc Neighbor Selection\n\n  Deep semantic matching aims to discriminate the relationship between\ndocuments based on deep neural networks. In recent years, it becomes\nincreasingly popular to organize documents with a graph structure, then\nleverage both the intrinsic document features and the extrinsic neighbor\nfeatures to derive discrimination. Most of the existing works mainly care about\nhow to utilize the presented neighbors, whereas limited effort is made to\nfilter appropriate neighbors. We argue that the neighbor features could be\nhighly noisy and partially useful. Thus, a lack of effective neighbor selection\nwill not only incur a great deal of unnecessary computation cost, but also\nrestrict the matching accuracy severely.\n  In this work, we propose a novel framework, Cascaded Deep Semantic Matching\n(CDSM), for accurate and efficient semantic matching on textual graphs. CDSM is\nhighlighted for its two-stage workflow. In the first stage, a lightweight\nCNN-based ad-hod neighbor selector is deployed to filter useful neighbors for\nthe matching task with a small computation cost. We design both one-step and\nmulti-step selection methods. In the second stage, a high-capacity graph-based\nmatching network is employed to compute fine-grained relevance scores based on\nthe well-selected neighbors. It is worth noting that CDSM is a generic\nframework which accommodates most of the mainstream graph-based semantic\nmatching networks. The major challenge is how the selector can learn to\ndiscriminate the neighbors usefulness which has no explicit labels. To cope\nwith this problem, we design a weak-supervision strategy for optimization,\nwhere we train the graph-based matching network at first and then the ad-hoc\nneighbor selector is learned on top of the annotations from the matching\nnetwork.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.15148,regular,pre_llm,2022,11,"{'ai_likelihood': 5.430645412868924e-06, 'text': 'Recent Advances in RecBole: Extensions with more Practical\n  Considerations\n\n  RecBole has recently attracted increasing attention from the research\ncommunity. As the increase of the number of users, we have received a number of\nsuggestions and update requests. This motivates us to make some significant\nimprovements on our library, so as to meet the user requirements and contribute\nto the research community. In order to show the recent update in RecBole, we\nwrite this technical report to introduce our latest improvements on RecBole. In\ngeneral, we focus on the flexibility and efficiency of RecBole in the past few\nmonths. More specifically, we have four development targets: (1) more flexible\ndata processing, (2) more efficient model training, (3) more reproducible\nconfigurations, and (4) more comprehensive user documentation. Readers can\ndownload the above updates at: https://github.com/RUCAIBox/RecBole.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.1536,regular,pre_llm,2022,11,"{'ai_likelihood': 1.158979203965929e-06, 'text': 'Learning Recommendations from User Actions in the Item-poor Insurance\n  Domain\n\n  While personalised recommendations are successful in domains like retail,\nwhere large volumes of user feedback on items are available, the generation of\nautomatic recommendations in data-sparse domains, like insurance purchasing, is\nan open problem. The insurance domain is notoriously data-sparse because the\nnumber of products is typically low (compared to retail) and they are usually\npurchased to last for a long time. Also, many users still prefer the telephone\nover the web for purchasing products, reducing the amount of web-logged user\ninteractions. To address this, we present a recurrent neural network\nrecommendation model that uses past user sessions as signals for learning\nrecommendations. Learning from past user sessions allows dealing with the data\nscarcity of the insurance domain. Specifically, our model learns from several\ntypes of user actions that are not always associated with items, and unlike all\nprior session-based recommendation models, it models relationships between\ninput sessions and a target action (purchasing insurance) that does not take\nplace within the input sessions. Evaluation on a real-world dataset from the\ninsurance domain (ca. 44K users, 16 items, 54K purchases, and 117K sessions)\nagainst several state-of-the-art baselines shows that our model outperforms the\nbaselines notably. Ablation analysis shows that this is mainly due to the\nlearning of dependencies across sessions in our model. We contribute the first\never session-based model for insurance recommendation, and make available our\ndataset to the research community.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.14155,regular,pre_llm,2022,11,"{'ai_likelihood': 3.410710228814019e-06, 'text': 'Caching Historical Embeddings in Conversational Search\n\n  Rapid response, namely low latency, is fundamental in search applications; it\nis particularly so in interactive search sessions, such as those encountered in\nconversational settings. An observation with a potential to reduce latency\nasserts that conversational queries exhibit a temporal locality in the lists of\ndocuments retrieved. Motivated by this observation, we propose and evaluate a\nclient-side document embedding cache, improving the responsiveness of\nconversational search systems. By leveraging state-of-the-art dense retrieval\nmodels to abstract document and query semantics, we cache the embeddings of\ndocuments retrieved for a topic introduced in the conversation, as they are\nlikely relevant to successive queries. Our document embedding cache implements\nan efficient metric index, answering nearest-neighbor similarity queries by\nestimating the approximate result sets returned. We demonstrate the efficiency\nachieved using our cache via reproducible experiments based on TREC CAsT\ndatasets, achieving a hit rate of up to 75% without degrading answer quality.\nOur achieved high cache hit rates significantly improve the responsiveness of\nconversational systems while likewise reducing the number of queries managed on\nthe search back-end.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.01976,regular,pre_llm,2022,11,"{'ai_likelihood': 3.311369154188368e-07, 'text': ""Enhancing Patent Retrieval using Text and Knowledge Graph Embeddings: A\n  Technical Note\n\n  Patent retrieval influences several applications within engineering design\nresearch, education, and practice as well as applications that concern\ninnovation, intellectual property, and knowledge management etc. In this\narticle, we propose a method to retrieve patents relevant to an initial set of\npatents, by synthesizing state-of-the-art techniques among natural language\nprocessing and knowledge graph embedding. Our method involves a patent\nembedding that captures text, citation, and inventor information, which\nindividually represent different facets of knowledge communicated through a\npatent document. We obtain text embeddings using Sentence-BERT applied to\ntitles and abstracts. We obtain citation and inventor embeddings through TransE\nthat is trained using the corresponding knowledge graphs. We identify using a\nclassification task that the concatenation of text, citation, and inventor\nembeddings offers a plausible representation of a patent. While the proposed\npatent embedding could be used to associate a pair of patents, we observe using\na recall task that multiple initial patents could be associated with a target\npatent using mean cosine similarity, which could then be utilized to rank all\ntarget patents and retrieve the most relevant ones. We apply the proposed\npatent retrieval method to a set of patents corresponding to a product family\nand an inventor's portfolio.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.01768,regular,pre_llm,2022,11,"{'ai_likelihood': 4.371007283528646e-06, 'text': 'Embedding Knowledge Graph of Patent Metadata to Measure Knowledge\n  Proximity\n\n  Knowledge proximity refers to the strength of association between any two\nentities in a structural form that embodies certain aspects of a knowledge\nbase. In this work, we operationalize knowledge proximity within the context of\nthe US Patent Database (knowledge base) using a knowledge graph (structural\nform) named PatNet built using patent metadata, including citations, inventors,\nassignees, and domain classifications. We train various graph embedding models\nusing PatNet to obtain the embeddings of entities and relations. The cosine\nsimilarity between the corresponding (or transformed) embeddings of entities\ndenotes the knowledge proximity between these. We compare the embedding models\nin terms of their performances in predicting target entities and explaining\ndomain expansion profiles of inventors and assignees. We then apply the\nembeddings of the best-preferred model to associate homogeneous (e.g.,\npatent-patent) and heterogeneous (e.g., inventor-assignee) pairs of entities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.08779,review,pre_llm,2022,12,"{'ai_likelihood': 5.672375361124675e-05, 'text': ""Personalized Federated Recommender Systems with Private and Partially\n  Federated AutoEncoders\n\n  Recommender Systems (RSs) have become increasingly important in many\napplication domains, such as digital marketing. Conventional RSs often need to\ncollect users' data, centralize them on the server-side, and form a global\nmodel to generate reliable recommendations. However, they suffer from two\ncritical limitations: the personalization problem that the RSs trained\ntraditionally may not be customized for individual users, and the privacy\nproblem that directly sharing user data is not encouraged. We propose\nPersonalized Federated Recommender Systems (PersonalFR), which introduces a\npersonalized autoencoder-based recommendation model with Federated Learning\n(FL) to address these challenges. PersonalFR guarantees that each user can\nlearn a personal model from the local dataset and other participating users'\ndata without sharing local data, data embeddings, or models. PersonalFR\nconsists of three main components, including AutoEncoder-based RSs (ARSs) that\nlearn the user-item interactions, Partially Federated Learning (PFL) that\nupdates the encoder locally and aggregates the decoder on the server-side, and\nPartial Compression (PC) that only computes and transmits active model\nparameters. Extensive experiments on two real-world datasets demonstrate that\nPersonalFR can achieve private and personalized performance comparable to that\ntrained by centralizing all users' data. Moreover, PersonalFR requires\nsignificantly less computation and communication overhead than standard FL\nbaselines.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.10762,review,pre_llm,2022,12,"{'ai_likelihood': 1.655684577094184e-07, 'text': ""AgAsk: An Agent to Help Answer Farmer's Questions From Scientific\n  Documents\n\n  Decisions in agriculture are increasingly data-driven; however, valuable\nagricultural knowledge is often locked away in free-text reports, manuals and\njournal articles. Specialised search systems are needed that can mine\nagricultural information to provide relevant answers to users' questions. This\npaper presents AgAsk -- an agent able to answer natural language agriculture\nquestions by mining scientific documents.\n  We carefully survey and analyse farmers' information needs. On the basis of\nthese needs we release an information retrieval test collection comprising real\nquestions, a large collection of scientific documents split in passages, and\nground truth relevance assessments indicating which passages are relevant to\neach question.\n  We implement and evaluate a number of information retrieval models to answer\nfarmers questions, including two state-of-the-art neural ranking models. We\nshow that neural rankers are highly effective at matching passages to questions\nin this context.\n  Finally, we propose a deployment architecture for AgAsk that includes a\nclient based on the Telegram messaging platform and retrieval model deployed on\ncommodity hardware.\n  The test collection we provide is intended to stimulate more research in\nmethods to match natural language to answers in scientific documents. While the\nretrieval models were evaluated in the agriculture domain, they are\ngeneralisable and of interest to others working on similar problems.\n  The test collection is available at:\n\\url{https://github.com/ielab/agvaluate}.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.04282,regular,pre_llm,2022,12,"{'ai_likelihood': 2.08947393629286e-05, 'text': 'Mitigating Spurious Correlations for Self-supervised Recommendation\n\n  Recent years have witnessed the great success of self-supervised learning\n(SSL) in recommendation systems. However, SSL recommender models are likely to\nsuffer from spurious correlations, leading to poor generalization. To mitigate\nspurious correlations, existing work usually pursues ID-based SSL\nrecommendation or utilizes feature engineering to identify spurious features.\nNevertheless, ID-based SSL approaches sacrifice the positive impact of\ninvariant features, while feature engineering methods require high-cost human\nlabeling. To address the problems, we aim to automatically mitigate the effect\nof spurious correlations. This objective requires to 1) automatically mask\nspurious features without supervision, and 2) block the negative effect\ntransmission from spurious features to other features during SSL. To handle the\ntwo challenges, we propose an invariant feature learning framework, which first\ndivides user-item interactions into multiple environments with distribution\nshifts and then learns a feature mask mechanism to capture invariant features\nacross environments. Based on the mask mechanism, we can remove the spurious\nfeatures for robust predictions and block the negative effect transmission via\nmask-guided feature augmentation. Extensive experiments on two datasets\ndemonstrate the effectiveness of the proposed framework in mitigating spurious\ncorrelations and improving the generalization abilities of SSL models. The code\nis available at https://github.com/Linxyhaha/IFL.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.14277,review,pre_llm,2022,12,"{'ai_likelihood': 2.3510720994737412e-06, 'text': 'System Log Parsing: A Survey\n\n  Modern information and communication systems have become increasingly\nchallenging to manage. The ubiquitous system logs contain plentiful information\nand are thus widely exploited as an alternative source for system management.\nAs log files usually encompass large amounts of raw data, manually analyzing\nthem is laborious and error-prone. Consequently, many research endeavors have\nbeen devoted to automatic log analysis. However, these works typically expect\nstructured input and struggle with the heterogeneous nature of raw system logs.\nLog parsing closes this gap by converting the unstructured system logs to\nstructured records. Many parsers were proposed during the last decades to\naccommodate various log analysis applications. However, due to the ample\nsolution space and lack of systematic evaluation, it is not easy for\npractitioners to find ready-made solutions that fit their needs.\n  This paper aims to provide a comprehensive survey on log parsing. We begin\nwith an exhaustive taxonomy of existing log parsers. Then we empirically\nanalyze the critical performance and operational features for 17 open-source\nsolutions both quantitatively and qualitatively, and whenever applicable\ndiscuss the merits of alternative approaches. We also elaborate on future\nchallenges and discuss the relevant research directions. We envision this\nsurvey as a helpful resource for system administrators and domain experts to\nchoose the most desirable open-source solution or implement new ones based on\napplication-specific requirements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.14464,review,pre_llm,2022,12,"{'ai_likelihood': 3.5762786865234375e-06, 'text': ""Result Diversification in Search and Recommendation: A Survey\n\n  Diversifying return results is an important research topic in retrieval\nsystems in order to satisfy both the various interests of customers and the\nequal market exposure of providers. There has been growing attention on\ndiversity-aware research during recent years, accompanied by a proliferation of\nliterature on methods to promote diversity in search and recommendation.\nHowever, diversity-aware studies in retrieval systems lack a systematic\norganization and are rather fragmented. In this survey, we are the first to\npropose a unified taxonomy for classifying the metrics and approaches of\ndiversification in both search and recommendation, which are two of the most\nextensively researched fields of retrieval systems. We begin the survey with a\nbrief discussion of why diversity is important in retrieval systems, followed\nby a summary of the various diversity concerns in search and recommendation,\nhighlighting their relationship and differences. For the survey's main body, we\npresent a unified taxonomy of diversification metrics and approaches in\nretrieval systems, from both the search and recommendation perspectives. In the\nlater part of the survey, we discuss the open research questions of\ndiversity-aware research in search and recommendation in an effort to inspire\nfuture innovations and encourage the implementation of diversity in real-world\nsystems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.02726,regular,pre_llm,2022,12,"{'ai_likelihood': 3.2782554626464844e-06, 'text': 'Dataset vs Reality: Understanding Model Performance from the Perspective\n  of Information Need\n\n  Deep learning technologies have brought us many models that outperform human\nbeings on a few benchmarks. An interesting question is: can these models well\nsolve real-world problems with similar settings (e.g., identical input/output)\nto the benchmark datasets? We argue that a model is trained to answer the same\ninformation need for which the training dataset is created. Although some\ndatasets may share high structural similarities, e.g., question-answer pairs\nfor the question answering (QA) task and image-caption pairs for the image\ncaptioning (IC) task, they may represent different research tasks aiming for\nanswering different information needs. To support our argument, we use the QA\ntask and IC task as two case studies and compare their widely used benchmark\ndatasets. From the perspective of information need in the context of\ninformation retrieval, we show the differences in the dataset creation\nprocesses, and the differences in morphosyntactic properties between datasets.\nThe differences in these datasets can be attributed to the different\ninformation needs of the specific research tasks. We encourage all researchers\nto consider the information need the perspective of a research task before\nutilizing a dataset to train a model. Likewise, while creating a dataset,\nresearchers may also incorporate the information need perspective as a factor\nto determine the degree to which the dataset accurately reflects the research\ntask they intend to tackle.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.07742,review,pre_llm,2022,12,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Analysis of information cascading and propagation barriers across\n  distinctive news events\n\n  News reporting on events that occur in our society can have different styles\nand structures as well as different dynamics of news spreading over time. News\npublishers have the potential to spread their news and reach out to a large\nnumber of readers worldwide. In this paper we would like to understand how well\nthey are doing it and which kind of obstacles the news may encounter when\nspreading. The news to be spread wider cross multiple barriers such as\nlinguistic (the most evident one as they get published in other natural\nlanguages), economic, geographical, political, time zone, and cultural\nbarriers. Observing potential differences between spreading of news on\ndifferent events published by multiple publishers can bring insights into what\nmay influence the differences in the spreading patterns. There are multiple\nreasons, possibly many hidden, influencing the speed and geographical spread of\nnews. This paper studies information cascading and propagation barriers,\napplying the proposed methodology on three distinctive kinds of events: Global\nWarming, earthquakes, and FIFA World Cup.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.1223,review,pre_llm,2022,12,"{'ai_likelihood': 6.622738308376736e-06, 'text': ""Recommending on graphs: a comprehensive review from a data perspective\n\n  Recent advances in graph-based learning approaches have demonstrated their\neffectiveness in modelling users' preferences and items' characteristics for\nRecommender Systems (RSS). Most of the data in RSS can be organized into graphs\nwhere various objects (e.g., users, items, and attributes) are explicitly or\nimplicitly connected and influence each other via various relations. Such a\ngraph-based organization brings benefits to exploiting potential properties in\ngraph learning (e.g., random walk and network embedding) techniques to enrich\nthe representations of the user and item nodes, which is an essential factor\nfor successful recommendations. In this paper, we provide a comprehensive\nsurvey of Graph Learning-based Recommender Systems (GLRSs). Specifically, we\nstart from a data-driven perspective to systematically categorize various\ngraphs in GLRSs and analyze their characteristics. Then, we discuss the\nstate-of-the-art frameworks with a focus on the graph learning module and how\nthey address practical recommendation challenges such as scalability, fairness,\ndiversity, explainability and so on. Finally, we share some potential research\ndirections in this rapidly growing area.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.00999,regular,pre_llm,2022,12,"{'ai_likelihood': 2.715322706434462e-06, 'text': 'Information Retrieval from the Digitized Books\n\n  Extracting the relevant information out of a large number of documents is a\nchallenging and tedious task. The quality of results generated by the\ntraditionally available full-text search engine and text-based image retrieval\nsystems is not optimal. Information retrieval (IR) tasks become more\nchallenging with the nontraditional language scripts, as in the case of Indic\nscripts. The authors have developed OCR (Optical Character Recognition) Search\nEngine to make an Information Retrieval & Extraction (IRE) system that\nreplicates the current state-of-the-art methods using the IRE and Natural\nLanguage Processing (NLP) techniques. Here we have presented the study of the\nmethods used for performing search and retrieval tasks. The details of this\nsystem, along with the statistics of the dataset (source: National Digital\nLibrary of India or NDLI), is also presented. Additionally, the ideas to\nfurther explore and add value to research in IRE are also discussed.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.0454,regular,pre_llm,2022,12,"{'ai_likelihood': 1.1920928955078125e-06, 'text': 'TinyKG: Memory-Efficient Training Framework for Knowledge Graph Neural\n  Recommender Systems\n\n  There has been an explosion of interest in designing various Knowledge Graph\nNeural Networks (KGNNs), which achieve state-of-the-art performance and provide\ngreat explainability for recommendation. The promising performance is mainly\nresulting from their capability of capturing high-order proximity messages over\nthe knowledge graphs. However, training KGNNs at scale is challenging due to\nthe high memory usage. In the forward pass, the automatic differentiation\nengines (\\textsl{e.g.}, TensorFlow/PyTorch) generally need to cache all\nintermediate activation maps in order to compute gradients in the backward\npass, which leads to a large GPU memory footprint. Existing work solves this\nproblem by utilizing multi-GPU distributed frameworks. Nonetheless, this poses\na practical challenge when seeking to deploy KGNNs in memory-constrained\nenvironments, especially for industry-scale graphs.\n  Here we present TinyKG, a memory-efficient GPU-based training framework for\nKGNNs for the tasks of recommendation. Specifically, TinyKG uses exact\nactivations in the forward pass while storing a quantized version of\nactivations in the GPU buffers. During the backward pass, these low-precision\nactivations are dequantized back to full-precision tensors, in order to compute\ngradients. To reduce the quantization errors, TinyKG applies a simple yet\neffective quantization algorithm to compress the activations, which ensures\nunbiasedness with low variance. As such, the training memory footprint of KGNNs\nis largely reduced with negligible accuracy loss. To evaluate the performance\nof our TinyKG, we conduct comprehensive experiments on real-world datasets. We\nfound that our TinyKG with INT2 quantization aggressively reduces the memory\nfootprint of activation maps with $7 \\times$, only with $2\\%$ loss in accuracy,\nallowing us to deploy KGNNs on memory-constrained devices.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.06552,regular,pre_llm,2022,12,"{'ai_likelihood': 4.867712656656901e-06, 'text': 'Domain Adaptation for Dense Retrieval through Self-Supervision by\n  Pseudo-Relevance Labeling\n\n  Although neural information retrieval has witnessed great improvements,\nrecent works showed that the generalization ability of dense retrieval models\non target domains with different distributions is limited, which contrasts with\nthe results obtained with interaction-based models. To address this issue,\nresearchers have resorted to adversarial learning and query generation\napproaches; both approaches nevertheless resulted in limited improvements. In\nthis paper, we propose to use a self-supervision approach in which\npseudo-relevance labels are automatically generated on the target domain. To do\nso, we first use the standard BM25 model on the target domain to obtain a first\nranking of documents, and then use the interaction-based model T53B to re-rank\ntop documents. We further combine this approach with knowledge distillation\nrelying on an interaction-based teacher model trained on the source domain. Our\nexperiments reveal that pseudo-relevance labeling using T53B and the MiniLM\nteacher performs on average better than other approaches and helps improve the\nstate-of-the-art query generation approach GPL when it is fine-tuned on the\npseudo-relevance labeled data.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.00096,regular,pre_llm,2022,12,"{'ai_likelihood': 2.086162567138672e-06, 'text': 'Sentiment Analysis of COVID-19 Public Activity Restriction (PPKM) Impact\n  using BERT Method\n\n  Covid-19 has grown rapidly in all parts of the world and is considered an\ninternational disaster because of its wide-reaching impact. The impact of\nCovid-19 has spread to Indonesia, especially in the slowdown in economic\ngrowth. This was influenced by the implementation of Community Activity\nRestrictions (PPKM) which limited community economic activities. This study\naims to analyze the mapping of public sentiment towards PPKM policies in\nIndonesia during the pandemic based on Twitter data. Knowing the mapping of\npublic sentiment regarding PPKM is expected to help stakeholders in the policy\nevaluation process for each region. The method used is BERT with IndoBERT\nspecific model. The results showed the evaluation value of IndoBERT f-1 score\nreached 84%, precision 86%, and recall 84%. Meanwhile for the evaluation of the\nuse of SVM f-1 score 70%, 72% precision, and 70% recall. Multinominal Na\\""ive\nBayes evaluation shows f-1 score 83%, precision 78%, and recall 80%. As a\nconclusion, BERT method with the IndoBERT model is proven to be higher than\nclassical methods such as SVM and Multinominal Na\\""ive Bayes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.13875,review,pre_llm,2022,12,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Political and Economic Patterns in COVID-19 News: From Lockdown to\n  Vaccination\n\n  The purpose of this study is to analyse COVID-19 related news published\nacross different geographical places, in order to gain insights in reporting\ndifferences. The COVID-19 pandemic had a major outbreak in January 2020 and was\nfollowed by different preventive measures, lockdown, and finally by the process\nof vaccination. To date, more comprehensive analysis of news related to\nCOVID-19 pandemic are missing, especially those which explain what aspects of\nthis pandemic are being reported by newspapers inserted in different economies\nand belonging to different political alignments. Since LDA is often less\ncoherent when there are news articles published across the world about an event\nand you look answers for specific queries. It is because of having semantically\ndifferent content. To address this challenge, we performed pooling of news\narticles based on information retrieval using TF-IDF score in a data processing\nstep and topic modeling using LDA with combination of 1 to 6 ngrams. We used\nVADER sentiment analyzer to analyze the differences in sentiments in news\narticles reported across different geographical places. The novelty of this\nstudy is to look at how COVID-19 pandemic was reported by the media, providing\na comparison among countries in different political and economic contexts. Our\nfindings suggest that the news reporting by newspapers with different political\nalignment support the reported content. Also, economic issues reported by\nnewspapers depend on economy of the place where a newspaper resides.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.00229,regular,pre_llm,2022,12,"{'ai_likelihood': 3.410710228814019e-06, 'text': ""NIR-Prompt: A Multi-task Generalized Neural Information Retrieval\n  Training Framework\n\n  Information retrieval aims to find information that meets users' needs from\nthe corpus. Different needs correspond to different IR tasks such as document\nretrieval, open-domain question answering, retrieval-based dialogue, etc.,\nwhile they share the same schema to estimate the relationship between texts. It\nindicates that a good IR model can generalize to different tasks and domains.\nHowever, previous studies indicate that state-of-the-art neural information\nretrieval (NIR) models, e.g, pre-trained language models (PLMs) are hard to\ngeneralize. Mainly because the end-to-end fine-tuning paradigm makes the model\noveremphasize task-specific signals and domain biases but loses the ability to\ncapture generalized essential signals. To address this problem, we propose a\nnovel NIR training framework named NIR-Prompt for retrieval and reranking\nstages based on the idea of decoupling signal capturing and combination.\nNIR-Prompt exploits Essential Matching Module (EMM) to capture the essential\nmatching signals and gets the description of tasks by Matching Description\nModule (MDM). The description is used as task-adaptation information to combine\nthe essential matching signals to adapt to different tasks. Experiments under\nin-domain multi-task, out-of-domain multi-task, and new task adaptation\nsettings show that NIR-Prompt can improve the generalization of PLMs in NIR for\nboth retrieval and reranking stages compared with baselines.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.03725,regular,pre_llm,2022,12,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Learning-To-Embed: Adopting Transformer based models for E-commerce\n  Products Representation Learning\n\n  Learning low-dimensional representation for large number of products present\nin an e-commerce catalogue plays a vital role as they are helpful in tasks like\nproduct ranking, product recommendation, finding similar products, modelling\nuser-behaviour etc. Recently, a lot of tasks in the NLP field are getting\ntackled using the Transformer based models and these deep models are widely\napplicable in the industries setting to solve various problems. With this\nmotivation, we apply transformer based model for learning contextual\nrepresentation of products in an e-commerce setting. In this work, we propose a\nnovel approach of pre-training transformer based model on a users generated\nsessions dataset obtained from a large fashion e-commerce platform to obtain\nlatent product representation. Once pre-trained, we show that the low-dimension\nrepresentation of the products can be obtained given the product attributes\ninformation as a textual sentence. We mainly pre-train BERT, RoBERTa, ALBERT\nand XLNET variants of transformer model and show a quantitative analysis of the\nproducts representation obtained from these models with respect to Next Product\nRecommendation(NPR) and Content Ranking(CR) tasks. For both the tasks, we\ncollect an evaluation data from the fashion e-commerce platform and observe\nthat XLNET model outperform other variants with a MRR of 0.5 for NPR and NDCG\nof 0.634 for CR. XLNET model also outperforms the Word2Vec based\nnon-transformer baseline on both the downstream tasks. To the best of our\nknowledge, this is the first and novel work for pre-training transformer based\nmodels using users generated sessions data containing products that are\nrepresented with rich attributes information for adoption in e-commerce\nsetting. These models can be further fine-tuned in order to solve various\ndownstream tasks in e-commerce, thereby eliminating the need to train a model\nfrom scratch.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.0412,regular,pre_llm,2022,12,"{'ai_likelihood': 9.271833631727431e-06, 'text': 'Denoising Self-attentive Sequential Recommendation\n\n  Transformer-based sequential recommenders are very powerful for capturing\nboth short-term and long-term sequential item dependencies. This is mainly\nattributed to their unique self-attention networks to exploit pairwise\nitem-item interactions within the sequence. However, real-world item sequences\nare often noisy, which is particularly true for implicit feedback. For example,\na large portion of clicks do not align well with user preferences, and many\nproducts end up with negative reviews or being returned. As such, the current\nuser action only depends on a subset of items, not on the entire sequences.\nMany existing Transformer-based models use full attention distributions, which\ninevitably assign certain credits to irrelevant items. This may lead to\nsub-optimal performance if Transformers are not regularized properly.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.09031,regular,pre_llm,2022,12,"{'ai_likelihood': 5.066394805908203e-06, 'text': 'Marginal-Certainty-aware Fair Ranking Algorithm\n\n  Ranking systems are ubiquitous in modern Internet services, including online\nmarketplaces, social media, and search engines. Traditionally, ranking systems\nonly focus on how to get better relevance estimation. When relevance estimation\nis available, they usually adopt a user-centric optimization strategy where\nranked lists are generated by sorting items according to their estimated\nrelevance. However, such user-centric optimization ignores the fact that item\nproviders also draw utility from ranking systems. It has been shown in existing\nresearch that such user-centric optimization will cause much unfairness to item\nproviders, followed by unfair opportunities and unfair economic gains for item\nproviders.\n  To address ranking fairness, many fair ranking methods have been proposed.\nHowever, as we show in this paper, these methods could be suboptimal as they\ndirectly rely on the relevance estimation without being aware of the\nuncertainty (i.e., the variance of the estimated relevance). To address this\nuncertainty, we propose a novel Marginal-Certainty-aware Fair algorithm named\nMCFair. MCFair jointly optimizes fairness and user utility, while relevance\nestimation is constantly updated in an online manner.\n  In MCFair, we first develop a ranking objective that includes uncertainty,\nfairness, and user utility. Then we directly use the gradient of the ranking\nobjective as the ranking score. We theoretically prove that MCFair based on\ngradients is optimal for the aforementioned ranking objective. Empirically, we\nfind that on semi-synthesized datasets, MCFair is effective and practical and\ncan deliver superior performance compared to state-of-the-art fair ranking\nmethods. To facilitate reproducibility, we release our code\nhttps://github.com/Taosheng-ty/WSDM22-MCFair.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.1391,review,pre_llm,2022,12,"{'ai_likelihood': 1.0, 'text': 'Recommender Systems in E-commerce\n\n  E-commerce recommender systems are becoming increasingly important in the\ncurrent digital world. They are used to personalize user experience, help\ncustomers find what they need quickly and efficiently, and increase revenue for\nthe business. However, there are several challenges associated with big\ndata-based e-commerce recommender systems. These challenges include limited\nresources, data validity period, cold start, long tail problem, scalability. In\nthis paper, we discuss the challenges and potential solutions to overcome these\nchallenges. We also discuss the different types of e-commerce recommender\nsystems, their advantages, and disadvantages. We conclude with some future\nresearch directions to improve the performance of e-commerce recommender\nsystems.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.10699462890625, 'GPT4': 4.690885543823242e-05, 'CLAUDE': 3.62396240234375e-05, 'GOOGLE': 0.890625, 'OPENAI_O_SERIES': 2.3424625396728516e-05, 'DEEPSEEK': 4.0531158447265625e-06, 'GROK': 4.470348358154297e-06, 'NOVA': 6.383657455444336e-05, 'OTHER': 0.002227783203125, 'HUMAN': 2.014636993408203e-05}}"
2212.11735,review,pre_llm,2022,12,"{'ai_likelihood': 4.635916815863716e-07, 'text': 'Response to Moffat\'s Comment on ""Towards Meaningful Statements in IR\n  Evaluation: Mapping Evaluation Measures to Interval Scales""\n\n  Moffat recently commented on our previous work. Our work focused on how\nlaying the foundations of our evaluation methodology into the theory of\nmeasurement can improve our knowledge and understanding of the evaluation\nmeasures we use in IR and how it can shed light on the different types of\nscales adopted by our evaluation measures; we also provided evidence, through\nextensive experimentation, on the impact of the different types of scales on\nthe statistical analyses, as well as on the impact of departing from their\nassumptions. Moreover, we investigated, for the first time in IR, the concept\nof meaningfulness, i.e. the invariance of the experimental statements and\ninferences you draw, and proposed it as a way to ensure more valid and\ngeneralizabile results. Moffat\'s comments build on: (i) misconceptions about\nthe representational theory of measurement, such as what an interval scale\nactually is and what axioms it has to comply with; (ii) they totally miss the\ncentral concept of meaningfulness. Therefore, we reply to Moffat\'s comments by\nproperly framing them in the representational theory of measurement and in the\nconcept of meaningfulness. All in all, we can only reiterate what we said\nseveral times: the goal of this research line is to theoretically ground our\nevaluation methodology - and IR is a field where it is extremely challenging to\nperform any theoretical advances - in order to aim for more robust and\ngeneralizable inferences - something we currently lack in the field. Possibly\nthere are other and better ways to achieve this objective and these proposals\ncould emerge from an open discussion in the field and from the work of others.\nOn the other hand, reducing everything to a contrast on what is (or pretend to\nbe) an interval scale or whether all or none evaluation measures are interval\nscales may be more a barrier from than a help in progressing towards this goal.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.10046,regular,pre_llm,2022,12,"{'ai_likelihood': 1.4735592736138238e-05, 'text': ""Causal Inference for Knowledge Graph based Recommendation\n\n  Knowledge Graph (KG), as a side-information, tends to be utilized to\nsupplement the collaborative filtering (CF) based recommendation model. By\nmapping items with the entities in KGs, prior studies mostly extract the\nknowledge information from the KGs and inject it into the representations of\nusers and items. Despite their remarkable performance, they fail to model the\nuser preference on attributes in the KG, since they ignore that (1) the\nstructure information of KG may hinder the user preference learning, and (2)\nthe user's interacted attributes will result in the bias issue on the\nsimilarity scores.\n  With the help of causality tools, we construct the causal-effect relation\nbetween the variables in KG-based recommendation and identify the reasons\ncausing the mentioned challenges. Accordingly, we develop a new framework,\ntermed Knowledge Graph-based Causal Recommendation (KGCR), which implements the\ndeconfounded user preference learning and adopts counterfactual inference to\neliminate bias in the similarity scoring. Ultimately, we evaluate our proposed\nmodel on three datasets, including Amazon-book, LastFM, and Yelp2018 datasets.\nBy conducting extensive experiments on the datasets, we demonstrate that KGCR\noutperforms several state-of-the-art baselines, such as KGNN-LS, KGAT and KGIN.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
