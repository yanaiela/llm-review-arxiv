[
  {
    "arxiv_id":2001.08371,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000295705,
      "text":"Wrapper Feature Selection Algorithm for the Optimization of an Indicator\n  System of Patent Value Assessment\n\n  Effective patent value assessment provides decision support for patent\ntransection and promotes the practical application of patent technology. The\nlimitations of previous research on patent value assessment were analyzed in\nthis work, and a wrapper-mode feature selection algorithm that is based on\nclassifier prediction accuracy was developed. Verification experiments on\nmultiple UCI standard datasets indicated that the algorithm effectively reduced\nthe size of the feature set and significantly enhanced the prediction accuracy\nof the classifier. When the algorithm was utilized to establish an indicator\nsystem of patent value assessment, the size of the system was reduced, and the\ngeneralization performance of the classifier was enhanced. Sequential forward\nselection was applied to further reduce the size of the indicator set and\ngenerate an optimal indicator system of patent value assessment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.05681,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000034107,
      "text":"Stream-Flow Forecasting of Small Rivers Based on LSTM\n\n  Stream-flow forecasting for small rivers has always been of great importance,\nyet comparatively challenging due to the special features of rivers with\nsmaller volume. Artificial Intelligence (AI) methods have been employed in this\narea for long, but improvement of forecast quality is still on the way. In this\npaper, we tried to provide a new method to do the forecast using the Long-Short\nTerm Memory (LSTM) deep learning model, which aims in the field of time-series\ndata. Utilizing LSTM, we collected the stream flow data from one hydrologic\nstation in Tunxi, China, and precipitation data from 11 rainfall stations\naround to forecast the stream flow data from that hydrologic station 6 hours in\nthe future. We evaluated the prediction results using three criteria: root mean\nsquare error (RMSE), mean absolute error (MAE), and coefficient of\ndetermination (R^2). By comparing LSTM's prediction with predictions of Support\nVector Regression (SVR) and Multilayer Perceptions (MLP) models, we showed that\nLSTM has better performance, achieving RMSE of 82.007, MAE of 27.752, and R^2\nof 0.970. We also did extended experiments on LSTM model, discussing influence\nfactors of its performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.05674,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000064241,
      "text":"Shifted and Squeezed 8-bit Floating Point format for Low-Precision\n  Training of Deep Neural Networks\n\n  Training with larger number of parameters while keeping fast iterations is an\nincreasingly adopted strategy and trend for developing better performing Deep\nNeural Network (DNN) models. This necessitates increased memory footprint and\ncomputational requirements for training. Here we introduce a novel methodology\nfor training deep neural networks using 8-bit floating point (FP8) numbers.\nReduced bit precision allows for a larger effective memory and increased\ncomputational speed. We name this method Shifted and Squeezed FP8 (S2FP8). We\nshow that, unlike previous 8-bit precision training methods, the proposed\nmethod works out-of-the-box for representative models: ResNet-50, Transformer\nand NCF. The method can maintain model accuracy without requiring fine-tuning\nloss scaling parameters or keeping certain layers in single precision. We\nintroduce two learnable statistics of the DNN tensors - shifted and squeezed\nfactors that are used to optimally adjust the range of the tensors in 8-bits,\nthus minimizing the loss in information due to quantization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.04292,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Geometric deep learning for computational mechanics Part I: Anisotropic\n  Hyperelasticity\n\n  This paper is the first attempt to use geometric deep learning and Sobolev\ntraining to incorporate non-Euclidean microstructural data such that\nanisotropic hyperelastic material machine learning models can be trained in the\nfinite deformation range. While traditional hyperelasticity models often\nincorporate homogenized measures of microstructural attributes, such as\nporosity averaged orientation of constitutes, these measures cannot reflect the\ntopological structures of the attributes. We fill this knowledge gap by\nintroducing the concept of weighted graph as a new mean to store topological\ninformation, such as the connectivity of anisotropic grains in assembles. Then,\nby leveraging a graph convolutional deep neural network architecture in the\nspectral domain, we introduce a mechanism to incorporate these non-Euclidean\nweighted graph data directly as input for training and for predicting the\nelastic responses of materials with complex microstructures. To ensure\nsmoothness and prevent non-convexity of the trained stored energy functional,\nwe introduce a Sobolev training technique for neural networks such that stress\nmeasure is obtained implicitly from taking directional derivatives of the\ntrained energy functional. By optimizing the neural network to approximate both\nthe energy functional output and the stress measure, we introduce a training\nprocedure the improves efficiency and generalize the learned energy functional\nfor different microstructures. The trained hybrid neural network model is then\nused to generate new stored energy functional for unseen microstructures in a\nparametric study to predict the influence of elastic anisotropy on the\nnucleation and propagation of fracture in the brittle regime.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.04889,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000181794,
      "text":"Physical-Virtual Collaboration Modeling for Intra-and Inter-Station\n  Metro Ridership Prediction\n\n  Due to the widespread applications in real-world scenarios, metro ridership\nprediction is a crucial but challenging task in intelligent transportation\nsystems. However, conventional methods either ignore the topological\ninformation of metro systems or directly learn on physical topology, and cannot\nfully explore the patterns of ridership evolution. To address this problem, we\nmodel a metro system as graphs with various topologies and propose a unified\nPhysical-Virtual Collaboration Graph Network (PVCGN), which can effectively\nlearn the complex ridership patterns from the tailor-designed graphs.\nSpecifically, a physical graph is directly built based on the realistic\ntopology of the studied metro system, while a similarity graph and a\ncorrelation graph are built with virtual topologies under the guidance of the\ninter-station passenger flow similarity and correlation. These complementary\ngraphs are incorporated into a Graph Convolution Gated Recurrent Unit (GC-GRU)\nfor spatial-temporal representation learning. Further, a Fully-Connected Gated\nRecurrent Unit (FC-GRU) is also applied to capture the global evolution\ntendency. Finally, we develop a Seq2Seq model with GC-GRU and FC-GRU to\nforecast the future metro ridership sequentially. Extensive experiments on two\nlarge-scale benchmarks (e.g., Shanghai Metro and Hangzhou Metro) well\ndemonstrate the superiority of our PVCGN for station-level metro ridership\nprediction. Moreover, we apply the proposed PVCGN to address the online\norigin-destination (OD) ridership prediction and the experiment results show\nthe universality of our method. Our code and benchmarks are available at\nhttps:\/\/github.com\/HCPLab-SYSU\/PVCGN.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.06761,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"Hybrid Embedded Deep Stacked Sparse Autoencoder with w_LPPD SVM Ensemble\n\n  Deep learning is a kind of feature learning method with strong nonliear\nfeature transformation and becomes more and more important in many fields of\nartificial intelligence. Deep autoencoder is one representative method of the\ndeep learning methods, and can effectively extract abstract the information of\ndatasets. However, it does not consider the complementarity between the deep\nfeatures and original features during deep feature transformation. Besides, it\nsuffers from small sample problem. In order to solve these problems, a novel\ndeep autoencoder - hybrid feature embedded stacked sparse autoencoder(HESSAE)\nhas been proposed in this paper. HFESAE is capable to learn discriminant deep\nfeatures with the help of embedding original features to filter weak\nhidden-layer outputs during training. For the issue that class representation\nability of abstract information is limited by small sample problem, a feature\nfusion strategy has been designed aiming to combining abstract information\nlearned by HFESAE with original feature and obtain hybrid features for feature\nreduction. The strategy is hybrid feature selection strategy based on L1\nregularization followed by an support vector machine(SVM) ensemble model, in\nwhich weighted local discriminant preservation projection (w_LPPD), is designed\nand employed on each base classifier. At the end of this paper, several\nrepresentative public datasets are used to verify the effectiveness of the\nproposed algorithm. The experimental results demonstrated that, the proposed\nfeature learning method yields superior performance compared to other existing\nand state of art feature learning algorithms including some representative deep\nautoencoder methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.03253,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Introduction to deep learning\n\n  Deep Learning (DL) has made a major impact on data science in the last\ndecade. This chapter introduces the basic concepts of this field. It includes\nboth the basic structures used to design deep neural networks and a brief\nsurvey of some of its popular use cases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.11919,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000101659,
      "text":"Network Cooperation with Progressive Disambiguation for Partial Label\n  Learning\n\n  Partial Label Learning (PLL) aims to train a classifier when each training\ninstance is associated with a set of candidate labels, among which only one is\ncorrect but is not accessible during the training phase. The common strategy\ndealing with such ambiguous labeling information is to disambiguate the\ncandidate label sets. Nonetheless, existing methods ignore the disambiguation\ndifficulty of instances and adopt the single-trend training mechanism. The\nformer would lead to the vulnerability of models to the false positive labels\nand the latter may arouse error accumulation problem. To remedy these two\ndrawbacks, this paper proposes a novel approach termed \"Network Cooperation\nwith Progressive Disambiguation\" (NCPD) for PLL. Specifically, we devise a\nprogressive disambiguation strategy of which the disambiguation operations are\nperformed on simple instances firstly and then gradually on more complicated\nones. Therefore, the negative impacts brought by the false positive labels of\ncomplicated instances can be effectively mitigated as the disambiguation\nability of the model has been strengthened via learning from the simple\ninstances. Moreover, by employing artificial neural networks as the backbone,\nwe utilize a network cooperation mechanism which trains two networks\ncollaboratively by letting them interact with each other. As two networks have\ndifferent disambiguation ability, such interaction is beneficial for both\nnetworks to reduce their respective disambiguation errors, and thus is much\nbetter than the existing algorithms with single-trend training process.\nExtensive experimental results on various benchmark and practical datasets\ndemonstrate the superiority of our NCPD to other state-of-the-art PLL methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.03869,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"A Framework for Semi-Automatic Precision and Accuracy Analysis for Fast\n  and Rigorous Deep Learning\n\n  Deep Neural Networks (DNN) represent a performance-hungry application.\nFloating-Point (FP) and custom floating-point-like arithmetic satisfies this\nhunger. While there is need for speed, inference in DNNs does not seem to have\nany need for precision. Many papers experimentally observe that DNNs can\nsuccessfully run at almost ridiculously low precision.\n  The aim of this paper is two-fold: first, to shed some theoretical light upon\nwhy a DNN's FP accuracy stays high for low FP precision. We observe that the\nloss of relative accuracy in the convolutional steps is recovered by the\nactivation layers, which are extremely well-conditioned. We give an\ninterpretation for the link between precision and accuracy in DNNs.\n  Second, the paper presents a software framework for semi-automatic FP error\nanalysis for the inference phase of deep-learning. Compatible with common\nTensorflow\/Keras models, it leverages the frugally-deep Python\/C++ library to\ntransform a neural network into C++ code in order to analyze the network's need\nfor precision. This rigorous analysis is based on Interval and Affine\narithmetics to compute absolute and relative error bounds for a DNN. We\ndemonstrate our tool with several examples.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.07051,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Retrain or not retrain? -- efficient pruning methods of deep CNN\n  networks\n\n  Convolutional neural networks (CNN) play a major role in image processing\ntasks like image classification, object detection, semantic segmentation. Very\noften CNN networks have from several to hundred stacked layers with several\nmegabytes of weights. One of the possible methods to reduce complexity and\nmemory footprint is pruning. Pruning is a process of removing weights which\nconnect neurons from two adjacent layers in the network. The process of finding\nnear optimal solution with specified drop in accuracy can be more sophisticated\nwhen DL model has higher number of convolutional layers. In the paper few\napproaches based on retraining and no retraining are described and compared\ntogether.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.08232,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000024504,
      "text":"CoLES: Contrastive Learning for Event Sequences with Self-Supervision\n\n  We address the problem of self-supervised learning on discrete event\nsequences generated by real-world users. Self-supervised learning incorporates\ncomplex information from the raw data in low-dimensional fixed-length vector\nrepresentations that could be easily applied in various downstream machine\nlearning tasks. In this paper, we propose a new method \"CoLES\", which adapts\ncontrastive learning, previously used for audio and computer vision domains, to\nthe discrete event sequences domain in a self-supervised setting. We deployed\nCoLES embeddings based on sequences of transactions at the large European\nfinancial services company. Usage of CoLES embeddings significantly improves\nthe performance of the pre-existing models on downstream tasks and produces\nsignificant financial gains, measured in hundreds of millions of dollars\nyearly. We also evaluated CoLES on several public event sequences datasets and\nshowed that CoLES representations consistently outperform other methods on\ndifferent downstream tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.08224,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0001326534,
      "text":"A Survey on Predictive Maintenance for Industry 4.0\n\n  Production issues at Volkswagen in 2016 lead to dramatic losses in sales of\nup to 400 million Euros per week. This example shows the huge financial impact\nof a working production facility for companies. Especially in the data-driven\ndomains of Industry 4.0 and Industrial IoT with intelligent, connected\nmachines, a conventional, static maintenance schedule seems to be\nold-fashioned. In this paper, we present a survey on the current state of the\nart in predictive maintenance for Industry 4.0. Based on a structured literate\nsurvey, we present a classification of predictive maintenance in the context of\nIndustry 4.0 and discuss recent developments in this area.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.04295,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000032451,
      "text":"A scheme for automatic differentiation of complex loss functions\n\n  For a real function, automatic differentiation is such a standard algorithm\nused to efficiently compute its gradient, that it is integrated in various\nneural network frameworks. However, despite the recent advances in using\ncomplex functions in machine learning and the well-established usefulness of\nautomatic differentiation, the support of automatic differentiation for complex\nfunctions is not as well-established and widespread as for real functions. In\nthis work we propose an efficient and seamless scheme to implement automatic\ndifferentiation for complex functions, which is a compatible generalization of\nthe current scheme for real functions. This scheme can significantly simplify\nthe implementation of neural networks which use complex numbers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.11127,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000019537,
      "text":"Kernel Operations on the GPU, with Autodiff, without Memory Overflows\n\n  The KeOps library provides a fast and memory-efficient GPU support for\ntensors whose entries are given by a mathematical formula, such as kernel and\ndistance matrices. KeOps alleviates the major bottleneck of tensor-centric\nlibraries for kernel and geometric applications: memory consumption. It also\nsupports automatic differentiation and outperforms standard GPU baselines,\nincluding PyTorch CUDA tensors or the Halide and TVM libraries. KeOps combines\noptimized C++\/CUDA schemes with binders for high-level languages: Python (Numpy\nand PyTorch), Matlab and GNU R. As a result, high-level \"quadratic\" codes can\nnow scale up to large data sets with millions of samples processed in seconds.\nKeOps brings graphics-like performances for kernel methods and is freely\navailable on standard repositories (PyPi, CRAN). To showcase its versatility,\nwe provide tutorials in a wide range of settings online at\n\\url{www.kernel-operations.io}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.06037,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000046028,
      "text":"Prediction of properties of steel alloys\n\n  We present a study of possible predictors based on four supervised machine\nlearning models for the prediction of four mechanical properties of the main\nindustrially used steels. The results were obtained from an experimental\ndatabase available in the literature which were used as input to train and\nevaluate the models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.05739,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"Technical report: Training Mixture Density Networks with full covariance\n  matrices\n\n  Mixture Density Networks are a tried and tested tool for modelling\nconditional probability distributions. As such, they constitute a great\nbaseline for novel approaches to this problem. In the standard formulation, an\nMDN takes some input and outputs parameters for a Gaussian mixture model with\nrestrictions on the mixture components' covariance. Since covariance between\nrandom variables is a central issue in the conditional modeling problems we\nwere investigating, I derived and implemented an MDN formulation with\nunrestricted covariances. It is likely that this has been done before, but I\ncould not find any resources online. For this reason, I have documented my\napproach in the form of this technical report, in hopes that it may be useful\nto others facing a similar situation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.09603,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000299348,
      "text":"Dynamic Sampling and Selective Masking for Communication-Efficient\n  Federated Learning\n\n  Federated learning (FL) is a novel machine learning setting that enables\non-device intelligence via decentralized training and federated optimization.\nDeep neural networks' rapid development facilitates the learning techniques for\nmodeling complex problems and emerges into federated deep learning under the\nfederated setting. However, the tremendous amount of model parameters burdens\nthe communication network with a high load of transportation. This paper\nintroduces two approaches for improving communication efficiency by dynamic\nsampling and top-$k$ selective masking. The former controls the fraction of\nselected client models dynamically, while the latter selects parameters with\ntop-$k$ largest values of difference for federated updating. Experiments on\nconvolutional image classification and recurrent language modeling are\nconducted on three public datasets to show our proposed methods' effectiveness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.04644,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"On the Ethics of Building AI in a Responsible Manner\n\n  The AI-alignment problem arises when there is a discrepancy between the goals\nthat a human designer specifies to an AI learner and a potential catastrophic\noutcome that does not reflect what the human designer really wants. We argue\nthat a formalism of AI alignment that does not distinguish between strategic\nand agnostic misalignments is not useful, as it deems all technology as\nun-safe. We propose a definition of a strategic-AI-alignment and prove that\nmost machine learning algorithms that are being used in practice today do not\nsuffer from the strategic-AI-alignment problem. However, without being careful,\ntoday's technology might lead to strategic misalignment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.1362,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Latent-Graph Learning for Disease Prediction\n\n  Recently, Graph Convolutional Networks (GCNs) have proven to be a powerful\nmachine learning tool for Computer-Aided Diagnosis (CADx) and disease\nprediction. A key component in these models is to build a population graph,\nwhere the graph adjacency matrix represents pair-wise patient similarities.\nUntil now, the similarity metrics have been defined manually, usually based on\nmeta-features like demographics or clinical scores. The definition of the\nmetric, however, needs careful tuning, as GCNs are very sensitive to the graph\nstructure. In this paper, we demonstrate for the first time in the CADx domain\nthat it is possible to learn a single, optimal graph towards the GCN's\ndownstream task of disease classification. To this end, we propose a novel,\nend-to-end trainable graph learning architecture for dynamic and localized\ngraph pruning. Unlike commonly employed spectral GCN approaches, our GCN is\nspatial and inductive, and can thus infer previously unseen patients as well.\nWe demonstrate significant classification improvements with our learned graph\non two CADx problems in medicine. We further explain and visualize this result\nusing an artificial dataset, underlining the importance of graph learning for\nmore accurate and robust inference with GCNs in medical applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.04815,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000038081,
      "text":"Learning Unsplit-field-based PML for the FDTD Method by Deep\n  Differentiable Forest\n\n  Alternative unsplit-filed-based absorbing boundary condition (ABC)\ncomputation approach for the finite-difference time-domain (FDTD) is\nefficiently proposed based on the deep differentiable forest. The deep\ndifferentiable forest (DDF) model is introduced to replace the conventional\nperfectly matched layer (PML) ABC during the computation process of FDTD. The\nfield component data on the interface of traditional PML are adopted to train\nthe DDF-based PML model. DDF has the advantages of both trees and neural\nnetworks. Its tree structure is easy to use and explain for the numerical PML\ndata. It has full differentiability like neural networks. DDF could be trained\nby powerful techniques from deep learning. So compared to the traditional PML\nimplementation, the proposed method can greatly reduce the size of FDTD\nphysical domain and the calculation complexity of FDTD due to the novel model\nwhich only involves the one-cell thickness of boundary layer. Numerical\nsimulations have been carried out to benchmark the performance of the proposed\napproach. Numerical results illustrate that the proposed method can not only\neasily replace the traditional PML, but also be integrated into the FDTD\ncomputation process with satisfactory numerical accuracy and compatibility to\nthe FDTD.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.04328,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"A Brief Prehistory of Double Descent\n\n  In their thought-provoking paper [1], Belkin et al. illustrate and discuss\nthe shape of risk curves in the context of modern high-complexity learners.\nGiven a fixed training sample size $n$, such curves show the risk of a learner\nas a function of some (approximate) measure of its complexity $N$. With $N$ the\nnumber of features, these curves are also referred to as feature curves. A\nsalient observation in [1] is that these curves can display, what they call,\ndouble descent: with increasing $N$, the risk initially decreases, attains a\nminimum, and then increases until $N$ equals $n$, where the training data is\nfitted perfectly. Increasing $N$ even further, the risk decreases a second and\nfinal time, creating a peak at $N=n$. This twofold descent may come as a\nsurprise, but as opposed to what [1] reports, it has not been overlooked\nhistorically. Our letter draws attention to some original, earlier findings, of\ninterest to contemporary machine learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.14382,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000026491,
      "text":"Transfer Learning for Thermal Comfort Prediction in Multiple Cities\n\n  HVAC (Heating, Ventilation and Air Conditioning) system is an important part\nof a building, which constitutes up to 40% of building energy usage. The main\npurpose of HVAC, maintaining appropriate thermal comfort, is crucial for the\nbest utilisation of energy usage. Besides, thermal comfort is also crucial for\nwell-being, health, and work productivity. Recently, data-driven thermal\ncomfort models have got better performance than traditional knowledge-based\nmethods (e.g. Predicted Mean Vote Model). An accurate thermal comfort model\nrequires a large amount of self-reported thermal comfort data from indoor\noccupants which undoubtedly remains a challenge for researchers. In this\nresearch, we aim to tackle this data-shortage problem and boost the performance\nof thermal comfort prediction. We utilise sensor data from multiple cities in\nthe same climate zone to learn thermal comfort patterns. We present a transfer\nlearning based multilayer perceptron model from the same climate zone\n(TL-MLP-C*) for accurate thermal comfort prediction. Extensive experimental\nresults on ASHRAE RP-884, the Scales Project and Medium US Office datasets show\nthat the performance of the proposed TL-MLP-C* exceeds the state-of-the-art\nmethods in accuracy, precision and F1-score.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.14476,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000083778,
      "text":"SIPA: A Simple Framework for Efficient Networks\n\n  With the success of deep learning in various fields and the advent of\nnumerous Internet of Things (IoT) devices, it is essential to lighten models\nsuitable for low-power devices. In keeping with this trend, MicroNet Challenge,\nwhich is the challenge to build efficient models from the view of both storage\nand computation, was hosted at NeurIPS 2019. To develop efficient models\nthrough this challenge, we propose a framework, coined as SIPA, consisting of\nfour stages: Searching, Improving, Pruning, and Accelerating. With the proposed\nframework, our team, OSI AI, compressed 334x the parameter storage and 357x the\nmath operation compared to WideResNet-28-10 and took 4th place in the CIFAR-100\ntrack at MicroNet Challenge 2019 with the top 10% highly efficient computation.\nOur source code is available from https:\/\/github.com\/Lee-Gihun\/MicroNet_OSI-AI.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.05244,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Efficient Sampled Softmax for Tensorflow\n\n  This short paper discusses an efficient implementation of \\emph{sampled\nsoftmax loss} for Tensorflow. The speedup over the default implementation is\nachieved due to simplification of the graph for the forward and backward\npasses.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.04892,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000028478,
      "text":"SR2CNN: Zero-Shot Learning for Signal Recognition\n\n  Signal recognition is one of significant and challenging tasks in the signal\nprocessing and communications field. It is often a common situation that\nthere's no training data accessible for some signal classes to perform a\nrecognition task. Hence, as widely-used in image processing field, zero-shot\nlearning (ZSL) is also very important for signal recognition. Unfortunately,\nZSL regarding this field has hardly been studied due to inexplicable signal\nsemantics. This paper proposes a ZSL framework, signal recognition and\nreconstruction convolutional neural networks (SR2CNN), to address relevant\nproblems in this situation. The key idea behind SR2CNN is to learn the\nrepresentation of signal semantic feature space by introducing a proper\ncombination of cross entropy loss, center loss and autoencoder loss, as well as\nadopting a suitable distance metric space such that semantic features have\ngreater minimal inter-class distance than maximal intra-class distance. The\nproposed SR2CNN can discriminate signals even if no training data is available\nfor some signal class. Moreover, SR2CNN can gradually improve itself in the aid\nof signal detection, because of constantly refined class center vectors in\nsemantic feature space. These merits are all verified by extensive experiments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.08152,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Continuous Representation of Molecules Using Graph Variational\n  Autoencoder\n\n  In order to continuously represent molecules, we propose a generative model\nin the form of a VAE which is operating on the 2D-graph structure of molecules.\nA side predictor is employed to prune the latent space and help the decoder in\ngenerating meaningful adjacency tensor of molecules. Other than the potential\napplicability in drug design and property prediction, we show the superior\nperformance of this technique in comparison to other similar methods based on\nthe SMILES representation of the molecules with RNN based encoder and decoder.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.12088,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000031127,
      "text":"SplitFed: When Federated Learning Meets Split Learning\n\n  Federated learning (FL) and split learning (SL) are two popular distributed\nmachine learning approaches. Both follow a model-to-data scenario; clients\ntrain and test machine learning models without sharing raw data. SL provides\nbetter model privacy than FL due to the machine learning model architecture\nsplit between clients and the server. Moreover, the split model makes SL a\nbetter option for resource-constrained environments. However, SL performs\nslower than FL due to the relay-based training across multiple clients. In this\nregard, this paper presents a novel approach, named splitfed learning (SFL),\nthat amalgamates the two approaches eliminating their inherent drawbacks, along\nwith a refined architectural configuration incorporating differential privacy\nand PixelDP to enhance data privacy and model robustness. Our analysis and\nempirical results demonstrate that (pure) SFL provides similar test accuracy\nand communication efficiency as SL while significantly decreasing its\ncomputation time per global epoch than in SL for multiple clients. Furthermore,\nas in SL, its communication efficiency over FL improves with the number of\nclients. Besides, the performance of SFL with privacy and robustness measures\nis further evaluated under extended experimental settings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.03451,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"An Experimental Study of Reduced-Voltage Operation in Modern FPGAs for\n  Neural Network Acceleration\n\n  We empirically evaluate an undervolting technique, i.e., underscaling the\ncircuit supply voltage below the nominal level, to improve the power-efficiency\nof Convolutional Neural Network (CNN) accelerators mapped to Field Programmable\nGate Arrays (FPGAs). Undervolting below a safe voltage level can lead to timing\nfaults due to excessive circuit latency increase. We evaluate the\nreliability-power trade-off for such accelerators. Specifically, we\nexperimentally study the reduced-voltage operation of multiple components of\nreal FPGAs, characterize the corresponding reliability behavior of CNN\naccelerators, propose techniques to minimize the drawbacks of reduced-voltage\noperation, and combine undervolting with architectural CNN optimization\ntechniques, i.e., quantization and pruning. We investigate the effect of\nenvironmental temperature on the reliability-power trade-off of such\naccelerators. We perform experiments on three identical samples of modern\nXilinx ZCU102 FPGA platforms with five state-of-the-art image classification\nCNN benchmarks. This approach allows us to study the effects of our\nundervolting technique for both software and hardware variability. We achieve\nmore than 3X power-efficiency (GOPs\/W) gain via undervolting. 2.6X of this gain\nis the result of eliminating the voltage guardband region, i.e., the safe\nvoltage region below the nominal level that is set by FPGA vendor to ensure\ncorrect functionality in worst-case environmental and circuit conditions. 43%\nof the power-efficiency gain is due to further undervolting below the\nguardband, which comes at the cost of accuracy loss in the CNN accelerator. We\nevaluate an effective frequency underscaling technique that prevents this\naccuracy loss, and find that it reduces the power-efficiency gain from 43% to\n25%.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.12401,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000051988,
      "text":"Wind Speed Prediction and Visualization Using Long Short-Term Memory\n  Networks (LSTM)\n\n  Climate change is one of the most concerning issues of this century. Emission\nfrom electric power generation is a crucial factor that drives the concern to\nthe next level. Renewable energy sources are widespread and available globally,\nhowever, one of the major challenges is to understand their characteristics in\na more informative way. This paper proposes the prediction of wind speed that\nsimplifies wind farm planning and feasibility study. Twelve artificial\nintelligence algorithms were used for wind speed prediction from collected\nmeteorological parameters. The model performances were compared to determine\nthe wind speed prediction accuracy. The results show a deep learning approach,\nlong short-term memory (LSTM) outperforms other models with the highest\naccuracy of 97.8%.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.01194,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"An empirical comparison of deep-neural-network architectures for next\n  activity prediction using context-enriched process event logs\n\n  Researchers have proposed a variety of predictive business process monitoring\n(PBPM) techniques aiming to predict future process behaviour during the process\nexecution. Especially, techniques for the next activity prediction anticipate\ngreat potential in improving operational business processes. To gain more\naccurate predictions, a plethora of these techniques rely on deep neural\nnetworks (DNNs) and consider information about the context, in which the\nprocess is running. However, an in-depth comparison of such techniques is\nmissing in the PBPM literature, which prevents researchers and practitioners\nfrom selecting the best solution for a given event log. To remedy this problem,\nwe empirically evaluate the predictive quality of three promising DNN\narchitectures, combined with five proven encoding techniques and based on five\ncontext-enriched real-life event logs. We provide four findings that can\nsupport researchers and practitioners in designing novel PBPM techniques for\npredicting the next activities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.02563,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000002616,
      "text":"EDD: Efficient Differentiable DNN Architecture and Implementation\n  Co-search for Embedded AI Solutions\n\n  High quality AI solutions require joint optimization of AI algorithms and\ntheir hardware implementations. In this work, we are the first to propose a\nfully simultaneous, efficient differentiable DNN architecture and\nimplementation co-search (EDD) methodology. We formulate the co-search problem\nby fusing DNN search variables and hardware implementation variables into one\nsolution space, and maximize both algorithm accuracy and hardware\nimplementation quality. The formulation is differentiable with respect to the\nfused variables, so that gradient descent algorithm can be applied to greatly\nreduce the search time. The formulation is also applicable for various devices\nwith different objectives. In the experiments, we demonstrate the effectiveness\nof our EDD methodology by searching for three representative DNNs, targeting\nlow-latency GPU implementation and FPGA implementations with both recursive and\npipelined architectures. Each model produced by EDD achieves similar accuracy\nas the best existing DNN models searched by neural architecture search (NAS)\nmethods on ImageNet, but with superior performance obtained within 12 GPU-hour\nsearches. Our DNN targeting GPU is 1.40x faster than the state-of-the-art\nsolution reported in Proxyless, and our DNN targeting FPGA delivers 1.45x\nhigher throughput than the state-of-the-art solution reported in DNNBuilder.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.04321,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"A Showcase of the Use of Autoencoders in Feature Learning Applications\n\n  Autoencoders are techniques for data representation learning based on\nartificial neural networks. Differently to other feature learning methods which\nmay be focused on finding specific transformations of the feature space, they\ncan be adapted to fulfill many purposes, such as data visualization, denoising,\nanomaly detection and semantic hashing. This work presents these applications\nand provides details on how autoencoders can perform them, including code\nsamples making use of an R package with an easy-to-use interface for\nautoencoder design and training, \\texttt{ruta}. Along the way, the explanations\non how each learning task has been achieved are provided with the aim to help\nthe reader design their own autoencoders for these or other objectives.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.1064,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"DETECT: A Hierarchical Clustering Algorithm for Behavioural Trends in\n  Temporal Educational Data\n\n  Techniques for clustering student behaviour offer many opportunities to\nimprove educational outcomes by providing insight into student learning.\nHowever, one important aspect of student behaviour, namely its evolution over\ntime, can often be challenging to identify using existing methods. This is\nbecause the objective functions used by these methods do not explicitly aim to\nfind cluster trends in time, so these trends may not be clearly represented in\nthe results. This paper presents `DETECT' (Detection of Educational Trends\nElicited by Clustering Time-series data), a novel divisive hierarchical\nclustering algorithm that incorporates temporal information into its objective\nfunction to prioritise the detection of behavioural trends. The resulting\nclusters are similar in structure to a decision tree, with a hierarchy of\nclusters defined by decision rules on features. DETECT is easy to apply, highly\ncustomisable, applicable to a wide range of educational datasets and yields\neasily interpretable results. Through a case study of two online programming\ncourses (N>600), this paper demonstrates two example applications of DETECT: 1)\nto identify how cohort behaviour develops over time and 2) to identify student\nbehaviours that characterise exercises where many students give up.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.12193,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000137091,
      "text":"Feature Statistics Guided Efficient Filter Pruning\n\n  Building compact convolutional neural networks (CNNs) with reliable\nperformance is a critical but challenging task, especially when deploying them\nin real-world applications. As a common approach to reduce the size of CNNs,\npruning methods delete part of the CNN filters according to some metrics such\nas $l1$-norm. However, previous methods hardly leverage the information\nvariance in a single feature map and the similarity characteristics among\nfeature maps. In this paper, we propose a novel filter pruning method, which\nincorporates two kinds of feature map selections: diversity-aware selection\n(DFS) and similarity-aware selection (SFS). DFS aims to discover features with\nlow information diversity while SFS removes features that have high\nsimilarities with others. We conduct extensive empirical experiments with\nvarious CNN architectures on publicly available datasets. The experimental\nresults demonstrate that our model obtains up to 91.6% parameter decrease and\n83.7% FLOPs reduction with almost no accuracy loss.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.04198,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"EnK: Encoding time-information in convolution\n\n  Recent development in deep learning techniques has attracted attention in\ndecoding and classification in EEG signals. Despite several efforts utilizing\ndifferent features of EEG signals, a significant research challenge is to use\ntime-dependent features in combination with local and global features. There\nhave been several efforts to remodel the deep learning convolution neural\nnetworks (CNNs) to capture time-dependency information by incorporating\nhand-crafted features, slicing the input data in a smaller time-windows, and\nrecurrent convolution. However, these approaches partially solve the problem,\nbut simultaneously hinder the CNN's capability to learn from unknown\ninformation that might be present in the data. To solve this, we have proposed\na novel time encoding kernel (EnK) approach, which introduces the increasing\ntime information during convolution operation in CNN. The encoded information\nby EnK lets CNN learn time-dependent features in-addition to local and global\nfeatures. We performed extensive experiments on several EEG datasets: cognitive\nconflict (CC), physical-human robot collaboration (pHRC), P300 visual-evoked\npotentials, movement-related cortical potentials (MRCP). EnK outperforms the\nstate-of-art by 12\\% (F1 score). Moreover, the EnK approach required only one\nadditional parameter to learn and can be applied to a virtually any CNN\narchitectures with minimal efforts. These results support our methodology and\nshow high potential to improve CNN performance in the context of time-series\ndata in general.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.16765,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000168549,
      "text":"Federated Mutual Learning\n\n  Federated learning (FL) enables collaboratively training deep learning models\non decentralized data. However, there are three types of heterogeneities in FL\nsetting bringing about distinctive challenges to the canonical federated\nlearning algorithm (FedAvg). First, due to the Non-IIDness of data, the global\nshared model may perform worse than local models that solely trained on their\nprivate data; Second, the objective of center server and clients may be\ndifferent, where center server seeks for a generalized model whereas client\npursue a personalized model, and clients may run different tasks; Third,\nclients may need to design their customized model for various scenes and tasks;\nIn this work, we present a novel federated learning paradigm, named Federated\nMutual Leaning (FML), dealing with the three heterogeneities. FML allows\nclients training a generalized model collaboratively and a personalized model\nindependently, and designing their private customized models. Thus, the\nNon-IIDness of data is no longer a bug but a feature that clients can be\npersonally served better. The experiments show that FML can achieve better\nperformance than alternatives in typical FL setting, and clients can be\nbenefited from FML with different models and tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.04403,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000048015,
      "text":"Global Robustness Verification Networks\n\n  The wide deployment of deep neural networks, though achieving great success\nin many domains, has severe safety and reliability concerns. Existing\nadversarial attack generation and automatic verification techniques cannot\nformally verify whether a network is globally robust, i.e., the absence or not\nof adversarial examples in the input space. To address this problem, we develop\na global robustness verification framework with three components: 1) a novel\nrule-based ``back-propagation'' finding which input region is responsible for\nthe class assignment by logic reasoning; 2) a new network architecture Sliding\nDoor Network (SDN) enabling feasible rule-based ``back-propagation''; 3) a\nregion-based global robustness verification (RGRV) approach. Moreover, we\ndemonstrate the effectiveness of our approach on both synthetic and real\ndatasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.07129,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Federated and continual learning for classification tasks in a society\n  of devices\n\n  Today we live in a context in which devices are increasingly interconnected\nand sensorized and are almost ubiquitous. Deep learning has become in recent\nyears a popular way to extract knowledge from the huge amount of data that\nthese devices are able to collect. Nevertheless, centralized state-of-the-art\nlearning methods have a number of drawbacks when facing real distributed\nproblems, in which the available information is usually private, partial,\nbiased and evolving over time. Federated learning is a popular framework that\nallows multiple distributed devices to train models remotely, collaboratively,\nand preserving data privacy. However, the current proposals in federated\nlearning focus on deep architectures that in many cases are not feasible to\nimplement in non-dedicated devices such as smartphones. Also, little research\nhas been done regarding the scenario where data distribution changes over time\nin unforeseen ways, causing what is known as concept drift. Therefore, in this\nwork we want to present Light Federated and Continual Consensus (LFedCon2), a\nnew federated and continual architecture that uses light, traditional learners.\nOur method allows powerless devices (such as smartphones or robots) to learn in\nreal time, locally, continuously, autonomously and from users, but also\nimproving models globally, in the cloud, combining what is learned locally, in\nthe devices. In order to test our proposal, we have applied it in a\nheterogeneous community of smartphone users to solve the problem of walking\nrecognition. The results show the advantages that LFedCon2 provides with\nrespect to other state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.04432,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"AdaDeep: A Usage-Driven, Automated Deep Model Compression Framework for\n  Enabling Ubiquitous Intelligent Mobiles\n\n  Recent breakthroughs in Deep Neural Networks (DNNs) have fueled a\ntremendously growing demand for bringing DNN-powered intelligence into mobile\nplatforms. While the potential of deploying DNNs on resource-constrained\nplatforms has been demonstrated by DNN compression techniques, the current\npractice suffers from two limitations: 1) merely stand-alone compression\nschemes are investigated even though each compression technique only suit for\ncertain types of DNN layers; and 2) mostly compression techniques are optimized\nfor DNNs' inference accuracy, without explicitly considering other\napplication-driven system performance (e.g., latency and energy cost) and the\nvarying resource availability across platforms (e.g., storage and processing\ncapability). To this end, we propose AdaDeep, a usage-driven, automated DNN\ncompression framework for systematically exploring the desired trade-off\nbetween performance and resource constraints, from a holistic system level.\nSpecifically, in a layer-wise manner, AdaDeep automatically selects the most\nsuitable combination of compression techniques and the corresponding\ncompression hyperparameters for a given DNN. Thorough evaluations on six\ndatasets and across twelve devices demonstrate that AdaDeep can achieve up to\n$18.6\\times$ latency reduction, $9.8\\times$ energy-efficiency improvement, and\n$37.3\\times$ storage reduction in DNNs while incurring negligible accuracy\nloss. Furthermore, AdaDeep also uncovers multiple novel combinations of\ncompression techniques.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.01173,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000086758,
      "text":"An Investigation on Deep Learning with Beta Stabilizer\n\n  Artificial neural networks (ANN) have been used in many applications such\nlike handwriting recognition and speech recognition. It is well-known that\nlearning rate is a crucial value in the training procedure for artificial\nneural networks. It is shown that the initial value of learning rate can\nconfoundedly affect the final result and this value is always set manually in\npractice. A new parameter called beta stabilizer has been introduced to reduce\nthe sensitivity of the initial learning rate. But this method has only been\nproposed for deep neural network (DNN) with sigmoid activation function. In\nthis paper we extended beta stabilizer to long short-term memory (LSTM) and\ninvestigated the effects of beta stabilizer parameters on different models,\nincluding LSTM and DNN with relu activation function. It is concluded that beta\nstabilizer parameters can reduce the sensitivity of learning rate with almost\nthe same performance on DNN with relu activation function and LSTM. However, it\nis shown that the effects of beta stabilizer on DNN with relu activation\nfunction and LSTM are fewer than the effects on DNN with sigmoid activation\nfunction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.12617,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"Attention Mechanism for Multivariate Time Series Recurrent Model\n  Interpretability Applied to the Ironmaking Industry\n\n  Data-driven model interpretability is a requirement to gain the acceptance of\nprocess engineers to rely on the prediction of a data-driven model to regulate\nindustrial processes in the ironmaking industry. In the research presented in\nthis paper, we focus on the development of an interpretable multivariate time\nseries forecasting deep learning architecture for the temperature of the hot\nmetal produced by a blast furnace. A Long Short-Term Memory (LSTM) based\narchitecture enhanced with attention mechanism and guided backpropagation is\nproposed to accommodate the prediction with a local temporal interpretability\nfor each input. Results are showing high potential for this architecture\napplied to blast furnace data and providing interpretability correctly\nreflecting the true complex variables relations dictated by the inherent blast\nfurnace process, and with reduced prediction error compared to a\nrecurrent-based deep learning architecture.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.10818,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"A Review of Meta-level Learning in the Context of Multi-component,\n  Multi-level Evolving Prediction Systems\n\n  The exponential growth of volume, variety and velocity of data is raising the\nneed for investigations of automated or semi-automated ways to extract useful\npatterns from the data. It requires deep expert knowledge and extensive\ncomputational resources to find the most appropriate mapping of learning\nmethods for a given problem. It becomes a challenge in the presence of numerous\nconfigurations of learning algorithms on massive amounts of data. So there is a\nneed for an intelligent recommendation engine that can advise what is the best\nlearning algorithm for a dataset. The techniques that are commonly used by\nexperts are based on a trial and error approach evaluating and comparing a\nnumber of possible solutions against each other, using their prior experience\non a specific domain, etc. The trial and error approach combined with the\nexpert's prior knowledge, though computationally and time expensive, have been\noften shown to work for stationary problems where the processing is usually\nperformed off-line. However, this approach would not normally be feasible to\napply to non-stationary problems where streams of data are continuously\narriving. Furthermore, in a non-stationary environment, the manual analysis of\ndata and testing of various methods whenever there is a change in the\nunderlying data distribution would be very difficult or simply infeasible. In\nthat scenario and within an on-line predictive system, there are several tasks\nwhere Meta-learning can be used to effectively facilitate best recommendations\nincluding 1) pre-processing steps, 2) learning algorithms or their combination,\n3) adaptivity mechanisms and their parameters, 4) recurring concept extraction,\nand 5) concept drift detection.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.0053,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000030796,
      "text":"MLPs to Find Extrema of Functionals\n\n  Multilayer perceptron (MLP) is a class of networks composed of multiple\nlayers of perceptrons, and it is essentially a mathematical function. Based on\nMLP, we develop a new numerical method to find the extrema of functionals. As\ndemonstrations, we present our solutions in three physic scenes. Ideally, the\nsame method is applicable to any cases where the objective curve\/surface can be\nfitted by second-order differentiable functions. This method can also be\nextended to cases where there are a finite number of non-differentiable (but\ncontinuous) points\/surfaces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.12567,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000036756,
      "text":"Wind speed prediction using multidimensional convolutional neural\n  networks\n\n  Accurate wind speed forecasting is of great importance for many economic,\nbusiness and management sectors. This paper introduces a new model based on\nconvolutional neural networks (CNNs) for wind speed prediction tasks. In\nparticular, we show that compared to classical CNN-based models, the proposed\nmodel is able to better characterise the spatio-temporal evolution of the wind\ndata by learning the underlying complex input-output relationships from\nmultiple dimensions (views) of the input data. The proposed model exploits the\nspatio-temporal multivariate multidimensional historical weather data for\nlearning new representations used for wind forecasting. We conduct experiments\non two real-life weather datasets. The datasets are measurements from cities in\nDenmark and in the Netherlands. The proposed model is compared with traditional\n2- and 3-dimensional CNN models, a 2D-CNN model with an attention layer and a\n2D-CNN model equipped with upscaling and depthwise separable convolutions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.01164,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Data-Driven Method for Enhanced Corrosion Assessment of Reinforced\n  Concrete Structures\n\n  Corrosion is a major problem affecting the durability of reinforced concrete\nstructures. Corrosion related maintenance and repair of reinforced concrete\nstructures cost multibillion USD per annum globally. It is often triggered by\nthe ingression of carbon dioxide and\/or chloride into the pores of concrete.\nEstimation of these corrosion causing factors using the conventional models\nresults in suboptimal assessment since they are incapable of capturing the\ncomplex interaction of parameters. Hygrothermal interaction also plays a role\nin aggravating the corrosion of reinforcement bar and this is usually\ncounteracted by applying surface-protection systems. These systems have\ndifferent degree of protection and they may even cause deterioration to the\nstructure unintentionally. The overall objective of this dissertation is to\nprovide a framework that enhances the assessment reliability of the corrosion\ncontrolling factors. The framework is realized through the development of\ndata-driven carbonation depth, chloride profile and hygrothermal performance\nprediction models. The carbonation depth prediction model integrates neural\nnetwork, decision tree, boosted and bagged ensemble decision trees. The\nensemble tree based chloride profile prediction models evaluate the\nsignificance of chloride ingress controlling variables from various\nperspectives. The hygrothermal interaction prediction models are developed\nusing neural networks to evaluate the status of corrosion and other unexpected\ndeteriorations in surface-treated concrete elements. Long-term data for all\nmodels were obtained from three different field experiments. The performance\ncomparison of the developed carbonation depth prediction model with the\nconventional one confirmed the prediction superiority of the data-driven model.\nThe variable ...\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.11985,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000082453,
      "text":"Shape-CD: Change-Point Detection in Time-Series Data with Shapes and\n  Neurons\n\n  Change-point detection in a time series aims to discover the time points at\nwhich some unknown underlying physical process that generates the time-series\ndata has changed. We found that existing approaches become less accurate when\nthe underlying process is complex and generates large varieties of patterns in\nthe time series. To address this shortcoming, we propose Shape-CD, a simple,\nfast, and accurate change point detection method. Shape-CD uses shape-based\nfeatures to model the patterns and a conditional neural field to model the\ntemporal correlations among the time regions. We evaluated the performance of\nShape-CD using four highly dynamic time-series datasets, including the\nExtraSensory dataset with up to 2000 classes. Shape-CD demonstrated improved\naccuracy (7-60% higher in AUC) and faster computational speed compared to\nexisting approaches. Furthermore, the Shape-CD model consists of only hundreds\nof parameters and require less data to train than other deep supervised\nlearning models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.12247,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Bracketing brackets with bras and kets\n\n  Brackets are an essential component in aircraft manufacture and design,\njoining parts together, supporting weight, holding wires, and strengthening\njoints. Hundreds or thousands of unique brackets are used in every aircraft,\nbut manufacturing a large number of distinct brackets is inefficient and\nexpensive. Fortunately, many so-called \"different\" brackets are in fact very\nsimilar or even identical to each other. In this manuscript, we present a\ndata-driven framework for constructing a comparatively small group of\nrepresentative brackets from a large catalog of current brackets, based on\nhierarchical clustering of bracket data. We find that for a modern commercial\naircraft, the full set of brackets can be reduced by 30\\% while still\ndescribing half of the test set sufficiently accurately. This approach is based\non designing an inner product that quantifies a multi-objective similarity\nbetween two brackets, which are the \"bra\" and the \"ket\" of the inner product.\nAlthough we demonstrate this algorithm to reduce the number of brackets in\naerospace manufacturing, it may be generally applied to any large-scale\ncomponent standardization effort.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.09527,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000041061,
      "text":"Abstraction based Output Range Analysis for Neural Networks\n\n  In this paper, we consider the problem of output range analysis for\nfeed-forward neural networks with ReLU activation functions. The existing\napproaches reduce the output range analysis problem to satisfiability and\noptimization solving, which are NP-hard problems, and whose computational\ncomplexity increases with the number of neurons in the network. To tackle the\ncomputational complexity, we present a novel abstraction technique that\nconstructs a simpler neural network with fewer neurons, albeit with interval\nweights called interval neural network (INN), which over-approximates the\noutput range of the given neural network. We reduce the output range analysis\non the INNs to solving a mixed integer linear programming problem. Our\nexperimental results highlight the trade-off between the computation time and\nthe precision of the computed output range.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.03519,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000034438,
      "text":"GateNet: Gating-Enhanced Deep Network for Click-Through Rate Prediction\n\n  Advertising and feed ranking are essential to many Internet companies such as\nFacebook. Among many real-world advertising and feed ranking systems, click\nthrough rate (CTR) prediction plays a central role. In recent years, many\nneural network based CTR models have been proposed and achieved success such as\nFactorization-Machine Supported Neural Networks, DeepFM and xDeepFM. Many of\nthem contain two commonly used components: embedding layer and MLP hidden\nlayers. On the other side, gating mechanism is also widely applied in many\nresearch fields such as computer vision(CV) and natural language\nprocessing(NLP). Some research has proved that gating mechanism improves the\ntrainability of non-convex deep neural networks. Inspired by these\nobservations, we propose a novel model named GateNet which introduces either\nthe feature embedding gate or the hidden gate to the embedding layer or hidden\nlayers of DNN CTR models, respectively. The feature embedding gate provides a\nlearnable feature gating module to select salient latent information from the\nfeature-level. The hidden gate helps the model to implicitly capture the\nhigh-order interaction more effectively. Extensive experiments conducted on\nthree real-world datasets demonstrate its effectiveness to boost the\nperformance of various state-of-the-art models such as FM, DeepFM and xDeepFM\non all datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.07363,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000004669,
      "text":"Predicting Account Receivables with Machine Learning\n\n  Being able to predict when invoices will be paid is valuable in multiple\nindustries and supports decision-making processes in most financial workflows.\nHowever, due to the complexity of data related to invoices and the fact that\nthe decision-making process is not registered in the accounts receivable\nsystem, performing this prediction becomes a challenge. In this paper, we\npresent a prototype able to support collectors in predicting the payment of\ninvoices. This prototype is part of a solution developed in partnership with a\nmultinational bank and it has reached up to 81% of prediction accuracy, which\nimproved the prioritization of customers and supported the daily work of\ncollectors. Our simulations show that the adoption of our model to prioritize\nthe work o collectors saves up to ~1.75 million dollars per month. The\nmethodology and results presented in this paper will allow researchers and\npractitioners in dealing with the problem of invoice payment prediction,\nproviding insights and examples of how to tackle issues present in real data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.11944,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000053313,
      "text":"A Consistent Diffusion-Based Algorithm for Semi-Supervised\n  Classification on Graphs\n\n  Semi-supervised classification on graphs aims at assigning labels to all\nnodes of a graph based on the labels known for a few nodes, called the seeds.\nThe most popular algorithm relies on the principle of heat diffusion, where the\nlabels of the seeds are spread by thermo-conductance and the temperature of\neach node is used as a score function for each label. Using a simple block\nmodel, we prove that this algorithm is not consistent unless the temperatures\nof the nodes are centered before classification. We show that this simple\nmodification of the algorithm is enough to get significant performance gains on\nreal data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.09202,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Conditional Wasserstein GAN-based Oversampling of Tabular Data for\n  Imbalanced Learning\n\n  Class imbalance is a common problem in supervised learning and impedes the\npredictive performance of classification models. Popular countermeasures\ninclude oversampling the minority class. Standard methods like SMOTE rely on\nfinding nearest neighbours and linear interpolations which are problematic in\ncase of high-dimensional, complex data distributions. Generative Adversarial\nNetworks (GANs) have been proposed as an alternative method for generating\nartificial minority examples as they can model complex distributions. However,\nprior research on GAN-based oversampling does not incorporate recent\nadvancements from the literature on generating realistic tabular data with\nGANs. Previous studies also focus on numerical variables whereas categorical\nfeatures are common in many business applications of classification methods\nsuch as credit scoring. The paper propoes an oversampling method based on a\nconditional Wasserstein GAN that can effectively model tabular datasets with\nnumerical and categorical variables and pays special attention to the\ndown-stream classification task through an auxiliary classifier loss. We\nbenchmark our method against standard oversampling methods and the imbalanced\nbaseline on seven real-world datasets. Empirical results evidence the\ncompetitiveness of GAN-based oversampling.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.0588,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"Lifelong Property Price Prediction: A Case Study for the Toronto Real\n  Estate Market\n\n  We present Luce, the first life-long predictive model for automated property\nvaluation. Luce addresses two critical issues of property valuation: the lack\nof recent sold prices and the sparsity of house data. It is designed to operate\non a limited volume of recent house transaction data. As a departure from prior\nwork, Luce organizes the house data in a heterogeneous information network\n(HIN) where graph nodes are house entities and attributes that are important\nfor house price valuation. We employ a Graph Convolutional Network (GCN) to\nextract the spatial information from the HIN for house-related data like\ngeographical locations, and then use a Long Short Term Memory (LSTM) network to\nmodel the temporal dependencies for house transaction data over time. Unlike\nprior work, Luce can make effective use of the limited house transactions data\nin the past few months to update valuation information for all house entities\nwithin the HIN. By providing a complete and up-to-date house valuation dataset,\nLuce thus massively simplifies the downstream valuation task for the targeting\nproperties. We demonstrate the benefit of Luce by applying it to large,\nreal-life datasets obtained from the Toronto real estate market. Extensive\nexperimental results show that Luce not only significantly outperforms prior\nproperty valuation methods but also often reaches and sometimes exceeds the\nvaluation accuracy given by independent experts when using the actual\nrealization price as the ground truth.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.09641,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"MPCC: Matching Priors and Conditionals for Clustering\n\n  Clustering is a fundamental task in unsupervised learning that depends\nheavily on the data representation that is used. Deep generative models have\nappeared as a promising tool to learn informative low-dimensional data\nrepresentations. We propose Matching Priors and Conditionals for Clustering\n(MPCC), a GAN-based model with an encoder to infer latent variables and cluster\ncategories from data, and a flexible decoder to generate samples from a\nconditional latent space. With MPCC we demonstrate that a deep generative model\ncan be competitive\/superior against discriminative methods in clustering tasks\nsurpassing the state of the art over a diverse set of benchmark datasets. Our\nexperiments show that adding a learnable prior and augmenting the number of\nencoder updates improve the quality of the generated samples, obtaining an\ninception score of 9.49 $\\pm$ 0.15 and improving the Fr\\'echet inception\ndistance over the state of the art by a 46.9% in CIFAR10.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.12642,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000019206,
      "text":"Bridging the Gap: Machine Learning to Resolve Improperly Modeled\n  Dynamics\n\n  We present a data-driven modeling strategy to overcome improperly modeled\ndynamics for systems exhibiting complex spatio-temporal behaviors. We propose a\nDeep Learning framework to resolve the differences between the true dynamics of\nthe system and the dynamics given by a model of the system that is either\ninaccurately or inadequately described. Our machine learning strategy leverages\ndata generated from the improper system model and observational data from the\nactual system to create a neural network to model the dynamics of the actual\nsystem. We evaluate the proposed framework using numerical solutions obtained\nfrom three increasingly complex dynamical systems. Our results show that our\nsystem is capable of learning a data-driven model that provides accurate\nestimates of the system states both in previously unobserved regions as well as\nfor future states. Our results show the power of state-of-the-art machine\nlearning frameworks in estimating an accurate prior of the system's true\ndynamics that can be used for prediction up to a finite horizon.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.10748,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"An empirical investigation of different classifiers, encoding and\n  ensemble schemes for next event prediction using business process event logs\n\n  There is a growing need for empirical benchmarks that support researchers and\npractitioners in selecting the best machine learning technique for given\nprediction tasks. In this paper, we consider the next event prediction task in\nbusiness process predictive monitoring and we extend our previously published\nbenchmark by studying the impact on the performance of different encoding\nwindows and of using ensemble schemes. The choice of whether to use ensembles\nand which scheme to use often depends on the type of data and classification\ntask. While there is a general understanding that ensembles perform well in\npredictive monitoring of business processes, next event prediction is a task\nfor which no other benchmarks involving ensembles are available. The proposed\nbenchmark helps researchers to select a high performing individual classifier\nor ensemble scheme given the variability at the case level of the event log\nunder consideration. Experimental results show that choosing an optimal number\nof events for feature encoding is challenging, resulting in the need to\nconsider each event log individually when selecting an optimal value. Ensemble\nschemes improve the performance of low performing classifiers in this task,\nsuch as SVM, whereas high performing classifiers, such as tree-based\nclassifiers, are not better off when ensemble schemes are considered.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.01072,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000018213,
      "text":"Spatial Classification With Limited Observations Based On Physics-Aware\n  Structural Constraint\n\n  Spatial classification with limited feature observations has been a\nchallenging problem in machine learning. The problem exists in applications\nwhere only a subset of sensors are deployed at certain spots or partial\nresponses are collected in field surveys. Existing research mostly focuses on\naddressing incomplete or missing data, e.g., data cleaning and imputation,\nclassification models that allow for missing feature values or model missing\nfeatures as hidden variables in the EM algorithm. These methods, however,\nassume that incomplete feature observations only happen on a small subset of\nsamples, and thus cannot solve problems where the vast majority of samples have\nmissing feature observations. To address this issue, we recently proposed a new\napproach that incorporates physics-aware structural constraint into the model\nrepresentation. Our approach assumes that a spatial contextual feature is\nobserved for all sample locations and establishes spatial structural constraint\nfrom the underlying spatial contextual feature map. We design efficient\nalgorithms for model parameter learning and class inference. This paper extends\nour recent approach by allowing feature values of samples in each class to\nfollow a multi-modal distribution. We propose learning algorithms for the\nextended model with multi-modal distribution. Evaluations on real-world\nhydrological applications show that our approach significantly outperforms\nbaseline methods in classification accuracy, and the multi-modal extension is\nmore robust than our early single-modal version especially when feature\ndistribution in training samples is multi-modal. Computational experiments show\nthat the proposed solution is computationally efficient on large datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.10753,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000046028,
      "text":"Evaluating Nonlinear Decision Trees for Binary Classification Tasks with\n  Other Existing Methods\n\n  Classification of datasets into two or more distinct classes is an important\nmachine learning task. Many methods are able to classify binary classification\ntasks with a very high accuracy on test data, but cannot provide any easily\ninterpretable explanation for users to have a deeper understanding of reasons\nfor the split of data into two classes. In this paper, we highlight and\nevaluate a recently proposed nonlinear decision tree approach with a number of\ncommonly used classification methods on a number of datasets involving a few to\na large number of features. The study reveals key issues such as effect of\nclassification on the method's parameter values, complexity of the classifier\nversus achieved accuracy, and interpretability of resulting classifiers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.11244,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Machine learning applied in the multi-scale 3D stress modelling\n\n  This paper proposes a methodology to estimate stress in the subsurface by a\nhybrid method combining finite element modeling and neural networks. This\nmethodology exploits the idea of obtaining a multi-frequency solution in the\nnumerical modeling of systems whose behavior involves a wide span of length\nscales. One low-frequency solution is obtained via inexpensive finite element\nmodeling at a coarse scale. The second solution provides the fine-grained\ndetails introduced by the heterogeneity of the free parameters at the fine\nscale. This high-frequency solution is estimated via neural networks -trained\nwith partial solutions obtained in high-resolution finite-element models. When\nthe coarse finite element solutions are combined with the neural network\nestimates, the results are within a 2\\% error of the results that would be\ncomputed with high-resolution finite element models. This paper discusses the\nbenefits and drawbacks of the method and illustrates their applicability via a\nworked example.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.09777,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Surrogate NAS Benchmarks: Going Beyond the Limited Search Spaces of\n  Tabular NAS Benchmarks\n\n  The most significant barrier to the advancement of Neural Architecture Search\n(NAS) is its demand for large computational resources, which hinders\nscientifically sound empirical evaluations of NAS methods. Tabular NAS\nbenchmarks have alleviated this problem substantially, making it possible to\nproperly evaluate NAS methods in seconds on commodity machines. However, an\nunintended consequence of tabular NAS benchmarks has been a focus on extremely\nsmall architectural search spaces since their construction relies on exhaustive\nevaluations of the space. This leads to unrealistic results that do not\ntransfer to larger spaces. To overcome this fundamental limitation, we propose\na methodology to create cheap NAS surrogate benchmarks for arbitrary search\nspaces. We exemplify this approach by creating surrogate NAS benchmarks on the\nexisting tabular NAS-Bench-101 and on two widely used NAS search spaces with up\nto $10^{21}$ architectures ($10^{13}$ times larger than any previous tabular\nNAS benchmark). We show that surrogate NAS benchmarks can model the true\nperformance of architectures better than tabular benchmarks (at a small\nfraction of the cost), that they lead to faithful estimates of how well\ndifferent NAS methods work on the original non-surrogate benchmark, and that\nthey can generate new scientific insight. We open-source all our code and\nbelieve that surrogate NAS benchmarks are an indispensable tool to extend\nscientifically sound work on NAS to large and exciting search spaces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.0892,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000012219,
      "text":"scikit-dyn2sel -- A Dynamic Selection Framework for Data Streams\n\n  Mining data streams is a challenge per se. It must be ready to deal with an\nenormous amount of data and with problems not present in batch machine\nlearning, such as concept drift. Therefore, applying a batch-designed\ntechnique, such as dynamic selection of classifiers (DCS) also presents a\nchallenge. The dynamic characteristic of ensembles that deal with streams\npresents barriers to the application of traditional DCS techniques in such\nclassifiers. scikit-dyn2sel is an open-source python library tailored for\ndynamic selection techniques in streaming data. scikit-dyn2sel's development\nfollows code quality and testing standards, including PEP8 compliance and\nautomated high test coverage using codecov.io and circleci.com. Source code,\ndocumentation, and examples are made available on GitHub at\nhttps:\/\/github.com\/luccaportes\/Scikit-DYN2SEL.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.02667,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Machine Learning for Health: Personalized Models for Forecasting of\n  Alzheimer Disease Progression\n\n  In this thesis the aim is to work on optimizing the modern machine learning\nmodels for personalized forecasting of Alzheimer Disease (AD) Progression from\nclinical trial data. The data comes from the TADPOLE challenge, which is one of\nthe largest publicly available datasets for AD research (ADNI dataset). The\ngoal of the project is to develop machine learning models that can be used to\nperform personalized forecasts of the participants cognitive changes (e.g.,\nADAS-Cog13 scores) over the time period of 6,12, 18 and 24 months in the future\nand the change in Clinical Status (CS) i.e., whether a person will convert to\nAD within 2 years or not. This is important for informing current clinical\ntrials and better design of future clinical trials for AD. We will work with\npersonalized Gaussian processes as machine learning models to predict\nADAS-Cog13 score and Cox model along with a classifier to predict the\nconversion in a patient within 2 years.This project is done with the\ncollaboration with researchers from the MIT MediaLab.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.00707,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0005494224,
      "text":"Pursuing a Prospective Perspective\n\n  Retrospective testing of predictive models does not consider the real-world\ncontext in which models are deployed. Prospective validation, on the other\nhand, enables meaningful comparisons between data generation processes by\nincorporating trained models and considering the subjective decisions that\naffect reproducibility. Prospective experiments are essential for consistent\nprogress in modeling.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.10989,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Congested Urban Networks Tend to Be Insensitive to Signal Settings:\n  Implications for Learning-Based Control\n\n  This paper highlights several properties of large urban networks that can\nhave an impact on machine learning methods applied to traffic signal control.\nIn particular, we show that the average network flow tends to be independent of\nthe signal control policy as density increases. This property, which so far has\nremained under the radar, implies that deep reinforcement learning (DRL)\nmethods becomes ineffective when trained under congested conditions, and might\nexplain DRL's limited success for traffic signal control. Our results apply to\nall possible grid networks thanks to a parametrization based on two network\nparameters: the ratio of the expected distance between consecutive traffic\nlights to the expected green time, and the turning probability at\nintersections. Networks with different parameters exhibit very different\nresponses to traffic signal control. Notably, we found that no control (i.e.\nrandom policy) can be an effective control strategy for a surprisingly large\nfamily of networks. The impact of the turning probability turned out to be very\nsignificant both for baseline and for DRL policies. It also explains the loss\nof symmetry observed for these policies, which is not captured by existing\ntheories that rely on corridor approximations without turns. Our findings also\nsuggest that supervised learning methods have enormous potential as they\nrequire very little examples to produce excellent policies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.12258,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000044703,
      "text":"Learning to Profile: User Meta-Profile Network for Few-Shot Learning\n\n  Meta-learning approaches have shown great success in vision and language\ndomains. However, few studies discuss the practice of meta-learning for\nlarge-scale industrial applications. Although e-commerce companies have spent\nmany efforts on learning representations to provide a better user experience,\nwe argue that such efforts cannot be stopped at this step. In addition to\nlearning a strong profile, the challenging question about how to effectively\ntransfer the learned representation is raised simultaneously. This paper\nintroduces the contributions that we made to address these challenges from\nthree aspects. 1) Meta-learning model: In the context of representation\nlearning with e-commerce user behavior data, we propose a meta-learning\nframework called the Meta-Profile Network, which extends the ideas of matching\nnetwork and relation network for knowledge transfer and fast adaptation; 2)\nEncoding strategy: To keep high fidelity of large-scale long-term sequential\nbehavior data, we propose a time-heatmap encoding strategy that allows the\nmodel to encode data effectively; 3) Deep network architecture: A multi-modal\nmodel combined with multi-task learning architecture is utilized to address the\ncross-domain knowledge learning and insufficient label problems. Moreover, we\nargue that an industrial model should not only have good performance in terms\nof accuracy, but also have better robustness and uncertainty performance under\nextreme conditions. We evaluate the performance of our model with extensive\ncontrol experiments in various extreme scenarios, i.e. out-of-distribution\ndetection, data insufficiency and class imbalance scenarios. The Meta-Profile\nNetwork shows significant improvement in the model performance when compared to\nbaseline models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.00542,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Hierarchical Deep Learning Classification of Unstructured Pathology\n  Reports to Automate ICD-O Morphology Grading\n\n  Timely cancer reporting data are required in order to understand the impact\nof cancer, inform public health resource planning and implement cancer policy\nespecially in Sub Saharan Africa where the reporting lag is behind world\naverages. Unstructured pathology reports, which contain tumor specific data,\nare the main source of information collected by cancer registries. Due to\nmanual processing and labelling of pathology reports using the International\nClassification of Disease for oncology (ICD-O) codes, by human coders employed\nby cancer registries, has led to a considerable lag in cancer reporting. We\npresent a hierarchical deep learning classification method that employs\nconvolutional neural network models to automate the classification of 1813\nanonymized breast cancer pathology reports with applicable ICD-O morphology\ncodes across 9 classes. We demonstrate that the hierarchical deep learning\nclassification method improves on performance in comparison to a flat\nmulticlass CNN model for ICD-O morphology classification of the same reports.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.09922,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"Machine Learning Approaches to Real Estate Market Prediction Problem: A\n  Case Study\n\n  Home sale prices are formed given the transaction actors economic interests,\nwhich include government, real estate dealers, and the general public who buy\nor sell properties. Generating an accurate property price prediction model is a\nmajor challenge for the real estate market. This work develops a property price\nclassification model using a ten year actual dataset, from January 2010 to\nNovember 2019. The real estate dataset is publicly available and was retrieved\nfrom Volusia County Property Appraiser of Florida website. In addition,\nsocio-economic factors such as Gross Domestic Product, Consumer Price Index,\nProducer Price Index, House Price Index, and Effective Federal Funds Rate are\ncollected and used in the prediction model. To solve this case study problem,\nseveral powerful machine learning algorithms, namely, Logistic Regression,\nRandom Forest, Voting Classifier, and XGBoost, are employed. They are\nintegrated with target encoding to develop an accurate property sale price\nprediction model with the aim to predict whether the closing sale price is\ngreater than or less than the listing sale price. To assess the performance of\nthe models, the accuracy, precision, recall, classification F1 score, and error\nrate of the models are determined. Among the four studied machine learning\nalgorithms, XGBoost delivers superior results and robustness of the model\ncompared to other models. The developed model can facilitate real estate\ninvestors, mortgage lenders and financial institutions to make better informed\ndecisions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.07387,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000064903,
      "text":"Deep Networks with Fast Retraining\n\n  Recent work [1] has utilized Moore-Penrose (MP) inverse in deep convolutional\nneural network (DCNN) learning, which achieves better generalization\nperformance over the DCNN with a stochastic gradient descent (SGD) pipeline.\nHowever, Yang's work has not gained much popularity in practice due to its high\nsensitivity of hyper-parameters and stringent demands of computational\nresources. To enhance its applicability, this paper proposes a novel MP\ninverse-based fast retraining strategy. In each training epoch, a random\nlearning strategy that controls the number of convolutional layers trained in\nthe backward pass is first utilized. Then, an MP inverse-based batch-by-batch\nlearning strategy, which enables the network to be implemented without access\nto industrial-scale computational resources, is developed to refine the dense\nlayer parameters. Experimental results empirically demonstrate that fast\nretraining is a unified strategy that can be used for all DCNNs. Compared to\nother learning strategies, the proposed learning pipeline has robustness\nagainst the hyper-parameters, and the requirement of computational resources is\nsignificantly reduced. [1] Y. Yang, J. Wu, X. Feng, and A. Thangarajah,\n\"Recomputation of dense layers for the perfor-238mance improvement of dcnn,\"\nIEEE Trans. Pattern Anal. Mach. Intell., 2019.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.11112,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000029471,
      "text":"ANNdotNET -- deep learning tool on .NET Platform\n\n  ANNdotNET is an open source project for deep learning written in C# with\nability to create, train, evaluate and export deep learning models. The project\nconsists of the Graphical User Interface module capable to visually prepare\ndata, fine tune hyper-parameters, design network architecture, evaluate and\ntest trained models. The ANNdotNET introduces the Visual Network Designer,\n(VND) for visually design almost any sequential deep learning network. Beside\nVND, ANNdotNET implements Machine Learning Engine, (MLE) based on CNTK - deep\nlearning framework, with ability to train and evaluate models on GPU. For model\nevaluation ANNdotNET contains rich set of visual and descriptive performance\nparameters, history of the training process and set of export\/deployment\noptions. The advantage of using ANNdotNET over the classic code based ML\napproach is more focus on deep learning network design and training process\ninstead of focusing on coding and debugging. It is ideal for engineers not\nfamiliar with supported programming languages. The project is hosted at\ngithub.com\/bhrnjica\/anndotnet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.06783,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Learning Hidden Patterns from Patient Multivariate Time Series Data\n  Using Convolutional Neural Networks: A Case Study of Healthcare Cost\n  Prediction\n\n  Objective: To develop an effective and scalable individual-level patient cost\nprediction method by automatically learning hidden temporal patterns from\nmultivariate time series data in patient insurance claims using a convolutional\nneural network (CNN) architecture.\n  Methods: We used three years of medical and pharmacy claims data from 2013 to\n2016 from a healthcare insurer, where data from the first two years were used\nto build the model to predict costs in the third year. The data consisted of\nthe multivariate time series of cost, visit and medical features that were\nshaped as images of patients' health status (i.e., matrices with time windows\non one dimension and the medical, visit and cost features on the other\ndimension). Patients' multivariate time series images were given to a CNN\nmethod with a proposed architecture. After hyper-parameter tuning, the proposed\narchitecture consisted of three building blocks of convolution and pooling\nlayers with an LReLU activation function and a customized kernel size at each\nlayer for healthcare data. The proposed CNN learned temporal patterns became\ninputs to a fully connected layer.\n  Conclusions: Feature learning through the proposed CNN configuration\nsignificantly improved individual-level healthcare cost prediction. The\nproposed CNN was able to outperform temporal pattern detection methods that\nlook for a pre-defined set of pattern shapes, since it is capable of extracting\na variable number of patterns with various shapes. Temporal patterns learned\nfrom medical, visit and cost data made significant contributions to the\nprediction performance. Hyper-parameter tuning showed that considering\nthree-month data patterns has the highest prediction accuracy. Our results\nshowed that patients' images extracted from multivariate time series data are\ndifferent from regular images, and hence require unique designs of CNN\narchitectures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.14025,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000220537,
      "text":"Machine-Learning Approach to Analyze the Status of Forklift Vehicles\n  with Irregular Movement in a Shipyard\n\n  In large shipyards, the management of equipment, which are used for building\na variety of ships, is critical. Because orders vary year to year, shipyard\nmanagers are required to determine methods to make the most of their limited\nresources. A particular difficulty that arises because of the nature and size\nof shipyards is the management of moving vehicles. In recent years,\nshipbuilding companies have attempted to manage and track the locations and\nmovements of vehicles using Global Positioning System (GPS) modules. However,\nbecause certain vehicles, such as forklifts, roam irregularly around a yard,\nidentifying their working status without being onsite is difficult. Location\ninformation alone is not sufficient to determine whether a vehicle is working,\nmoving, waiting, or resting. This study proposes an approach based on machine\nlearning to identify the work status of each forklift. We use the DBSCAN and\nk-means algorithms to identify the area in which a particular forklift is\noperating and the type of work it is performing. We developed a business\nintelligence system to collect information from forklifts equipped with GPS and\nInternet of Things (IoT) devices. The system provides visual information on the\nstatus of individual forklifts and helps in the efficient management of their\nmovements within large shipyards.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.05094,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Why I'm not Answering: Understanding Determinants of Classification of\n  an Abstaining Classifier for Cancer Pathology Reports\n\n  Safe deployment of deep learning systems in critical real world applications\nrequires models to make very few mistakes, and only under predictable\ncircumstances. In this work, we address this problem using an abstaining\nclassifier that is tuned to have $>$95% accuracy, and then identify the\ndeterminants of abstention using LIME. Essentially, we are training our model\nto learn the attributes of pathology reports that are likely to lead to\nincorrect classifications, albeit at the cost of reduced sensitivity. We\ndemonstrate an abstaining classifier in a multitask setting for classifying\ncancer pathology reports from the NCI SEER cancer registries on six tasks of\ninterest. For these tasks, we reduce the classification error rate by factors\nof 2--5 by abstaining on 25--45% of the reports. For the specific task of\nclassifying cancer site, we are able to identify metastasis, reports involving\nlymph nodes, and discussion of multiple cancer sites as responsible for many of\nthe classification mistakes, and observe that the extent and types of mistakes\nvary systematically with cancer site (e.g., breast, lung, and prostate). When\ncombining across three of the tasks, our model classifies 50% of the reports\nwith an accuracy greater than 95% for three of the six tasks\\edit, and greater\nthan 85% for all six tasks on the retained samples. Furthermore, we show that\nLIME provides a better determinant of classification than measures of word\noccurrence alone. By combining a deep abstaining classifier with feature\nidentification using LIME, we are able to identify concepts responsible for\nboth correctness and abstention when classifying cancer sites from pathology\nreports. The improvement of LIME over keyword searches is statistically\nsignificant, presumably because words are assessed in context and have been\nidentified as a local determinant of classification.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.11719,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000059605,
      "text":"Deep Neural Networks with Short Circuits for Improved Gradient Learning\n\n  Deep neural networks have achieved great success both in computer vision and\nnatural language processing tasks. However, mostly state-of-art methods highly\nrely on external training or computing to improve the performance. To alleviate\nthe external reliance, we proposed a gradient enhancement approach, conducted\nby the short circuit neural connections, to improve the gradient learning of\ndeep neural networks. The proposed short circuit is a unidirectional connection\nthat single back propagates the sensitive from the deep layer to the shallows.\nMoreover, the short circuit formulates to be a gradient truncation of its\ncrossing layers which can plug into the backbone deep neural networks without\nintroducing external training parameters. Extensive experiments demonstrate\ndeep neural networks with our short circuit gain a large margin over the\nbaselines on both computer vision and natural language processing tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.0373,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Large-scale Neural Solvers for Partial Differential Equations\n\n  Solving partial differential equations (PDE) is an indispensable part of many\nbranches of science as many processes can be modelled in terms of PDEs.\nHowever, recent numerical solvers require manual discretization of the\nunderlying equation as well as sophisticated, tailored code for distributed\ncomputing. Scanning the parameters of the underlying model significantly\nincreases the runtime as the simulations have to be cold-started for each\nparameter configuration. Machine Learning based surrogate models denote\npromising ways for learning complex relationship among input, parameter and\nsolution. However, recent generative neural networks require lots of training\ndata, i.e. full simulation runs making them costly. In contrast, we examine the\napplicability of continuous, mesh-free neural solvers for partial differential\nequations, physics-informed neural networks (PINNs) solely requiring\ninitial\/boundary values and validation points for training but no simulation\ndata. The induced curse of dimensionality is approached by learning a domain\ndecomposition that steers the number of neurons per unit volume and\nsignificantly improves runtime. Distributed training on large-scale cluster\nsystems also promises great utilization of large quantities of GPUs which we\nassess by a comprehensive evaluation study. Finally, we discuss the accuracy of\nGatedPINN with respect to analytical solutions -- as well as state-of-the-art\nnumerical solvers, such as spectral solvers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.14627,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"A Traffic Light Dynamic Control Algorithm with Deep Reinforcement\n  Learning Based on GNN Prediction\n\n  Today's intelligent traffic light control system is based on the current road\ntraffic conditions for traffic regulation. However, these approaches cannot\nexploit the future traffic information in advance. In this paper, we propose\nGPlight, a deep reinforcement learning (DRL) algorithm integrated with graph\nneural network (GNN) , to relieve the traffic congestion for multi-intersection\nintelligent traffic control system. In GPlight, the graph neural network (GNN)\nis first used to predict the future short-term traffic flow at the\nintersections. Then, the results of traffic flow prediction are used in traffic\nlight control, and the agent combines the predicted results with the observed\ncurrent traffic conditions to dynamically control the phase and duration of the\ntraffic lights at the intersection. Experiments on both synthetic and two\nreal-world data-sets of Hangzhou and New-York verify the effectiveness and\nrationality of the GPlight algorithm.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.02011,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"A Physics-Informed Machine Learning Approach for Solving Heat Transfer\n  Equation in Advanced Manufacturing and Engineering Applications\n\n  A physics-informed neural network is developed to solve conductive heat\ntransfer partial differential equation (PDE), along with convective heat\ntransfer PDEs as boundary conditions (BCs), in manufacturing and engineering\napplications where parts are heated in ovens. Since convective coefficients are\ntypically unknown, current analysis approaches based on trial and error finite\nelement (FE) simulations are slow. The loss function is defined based on errors\nto satisfy PDE, BCs and initial condition. An adaptive normalizing scheme is\ndeveloped to reduce loss terms simultaneously. In addition, theory of heat\ntransfer is used for feature engineering. The predictions for 1D and 2D cases\nare validated by comparing with FE results. It is shown that using engineered\nfeatures, heat transfer beyond the training zone can be predicted. Trained\nmodel allows for fast evaluation of a range of BCs to develop feedback loops,\nrealizing Industry 4.0 concept of active manufacturing control based on sensor\ndata.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.00382,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000043048,
      "text":"A Deep Learning Framework for COVID Outbreak Prediction\n\n  The outbreak of COVID-19 i.e. a variation of coronavirus, also known as novel\ncorona virus causing respiratory disease is a big concern worldwide since the\nend of December 2019. As of September 12, 2020, it has turned into an epidemic\noutbreak with more than 29 million confirmed cases and around 1 million\nreported deaths worldwide. It has created an urgent need to monitor and\nforecast COVID-19 spread behavior to better control this spread. Among all the\npopular models for COVID-19 forecasting, statistical models are receiving much\nattention in media. However, statistical models are showing less accuracy for\nlong term forecasting, as there is high level of uncertainty and required data\nis also not sufficiently available. In this paper, we propose a comparative\nanalysis of deep learning models to forecast the COVID-19 outbreak as an\nalternative to statistical models. We propose a new Attention-based\nencoder-decoder model, named Attention-Long Short Term Memory (AttentionLSTM).\nLSTM based neural network layer architecture incorporates the idea of\nfine-grained attention mechanism i.e., attention on hidden state dimensions\ninstead of hidden state vector itself, which is capable of highlighting the\nimportance and contribution of each hidden state dimension. It helps in\ndetection on crucial temporal information, resulting in a highly interpretable\nnetwork. Additionally, we implement a learnable vector embedding for time. As,\ntime in a vector representation can be easily added with many architectures.\nThis vector representation is called Time2Vec. We have used COVID-19 data\nrepository by the Center for Systems Science and Engineering (CSSE) at Johns\nHopkins University to assess the proposed model's performance. The proposed\nmodel give superior forecasting accuracy compared to other existing methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.05783,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000023312,
      "text":"Multiclass Model for Agriculture development using Multivariate\n  Statistical method\n\n  Mahalanobis taguchi system (MTS) is a multi-variate statistical method\nextensively used for feature selection and binary classification problems. The\ncalculation of orthogonal array and signal-to-noise ratio in MTS makes the\nalgorithm complicated when more number of factors are involved in the\nclassification problem. Also the decision is based on the accuracy of normal\nand abnormal observations of the dataset. In this paper, a multiclass model\nusing Improved Mahalanobis Taguchi System (IMTS) is proposed based on normal\nobservations and Mahalanobis distance for agriculture development. Twenty-six\ninput factors relevant to crop cultivation have been identified and clustered\ninto six main factors for the development of the model. The multiclass model is\ndeveloped with the consideration of the relative importance of the factors. An\nobjective function is defined for the classification of three crops, namely\npaddy, sugarcane and groundnut. The classification results are verified against\nthe results obtained from the agriculture experts working in the field. The\nproposed classifier provides 100% accuracy, recall, precision and 0% error rate\nwhen compared with other traditional classifier models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.06421,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000159608,
      "text":"High correlated variables creator machine: Prediction of the compressive\n  strength of concrete\n\n  In this paper, we introduce a novel hybrid model for predicting the\ncompressive strength of concrete using ultrasonic pulse velocity (UPV) and\nrebound number (RN). First, 516 data from 8 studies of UPV and rebound hammer\n(RH) tests was collected. Then, high correlated variables creator machine\n(HVCM) is used to create the new variables that have a better correlation with\nthe output and improve the prediction models. Three single models, including a\nstep-by-step regression (SBSR), gene expression programming (GEP) and an\nadaptive neuro-fuzzy inference system (ANFIS) as well as three hybrid models,\ni.e. HCVCM-SBSR, HCVCM-GEP and HCVCM-ANFIS, were employed to predict the\ncompressive strength of concrete. The statistical parameters and error terms\nsuch as coefficient of determination, root mean square error (RMSE), normalized\nmean square error (NMSE), fractional bias, the maximum positive and negative\nerrors, and mean absolute percentage error (MAPE), were computed to evaluate\nand compare the models. The results show that HCVCM-ANFIS can predict the\ncompressive strength of concrete better than all other models. HCVCM improves\nthe accuracy of ANFIS by 5% in the coefficient of determination, 10% in RMSE,\n3% in NMSE, 20% in MAPE, and 7% in the maximum negative error.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.01454,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Say No to the Discrimination: Learning Fair Graph Neural Networks with\n  Limited Sensitive Attribute Information\n\n  Graph neural networks (GNNs) have shown great power in modeling graph\nstructured data. However, similar to other machine learning models, GNNs may\nmake predictions biased on protected sensitive attributes, e.g., skin color and\ngender. Because machine learning algorithms including GNNs are trained to\nreflect the distribution of the training data which often contains historical\nbias towards sensitive attributes. In addition, the discrimination in GNNs can\nbe magnified by graph structures and the message-passing mechanism. As a\nresult, the applications of GNNs in sensitive domains such as crime rate\nprediction would be largely limited. Though extensive studies of fair\nclassification have been conducted on i.i.d data, methods to address the\nproblem of discrimination on non-i.i.d data are rather limited. Furthermore,\nthe practical scenario of sparse annotations in sensitive attributes is rarely\nconsidered in existing works. Therefore, we study the novel and important\nproblem of learning fair GNNs with limited sensitive attribute information.\nFairGNN is proposed to eliminate the bias of GNNs whilst maintaining high node\nclassification accuracy by leveraging graph structures and limited sensitive\ninformation. Our theoretical analysis shows that FairGNN can ensure the\nfairness of GNNs under mild conditions given limited nodes with known sensitive\nattributes. Extensive experiments on real-world datasets also demonstrate the\neffectiveness of FairGNN in debiasing and keeping high accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.12656,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000028478,
      "text":"Bidirectional Representation Learning from Transformers using Multimodal\n  Electronic Health Record Data to Predict Depression\n\n  Advancements in machine learning algorithms have had a beneficial impact on\nrepresentation learning, classification, and prediction models built using\nelectronic health record (EHR) data. Effort has been put both on increasing\nmodels' overall performance as well as improving their interpretability,\nparticularly regarding the decision-making process. In this study, we present a\ntemporal deep learning model to perform bidirectional representation learning\non EHR sequences with a transformer architecture to predict future diagnosis of\ndepression. This model is able to aggregate five heterogenous and\nhigh-dimensional data sources from the EHR and process them in a temporal\nmanner for chronic disease prediction at various prediction windows. We applied\nthe current trend of pretraining and fine-tuning on EHR data to outperform the\ncurrent state-of-the-art in chronic disease prediction, and to demonstrate the\nunderlying relation between EHR codes in the sequence. The model generated the\nhighest increases of precision-recall area under the curve (PRAUC) from 0.70 to\n0.76 in depression prediction compared to the best baseline model. Furthermore,\nthe self-attention weights in each sequence quantitatively demonstrated the\ninner relationship between various codes, which improved the model's\ninterpretability. These results demonstrate the model's ability to utilize\nheterogeneous EHR data to predict depression while achieving high accuracy and\ninterpretability, which may facilitate constructing clinical decision support\nsystems in the future for chronic disease screening and early detection.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.06365,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Fraud Detection using Data-Driven approach\n\n  The extensive use of the internet is continuously drifting businesses to\nincorporate their services in the online environment. One of the first\nspectrums to embrace this evolution was the banking sector. In fact, the first\nknown online banking service came in 1980. It was deployed from a community\nbank located in Knoxville, called the United American Bank. Since then,\ninternet banking has been offering ease and efficiency to costumers in\ncompleting their daily banking tasks.\n  The ever increasing use of internet banking and a large number of online\ntransactions increased fraudulent behavior also. As if fraud increase was not\nenough, the massive number of online transactions further increased the data\ncomplexity. Modern data sources are not only complex but generated at high\nspeed and in real-time as well. This presents a serious problem and a definite\nreason why more advanced solutions are desired to protect financial service\ncompanies and credit cardholders.\n  Therefore, this research paper aims to construct an efficient fraud detection\nmodel which is adaptive to customer behavior changes and tends to decrease\nfraud manipulation, by detecting and filtering fraud in real-time. In order to\nachieve this aim, a review of various methods is conducted, adding above a\npersonal experience working in the Banking sector, specifically in the Fraud\nDetection office. Unlike the majority of reviewed methods, the proposed model\nin this research paper is able to detect fraud in the moment of occurrence\nusing an incremental classifier. The evaluation of synthetic data, based on\nfraud scenarios selected in collaboration with domain experts that replicate\ntypical, real-world attacks, shows that this approach correctly ranks complex\nfrauds. In particular, our proposal detects fraudulent behavior and anomalies\nwith up to 97\\% detection rate while maintaining a satisfyingly low cost.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.02784,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"An FPGA Accelerated Method for Training Feed-forward Neural Networks\n  Using Alternating Direction Method of Multipliers and LSMR\n\n  In this project, we have successfully designed, implemented, deployed and\ntested a novel FPGA accelerated algorithm for neural network training. The\nalgorithm itself was developed in an independent study option. This training\nmethod is based on Alternating Direction Method of Multipliers algorithm, which\nhas strong parallel characteristics and avoids procedures such as matrix\ninversion that are problematic in hardware designs by employing LSMR. As an\nintermediate stage, we fully implemented the ADMM-LSMR method in C language for\nfeed-forward neural networks with a flexible number of layers and hidden size.\nWe demonstrated that the method can operate with fixed-point arithmetic without\ncompromising the accuracy. Next, we devised an FPGA accelerated version of the\nalgorithm using Intel FPGA SDK for OpenCL and performed extensive optimisation\nstages followed by successful deployment of the program on an Intel Arria 10 GX\nFPGA. The FPGA accelerated program showed up to 6 times speed up comparing to\nequivalent CPU implementation while achieving promising accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.06027,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"ReviewViz: Assisting Developers Perform Empirical Study on Energy\n  Consumption Related Reviews for Mobile Applications\n\n  Improving the energy efficiency of mobile applications is a topic that has\ngained a lot of attention recently. It has been addressed in a number of ways\nsuch as identifying energy bugs and developing a catalog of energy patterns.\nPrevious work shows that users discuss the battery-related issues (energy\ninefficiency or energy consumption) of the apps in their reviews. However,\nthere is no work that addresses the automatic extraction of battery-related\nissues from users' feedback. In this paper, we report on a visualization tool\nthat is developed to empirically study machine learning algorithms and text\nfeatures to automatically identify the energy consumption specific reviews with\nthe highest accuracy. Other than the common machine learning algorithms, we\nutilize deep learning models with different word embeddings to compare the\nresults. Furthermore, to help the developers extract the main topics that are\ndiscussed in the reviews, two states of the art topic modeling algorithms are\napplied. The visualizations of the topics represent the keywords that are\nextracted for each topic along with a comparison with the results of string\nmatching. The developed web-browser based interactive visualization tool is a\nnovel framework developed with the intention of giving the app developers\ninsights about running time and accuracy of machine learning and deep learning\nmodels as well as extracted topics. The tool makes it easier for the developers\nto traverse through the extensive result set generated by the text\nclassification and topic modeling algorithms. The dynamic-data structure used\nfor the tool stores the baseline-results of the discussed approaches and is\nupdated when applied on new datasets. The tool is open-sourced to replicate the\nresearch results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.0678,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000062916,
      "text":"Healthcare Cost Prediction: Leveraging Fine-grain Temporal Patterns\n\n  Objective: To design and assess a method to leverage individuals' temporal\ndata for predicting their healthcare cost. To achieve this goal, we first used\npatients' temporal data in their fine-grain form as opposed to coarse-grain\nform. Second, we devised novel spike detection features to extract temporal\npatterns that improve the performance of cost prediction. Third, we evaluated\nthe effectiveness of different types of temporal features based on cost\ninformation, visit information and medical information for the prediction task.\n  Materials and methods: We used three years of medical and pharmacy claims\ndata from 2013 to 2016 from a healthcare insurer, where the first two years\nwere used to build the model to predict the costs in the third year. To prepare\nthe data for modeling and prediction, the time series data of cost, visit and\nmedical information were extracted in the form of fine-grain features (i.e.,\nsegmenting each time series into a sequence of consecutive windows and\nrepresenting each window by various statistics such as sum). Then, temporal\npatterns of the time series were extracted and added to fine-grain features\nusing a novel set of spike detection features (i.e., the fluctuation of data\npoints). Gradient Boosting was applied on the final set of extracted features.\nMoreover, the contribution of each type of data (i.e., cost, visit and medical)\nwas assessed.\n  Conclusions: Leveraging fine-grain temporal patterns for healthcare cost\nprediction significantly improves prediction performance. Enhancing fine-grain\nfeatures with extraction of temporal cost and visit patterns significantly\nimproved the performance. However, medical features did not have a significant\neffect on prediction performance. Gradient Boosting outperformed all other\nprediction models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.12566,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"An Explainable Model for EEG Seizure Detection based on Connectivity\n  Features\n\n  Epilepsy which is characterized by seizures is studied using EEG signals by\nrecording the electrical activity of the brain. Different types of\ncommunication between different parts of the brain are characterized by many\nstate of the art connectivity measures which can be directed and undirected. We\npropose to employ a set of undirected (spectral matrix, the inverse of the\nspectral matrix, coherence, partial coherence, and phaselocking value) and\ndirected features (directed coherence, the partial directed coherence) to learn\na deep neural network that detects whether a particular data window belongs to\na seizure or not, which is a new approach to standard seizure classification.\nTaking our data as a sequence of ten sub-windows, we aim at designing an\noptimal deep learning model using attention, CNN, BiLstm, and fully connected\nlayers. We also compute the relevance using the weights of the learned model\nbased on the activation values of the receptive fields at a particular layer.\nOur best model architecture resulted in 97.03% accuracy using balanced MITBIH\ndata subset. Also, we were able to explain the relevance of each feature across\nall patients. We were able to experimentally validate some of the scientific\nfacts concerning seizures by studying the impact of the contributions of the\nactivations on the decision.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.10312,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"Stacked Generalization for Human Activity Recognition\n\n  This short paper aims to discuss the effectiveness and performance of\nclassical machine learning approaches for Human Activity Recognition (HAR). It\nproposes two important models - Extra Trees and Stacked Classifier with the\nemphasize on the best practices, heuristics and measures that are required to\nmaximize the performance of those models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.06825,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Frequency-based Multi Task learning With Attention Mechanism for Fault\n  Detection In Power Systems\n\n  The prompt and accurate detection of faults and abnormalities in electric\ntransmission lines is a critical challenge in smart grid systems. Existing\nmethods mostly rely on model-based approaches, which may not capture all the\naspects of these complex temporal series. Recently, the availability of data\nsets collected using advanced metering devices, such as Micro-Phasor\nMeasurement units ($\\mu$ PMU), which provide measurements at microsecond\ntimescale, boosted the development of data-driven methodologies. In this paper,\nwe introduce a novel deep learning-based approach for fault detection and test\nit on a real data set, namely, the Kaggle platform for a partial discharge\ndetection task. Our solution adopts a Long-Short Term Memory architecture with\nattention mechanism to extract time series features, and uses a\n1D-Convolutional Neural Network structure to exploit frequency information of\nthe signal for prediction. Additionally, we propose an unsupervised method to\ncluster signals based on their frequency components, and apply multi task\nlearning on different clusters. The method we propose outperforms the winner\nsolutions in the Kaggle competition and other state of the art methods in many\nperformance metrics, and improves the interpretability of analysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.1349,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000412597,
      "text":"Learning Fast Approximations of Sparse Nonlinear Regression\n\n  The idea of unfolding iterative algorithms as deep neural networks has been\nwidely applied in solving sparse coding problems, providing both solid\ntheoretical analysis in convergence rate and superior empirical performance.\nHowever, for sparse nonlinear regression problems, a similar idea is rarely\nexploited due to the complexity of nonlinearity. In this work, we bridge this\ngap by introducing the Nonlinear Learned Iterative Shrinkage Thresholding\nAlgorithm (NLISTA), which can attain a linear convergence under suitable\nconditions. Experiments on synthetic data corroborate our theoretical results\nand show our method outperforms state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.05719,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000076824,
      "text":"Revisiting Neural Architecture Search\n\n  Neural Architecture Search (NAS) is a collection of methods to craft the way\nneural networks are built. Current NAS methods are far from ab initio and\nautomatic, as they use manual backbone architectures or micro building blocks\n(cells), which have had minor breakthroughs in performance compared to random\nbaselines. They also involve a significant manual expert effort in various\ncomponents of the NAS pipeline. This raises a natural question - Are the\ncurrent NAS methods still heavily dependent on manual effort in the search\nspace design and wiring like it was done when building models before the advent\nof NAS? In this paper, instead of merely chasing slight improvements over\nstate-of-the-art (SOTA) performance, we revisit the fundamental approach to NAS\nand propose a novel approach called ReNAS that can search for the complete\nneural network without much human effort and is a step closer towards\nAutoML-nirvana. Our method starts from a complete graph mapped to a neural\nnetwork and searches for the connections and operations by balancing the\nexploration and exploitation of the search space. The results are on-par with\nthe SOTA performance with methods that leverage handcrafted blocks. We believe\nthat this approach may lead to newer NAS strategies for a variety of network\ntypes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.15237,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Bandit Policies for Reliable Cellular Network Handovers in Extreme\n  Mobility\n\n  The demand for seamless Internet access under extreme user mobility, such as\non high-speed trains and vehicles, has become a norm rather than an exception.\nHowever, the 4G\/5G mobile network is not always reliable to meet this demand,\nwith non-negligible failures during the handover between base stations. A\nfundamental challenge of reliability is to balance the exploration of more\nmeasurements for satisfactory handover, and exploitation for timely handover\n(before the fast-moving user leaves the serving base station's radio coverage).\nThis paper formulates this trade-off in extreme mobility as a composition of\ntwo distinct multi-armed bandit problems. We propose Bandit and Threshold\nTuning (BATT) to minimize the regret of handover failures in extreme mobility.\nBATT uses $\\epsilon$-binary-search to optimize the threshold of the serving\ncell's signal strength to initiate the handover procedure with\n$\\mathcal{O}(\\log J \\log T)$ regret.It further devises opportunistic Thompson\nsampling, which optimizes the sequence of the target cells to measure for\nreliable handover with $\\mathcal{O}(\\log T)$ regret.Our experiment over a real\nLTE dataset from Chinese high-speed rails validates significant regret\nreduction and a 29.1% handover failure reduction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.02056,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000025498,
      "text":"Specialized federated learning using a mixture of experts\n\n  In federated learning, clients share a global model that has been trained on\ndecentralized local client data. Although federated learning shows significant\npromise as a key approach when data cannot be shared or centralized, current\nmethods show limited privacy properties and have shortcomings when applied to\ncommon real-world scenarios, especially when client data is heterogeneous. In\nthis paper, we propose an alternative method to learn a personalized model for\neach client in a federated setting, with greater generalization abilities than\nprevious methods. To achieve this personalization we propose a federated\nlearning framework using a mixture of experts to combine the specialist nature\nof a locally trained model with the generalist knowledge of a global model. We\nevaluate our method on a variety of datasets with different levels of data\nheterogeneity, and our results show that the mixture of experts model is better\nsuited as a personalized model for devices in these settings, outperforming\nboth fine-tuned global models and local specialists.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.08161,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"ALdataset: a benchmark for pool-based active learning\n\n  Active learning (AL) is a subfield of machine learning (ML) in which a\nlearning algorithm could achieve good accuracy with less training samples by\ninteractively querying a user\/oracle to label new data points. Pool-based AL is\nwell-motivated in many ML tasks, where unlabeled data is abundant, but their\nlabels are hard to obtain. Although many pool-based AL methods have been\ndeveloped, the lack of a comparative benchmarking and integration of techniques\nmakes it difficult to: 1) determine the current state-of-the-art technique; 2)\nevaluate the relative benefit of new methods for various properties of the\ndataset; 3) understand what specific problems merit greater attention; and 4)\nmeasure the progress of the field over time. To conduct easier comparative\nevaluation among AL methods, we present a benchmark task for pool-based active\nlearning, which consists of benchmarking datasets and quantitative metrics that\nsummarize overall performance. We present experiment results for various active\nlearning strategies, both recently proposed and classic highly-cited methods,\nand draw insights from the results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.08861,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000019206,
      "text":"Deep Learning in the Era of Edge Computing: Challenges and Opportunities\n\n  The era of edge computing has arrived. Although the Internet is the backbone\nof edge computing, its true value lies at the intersection of gathering data\nfrom sensors and extracting meaningful information from the sensor data. We\nenvision that in the near future, majority of edge devices will be equipped\nwith machine intelligence powered by deep learning. However, deep\nlearning-based approaches require a large volume of high-quality data to train\nand are very expensive in terms of computation, memory, and power consumption.\nIn this chapter, we describe eight research challenges and promising\nopportunities at the intersection of computer systems, networking, and machine\nlearning. Solving those challenges will enable resource-limited edge devices to\nleverage the amazing capability of deep learning. We hope this chapter could\ninspire new research that will eventually lead to the realization of the vision\nof intelligent edge.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.04081,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"SWIFT: Scalable Wasserstein Factorization for Sparse Nonnegative Tensors\n\n  Existing tensor factorization methods assume that the input tensor follows\nsome specific distribution (i.e. Poisson, Bernoulli, and Gaussian), and solve\nthe factorization by minimizing some empirical loss functions defined based on\nthe corresponding distribution. However, it suffers from several drawbacks: 1)\nIn reality, the underlying distributions are complicated and unknown, making it\ninfeasible to be approximated by a simple distribution. 2) The correlation\nacross dimensions of the input tensor is not well utilized, leading to\nsub-optimal performance. Although heuristics were proposed to incorporate such\ncorrelation as side information under Gaussian distribution, they can not\neasily be generalized to other distributions. Thus, a more principled way of\nutilizing the correlation in tensor factorization models is still an open\nchallenge. Without assuming any explicit distribution, we formulate the tensor\nfactorization as an optimal transport problem with Wasserstein distance, which\ncan handle non-negative inputs.\n  We introduce SWIFT, which minimizes the Wasserstein distance that measures\nthe distance between the input tensor and that of the reconstruction. In\nparticular, we define the N-th order tensor Wasserstein loss for the widely\nused tensor CP factorization and derive the optimization algorithm that\nminimizes it. By leveraging sparsity structure and different equivalent\nformulations for optimizing computational efficiency, SWIFT is as scalable as\nother well-known CP algorithms. Using the factor matrices as features, SWIFT\nachieves up to 9.65% and 11.31% relative improvement over baselines for\ndownstream prediction tasks. Under the noisy conditions, SWIFT achieves up to\n15% and 17% relative improvements over the best competitors for the prediction\ntasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.12785,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000014239,
      "text":"ShiftAddNet: A Hardware-Inspired Deep Network\n\n  Multiplication (e.g., convolution) is arguably a cornerstone of modern deep\nneural networks (DNNs). However, intensive multiplications cause expensive\nresource costs that challenge DNNs' deployment on resource-constrained edge\ndevices, driving several attempts for multiplication-less deep networks. This\npaper presented ShiftAddNet, whose main inspiration is drawn from a common\npractice in energy-efficient hardware implementation, that is, multiplication\ncan be instead performed with additions and logical bit-shifts. We leverage\nthis idea to explicitly parameterize deep networks in this way, yielding a new\ntype of deep network that involves only bit-shift and additive weight layers.\nThis hardware-inspired ShiftAddNet immediately leads to both energy-efficient\ninference and training, without compromising the expressive capacity compared\nto standard DNNs. The two complementary operation types (bit-shift and add)\nadditionally enable finer-grained control of the model's learning capacity,\nleading to more flexible trade-off between accuracy and (training) efficiency,\nas well as improved robustness to quantization and pruning. We conduct\nextensive experiments and ablation studies, all backed up by our FPGA-based\nShiftAddNet implementation and energy measurements. Compared to existing DNNs\nor other multiplication-less models, ShiftAddNet aggressively reduces over 80%\nhardware-quantified energy cost of DNNs training and inference, while offering\ncomparable or better accuracies. Codes and pre-trained models are available at\nhttps:\/\/github.com\/RICE-EIC\/ShiftAddNet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.0841,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Automatic Feasibility Study via Data Quality Analysis for ML: A\n  Case-Study on Label Noise\n\n  In our experience of working with domain experts who are using today's AutoML\nsystems, a common problem we encountered is what we call \"unrealistic\nexpectations\" -- when users are facing a very challenging task with a noisy\ndata acquisition process, while being expected to achieve startlingly high\naccuracy with machine learning (ML). Many of these are predestined to fail from\nthe beginning. In traditional software engineering, this problem is addressed\nvia a feasibility study, an indispensable step before developing any software\nsystem. In this paper, we present Snoopy, with the goal of supporting data\nscientists and machine learning engineers performing a systematic and\ntheoretically founded feasibility study before building ML applications. We\napproach this problem by estimating the irreducible error of the underlying\ntask, also known as the Bayes error rate (BER), which stems from data quality\nissues in datasets used to train or evaluate ML model artifacts. We design a\npractical Bayes error estimator that is compared against baseline feasibility\nstudy candidates on 6 datasets (with additional real and synthetic noise of\ndifferent levels) in computer vision and natural language processing.\nFurthermore, by including our systematic feasibility study with additional\nsignals into the iterative label cleaning process, we demonstrate in end-to-end\nexperiments how users are able to save substantial labeling time and monetary\nefforts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.15533,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"How do Offline Measures for Exploration in Reinforcement Learning\n  behave?\n\n  Sufficient exploration is paramount for the success of a reinforcement\nlearning agent. Yet, exploration is rarely assessed in an algorithm-independent\nway. We compare the behavior of three data-based, offline exploration metrics\ndescribed in the literature on intuitive simple distributions and highlight\nproblems to be aware of when using them. We propose a fourth metric,uniform\nrelative entropy, and implement it using either a k-nearest-neighbor or a\nnearest-neighbor-ratio estimator, highlighting that the implementation choices\nhave a profound impact on these measures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.13116,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"An empirical study of domain-agnostic semi-supervised learning via\n  energy-based models: joint-training and pre-training\n\n  A class of recent semi-supervised learning (SSL) methods heavily rely on\ndomain-specific data augmentations. In contrast, generative SSL methods involve\nunsupervised learning based on generative models by either joint-training or\npre-training, and are more appealing from the perspective of being\ndomain-agnostic, since they do not inherently require data augmentations.\nJoint-training estimates the joint distribution of observations and labels,\nwhile pre-training is taken over observations only. Recently, energy-based\nmodels (EBMs) have achieved promising results for generative modeling.\nJoint-training via EBMs for SSL has been explored with encouraging results\nacross different data modalities. In this paper, we make two contributions.\nFirst, we explore pre-training via EBMs for SSL and compare it to\njoint-training. Second, a suite of experiments are conducted over domains of\nimage classification and natural language labeling to give a realistic whole\npicture of the performances of EBM based SSL methods. It is found that\njoint-training EBMs outperform pre-training EBMs marginally but nearly\nconsistently.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.05473,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"The Impact of Isolation Kernel on Agglomerative Hierarchical Clustering\n  Algorithms\n\n  Agglomerative hierarchical clustering (AHC) is one of the popular clustering\napproaches. Existing AHC methods, which are based on a distance measure, have\none key issue: it has difficulty in identifying adjacent clusters with varied\ndensities, regardless of the cluster extraction methods applied on the\nresultant dendrogram. In this paper, we identify the root cause of this issue\nand show that the use of a data-dependent kernel (instead of distance or\nexisting kernel) provides an effective means to address it. We analyse the\ncondition under which existing AHC methods fail to extract clusters\neffectively; and the reason why the data-dependent kernel is an effective\nremedy. This leads to a new approach to kernerlise existing hierarchical\nclustering algorithms such as existing traditional AHC algorithms, HDBSCAN, GDL\nand PHA. In each of these algorithms, our empirical evaluation shows that a\nrecently introduced Isolation Kernel produces a higher quality or purer\ndendrogram than distance, Gaussian Kernel and adaptive Gaussian Kernel.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.11598,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000032451,
      "text":"An Efficient Adversarial Attack for Tree Ensembles\n\n  We study the problem of efficient adversarial attacks on tree based ensembles\nsuch as gradient boosting decision trees (GBDTs) and random forests (RFs).\nSince these models are non-continuous step functions and gradient does not\nexist, most existing efficient adversarial attacks are not applicable. Although\ndecision-based black-box attacks can be applied, they cannot utilize the\nspecial structure of trees. In our work, we transform the attack problem into a\ndiscrete search problem specially designed for tree ensembles, where the goal\nis to find a valid \"leaf tuple\" that leads to mis-classification while having\nthe shortest distance to the original input. With this formulation, we show\nthat a simple yet effective greedy algorithm can be applied to iteratively\noptimize the adversarial example by moving the leaf tuple to its neighborhood\nwithin hamming distance 1. Experimental results on several large GBDT and RF\nmodels with up to hundreds of trees demonstrate that our method can be\nthousands of times faster than the previous mixed-integer linear programming\n(MILP) based approach, while also providing smaller (better) adversarial\nexamples than decision-based black-box attacks on general $\\ell_p$ ($p=1, 2,\n\\infty$) norm perturbations. Our code is available at\nhttps:\/\/github.com\/chong-z\/tree-ensemble-attack.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.13303,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000049008,
      "text":"Trajectory-wise Multiple Choice Learning for Dynamics Generalization in\n  Reinforcement Learning\n\n  Model-based reinforcement learning (RL) has shown great potential in various\ncontrol tasks in terms of both sample-efficiency and final performance.\nHowever, learning a generalizable dynamics model robust to changes in dynamics\nremains a challenge since the target transition dynamics follow a multi-modal\ndistribution. In this paper, we present a new model-based RL algorithm, coined\ntrajectory-wise multiple choice learning, that learns a multi-headed dynamics\nmodel for dynamics generalization. The main idea is updating the most accurate\nprediction head to specialize each head in certain environments with similar\ndynamics, i.e., clustering environments. Moreover, we incorporate context\nlearning, which encodes dynamics-specific information from past experiences\ninto the context latent vector, enabling the model to perform online adaptation\nto unseen environments. Finally, to utilize the specialized prediction heads\nmore effectively, we propose an adaptive planning method, which selects the\nmost accurate prediction head over a recent experience. Our method exhibits\nsuperior zero-shot generalization performance across a variety of control\ntasks, compared to state-of-the-art RL methods. Source code and videos are\navailable at https:\/\/sites.google.com\/view\/trajectory-mcl.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.15377,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000059936,
      "text":"Supervised sequential pattern mining of event sequences in sport to\n  identify important patterns of play: an application to rugby union\n\n  Given a set of sequences comprised of time-ordered events, sequential pattern\nmining is useful to identify frequent subsequences from different sequences or\nwithin the same sequence. However, in sport, these techniques cannot determine\nthe importance of particular patterns of play to good or bad outcomes, which is\noften of greater interest to coaches and performance analysts. In this study,\nwe apply a recently proposed supervised sequential pattern mining algorithm\ncalled safe pattern pruning (SPP) to 490 labelled event sequences representing\npassages of play from one rugby team's matches from the 2018 Japan Top League.\nWe compare the SPP-obtained patterns that are the most discriminative between\nscoring and non-scoring outcomes from both the team's and opposition teams'\nperspectives, with the most frequent patterns obtained with well-known\nunsupervised sequential pattern mining algorithms when applied to subsets of\nthe original dataset, split on the label. Our obtained results found that\nlinebreaks, successful lineouts, regained kicks in play, repeated\nphase-breakdown play, and failed exit plays by the opposition team were\nidentified as as the patterns that discriminated most between the team scoring\nand not scoring. Opposition team linebreaks, errors made by the team,\nopposition team lineouts, and repeated phase-breakdown play by the opposition\nteam were identified as the patterns that discriminated most between the\nopposition team scoring and not scoring. It was also found that, by virtue of\nits supervised nature as well as its pruning and safe-screening properties, SPP\nobtained a greater variety of generally more sophisticated patterns than the\nunsupervised models, which are likely to be of more utility to coaches and\nperformance analysts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.14816,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000035432,
      "text":"Higher Order Linear Transformer\n\n  Following up on the linear transformer part of the article from Katharopoulos\net al., that takes this idea from Shen et al., the trick that produces a linear\ncomplexity for the attention mechanism is re-used and extended to a\nsecond-order approximation of the softmax normalization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.09467,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000021855,
      "text":"ARENA: A Data-driven Radio Access Networks Analysis of Football Events\n\n  Mass events represent one of the most challenging scenarios for mobile\nnetworks because, although their date and time are usually known in advance,\nthe actual demand for resources is difficult to predict due to its dependency\non many different factors. Based on data provided by a major European carrier\nduring mass events in a football stadium comprising up to 30.000 people, 16\nbase station sectors and $1$Km$^2$ area, we performed a data-driven analysis of\nthe radio access network infrastructure dynamics during such events. Given the\ninsights obtained from the analysis, we developed ARENA, a model-free deep\nlearning Radio Access Network (RAN) capacity forecasting solution that, taking\nas input past network monitoring data and events context information, provides\nguidance to mobile operators on the expected RAN capacity needed during a\nfuture event. Our results, validated against real events contained in the\ndataset, illustrate the effectiveness of our proposed solution.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.06266,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Model-Based Reinforcement Learning for Type 1Diabetes Blood Glucose\n  Control\n\n  In this paper we investigate the use of model-based reinforcement learning to\nassist people with Type 1 Diabetes with insulin dose decisions. The proposed\narchitecture consists of multiple Echo State Networks to predict blood glucose\nlevels combined with Model Predictive Controller for planning. Echo State\nNetwork is a version of recurrent neural networks which allows us to learn long\nterm dependencies in the input of time series data in an online manner.\nAdditionally, we address the quantification of uncertainty for a more robust\ncontrol. Here, we used ensembles of Echo State Networks to capture model\n(epistemic) uncertainty. We evaluated the approach with the FDA-approved\nUVa\/Padova Type 1 Diabetes simulator and compared the results against baseline\nalgorithms such as Basal-Bolus controller and Deep Q-learning. The results\nsuggest that the model-based reinforcement learning algorithm can perform\nequally or better than the baseline algorithms for the majority of virtual Type\n1 Diabetes person profiles tested.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.14957,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Dimensionality Reduction and Anomaly Detection for CPPS Data using\n  Autoencoder\n\n  Unsupervised anomaly detection (AD) is a major topic in the field of\nCyber-Physical Production Systems (CPPSs). A closely related concern is\ndimensionality reduction (DR) which is: 1) often used as a preprocessing step\nin an AD solution, 2) a sort of AD, if a measure of observation conformity to\nthe learned data manifold is provided.\n  We argue that the two aspects can be complementary in a CPPS anomaly\ndetection solution. In this work, we focus on the nonlinear autoencoder (AE) as\na DR\/AD approach. The contribution of this work is: 1) we examine the\nsuitability of AE reconstruction error as an AD decision criterion in CPPS\ndata. 2) we analyze its relation to a potential second-phase AD approach in the\nAE latent space 3) we evaluate the performance of the approach on three\nreal-world datasets. Moreover, the approach outperforms state-of-the-art\ntechniques, alongside a relatively simple and straightforward application.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.09094,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000019537,
      "text":"Multi-Agent Reinforcement Learning in NOMA-aided UAV Networks for\n  Cellular Offloading\n\n  A novel framework is proposed for cellular offloading with the aid of\nmultiple unmanned aerial vehicles (UAVs), while the non-orthogonal multiple\naccess (NOMA) technique is employed at each UAV to further improve the spectrum\nefficiency of the wireless network. The optimization problem of joint\nthree-dimensional (3D) trajectory design and power allocation is formulated for\nmaximizing the throughput. Since ground mobile users are considered as roaming\ncontinuously, the UAVs need to be re-deployed timely based on the movement of\nusers. In an effort to solve this pertinent dynamic problem, a K-means based\nclustering algorithm is first adopted for periodically partitioning users.\nAfterward, a mutual deep Q-network (MDQN) algorithm is proposed to jointly\ndetermine the optimal 3D trajectory and power allocation of UAVs. In contrast\nto the conventional DQN algorithm, the MDQN algorithm enables the experience of\nmulti-agent to be input into a shared neural network to shorten the training\ntime with the assistance of state abstraction. Numerical results demonstrate\nthat: 1) the proposed MDQN algorithm is capable of converging under minor\nconstraints and has a faster convergence rate than the conventional DQN\nalgorithm in the multi-agent case; 2) The achievable sum rate of the NOMA\nenhanced UAV network is 23% superior to the case of orthogonal multiple access\n(OMA); 3) By designing the optimal 3D trajectory of UAVs with the aid of the\nMDON algorithm, the sum rate of the network enjoys 142% and 56% gains than that\nof invoking the circular trajectory and the 2D trajectory, respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.05816,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"Duality-Induced Regularizer for Tensor Factorization Based Knowledge\n  Graph Completion\n\n  Tensor factorization based models have shown great power in knowledge graph\ncompletion (KGC). However, their performance usually suffers from the\noverfitting problem seriously. This motivates various regularizers -- such as\nthe squared Frobenius norm and tensor nuclear norm regularizers -- while the\nlimited applicability significantly limits their practical usage. To address\nthis challenge, we propose a novel regularizer -- namely, DUality-induced\nRegulArizer (DURA) -- which is not only effective in improving the performance\nof existing models but widely applicable to various methods. The major novelty\nof DURA is based on the observation that, for an existing tensor factorization\nbased KGC model (primal), there is often another distance based KGC model\n(dual) closely associated with it. Experiments show that DURA yields consistent\nand significant improvements on benchmarks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.04437,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000045697,
      "text":"Interpretable collaborative data analysis on distributed data\n\n  This paper proposes an interpretable non-model sharing collaborative data\nanalysis method as one of the federated learning systems, which is an emerging\ntechnology to analyze distributed data. Analyzing distributed data is essential\nin many applications such as medical, financial, and manufacturing data\nanalyses due to privacy, and confidentiality concerns. In addition,\ninterpretability of the obtained model has an important role for practical\napplications of the federated learning systems. By centralizing intermediate\nrepresentations, which are individually constructed in each party, the proposed\nmethod obtains an interpretable model, achieving a collaborative analysis\nwithout revealing the individual data and learning model distributed over local\nparties. Numerical experiments indicate that the proposed method achieves\nbetter recognition performance for artificial and real-world problems than\nindividual analysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.14779,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Data-Free Model Extraction\n\n  Current model extraction attacks assume that the adversary has access to a\nsurrogate dataset with characteristics similar to the proprietary data used to\ntrain the victim model. This requirement precludes the use of existing model\nextraction techniques on valuable models, such as those trained on rare or hard\nto acquire datasets. In contrast, we propose data-free model extraction methods\nthat do not require a surrogate dataset. Our approach adapts techniques from\nthe area of data-free knowledge transfer for model extraction. As part of our\nstudy, we identify that the choice of loss is critical to ensuring that the\nextracted model is an accurate replica of the victim model. Furthermore, we\naddress difficulties arising from the adversary's limited access to the victim\nmodel in a black-box setting. For example, we recover the model's logits from\nits probability predictions to approximate gradients. We find that the proposed\ndata-free model extraction approach achieves high-accuracy with reasonable\nquery complexity -- 0.99x and 0.92x the victim model accuracy on SVHN and\nCIFAR-10 datasets given 2M and 20M queries respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.13548,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000094705,
      "text":"Self-Supervised Time Series Representation Learning by Inter-Intra\n  Relational Reasoning\n\n  Self-supervised learning achieves superior performance in many domains by\nextracting useful representations from the unlabeled data. However, most of\ntraditional self-supervised methods mainly focus on exploring the inter-sample\nstructure while less efforts have been concentrated on the underlying\nintra-temporal structure, which is important for time series data. In this\npaper, we present SelfTime: a general self-supervised time series\nrepresentation learning framework, by exploring the inter-sample relation and\nintra-temporal relation of time series to learn the underlying structure\nfeature on the unlabeled time series. Specifically, we first generate the\ninter-sample relation by sampling positive and negative samples of a given\nanchor sample, and intra-temporal relation by sampling time pieces from this\nanchor. Then, based on the sampled relation, a shared feature extraction\nbackbone combined with two separate relation reasoning heads are employed to\nquantify the relationships of the sample pairs for inter-sample relation\nreasoning, and the relationships of the time piece pairs for intra-temporal\nrelation reasoning, respectively. Finally, the useful representations of time\nseries are extracted from the backbone under the supervision of relation\nreasoning heads. Experimental results on multiple real-world time series\ndatasets for time series classification task demonstrate the effectiveness of\nthe proposed method. Code and data are publicly available at\nhttps:\/\/haoyfan.github.io\/.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.01104,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000035432,
      "text":"Efficient PAC Learning from the Crowd with Pairwise Comparisons\n\n  We study crowdsourced PAC learning of threshold functions, where the labels\nare gathered from a pool of annotators some of whom may behave adversarially.\nThis is yet a challenging problem and until recently has computationally and\nquery efficient PAC learning algorithm been established by Awasthi et al.\n(2017). In this paper, we show that by leveraging the more easily acquired\npairwise comparison queries, it is possible to exponentially reduce the label\ncomplexity while retaining the overall query complexity and runtime. Our main\nalgorithmic contributions are a comparison-equipped labeling scheme that can\nfaithfully recover the true labels of a small set of instances, and a\nlabel-efficient filtering process that in conjunction with the small labeled\nset can reliably infer the true labels of a large instance set.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.06015,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"GANMEX: One-vs-One Attributions Guided by GAN-based Counterfactual\n  Explanation Baselines\n\n  Attribution methods have been shown as promising approaches for identifying\nkey features that led to learned model predictions. While most existing\nattribution methods rely on a baseline input for performing feature\nperturbations, limited research has been conducted to address the baseline\nselection issues. Poor choices of baselines limit the ability of one-vs-one\n(1-vs-1) explanations for multi-class classifiers, which means the attribution\nmethods were not able to explain why an input belongs to its original class but\nnot the other specified target class. 1-vs-1 explanation is crucial when\ncertain classes are more similar than others, e.g. two bird types among\nmultiple animals, by focusing on key differentiating features rather than\nshared features across classes. In this paper, we present GAN-based Model\nEXplainability (GANMEX), a novel approach applying Generative Adversarial\nNetworks (GAN) by incorporating the to-be-explained classifier as part of the\nadversarial networks. Our approach effectively selects the counterfactual\nbaseline as the closest realistic sample belong to the target class, which\nallows attribution methods to provide true 1-vs-1 explanations. We showed that\nGANMEX baselines improved the saliency maps and led to stronger performance on\nperturbation-based evaluation metrics over the existing baselines. Existing\nattribution results are known for being insensitive to model randomization, and\nwe demonstrated that GANMEX baselines led to better outcome under the cascading\nrandomization of the model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.02842,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000038412,
      "text":"Depth Self-Optimized Learning Toward Data Science\n\n  We propose a two-stage model called Depth Self-Optimized Learning (DSOL),\nwhich aims to realize ANN depth self-configuration, self-optimization as well\nas ANN training without manual intervention. In the first stage of DSOL, it\nwill configure ANN of specific depth according to a specific dataset. In the\nsecond stage, DSOL will continuously optimize ANN based on Reinforcement\nLearning (RL). Finally, the optimal depth is returned to the first stage of\nDSOL for training, so that DSOL can configure the appropriate ANN depth and\nperform more reasonable optimization when processing similar datasets again. In\nthe experiment, we ran DSOL on the Iris and Boston housing datasets, and the\nresults showed that DSOL performed well. We have uploaded the experiment\nrecords and code to our Github.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.14246,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Monte Carlo Tree Search for a single target search game on a 2-D lattice\n\n  Monte Carlo Tree Search (MCTS) is a branch of stochastic modeling that\nutilizes decision trees for optimization, mostly applied to artificial\nintelligence (AI) game players. This project imagines a game in which an AI\nplayer searches for a stationary target within a 2-D lattice. We analyze its\nbehavior with different target distributions and compare its efficiency to the\nLevy Flight Search, a model for animal foraging behavior. In addition to\nsimulated data analysis we prove two theorems about the convergence of MCTS\nwhen computation constraints neglected.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.14381,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Self-supervised Visual Reinforcement Learning with Object-centric\n  Representations\n\n  Autonomous agents need large repertoires of skills to act reasonably on new\ntasks that they have not seen before. However, acquiring these skills using\nonly a stream of high-dimensional, unstructured, and unlabeled observations is\na tricky challenge for any autonomous agent. Previous methods have used\nvariational autoencoders to encode a scene into a low-dimensional vector that\ncan be used as a goal for an agent to discover new skills. Nevertheless, in\ncompositional\/multi-object environments it is difficult to disentangle all the\nfactors of variation into such a fixed-length representation of the whole\nscene. We propose to use object-centric representations as a modular and\nstructured observation space, which is learned with a compositional generative\nworld model. We show that the structure in the representations in combination\nwith goal-conditioned attention policies helps the autonomous agent to discover\nand learn useful skills. These skills can be further combined to address\ncompositional tasks like the manipulation of several different objects.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.1185,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Solving The Lunar Lander Problem under Uncertainty using Reinforcement\n  Learning\n\n  Reinforcement Learning (RL) is an area of machine learning concerned with\nenabling an agent to navigate an environment with uncertainty in order to\nmaximize some notion of cumulative long-term reward. In this paper, we\nimplement and analyze two different RL techniques, Sarsa and Deep QLearning, on\nOpenAI Gym's LunarLander-v2 environment. We then introduce additional\nuncertainty to the original problem to test the robustness of the mentioned\ntechniques. With our best models, we are able to achieve average rewards of\n170+ with the Sarsa agent and 200+ with the Deep Q-Learning agent on the\noriginal problem. We also show that these techniques are able to overcome the\nadditional uncertainities and achieve positive average rewards of 100+ with\nboth agents. We then perform a comparative analysis of the two techniques to\nconclude which agent peforms better.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.15069,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Graph convolutions that can finally model local structure\n\n  Despite quick progress in the last few years, recent studies have shown that\nmodern graph neural networks can still fail at very simple tasks, like\ndetecting small cycles. This hints at the fact that current networks fail to\ncatch information about the local structure, which is problematic if the\ndownstream task heavily relies on graph substructure analysis, as in the\ncontext of chemistry. We propose a very simple correction to the now standard\nGIN convolution that enables the network to detect small cycles with nearly no\ncost in terms of computation time and number of parameters. Tested on real life\nmolecule property datasets, our model consistently improves performance on\nlarge multi-tasked datasets over all baselines, both globally and on a per-task\nsetting.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.13538,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000002914,
      "text":"Rethinking Uncertainty in Deep Learning: Whether and How it Improves\n  Robustness\n\n  Deep neural networks (DNNs) are known to be prone to adversarial attacks, for\nwhich many remedies are proposed. While adversarial training (AT) is regarded\nas the most robust defense, it suffers from poor performance both on clean\nexamples and under other types of attacks, e.g. attacks with larger\nperturbations. Meanwhile, regularizers that encourage uncertain outputs, such\nas entropy maximization (EntM) and label smoothing (LS) can maintain accuracy\non clean examples and improve performance under weak attacks, yet their ability\nto defend against strong attacks is still in doubt. In this paper, we revisit\nuncertainty promotion regularizers, including EntM and LS, in the field of\nadversarial learning. We show that EntM and LS alone provide robustness only\nunder small perturbations. Contrarily, we show that uncertainty promotion\nregularizers complement AT in a principled manner, consistently improving\nperformance on both clean examples and under various attacks, especially\nattacks with large perturbations. We further analyze how uncertainty promotion\nregularizers enhance the performance of AT from the perspective of Jacobian\nmatrices $\\nabla_X f(X;\\theta)$, and find out that EntM effectively shrinks the\nnorm of Jacobian matrices and hence promotes robustness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.06428,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000063247,
      "text":"Discriminative, Generative and Self-Supervised Approaches for\n  Target-Agnostic Learning\n\n  Supervised learning, characterized by both discriminative and generative\nlearning, seeks to predict the values of single (or sometimes multiple)\npredefined target attributes based on a predefined set of predictor attributes.\nFor applications where the information available and predictions to be made may\nvary from instance to instance, we propose the task of target-agnostic learning\nwhere arbitrary disjoint sets of attributes can be used for each of predictors\nand targets for each to-be-predicted instance. For this task, we survey a wide\nrange of techniques available for handling missing values, self-supervised\ntraining and pseudo-likelihood training, and adapt them to a suite of\nalgorithms that are suitable for the task. We conduct extensive experiments on\nthis suite of algorithms on a large collection of categorical, continuous and\ndiscretized datasets, and report their performance in terms of both\nclassification and regression errors. We also report the training and\nprediction time of these algorithms when handling large-scale datasets. Both\ngenerative and self-supervised learning models are shown to perform well at the\ntask, although their characteristics towards the different types of data are\nquite different. Nevertheless, our derived theorem for the pseudo-likelihood\ntheory also shows that they are related for inferring a joint distribution\nmodel based on the pseudo-likelihood training.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.06833,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000050995,
      "text":"End-to-End Learning from Noisy Crowd to Supervised Machine Learning\n  Models\n\n  Labeling real-world datasets is time consuming but indispensable for\nsupervised machine learning models. A common solution is to distribute the\nlabeling task across a large number of non-expert workers via crowd-sourcing.\nDue to the varying background and experience of crowd workers, the obtained\nlabels are highly prone to errors and even detrimental to the learning models.\nIn this paper, we advocate using hybrid intelligence, i.e., combining deep\nmodels and human experts, to design an end-to-end learning framework from noisy\ncrowd-sourced data, especially in an on-line scenario. We first summarize the\nstate-of-the-art solutions that address the challenges of noisy labels from\nnon-expert crowd and learn from multiple annotators. We show how label\naggregation can benefit from estimating the annotators' confusion matrices to\nimprove the learning process. Moreover, with the help of an expert labeler as\nwell as classifiers, we cleanse aggregated labels of highly informative samples\nto enhance the final classification accuracy. We demonstrate the effectiveness\nof our strategies on several image datasets, i.e. UCI and CIFAR-10, using SVM\nand deep neural networks. Our evaluation shows that our on-line label\naggregation with confusion matrix estimation reduces the error rate of labels\nby over 30%. Furthermore, relabeling only 10% of the data using the expert's\nresults in over 90% classification accuracy with SVM.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.02141,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"Control with adaptive Q-learning\n\n  This paper evaluates adaptive Q-learning (AQL) and single-partition adaptive\nQ-learning (SPAQL), two algorithms for efficient model-free episodic\nreinforcement learning (RL), in two classical control problems (Pendulum and\nCartpole). AQL adaptively partitions the state-action space of a Markov\ndecision process (MDP), while learning the control policy, i. e., the mapping\nfrom states to actions. The main difference between AQL and SPAQL is that the\nlatter learns time-invariant policies, where the mapping from states to actions\ndoes not depend explicitly on the time step. This paper also proposes the SPAQL\nwith terminal state (SPAQL-TS), an improved version of SPAQL tailored for the\ndesign of regulators for control problems. The time-invariant policies are\nshown to result in a better performance than the time-variant ones in both\nproblems studied. These algorithms are particularly fitted to RL problems where\nthe action space is finite, as is the case with the Cartpole problem. SPAQL-TS\nsolves the OpenAI Gym Cartpole problem, while also displaying a higher sample\nefficiency than trust region policy optimization (TRPO), a standard RL\nalgorithm for solving control tasks. Moreover, the policies learned by SPAQL\nare interpretable, while TRPO policies are typically encoded as neural\nnetworks, and therefore hard to interpret. Yielding interpretable policies\nwhile being sample-efficient are the major advantages of SPAQL.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.01023,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Learning transition times in event sequences: the Event-Based Hidden\n  Markov Model of disease progression\n\n  Progressive diseases worsen over time and are characterised by monotonic\nchange in features that track disease progression. Here we connect ideas from\ntwo formerly separate methodologies -- event-based and hidden Markov modelling\n-- to derive a new generative model of disease progression. Our model can\nuniquely infer the most likely group-level sequence and timing of events\n(natural history) from limited datasets. Moreover, it can infer and predict\nindividual-level trajectories (prognosis) even when data are missing, giving it\nhigh clinical utility. Here we derive the model and provide an inference scheme\nbased on the expectation maximisation algorithm. We use clinical, imaging and\nbiofluid data from the Alzheimer's Disease Neuroimaging Initiative to\ndemonstrate the validity and utility of our model. First, we train our model to\nuncover a new group-level sequence of feature changes in Alzheimer's disease\nover a period of ${\\sim}17.3$ years. Next, we demonstrate that our model\nprovides improved utility over a continuous time hidden Markov model by area\nunder the receiver operator characteristic curve ${\\sim}0.23$. Finally, we\ndemonstrate that our model maintains predictive accuracy with up to $50\\%$\nmissing data. These results support the clinical validity of our model and its\nbroader utility in resource-limited medical applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.0301,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Data Augmentation via Structured Adversarial Perturbations\n\n  Data augmentation is a major component of many machine learning methods with\nstate-of-the-art performance. Common augmentation strategies work by drawing\nrandom samples from a space of transformations. Unfortunately, such sampling\napproaches are limited in expressivity, as they are unable to scale to rich\ntransformations that depend on numerous parameters due to the curse of\ndimensionality. Adversarial examples can be considered as an alternative scheme\nfor data augmentation. By being trained on the most difficult modifications of\nthe inputs, the resulting models are then hopefully able to handle other,\npresumably easier, modifications as well. The advantage of adversarial\naugmentation is that it replaces sampling with the use of a single, calculated\nperturbation that maximally increases the loss. The downside, however, is that\nthese raw adversarial perturbations appear rather unstructured; applying them\noften does not produce a natural transformation, contrary to a desirable data\naugmentation technique. To address this, we propose a method to generate\nadversarial examples that maintain some desired natural structure. We first\nconstruct a subspace that only contains perturbations with the desired\nstructure. We then project the raw adversarial gradient onto this space to\nselect a structured transformation that would maximally increase the loss when\napplied. We demonstrate this approach through two types of image\ntransformations: photometric and geometric. Furthermore, we show that training\non such structured adversarial images improves generalization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.04272,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000158946,
      "text":"Time Synchronized State Estimation for Incompletely Observed\n  Distribution Systems Using Deep Learning Considering Realistic Measurement\n  Noise\n\n  Time-synchronized state estimation is a challenge for distribution systems\nbecause of limited real-time observability. This paper addresses this challenge\nby formulating a deep learning (DL)-based approach to perform unbalanced\nthree-phase distribution system state estimation (DSSE). Initially, a\ndata-driven approach for judicious measurement selection to facilitate reliable\nstate estimation is provided. Then, a deep neural network (DNN) is trained to\nperform DSSE for systems that are incompletely observed by synchrophasor\nmeasurement devices (SMDs). Robustness of the proposed methodology is\ndemonstrated by considering realistic measurement error models for SMDs. A\ncomparative study of the DNN-based DSSE with classical linear state estimation\nindicates that the DL-based approach gives better accuracy with a significantly\nsmaller number of SMDs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.11235,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"An Empirical Study of Representation Learning for Reinforcement Learning\n  in Healthcare\n\n  Reinforcement Learning (RL) has recently been applied to sequential\nestimation and prediction problems identifying and developing hypothetical\ntreatment strategies for septic patients, with a particular focus on offline\nlearning with observational data. In practice, successful RL relies on\ninformative latent states derived from sequential observations to develop\noptimal treatment strategies. To date, how best to construct such states in a\nhealthcare setting is an open question. In this paper, we perform an empirical\nstudy of several information encoding architectures using data from septic\npatients in the MIMIC-III dataset to form representations of a patient state.\nWe evaluate the impact of representation dimension, correlations with\nestablished acuity scores, and the treatment policies derived from them. We\nfind that sequentially formed state representations facilitate effective policy\nlearning in batch settings, validating a more thoughtful approach to\nrepresentation learning that remains faithful to the sequential and partial\nnature of healthcare data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.11619,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000032783,
      "text":"Neural collapse with unconstrained features\n\n  Neural collapse is an emergent phenomenon in deep learning that was recently\ndiscovered by Papyan, Han and Donoho. We propose a simple \"unconstrained\nfeatures model\" in which neural collapse also emerges empirically. By studying\nthis model, we provide some explanation for the emergence of neural collapse in\nterms of the landscape of empirical risk.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.02312,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"ReMix: Calibrated Resampling for Class Imbalance in Deep learning\n\n  Class imbalance is a problem of significant importance in applied deep\nlearning where trained models are exploited for decision support and automated\ndecisions in critical areas such as health and medicine, transportation, and\nfinance. The challenge of learning deep models from imbalanced training data\nremains high, and the state-of-the-art solutions are typically data dependent\nand primarily focused on image data. Real-world imbalanced classification\nproblems, however, are much more diverse thus necessitating a general solution\nthat can be applied to tabular, image and text data. In this paper, we propose\nReMix, a training technique that leverages batch resampling, instance mixing\nand soft-labels to enable the induction of robust deep models for imbalanced\nlearning. Our results show that dense nets and CNNs trained with ReMix\ngenerally outperform the alternatives according to the g-mean and are better\ncalibrated according to the balanced Brier score.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.02792,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Weight Update Skipping: Reducing Training Time for Artificial Neural\n  Networks\n\n  Artificial Neural Networks (ANNs) are known as state-of-the-art techniques in\nMachine Learning (ML) and have achieved outstanding results in data-intensive\napplications, such as recognition, classification, and segmentation. These\nnetworks mostly use deep layers of convolution or fully connected layers with\nmany filters in each layer, demanding a large amount of data and tunable\nhyperparameters to achieve competitive accuracy. As a result, storage,\ncommunication, and computational costs of training (in particular training\ntime) become limiting factors to scale them up. In this paper, we propose a new\ntraining methodology for ANNs that exploits the observation of improvement of\naccuracy shows temporal variations which allow us to skip updating weights when\nthe variation is minuscule. During such time windows, we keep updating bias\nwhich ensures the network still trains and avoids overfitting; however, we\nselectively skip updating weights (and their time-consuming computations). Such\na training approach virtually achieves the same accuracy with considerably less\ncomputational cost, thus lower training time. We propose two methods for\nupdating weights and evaluate them by analyzing four state-of-the-art models,\nAlexNet, VGG-11, VGG-16, ResNet-18 on CIFAR datasets. On average, our two\nproposed methods called WUS and WUS+LR reduced the training time (compared to\nthe baseline) by 54%, and 50%, respectively on CIFAR-10; and 43% and 35% on\nCIFAR-100, respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01489,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000047353,
      "text":"Distributed Machine Learning for Wireless Communication Networks:\n  Techniques, Architectures, and Applications\n\n  Distributed machine learning (DML) techniques, such as federated learning,\npartitioned learning, and distributed reinforcement learning, have been\nincreasingly applied to wireless communications. This is due to improved\ncapabilities of terminal devices, explosively growing data volume, congestion\nin the radio interfaces, and increasing concern of data privacy. The unique\nfeatures of wireless systems, such as large scale, geographically dispersed\ndeployment, user mobility, and massive amount of data, give rise to new\nchallenges in the design of DML techniques. There is a clear gap in the\nexisting literature in that the DML techniques are yet to be systematically\nreviewed for their applicability to wireless systems. This survey bridges the\ngap by providing a contemporary and comprehensive survey of DML techniques with\na focus on wireless networks. Specifically, we review the latest applications\nof DML in power control, spectrum management, user association, and edge cloud\ncomputing. The optimality, scalability, convergence rate, computation cost, and\ncommunication overhead of DML are analyzed. We also discuss the potential\nadversarial attacks faced by DML applications, and describe state-of-the-art\ncountermeasures to preserve privacy and security. Last but not least, we point\nout a number of key issues yet to be addressed, and collate potentially\ninteresting and challenging topics for future research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.10069,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000192722,
      "text":"Fairness and Accuracy in Federated Learning\n\n  In the federated learning setting, multiple clients jointly train a model\nunder the coordination of the central server, while the training data is kept\non the client to ensure privacy. Normally, inconsistent distribution of data\nacross different devices in a federated network and limited communication\nbandwidth between end devices impose both statistical heterogeneity and\nexpensive communication as major challenges for federated learning. This paper\nproposes an algorithm to achieve more fairness and accuracy in federated\nlearning (FedFa). It introduces an optimization scheme that employs a double\nmomentum gradient, thereby accelerating the convergence rate of the model. An\nappropriate weight selection algorithm that combines the information quantity\nof training accuracy and training frequency to measure the weights is proposed.\nThis procedure assists in addressing the issue of unfairness in federated\nlearning due to preferences for certain clients. Our results show that the\nproposed FedFa algorithm outperforms the baseline algorithm in terms of\naccuracy and fairness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.14843,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"Learning Adversarial Markov Decision Processes with Delayed Feedback\n\n  Reinforcement learning typically assumes that agents observe feedback for\ntheir actions immediately, but in many real-world applications (like\nrecommendation systems) feedback is observed in delay. This paper studies\nonline learning in episodic Markov decision processes (MDPs) with unknown\ntransitions, adversarially changing costs and unrestricted delayed feedback.\nThat is, the costs and trajectory of episode $k$ are revealed to the learner\nonly in the end of episode $k + d^k$, where the delays $d^k$ are neither\nidentical nor bounded, and are chosen by an oblivious adversary. We present\nnovel algorithms based on policy optimization that achieve near-optimal\nhigh-probability regret of $\\sqrt{K + D}$ under full-information feedback,\nwhere $K$ is the number of episodes and $D = \\sum_{k} d^k$ is the total delay.\nUnder bandit feedback, we prove similar $\\sqrt{K + D}$ regret assuming the\ncosts are stochastic, and $(K + D)^{2\/3}$ regret in the general case. We are\nthe first to consider regret minimization in the important setting of MDPs with\ndelayed feedback.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.07823,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Annealed Importance Sampling with q-Paths\n\n  Annealed importance sampling (AIS) is the gold standard for estimating\npartition functions or marginal likelihoods, corresponding to importance\nsampling over a path of distributions between a tractable base and an\nunnormalized target. While AIS yields an unbiased estimator for any path,\nexisting literature has been primarily limited to the geometric mixture or\nmoment-averaged paths associated with the exponential family and KL divergence.\nWe explore AIS using $q$-paths, which include the geometric path as a special\ncase and are related to the homogeneous power mean, deformed exponential\nfamily, and $\\alpha$-divergence.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.05321,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Securing Deep Spiking Neural Networks against Adversarial Attacks\n  through Inherent Structural Parameters\n\n  Deep Learning (DL) algorithms have gained popularity owing to their practical\nproblem-solving capacity. However, they suffer from a serious integrity threat,\ni.e., their vulnerability to adversarial attacks. In the quest for DL\ntrustworthiness, recent works claimed the inherent robustness of Spiking Neural\nNetworks (SNNs) to these attacks, without considering the variability in their\nstructural spiking parameters. This paper explores the security enhancement of\nSNNs through internal structural parameters. Specifically, we investigate the\nSNNs robustness to adversarial attacks with different values of the neuron's\nfiring voltage thresholds and time window boundaries. We thoroughly study SNNs\nsecurity under different adversarial attacks in the strong white-box setting,\nwith different noise budgets and under variable spiking parameters. Our results\nshow a significant impact of the structural parameters on the SNNs' security,\nand promising sweet spots can be reached to design trustworthy SNNs with 85%\nhigher robustness than a traditional non-spiking DL system. To the best of our\nknowledge, this is the first work that investigates the impact of structural\nparameters on SNNs robustness to adversarial attacks. The proposed\ncontributions and the experimental framework is available online to the\ncommunity for reproducible research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.096,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000039736,
      "text":"Deep Fusion Clustering Network\n\n  Deep clustering is a fundamental yet challenging task for data analysis.\nRecently we witness a strong tendency of combining autoencoder and graph neural\nnetworks to exploit structure information for clustering performance\nenhancement. However, we observe that existing literature 1) lacks a dynamic\nfusion mechanism to selectively integrate and refine the information of graph\nstructure and node attributes for consensus representation learning; 2) fails\nto extract information from both sides for robust target distribution (i.e.,\n\"groundtruth\" soft labels) generation. To tackle the above issues, we propose a\nDeep Fusion Clustering Network (DFCN). Specifically, in our network, an\ninterdependency learning-based Structure and Attribute Information Fusion\n(SAIF) module is proposed to explicitly merge the representations learned by an\nautoencoder and a graph autoencoder for consensus representation learning.\nAlso, a reliable target distribution generation measure and a triplet\nself-supervision strategy, which facilitate cross-modality information\nexploitation, are designed for network training. Extensive experiments on six\nbenchmark datasets have demonstrated that the proposed DFCN consistently\noutperforms the state-of-the-art deep clustering methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01166,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000386437,
      "text":"Improving Interpretability in Medical Imaging Diagnosis using\n  Adversarial Training\n\n  We investigate the influence of adversarial training on the interpretability\nof convolutional neural networks (CNNs), specifically applied to diagnosing\nskin cancer. We show that gradient-based saliency maps of adversarially trained\nCNNs are significantly sharper and more visually coherent than those of\nstandardly trained CNNs. Furthermore, we show that adversarially trained\nnetworks highlight regions with significant color variation within the lesion,\na common characteristic of melanoma. We find that fine-tuning a robust network\nwith a small learning rate further improves saliency maps' sharpness. Lastly,\nwe provide preliminary work suggesting that robustifying the first layers to\nextract robust low-level features leads to visually coherent explanations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.094,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000093381,
      "text":"Stochastic Compositional Gradient Descent under Compositional\n  Constraints\n\n  This work studies constrained stochastic optimization problems where the\nobjective and constraint functions are convex and expressed as compositions of\nstochastic functions. The problem arises in the context of fair classification,\nfair regression, and the design of queuing systems. Of particular interest is\nthe large-scale setting where an oracle provides the stochastic gradients of\nthe constituent functions, and the goal is to solve the problem with a minimal\nnumber of calls to the oracle. Owing to the compositional form, the stochastic\ngradients provided by the oracle do not yield unbiased estimates of the\nobjective or constraint gradients. Instead, we construct approximate gradients\nby tracking the inner function evaluations, resulting in a quasi-gradient\nsaddle point algorithm. We prove that the proposed algorithm is guaranteed to\nfind the optimal and feasible solution almost surely. We further establish that\nthe proposed algorithm requires $\\mathcal{O}(1\/\\epsilon^4)$ data samples in\norder to obtain an $\\epsilon$-approximate optimal point while also ensuring\nzero constraint violation. The result matches the sample complexity of the\nstochastic compositional gradient descent method for unconstrained problems and\nimproves upon the best-known sample complexity results for the constrained\nsettings. The efficacy of the proposed algorithm is tested on both fair\nclassification and fair regression problems. The numerical results show that\nthe proposed algorithm outperforms the state-of-the-art algorithms in terms of\nthe convergence rate.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01633,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000059936,
      "text":"What Makes a Star Teacher? A Hierarchical BERT Model for Evaluating\n  Teacher's Performance in Online Education\n\n  Education has a significant impact on both society and personal life. With\nthe development of technology, online education has been growing rapidly over\nthe past decade. While there are several online education studies on student\nbehavior analysis, the course concept mining, and course recommendations (Feng,\nTang, and Liu 2019; Pan et al. 2017), there is little research on evaluating\nteachers' performance in online education. In this paper, we conduct a\nsystematic study to understand and effectively predict teachers' performance\nusing the subtitles of 1,085 online courses. Our model-free analysis shows that\nteachers' verbal cues (e.g., question strategy, emotional appealing, and\nhedging) and their course structure design are both significantly correlated\nwith teachers' performance evaluation. Based on these insights, we then propose\na hierarchical course BERT model to predict teachers' performance in online\neducation. Our proposed model can capture the hierarchical structure within\neach course as well as the deep semantic features extracted from the course\ncontent. Experiment results show that our proposed method achieves significant\ngain over several state-of-the-art methods. Our study provides a significant\nsocial impact in helping teachers improve their teaching style and enhance\ntheir instructional material design for more effective online teaching in the\nfuture.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.07244,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Bayesian Neural Ordinary Differential Equations\n\n  Recently, Neural Ordinary Differential Equations has emerged as a powerful\nframework for modeling physical simulations without explicitly defining the\nODEs governing the system, but instead learning them via machine learning.\nHowever, the question: \"Can Bayesian learning frameworks be integrated with\nNeural ODE's to robustly quantify the uncertainty in the weights of a Neural\nODE?\" remains unanswered. In an effort to address this question, we primarily\nevaluate the following categories of inference methods: (a) The No-U-Turn MCMC\nsampler (NUTS), (b) Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) and (c)\nStochastic Langevin Gradient Descent (SGLD). We demonstrate the successful\nintegration of Neural ODEs with the above Bayesian inference frameworks on\nclassical physical systems, as well as on standard machine learning datasets\nlike MNIST, using GPU acceleration. On the MNIST dataset, we achieve a\nposterior sample accuracy of 98.5% on the test ensemble of 10,000 images.\nSubsequently, for the first time, we demonstrate the successful integration of\nvariational inference with normalizing flows and Neural ODEs, leading to a\npowerful Bayesian Neural ODE object. Finally, considering a predator-prey model\nand an epidemiological system, we demonstrate the probabilistic identification\nof model specification in partially-described dynamical systems using universal\nordinary differential equations. Together, this gives a scientific machine\nlearning tool for probabilistic estimation of epistemic uncertainties.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01602,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Margin-Based Transfer Bounds for Meta Learning with Deep Feature\n  Embedding\n\n  By transferring knowledge learned from seen\/previous tasks, meta learning\naims to generalize well to unseen\/future tasks. Existing meta-learning\napproaches have shown promising empirical performance on various multiclass\nclassification problems, but few provide theoretical analysis on the\nclassifiers' generalization ability on future tasks. In this paper, under the\nassumption that all classification tasks are sampled from the same\nmeta-distribution, we leverage margin theory and statistical learning theory to\nestablish three margin-based transfer bounds for meta-learning based multiclass\nclassification (MLMC). These bounds reveal that the expected error of a given\nclassification algorithm for a future task can be estimated with the average\nempirical error on a finite number of previous tasks, uniformly over a class of\npreprocessing feature maps\/deep neural networks (i.e. deep feature embeddings).\nTo validate these bounds, instead of the commonly-used cross-entropy loss, a\nmulti-margin loss is employed to train a number of representative MLMC models.\nExperiments on three benchmarks show that these margin-based models still\nachieve competitive performance, validating the practical value of our\nmargin-based theoretical analysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.07341,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"A One-Size-Fits-All Solution to Conservative Bandit Problems\n\n  In this paper, we study a family of conservative bandit problems (CBPs) with\nsample-path reward constraints, i.e., the learner's reward performance must be\nat least as well as a given baseline at any time. We propose a\nOne-Size-Fits-All solution to CBPs and present its applications to three\nencompassed problems, i.e. conservative multi-armed bandits (CMAB),\nconservative linear bandits (CLB) and conservative contextual combinatorial\nbandits (CCCB). Different from previous works which consider high probability\nconstraints on the expected reward, we focus on a sample-path constraint on the\nactually received reward, and achieve better theoretical guarantees\n($T$-independent additive regrets instead of $T$-dependent) and empirical\nperformance. Furthermore, we extend the results and consider a novel\nconservative mean-variance bandit problem (MV-CBP), which measures the learning\nperformance with both the expected reward and variability. For this extended\nproblem, we provide a novel algorithm with $O(1\/T)$ normalized additive regrets\n($T$-independent in the cumulative form) and validate this result through\nempirical evaluation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.12533,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000043379,
      "text":"Motif-Driven Contrastive Learning of Graph Representations\n\n  Pre-training Graph Neural Networks (GNN) via self-supervised contrastive\nlearning has recently drawn lots of attention. However, most existing works\nfocus on node-level contrastive learning, which cannot capture global graph\nstructure. The key challenge to conducting subgraph-level contrastive learning\nis to sample informative subgraphs that are semantically meaningful. To solve\nit, we propose to learn graph motifs, which are frequently-occurring subgraph\npatterns (e.g. functional groups of molecules), for better subgraph sampling.\nOur framework MotIf-driven Contrastive leaRning Of Graph representations\n(MICRO-Graph) can: 1) use GNNs to extract motifs from large graph datasets; 2)\nleverage learned motifs to sample informative subgraphs for contrastive\nlearning of GNN. We formulate motif learning as a differentiable clustering\nproblem, and adopt EM-clustering to group similar and significant subgraphs\ninto several motifs. Guided by these learned motifs, a sampler is trained to\ngenerate more informative subgraphs, and these subgraphs are used to train GNNs\nthrough graph-to-subgraph contrastive learning. By pre-training on the\nogbg-molhiv dataset with MICRO-Graph, the pre-trained GNN achieves 2.04%\nROC-AUC average performance enhancement on various downstream benchmark\ndatasets, which is significantly higher than other state-of-the-art\nself-supervised learning baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.07233,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"Achieving Adversarial Robustness Requires An Active Teacher\n\n  A new understanding of adversarial examples and adversarial robustness is\nproposed by decoupling the data generator and the label generator (which we\ncall the teacher). In our framework, adversarial robustness is a conditional\nconcept---the student model is not absolutely robust, but robust with respect\nto the teacher. Based on the new understanding, we claim that adversarial\nexamples exist because the student cannot obtain sufficient information of the\nteacher from the training data. Various ways of achieving robustness is\ncompared. Theoretical and numerical evidence shows that to efficiently attain\nrobustness, a teacher that actively provides its information to the student may\nbe necessary.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.05324,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000112255,
      "text":"Modeling Disease Progression Trajectories from Longitudinal\n  Observational Data\n\n  Analyzing disease progression patterns can provide useful insights into the\ndisease processes of many chronic conditions. These analyses may help inform\nrecruitment for prevention trials or the development and personalization of\ntreatments for those affected. We learn disease progression patterns using\nHidden Markov Models (HMM) and distill them into distinct trajectories using\nvisualization methods. We apply it to the domain of Type 1 Diabetes (T1D) using\nlarge longitudinal observational data from the T1DI study group. Our method\ndiscovers distinct disease progression trajectories that corroborate with\nrecently published findings. In this paper, we describe the iterative process\nof developing the model. These methods may also be applied to other chronic\nconditions that evolve over time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01065,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000015563,
      "text":"BSODA: A Bipartite Scalable Framework for Online Disease Diagnosis\n\n  A growing number of people are seeking healthcare advice online. Usually,\nthey diagnose their medical conditions based on the symptoms they are\nexperiencing, which is also known as self-diagnosis. From the machine learning\nperspective, online disease diagnosis is a sequential feature (symptom)\nselection and classification problem. Reinforcement learning (RL) methods are\nthe standard approaches to this type of tasks. Generally, they perform well\nwhen the feature space is small, but frequently become inefficient in tasks\nwith a large number of features, such as the self-diagnosis. To address the\nchallenge, we propose a non-RL Bipartite Scalable framework for Online Disease\ndiAgnosis, called BSODA. BSODA is composed of two cooperative branches that\nhandle symptom-inquiry and disease-diagnosis, respectively. The inquiry branch\ndetermines which symptom to collect next by an information-theoretic reward. We\nemploy a Product-of-Experts encoder to significantly improve the handling of\npartial observations of a large number of features. Besides, we propose several\napproximation methods to substantially reduce the computational cost of the\nreward to a level that is acceptable for online services. Additionally, we\nleverage the diagnosis model to estimate the reward more precisely. For the\ndiagnosis branch, we use a knowledge-guided self-attention model to perform\npredictions. In particular, BSODA determines when to stop inquiry and output\npredictions using both the inquiry and diagnosis models. We demonstrate that\nBSODA outperforms the state-of-the-art methods on several public datasets.\nMoreover, we propose a novel evaluation method to test the transferability of\nsymptom checking methods from synthetic to real-world tasks. Compared to\nexisting RL baselines, BSODA is more effectively scalable to large search\nspaces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.06825,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Physics-Informed Machine Learning Simulator for Wildfire Propagation\n\n  The aim of this work is to evaluate the feasibility of re-implementing some\nkey parts of the widely used Weather Research and Forecasting WRF-SFIRE\nsimulator by replacing its core differential equations numerical solvers with\nstate-of-the-art physics-informed machine learning techniques to solve ODEs and\nPDEs, in order to transform it into a real-time simulator for wildfire spread\nprediction. The main programming language used is Julia, a compiled language\nwhich offers better perfomance than interpreted ones, providing Just in Time\n(JIT) compilation with different optimization levels. Moreover, Julia is\nparticularly well suited for numerical computation and for the solution of\ncomplex physical models, both considering the syntax and the presence of some\nspecific libraries such as DifferentialEquations.jl and ModellingToolkit.jl.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.11589,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Making transport more robust and interpretable by moving data through a\n  small number of anchor points\n\n  Optimal transport (OT) is a widely used technique for distribution alignment,\nwith applications throughout the machine learning, graphics, and vision\ncommunities. Without any additional structural assumptions on trans-port,\nhowever, OT can be fragile to outliers or noise, especially in high dimensions.\nHere, we introduce a new form of structured OT that simultaneously learns\nlow-dimensional structure in data while leveraging this structure to solve the\nalignment task. Compared with OT, the resulting transport plan has better\nstructural interpretability, highlighting the connections between individual\ndata points and local geometry, and is more robust to noise and sampling. We\napply the method to synthetic as well as real datasets, where we show that our\nmethod can facilitate alignment in noisy settings and can be used to both\ncorrect and interpret domain shift.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.09108,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000004669,
      "text":"Adaptive Neighbourhoods for the Discovery of Adversarial Examples\n\n  Deep Neural Networks (DNNs) have often supplied state-of-the-art results in\npattern recognition tasks. Despite their advances, however, the existence of\nadversarial examples have caught the attention of the community. Many existing\nworks have proposed methods for searching for adversarial examples within\nfixed-sized regions around training points. Our work complements and improves\nthese existing approaches by adapting the size of these regions based on the\nproblem complexity and data sampling density. This makes such approaches more\nappropriate for other types of data and may further improve adversarial\ntraining methods by increasing the region sizes without creating incorrect\nlabels.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.02308,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"Coding for Distributed Multi-Agent Reinforcement Learning\n\n  This paper aims to mitigate straggler effects in synchronous distributed\nlearning for multi-agent reinforcement learning (MARL) problems. Stragglers\narise frequently in a distributed learning system, due to the existence of\nvarious system disturbances such as slow-downs or failures of compute nodes and\ncommunication bottlenecks. To resolve this issue, we propose a coded\ndistributed learning framework, which speeds up the training of MARL algorithms\nin the presence of stragglers, while maintaining the same accuracy as the\ncentralized approach. As an illustration, a coded distributed version of the\nmulti-agent deep deterministic policy gradient(MADDPG) algorithm is developed\nand evaluated. Different coding schemes, including maximum distance separable\n(MDS)code, random sparse code, replication-based code, and regular low density\nparity check (LDPC) code are also investigated. Simulations in several\nmulti-robot problems demonstrate the promising performance of the proposed\nframework.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.06178,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"Learning to Sample from Censored Markov Random Fields\n\n  We study learning Censor Markov Random Fields (abbreviated CMRFs). These are\nMarkov Random Fields where some of the nodes are censored (not observed). We\npresent an algorithm for learning high-temperature CMRFs within o(n)\ntransportation distance. Crucially our algorithm makes no assumption about the\nstructure of the graph or the number or location of the observed nodes. We\nobtain stronger results for high girth high-temperature CMRFs as well as\ncomputational lower bounds indicating that our results can not be qualitatively\nimproved.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.02055,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Geometric Entropic Exploration\n\n  Exploration is essential for solving complex Reinforcement Learning (RL)\ntasks. Maximum State-Visitation Entropy (MSVE) formulates the exploration\nproblem as a well-defined policy optimization problem whose solution aims at\nvisiting all states as uniformly as possible. This is in contrast to standard\nuncertainty-based approaches where exploration is transient and eventually\nvanishes. However, existing approaches to MSVE are theoretically justified only\nfor discrete state-spaces as they are oblivious to the geometry of continuous\ndomains. We address this challenge by introducing Geometric Entropy\nMaximisation (GEM), a new algorithm that maximises the geometry-aware Shannon\nentropy of state-visits in both discrete and continuous domains. Our key\ntheoretical contribution is casting geometry-aware MSVE exploration as a\ntractable problem of optimising a simple and novel noise-contrastive objective\nfunction. In our experiments, we show the efficiency of GEM in solving several\nRL problems with sparse rewards, compared against other deep RL exploration\napproaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.12632,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"The Deep Radial Basis Function Data Descriptor (D-RBFDD) Network: A\n  One-Class Neural Network for Anomaly Detection\n\n  Anomaly detection is a challenging problem in machine learning, and is even\nmore so when dealing with instances that are captured in low-level, raw data\nrepresentations without a well-behaved set of engineered features. The Radial\nBasis Function Data Descriptor (RBFDD) network is an effective solution for\nanomaly detection, however, it is a shallow model that does not deal\neffectively with raw data representations. This paper investigates approaches\nto modifying the RBFDD network to transform it into a deep one-class classifier\nsuitable for anomaly detection problems with low-level raw data\nrepresentations. We show that approaches based on transfer learning are not\neffective and our results suggest that this is because the latent\nrepresentations learned by generic classification models are not suitable for\nanomaly detection. Instead we show that an approach that adds multiple\nconvolutional layers before the RBF layer, to form a Deep Radial Basis Function\nData Descriptor (D-RBFDD) network, is very effective. This is shown in a set of\nevaluation experiments using multiple anomaly detection scenarios created from\npublicly available image classification datasets, and a real-world anomaly\ndetection dataset in which different types of arrhythmia are detected in\nelectrocardiogram (ECG) data. Our experiments show that the D-RBFDD network\nout-performs state-of-the-art anomaly detection methods including the Deep\nSupport Vector Data Descriptor (Deep SVDD), One-Class SVM, and Isolation Forest\non the image datasets, and produces competitive results for the ECG dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.09868,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"CPT: Efficient Deep Neural Network Training via Cyclic Precision\n\n  Low-precision deep neural network (DNN) training has gained tremendous\nattention as reducing precision is one of the most effective knobs for boosting\nDNNs' training time\/energy efficiency. In this paper, we attempt to explore\nlow-precision training from a new perspective as inspired by recent findings in\nunderstanding DNN training: we conjecture that DNNs' precision might have a\nsimilar effect as the learning rate during DNN training, and advocate dynamic\nprecision along the training trajectory for further boosting the time\/energy\nefficiency of DNN training. Specifically, we propose Cyclic Precision Training\n(CPT) to cyclically vary the precision between two boundary values which can be\nidentified using a simple precision range test within the first few training\nepochs. Extensive simulations and ablation studies on five datasets and eleven\nmodels demonstrate that CPT's effectiveness is consistent across various\nmodels\/tasks (including classification and language modeling). Furthermore,\nthrough experiments and visualization we show that CPT helps to (1) converge to\na wider minima with a lower generalization error and (2) reduce training\nvariance which we believe opens up a new design knob for simultaneously\nimproving the optimization and efficiency of DNN training. Our codes are\navailable at: https:\/\/github.com\/RICE-EIC\/CPT.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.0658,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"Physics-Informed Deep Learning for Traffic State Estimation\n\n  Traffic state estimation (TSE), which reconstructs the traffic variables\n(e.g., density) on road segments using partially observed data, plays an\nimportant role on efficient traffic control and operation that intelligent\ntransportation systems (ITS) need to provide to people. Over decades, TSE\napproaches bifurcate into two main categories, model-driven approaches and\ndata-driven approaches. However, each of them has limitations: the former\nhighly relies on existing physical traffic flow models, such as\nLighthill-Whitham-Richards (LWR) models, which may only capture limited\ndynamics of real-world traffic, resulting in low-quality estimation, while the\nlatter requires massive data in order to perform accurate and generalizable\nestimation. To mitigate the limitations, this paper introduces a\nphysics-informed deep learning (PIDL) framework to efficiently conduct\nhigh-quality TSE with small amounts of observed data. PIDL contains both\nmodel-driven and data-driven components, making possible the integration of the\nstrong points of both approaches while overcoming the shortcomings of either.\nThis paper focuses on highway TSE with observed data from loop detectors, using\ntraffic density as the traffic variables. We demonstrate the use of PIDL to\nsolve (with data from loop detectors) two popular physical traffic flow models,\ni.e., Greenshields-based LWR and three-parameter-based LWR, and discover the\nmodel parameters. We then evaluate the PIDL-based highway TSE using the Next\nGeneration SIMulation (NGSIM) dataset. The experimental results show the\nadvantages of the PIDL-based approach in terms of estimation accuracy and data\nefficiency over advanced baseline TSE methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.08742,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000053313,
      "text":"Soft Genetic Programming Binary Classifiers\n\n  The study of the classifier's design and it's usage is one of the most\nimportant machine learning areas. With the development of automatic machine\nlearning methods, various approaches are used to build a robust classifier\nmodel. Due to some difficult implementation and customization complexity,\ngenetic programming (GP) methods are not often used to construct classifiers.\nGP classifiers have several limitations and disadvantages. However, the concept\nof \"soft\" genetic programming (SGP) has been developed, which allows the\nlogical operator tree to be more flexible and find dependencies in datasets,\nwhich gives promising results in most cases. This article discusses a method\nfor constructing binary classifiers using the SGP technique. The test results\nare presented. Source code - https:\/\/github.com\/survexman\/sgp_classifier.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.00159,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Fidel: Reconstructing Private Training Samples from Weight Updates in\n  Federated Learning\n\n  With the increasing number of data collectors such as smartphones, immense\namounts of data are available. Federated learning was developed to allow for\ndistributed learning on a massive scale whilst still protecting each users'\nprivacy. This privacy is claimed by the notion that the centralized server does\nnot have any access to a client's data, solely the client's model update. In\nthis paper, we evaluate a novel attack method within regular federated learning\nwhich we name the First Dense Layer Attack (Fidel). The methodology of using\nthis attack is discussed, and as a proof of viability we show how this attack\nmethod can be used to great effect for densely connected networks and\nconvolutional neural networks. We evaluate some key design decisions and show\nthat the usage of ReLu and Dropout are detrimental to the privacy of a client's\nlocal dataset. We show how to recover on average twenty out of thirty private\ndata samples from a client's model update employing a fully connected neural\nnetwork with very little computational resources required. Similarly, we show\nthat over thirteen out of twenty samples can be recovered from a convolutional\nneural network update. An open source implementation of this attack can be\nfound here https:\/\/github.com\/Davidenthoven\/Fidel-Reconstruction-Demo\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.07077,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000052651,
      "text":"Yet Another Representation of Binary Decision Trees: A Mathematical\n  Demonstration\n\n  A decision tree looks like a simple directed acyclic computational graph,\nwhere only the leaf nodes specify the output values and the non-terminals\nspecify their tests or split conditions. From the numerical perspective, we\nexpress decision trees in the language of computational graph. We explicitly\nparameterize the test phase, traversal phase and prediction phase of decision\ntrees based on the bitvectors of non-terminal nodes. As shown, the decision\ntree is a shallow binary network in some sense. Especially, we introduce the\nbitvector matrix to implement the tree traversal in numerical approach, where\nthe core is to convert the logical `AND' operation to arithmetic operations.\nAnd we apply this numerical representation to extend and unify diverse decision\ntrees in concept.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.08152,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Rank the Episodes: A Simple Approach for Exploration in\n  Procedurally-Generated Environments\n\n  Exploration under sparse reward is a long-standing challenge of model-free\nreinforcement learning. The state-of-the-art methods address this challenge by\nintroducing intrinsic rewards to encourage exploration in novel states or\nuncertain environment dynamics. Unfortunately, methods based on intrinsic\nrewards often fall short in procedurally-generated environments, where a\ndifferent environment is generated in each episode so that the agent is not\nlikely to visit the same state more than once. Motivated by how humans\ndistinguish good exploration behaviors by looking into the entire episode, we\nintroduce RAPID, a simple yet effective episode-level exploration method for\nprocedurally-generated environments. RAPID regards each episode as a whole and\ngives an episodic exploration score from both per-episode and long-term views.\nThose highly scored episodes are treated as good exploration behaviors and are\nstored in a small ranking buffer. The agent then imitates the episodes in the\nbuffer to reproduce the past good exploration behaviors. We demonstrate our\nmethod on several procedurally-generated MiniGrid environments, a\nfirst-person-view 3D Maze navigation task from MiniWorld, and several sparse\nMuJoCo tasks. The results show that RAPID significantly outperforms the\nstate-of-the-art intrinsic reward strategies in terms of sample efficiency and\nfinal performance. The code is available at https:\/\/github.com\/daochenzha\/rapid\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.05892,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000014239,
      "text":"A Ternary Bi-Directional LSTM Classification for Brain Activation\n  Pattern Recognition Using fNIRS\n\n  Functional near-infrared spectroscopy (fNIRS) is a non-invasive, low-cost\nmethod used to study the brain's blood flow pattern. Such patterns can enable\nus to classify performed by a subject. In recent research, most classification\nsystems use traditional machine learning algorithms for the classification of\ntasks. These methods, which are easier to implement, usually suffer from low\naccuracy. Further, a complex pre-processing phase is required for data\npreparation before implementing traditional machine learning methods. The\nproposed system uses a Bi-Directional LSTM based deep learning architecture for\ntask classification, including mental arithmetic, motor imagery, and idle state\nusing fNIRS data. Further, this system will require less pre-processing than\nthe traditional approach, saving time and computational resources while\nobtaining an accuracy of 81.48\\%, which is considerably higher than the\naccuracy obtained using conventional machine learning algorithms for the same\ndata set.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.07524,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"DuelGAN: A Duel Between Two Discriminators Stabilizes the GAN Training\n\n  In this paper, we introduce DuelGAN, a generative adversarial network (GAN)\nsolution to improve the stability of the generated samples and to mitigate mode\ncollapse. Built upon the Vanilla GAN's two-player game between the\ndiscriminator $D_1$ and the generator $G$, we introduce a peer discriminator\n$D_2$ to the min-max game. Similar to previous work using two discriminators,\nthe first role of both $D_1$, $D_2$ is to distinguish between generated samples\nand real ones, while the generator tries to generate high-quality samples which\nare able to fool both discriminators. Different from existing methods, we\nintroduce another game between $D_1$ and $D_2$ to discourage their agreement\nand therefore increase the level of diversity of the generated samples. This\nproperty alleviates the issue of early mode collapse by preventing $D_1$ and\n$D_2$ from converging too fast. We provide theoretical analysis for the\nequilibrium of the min-max game formed among $G, D_1, D_2$. We offer\nconvergence behavior of DuelGAN as well as stability of the min-max game. It's\nworth mentioning that DuelGAN operates in the unsupervised setting, and the\nduel between $D_1$ and $D_2$ does not need any label supervision. Experiments\nresults on a synthetic dataset and on real-world image datasets (MNIST, Fashion\nMNIST, CIFAR-10, STL-10, CelebA, VGG, and FFHQ) demonstrate that DuelGAN\noutperforms competitive baseline work in generating diverse and high-quality\nsamples, while only introduces negligible computation cost.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.08862,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Breaking the Deadly Triad with a Target Network\n\n  The deadly triad refers to the instability of a reinforcement learning\nalgorithm when it employs off-policy learning, function approximation, and\nbootstrapping simultaneously. In this paper, we investigate the target network\nas a tool for breaking the deadly triad, providing theoretical support for the\nconventional wisdom that a target network stabilizes training. We first propose\nand analyze a novel target network update rule which augments the commonly used\nPolyak-averaging style update with two projections. We then apply the target\nnetwork and ridge regularization in several divergent algorithms and show their\nconvergence to regularized TD fixed points. Those algorithms are off-policy\nwith linear function approximation and bootstrapping, spanning both policy\nevaluation and control, as well as both discounted and average-reward settings.\nIn particular, we provide the first convergent linear $Q$-learning algorithms\nunder nonrestrictive and changing behavior policies without bi-level\noptimization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.04285,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000050333,
      "text":"Explainable Deep Behavioral Sequence Clustering for Transaction Fraud\n  Detection\n\n  In e-commerce industry, user behavior sequence data has been widely used in\nmany business units such as search and merchandising to improve their products.\nHowever, it is rarely used in financial services not only due to its 3V\ncharacteristics - i.e. Volume, Velocity and Variety - but also due to its\nunstructured nature. In this paper, we propose a Financial Service scenario\nDeep learning based Behavior data representation method for Clustering\n(FinDeepBehaviorCluster) to detect fraudulent transactions. To utilize the\nbehavior sequence data, we treat click stream data as event sequence, use time\nattention based Bi-LSTM to learn the sequence embedding in an unsupervised\nfashion, and combine them with intuitive features generated by risk experts to\nform a hybrid feature representation. We also propose a GPU powered HDBSCAN\n(pHDBSCAN) algorithm, which is an engineering optimization for the original\nHDBSCAN algorithm based on FAISS project, so that clustering can be carried out\non hundreds of millions of transactions within a few minutes. The computation\nefficiency of the algorithm has increased 500 times compared with the original\nimplementation, which makes flash fraud pattern detection feasible. Our\nexperimental results show that the proposed FinDeepBehaviorCluster framework is\nable to catch missed fraudulent transactions with considerable business values.\nIn addition, rule extraction method is applied to extract patterns from risky\nclusters using intuitive features, so that narrative descriptions can be\nattached to the risky clusters for case investigation, and unknown risk\npatterns can be mined for real-time fraud detection. In summary,\nFinDeepBehaviorCluster as a complementary risk management strategy to the\nexisting real-time fraud detection engine, can further increase our fraud\ndetection and proactive risk defense capabilities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.06802,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Measure-conditional Discriminator with Stationary Optimum for GANs and\n  Statistical Distance Surrogates\n\n  We propose a simple but effective modification of the discriminators, namely\nmeasure-conditional discriminators, as a plug-and-play module for different\nGANs. By taking the generated distributions as part of input so that the target\noptimum for the discriminator is stationary, the proposed discriminator is more\nrobust than the vanilla one. A variant of the measure-conditional discriminator\ncan also handle multiple target distributions, or act as a surrogate model of\nstatistical distances such as KL divergence with applications to transfer\nlearning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.09808,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000534786,
      "text":"Analytical Characterization and Design Space Exploration for\n  Optimization of CNNs\n\n  Moving data through the memory hierarchy is a fundamental bottleneck that can\nlimit the performance of core algorithms of machine learning, such as\nconvolutional neural networks (CNNs). Loop-level optimization, including loop\ntiling and loop permutation, are fundamental transformations to reduce data\nmovement. However, the search space for finding the best loop-level\noptimization configuration is explosively large. This paper develops an\nanalytical modeling approach for finding the best loop-level optimization\nconfiguration for CNNs on multi-core CPUs. Experimental evaluation shows that\nthis approach achieves comparable or better performance than state-of-the-art\nlibraries and auto-tuning based optimizers for CNNs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.03785,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Predictive Analysis of Chikungunya\n\n  Chikungunya is an emerging threat for health security all over the world\nwhich is spreading very fast. Researches for proper forecasting of the\nincidence rate of chikungunya has been going on in many places in which DARPA\nhas done a very extensive summarized result from 2014 to 2017 with the data of\nsuspected cases, confirmed cases, deaths, population and incidence rate in\ndifferent countries. In this project, we have analysed the dataset from DARPA\nand extended it to predict the incidence rate using different features of\nweather like temperature, humidity, dewiness, wind and pressure along with the\nlatitude and longitude of every country. We had to use different APIs to find\nout these extra features from 2014-2016. After creating a pure dataset, we have\nused Linear Regression to predict the incidence rate and calculated the\naccuracy and error rate.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.12017,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"A Fully Rigorous Proof of the Derivation of Xavier and He's\n  Initialization for Deep ReLU Networks\n\n  A fully rigorous proof of the derivation of Xavier\/He's initialization for\nReLU nets is given.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.12615,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"DigitalExposome: Quantifying the Urban Environment Influence on\n  Wellbeing based on Real-Time Multi-Sensor Fusion and Deep Belief Network\n\n  In this paper, we define the term 'DigitalExposome' as a conceptual framework\nthat takes us closer towards understanding the relationship between\nenvironment, personal characteristics, behaviour and wellbeing using multimodel\nmobile sensing technology. Specifically, we simultaneously collected (for the\nfirst time) multi-sensor data including urban environmental factors (e.g. air\npollution including: PM1, PM2.5, PM10, Oxidised, Reduced, NH3 and Noise, People\nCount in the vicinity), body reaction (physiological reactions including: EDA,\nHR, HRV, Body Temperature, BVP and movement) and individuals' perceived\nresponses (e.g. self-reported valence) in urban settings. Our users followed a\npre-specified urban path and collected the data using a comprehensive sensing\nedge devices. The data is instantly fused, time-stamped and geo-tagged at the\npoint of collection. A range of multivariate statistical analysis techniques\nhave been applied including Principle Component Analysis, Regression and\nspatial visualisations to unravel the relationship between the variables.\nResults showed that EDA and Heart Rate Variability HRV are noticeably impacted\nby the level of Particulate Matters (PM) in the environment well with the\nenvironmental variables. Furthermore, we adopted Deep Belief Network to extract\nfeatures from the multimodel data feed which outperformed Convolutional Neural\nNetwork and achieved up to (a=80.8%, {\\sigma}=0.001) accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.07655,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Dense for the Price of Sparse: Improved Performance of Sparsely\n  Initialized Networks via a Subspace Offset\n\n  That neural networks may be pruned to high sparsities and retain high\naccuracy is well established. Recent research efforts focus on pruning\nimmediately after initialization so as to allow the computational savings\nafforded by sparsity to extend to the training process. In this work, we\nintroduce a new `DCT plus Sparse' layer architecture, which maintains\ninformation propagation and trainability even with as little as 0.01% trainable\nkernel parameters remaining. We show that standard training of networks built\nwith these layers, and pruned at initialization, achieves state-of-the-art\naccuracy for extreme sparsities on a variety of benchmark network architectures\nand datasets. Moreover, these results are achieved using only simple heuristics\nto determine the locations of the trainable parameters in the network, and thus\nwithout having to initially store or compute with the full, unpruned network,\nas is required by competing prune-at-initialization algorithms. Switching from\nstandard sparse layers to DCT plus Sparse layers does not increase the storage\nfootprint of a network and incurs only a small additional computational\noverhead.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.12661,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Online Learning for Unknown Partially Observable MDPs\n\n  Solving Partially Observable Markov Decision Processes (POMDPs) is hard.\nLearning optimal controllers for POMDPs when the model is unknown is harder.\nOnline learning of optimal controllers for unknown POMDPs, which requires\nefficient learning using regret-minimizing algorithms that effectively tradeoff\nexploration and exploitation, is even harder, and no solution exists currently.\nIn this paper, we consider infinite-horizon average-cost POMDPs with unknown\ntransition model, though a known observation model. We propose a natural\nposterior sampling-based reinforcement learning algorithm (PSRL-POMDP) and show\nthat it achieves a regret bound of $O(\\log T)$, where $T$ is the time horizon,\nwhen the parameter set is finite. In the general case (continuous parameter\nset), we show that the algorithm achieves $O (T^{2\/3})$ regret under two\ntechnical assumptions. To the best of our knowledge, this is the first online\nRL algorithm for POMDPs and has sub-linear regret.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.06162,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Defuse: Harnessing Unrestricted Adversarial Examples for Debugging\n  Models Beyond Test Accuracy\n\n  We typically compute aggregate statistics on held-out test data to assess the\ngeneralization of machine learning models. However, statistics on test data\noften overstate model generalization, and thus, the performance of deployed\nmachine learning models can be variable and untrustworthy. Motivated by these\nconcerns, we develop methods to automatically discover and correct model errors\nbeyond those available in the data. We propose Defuse, a method that generates\nnovel model misclassifications, categorizes these errors into high-level model\nbugs, and efficiently labels and fine-tunes on the errors to correct them. To\ngenerate misclassified data, we propose an algorithm inspired by adversarial\nmachine learning techniques that uses a generative model to find naturally\noccurring instances misclassified by a model. Further, we observe that the\ngenerative models have regions in their latent space with higher concentrations\nof misclassifications. We call these regions misclassification regions and find\nthey have several useful properties. Each region contains a specific type of\nmodel bug; for instance, a misclassification region for an MNIST classifier\ncontains a style of skinny 6 that the model mistakes as a 1. We can also assign\na single label to each region, facilitating low-cost labeling. We propose a\nmethod to learn the misclassification regions and use this insight to both\ncategorize errors and correct them. In practice, Defuse finds and corrects\nnovel errors in classifiers. For example, Defuse shows that a high-performance\ntraffic sign classifier mistakes certain 50km\/h signs as 80km\/h. Defuse\ncorrects the error after fine-tuning while maintaining generalization on the\ntest set.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.02329,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Modeling Financial Products and their Supply Chains\n\n  The objective of this paper is to explore how financial big data and machine\nlearning methods can be applied to model and understand financial products. We\nfocus on residential mortgage backed securities, resMBS, which were at the\nheart of the 2008 US financial crisis. These securities are contained within a\nprospectus and have a complex waterfall payoff structure. Multiple financial\ninstitutions form a supply chain to create prospectuses. To model this supply\nchain, we use unsupervised probabilistic methods, particularly dynamic topics\nmodels (DTM), to extract a set of features (topics) reflecting community\nformation and temporal evolution along the chain. We then provide insight into\nthe performance of the resMBS securities and the impact of the supply chain\nthrough a series of increasingly comprehensive models. First, models at the\nsecurity level directly identify salient features of resMBS securities that\nimpact their performance. We then extend the model to include prospectus level\nfeatures and demonstrate that the composition of the prospectus is significant.\nOur model also shows that communities along the supply chain that are\nassociated with the generation of the prospectuses and securities have an\nimpact on performance. We are the first to show that toxic communities that are\nclosely linked to financial institutions that played a key role in the subprime\ncrisis can increase the risk of failure of resMBS securities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.10527,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000032783,
      "text":"Delayed Rewards Calibration via Reward Empirical Sufficiency\n\n  Appropriate credit assignment for delay rewards is a fundamental challenge\nfor reinforcement learning. To tackle this problem, we introduce a delay reward\ncalibration paradigm inspired from a classification perspective. We hypothesize\nthat well-represented state vectors share similarities with each other since\nthey contain the same or equivalent essential information. To this end, we\ndefine an empirical sufficient distribution, where the state vectors within the\ndistribution will lead agents to environmental reward signals in the consequent\nsteps. Therefore, a purify-trained classifier is designed to obtain the\ndistribution and generate the calibrated rewards. We examine the correctness of\nsufficient state extraction by tracking the real-time extraction and building\ndifferent reward functions in environments. The results demonstrate that the\nclassifier could generate timely and accurate calibrated rewards. Moreover, the\nrewards are able to make the model training process more efficient. Finally, we\nidentify and discuss that the sufficient states extracted by our model resonate\nwith the observations of humans.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.07818,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Certified Robustness to Programmable Transformations in LSTMs\n\n  Deep neural networks for natural language processing are fragile in the face\nof adversarial examples -- small input perturbations, like synonym substitution\nor word duplication, which cause a neural network to change its prediction. We\npresent an approach to certifying the robustness of LSTMs (and extensions of\nLSTMs) and training models that can be efficiently certified. Our approach can\ncertify robustness to intractably large perturbation spaces defined\nprogrammatically in a language of string transformations. Our evaluation shows\nthat (1) our approach can train models that are more robust to combinations of\nstring transformations than those produced using existing techniques; (2) our\napproach can show high certification accuracy of the resulting models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.0978,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000205967,
      "text":"A Deep Graph Wavelet Convolutional Neural Network for Semi-supervised\n  Node Classification\n\n  Graph convolutional neural network provides good solutions for node\nclassification and other tasks with non-Euclidean data. There are several graph\nconvolutional models that attempt to develop deep networks but do not cause\nserious over-smoothing at the same time. Considering that the wavelet transform\ngenerally has a stronger ability to extract useful information than the Fourier\ntransform, we propose a new deep graph wavelet convolutional network (DeepGWC)\nfor semi-supervised node classification tasks. Based on the optimized static\nfiltering matrix parameters of vanilla graph wavelet neural networks and the\ncombination of Fourier bases and wavelet ones, DeepGWC is constructed together\nwith the reuse of residual connection and identity mappings in network\narchitectures. Extensive experiments on three benchmark datasets including\nCora, Citeseer, and Pubmed are conducted. The experimental results demonstrate\nthat our DeepGWC outperforms existing graph deep models with the help of\nadditional wavelet bases and achieves new state-of-the-art performances\neventually.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.07861,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000034107,
      "text":"Low Curvature Activations Reduce Overfitting in Adversarial Training\n\n  Adversarial training is one of the most effective defenses against\nadversarial attacks. Previous works suggest that overfitting is a dominant\nphenomenon in adversarial training leading to a large generalization gap\nbetween test and train accuracy in neural networks. In this work, we show that\nthe observed generalization gap is closely related to the choice of the\nactivation function. In particular, we show that using activation functions\nwith low (exact or approximate) curvature values has a regularization effect\nthat significantly reduces both the standard and robust generalization gaps in\nadversarial training. We observe this effect for both differentiable\/smooth\nactivations such as SiLU as well as non-differentiable\/non-smooth activations\nsuch as LeakyReLU. In the latter case, the \"approximate\" curvature of the\nactivation is low. Finally, we show that for activation functions with low\ncurvature, the double descent phenomenon for adversarially trained models does\nnot occur.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.11628,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000030796,
      "text":"FINE Samples for Learning with Noisy Labels\n\n  Modern deep neural networks (DNNs) become frail when the datasets contain\nnoisy (incorrect) class labels. Robust techniques in the presence of noisy\nlabels can be categorized into two folds: developing noise-robust functions or\nusing noise-cleansing methods by detecting the noisy data. Recently,\nnoise-cleansing methods have been considered as the most competitive\nnoisy-label learning algorithms. Despite their success, their noisy label\ndetectors are often based on heuristics more than a theory, requiring a robust\nclassifier to predict the noisy data with loss values. In this paper, we\npropose a novel detector for filtering label noise. Unlike most existing\nmethods, we focus on each data's latent representation dynamics and measure the\nalignment between the latent distribution and each representation using the\neigendecomposition of the data gram matrix. Our framework, coined as filtering\nnoisy instances via their eigenvectors (FINE), provides a robust detector with\nderivative-free simple methods having theoretical guarantees. Under our\nframework, we propose three applications of the FINE: sample-selection\napproach, semi-supervised learning approach, and collaboration with\nnoise-robust loss functions. Experimental results show that the proposed\nmethods consistently outperform corresponding baselines for all three\napplications on various benchmark datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.10739,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"Dissecting the Diffusion Process in Linear Graph Convolutional Networks\n\n  Graph Convolutional Networks (GCNs) have attracted more and more attentions\nin recent years. A typical GCN layer consists of a linear feature propagation\nstep and a nonlinear transformation step. Recent works show that a linear GCN\ncan achieve comparable performance to the original non-linear GCN while being\nmuch more computationally efficient. In this paper, we dissect the feature\npropagation steps of linear GCNs from a perspective of continuous graph\ndiffusion, and analyze why linear GCNs fail to benefit from more propagation\nsteps. Following that, we propose Decoupled Graph Convolution (DGC) that\ndecouples the terminal time and the feature propagation steps, making it more\nflexible and capable of exploiting a very large number of feature propagation\nsteps. Experiments demonstrate that our proposed DGC improves linear GCNs by a\nlarge margin and makes them competitive with many modern variants of non-linear\nGCNs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.03267,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Estimating 2-Sinkhorn Divergence between Gaussian Processes from\n  Finite-Dimensional Marginals\n\n  \\emph{Optimal Transport} (OT) has emerged as an important computational tool\nin machine learning and computer vision, providing a geometrical framework for\nstudying probability measures. OT unfortunately suffers from the curse of\ndimensionality and requires regularization for practical computations, of which\nthe \\emph{entropic regularization} is a popular choice, which can be\n'unbiased', resulting in a \\emph{Sinkhorn divergence}. In this work, we study\nthe convergence of estimating the 2-Sinkhorn divergence between \\emph{Gaussian\nprocesses} (GPs) using their finite-dimensional marginal distributions. We show\nalmost sure convergence of the divergence when the marginals are sampled\naccording to some base measure. Furthermore, we show that using $n$ marginals\nthe estimation error of the divergence scales in a dimension-free way as\n$\\mathcal{O}\\left(\\epsilon^ {-1}n^{-\\frac{1}{2}}\\right)$, where $\\epsilon$ is\nthe magnitude of entropic regularization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.07751,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000028478,
      "text":"Deep Co-Attention Network for Multi-View Subspace Learning\n\n  Many real-world applications involve data from multiple modalities and thus\nexhibit the view heterogeneity. For example, user modeling on social media\nmight leverage both the topology of the underlying social network and the\ncontent of the users' posts; in the medical domain, multiple views could be\nX-ray images taken at different poses. To date, various techniques have been\nproposed to achieve promising results, such as canonical correlation analysis\nbased methods, etc. In the meanwhile, it is critical for decision-makers to be\nable to understand the prediction results from these methods. For example,\ngiven the diagnostic result that a model provided based on the X-ray images of\na patient at different poses, the doctor needs to know why the model made such\na prediction. However, state-of-the-art techniques usually suffer from the\ninability to utilize the complementary information of each view and to explain\nthe predictions in an interpretable manner.\n  To address these issues, in this paper, we propose a deep co-attention\nnetwork for multi-view subspace learning, which aims to extract both the common\ninformation and the complementary information in an adversarial setting and\nprovide robust interpretations behind the prediction to the end-users via the\nco-attention mechanism. In particular, it uses a novel cross reconstruction\nloss and leverages the label information to guide the construction of the\nlatent representation by incorporating the classifier into our model. This\nimproves the quality of latent representation and accelerates the convergence\nspeed. Finally, we develop an efficient iterative algorithm to find the optimal\nencoders and discriminator, which are evaluated extensively on synthetic and\nreal-world data sets. We also conduct a case study to demonstrate how the\nproposed method robustly interprets the predictions on an image data set.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.01006,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000057618,
      "text":"GaNDLF: A Generally Nuanced Deep Learning Framework for Scalable\n  End-to-End Clinical Workflows in Medical Imaging\n\n  Deep Learning (DL) has the potential to optimize machine learning in both the\nscientific and clinical communities. However, greater expertise is required to\ndevelop DL algorithms, and the variability of implementations hinders their\nreproducibility, translation, and deployment. Here we present the\ncommunity-driven Generally Nuanced Deep Learning Framework (GaNDLF), with the\ngoal of lowering these barriers. GaNDLF makes the mechanism of DL development,\ntraining, and inference more stable, reproducible, interpretable, and scalable,\nwithout requiring an extensive technical background. GaNDLF aims to provide an\nend-to-end solution for all DL-related tasks in computational precision\nmedicine. We demonstrate the ability of GaNDLF to analyze both radiology and\nhistology images, with built-in support for k-fold cross-validation, data\naugmentation, multiple modalities and output classes. Our quantitative\nperformance evaluation on numerous use cases, anatomies, and computational\ntasks supports GaNDLF as a robust application framework for deployment in\nclinical workflows.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.06039,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000038743,
      "text":"An Ensemble Deep Convolutional Neural Network Model for Electricity\n  Theft Detection in Smart Grids\n\n  Smart grids extremely rely on Information and Communications Technology (ICT)\nand smart meters to control and manage numerous parameters of the network.\nHowever, using these infrastructures make smart grids more vulnerable to cyber\nthreats especially electricity theft. Electricity Theft Detection (EDT)\nalgorithms are typically used for such purpose since this Non-Technical Loss\n(NTL) may lead to significant challenges in the power system. In this paper, an\nEnsemble Deep Convolutional Neural Network (EDCNN) algorithm for ETD in smart\ngrids has been proposed. As the first layer of the model, a random under\nbagging technique is applied to deal with the imbalance data, and then Deep\nConvolutional Neural Networks (DCNN) are utilized on each subset. Finally, a\nvoting system is embedded, in the last part. The evaluation results based on\nthe Area Under Curve (AUC), precision, recall, f1-score, and accuracy verify\nthe efficiency of the proposed method compared to the existing method in the\nliterature.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.00606,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000028809,
      "text":"Unsupervised Domain Adaptation for Cross-Subject Few-Shot Neurological\n  Symptom Detection\n\n  Modern machine learning tools have shown promise in detecting symptoms of\nneurological disorders. However, current approaches typically train a unique\nclassifier for each subject. This subject-specific training scheme requires\nlong labeled recordings from each patient, thus failing to detect symptoms in\nnew patients with limited recordings. This paper introduces an unsupervised\ndomain adaptation approach based on adversarial networks to enable few-shot,\ncross-subject epileptic seizure detection. Using adversarial learning, features\nfrom multiple patients were encoded into a subject-invariant space and a\ndiscriminative model was trained on subject-invariant features to make\npredictions. We evaluated this approach on the intracranial EEG (iEEG)\nrecordings from 9 patients with epilepsy. Our approach enabled cross-subject\nseizure detection with a 9.4\\% improvement in 1-shot classification accuracy\ncompared to the conventional subject-specific scheme.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.00837,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Machine learning pipeline for battery state of health estimation\n\n  Lithium-ion batteries are ubiquitous in modern day applications ranging from\nportable electronics to electric vehicles. Irrespective of the application,\nreliable real-time estimation of battery state of health (SOH) by on-board\ncomputers is crucial to the safe operation of the battery, ultimately\nsafeguarding asset integrity. In this paper, we design and evaluate a machine\nlearning pipeline for estimation of battery capacity fade - a metric of battery\nhealth - on 179 cells cycled under various conditions. The pipeline estimates\nbattery SOH with an associated confidence interval by using two parametric and\ntwo non-parametric algorithms. Using segments of charge voltage and current\ncurves, the pipeline engineers 30 features, performs automatic feature\nselection and calibrates the algorithms. When deployed on cells operated under\nthe fast-charging protocol, the best model achieves a root mean squared percent\nerror of 0.45\\%. This work provides insights into the design of scalable\ndata-driven models for battery SOH estimation, emphasising the value of\nconfidence bounds around the prediction. The pipeline methodology combines\nexperimental data with machine learning modelling and can be generalized to\nother critical components that require real-time estimation of SOH.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.10403,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"GLAM: Graph Learning by Modeling Affinity to Labeled Nodes for Graph\n  Neural Networks\n\n  Graph Neural Networks have shown excellent performance on semi-supervised\nclassification tasks. However, they assume access to a graph that may not be\noften available in practice. In the absence of any graph, constructing\nk-Nearest Neighbor (kNN) graphs from the given data have shown to give\nimprovements when used with GNNs over other semi-supervised methods. This paper\nproposes a semi-supervised graph learning method for cases when there are no\ngraphs available. This method learns a graph as a convex combination of the\nunsupervised kNN graph and a supervised label-affinity graph. The\nlabel-affinity graph directly captures all the nodes' label-affinity with the\nlabeled nodes, i.e., how likely a node has the same label as the labeled nodes.\nThis affinity measure contrasts with the kNN graph where the metric measures\ncloseness in the feature space. Our experiments suggest that this approach\ngives close to or better performance (up to 1.5%), while being simpler and\nfaster (up to 70x) to train, than state-of-the-art graph learning methods. We\nalso conduct several experiments to highlight the importance of individual\ncomponents and contrast them with state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.07666,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"A closer look at temporal variability in dynamic online learning\n\n  This work focuses on the setting of dynamic regret in the context of online\nlearning with full information. In particular, we analyze regret bounds with\nrespect to the temporal variability of the loss functions. By assuming that the\nsequence of loss functions does not vary much with time, we show that it is\npossible to incur improved regret bounds compared to existing results. The key\nto our approach is to use the loss function (and not its gradient) during the\noptimization process. Building on recent advances in the analysis of Implicit\nalgorithms, we propose an adaptation of the Implicit version of Online Mirror\nDescent to the dynamic setting. Our proposed algorithm is adaptive not only to\nthe temporal variability of the loss functions, but also to the path length of\nthe sequence of comparators when an upper bound is known. Furthermore, our\nanalysis reveals that our results are tight and cannot be improved without\nfurther assumptions. Next, we show how our algorithm can be applied to the\nsetting of learning with expert advice or to settings with composite loss\nfunctions. Finally, when an upper bound to the path-length is not fixed\nbeforehand we show how to combine a greedy strategy with existing\nstrongly-adaptive algorithms to compete optimally against different sequences\nof comparators simultaneously.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.06083,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000136428,
      "text":"Partially Observed Exchangeable Modeling\n\n  Modeling dependencies among features is fundamental for many machine learning\ntasks. Although there are often multiple related instances that may be\nleveraged to inform conditional dependencies, typical approaches only model\nconditional dependencies over individual instances. In this work, we propose a\nnovel framework, partially observed exchangeable modeling (POEx) that takes in\na set of related partially observed instances and infers the conditional\ndistribution for the unobserved dimensions over multiple elements. Our approach\njointly models the intra-instance (among features in a point) and\ninter-instance (among multiple points in a set) dependencies in data. POEx is a\ngeneral framework that encompasses many existing tasks such as point cloud\nexpansion and few-shot generation, as well as new tasks like few-shot\nimputation. Despite its generality, extensive empirical evaluations show that\nour model achieves state-of-the-art performance across a range of applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.10859,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000073844,
      "text":"Recursive Least Squares Based Refinement Network for the Rollout\n  Trajectory Prediction Methods\n\n  Trajectory prediction plays a pivotal role in the field of intelligent\nvehicles. It currently suffers from several challenges,e.g., accumulative error\nin rollout process and weak adaptability in various scenarios. This paper\nproposes a parametric-learning recursive least squares (RLS) estimation based\non deep neural network for trajectory prediction. We design a flexible plug-in\nmodule which can be readily implanted into rollout approaches. Goal points are\nproposed to capture the long-term prediction stability from the global\nperspective. We carried experiments out on the NGSIM dataset. The promising\nresults indicate that our method could improve rollout trajectory prediction\nmethods effectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.13752,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"Estimating Koopman operators for nonlinear dynamical systems: a\n  nonparametric approach\n\n  The Koopman operator is a mathematical tool that allows for a linear\ndescription of non-linear systems, but working in infinite dimensional spaces.\nDynamic Mode Decomposition and Extended Dynamic Mode Decomposition are amongst\nthe most popular finite dimensional approximation. In this paper we capture\ntheir core essence as a dual version of the same framework, incorporating them\ninto the Kernel framework. To do so, we leverage the RKHS as a suitable space\nfor learning the Koopman dynamics, thanks to its intrinsic finite-dimensional\nnature, shaped by data. We finally establish a strong link between kernel\nmethods and Koopman operators, leading to the estimation of the latter through\nKernel functions. We provide also simulations for comparison with standard\nprocedures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.04662,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000649028,
      "text":"Anomaly Detection Based on Selection and Weighting in Latent Space\n\n  With the high requirements of automation in the era of Industry 4.0, anomaly\ndetection plays an increasingly important role in higher safety and reliability\nin the production and manufacturing industry. Recently, autoencoders have been\nwidely used as a backend algorithm for anomaly detection. Different techniques\nhave been developed to improve the anomaly detection performance of\nautoencoders. Nonetheless, little attention has been paid to the latent\nrepresentations learned by autoencoders. In this paper, we propose a novel\nselection-and-weighting-based anomaly detection framework called SWAD. In\nparticular, the learned latent representations are individually selected and\nweighted. Experiments on both benchmark and real-world datasets have shown the\neffectiveness and superiority of SWAD. On the benchmark datasets, the SWAD\nframework has reached comparable or even better performance than the\nstate-of-the-art approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.05199,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Dynamic Pricing and Learning under the Bass Model\n\n  We consider a novel formulation of the dynamic pricing and demand learning\nproblem, where the evolution of demand in response to posted prices is governed\nby a stochastic variant of the popular Bass model with parameters $\\alpha,\n\\beta$ that are linked to the so-called \"innovation\" and \"imitation\" effects.\nUnlike the more commonly used i.i.d. and contextual demand models, in this\nmodel the posted price not only affects the demand and the revenue in the\ncurrent round but also the future evolution of demand, and hence the fraction\nof potential market size $m$ that can be ultimately captured. In this paper, we\nconsider the more challenging incomplete information problem where dynamic\npricing is applied in conjunction with learning the unknown parameters, with\nthe objective of optimizing the cumulative revenues over a given selling\nhorizon of length $T$. Equivalently, the goal is to minimize the regret which\nmeasures the revenue loss of the algorithm relative to the optimal expected\nrevenue achievable under the stochastic Bass model with market size $m$ and\ntime horizon $T$. Our main contribution is the development of an algorithm that\nsatisfies a high probability regret guarantee of order $\\tilde O(m^{2\/3})$;\nwhere the market size $m$ is known a priori. Moreover, we show that no\nalgorithm can incur smaller order of loss by deriving a matching lower bound.\nUnlike most regret analysis results, in the present problem the market size $m$\nis the fundamental driver of the complexity; our lower bound in fact, indicates\nthat for any fixed $\\alpha, \\beta$, most non-trivial instances of the problem\nhave constant $T$ and large $m$. We believe that this insight sets the problem\nof dynamic pricing under the Bass model apart from the typical i.i.d. setting\nand multi-armed bandit based models for dynamic pricing, which typically focus\nonly on the asymptotics with respect to time horizon $T$.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.01689,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Self-supervised Symmetric Nonnegative Matrix Factorization\n\n  Symmetric nonnegative matrix factorization (SNMF) has demonstrated to be a\npowerful method for data clustering. However, SNMF is mathematically formulated\nas a non-convex optimization problem, making it sensitive to the initialization\nof variables. Inspired by ensemble clustering that aims to seek a better\nclustering result from a set of clustering results, we propose self-supervised\nSNMF (S$^3$NMF), which is capable of boosting clustering performance\nprogressively by taking advantage of the sensitivity to initialization\ncharacteristic of SNMF, without relying on any additional information.\nSpecifically, we first perform SNMF repeatedly with a random nonnegative matrix\nfor initialization each time, leading to multiple decomposed matrices. Then, we\nrank the quality of the resulting matrices with adaptively learned weights,\nfrom which a new similarity matrix that is expected to be more discriminative\nis reconstructed for SNMF again. These two steps are iterated until the\nstopping criterion\/maximum number of iterations is achieved. We mathematically\nformulate S$^3$NMF as a constraint optimization problem, and provide an\nalternative optimization algorithm to solve it with the theoretical convergence\nguaranteed. Extensive experimental results on $10$ commonly used benchmark\ndatasets demonstrate the significant advantage of our S$^3$NMF over $12$\nstate-of-the-art methods in terms of $5$ quantitative metrics. The source code\nis publicly available at https:\/\/github.com\/jyh-learning\/SSSNMF.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.01629,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"DeepCert: Verification of Contextually Relevant Robustness for Neural\n  Network Image Classifiers\n\n  We introduce DeepCert, a tool-supported method for verifying the robustness\nof deep neural network (DNN) image classifiers to contextually relevant\nperturbations such as blur, haze, and changes in image contrast. While the\nrobustness of DNN classifiers has been the subject of intense research in\nrecent years, the solutions delivered by this research focus on verifying DNN\nrobustness to small perturbations in the images being classified, with\nperturbation magnitude measured using established Lp norms. This is useful for\nidentifying potential adversarial attacks on DNN image classifiers, but cannot\nverify DNN robustness to contextually relevant image perturbations, which are\ntypically not small when expressed with Lp norms. DeepCert addresses this\nunderexplored verification problem by supporting:(1) the encoding of real-world\nimage perturbations; (2) the systematic evaluation of contextually relevant DNN\nrobustness, using both testing and formal verification; (3) the generation of\ncontextually relevant counterexamples; and, through these, (4) the selection of\nDNN image classifiers suitable for the operational context (i)envisaged when a\npotentially safety-critical system is designed, or (ii)observed by a deployed\nsystem. We demonstrate the effectiveness of DeepCert by showing how it can be\nused to verify the robustness of DNN image classifiers build for two benchmark\ndatasets (`German Traffic Sign' and `CIFAR-10') to multiple contextually\nrelevant perturbations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.10824,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000033114,
      "text":"Enhancing Robustness of On-line Learning Models on Highly Noisy Data\n\n  Classification algorithms have been widely adopted to detect anomalies for\nvarious systems, e.g., IoT, cloud and face recognition, under the common\nassumption that the data source is clean, i.e., features and labels are\ncorrectly set. However, data collected from the wild can be unreliable due to\ncareless annotations or malicious data transformation for incorrect anomaly\ndetection. In this paper, we extend a two-layer on-line data selection\nframework: Robust Anomaly Detector (RAD) with a newly designed ensemble\nprediction where both layers contribute to the final anomaly detection\ndecision. To adapt to the on-line nature of anomaly detection, we consider\nadditional features of conflicting opinions of classifiers, repetitive\ncleaning, and oracle knowledge. We on-line learn from incoming data streams and\ncontinuously cleanse the data, so as to adapt to the increasing learning\ncapacity from the larger accumulated data set. Moreover, we explore the concept\nof oracle learning that provides additional information of true labels for\ndifficult data points. We specifically focus on three use cases, (i) detecting\n10 classes of IoT attacks, (ii) predicting 4 classes of task failures of big\ndata jobs, and (iii) recognising 100 celebrities faces. Our evaluation results\nshow that RAD can robustly improve the accuracy of anomaly detection, to reach\nup to 98.95% for IoT device attacks (i.e., +7%), up to 85.03% for cloud task\nfailures (i.e., +14%) under 40% label noise, and for its extension, it can\nreach up to 77.51% for face recognition (i.e., +39%) under 30% label noise. The\nproposed RAD and its extensions are general and can be applied to different\nanomaly detection algorithms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.03543,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000031127,
      "text":"Artificial Neural Networks generated by Low Discrepancy Sequences\n\n  Artificial neural networks can be represented by paths. Generated as random\nwalks on a dense network graph, we find that the resulting sparse networks\nallow for deterministic initialization and even weights with fixed sign. Such\nnetworks can be trained sparse from scratch, avoiding the expensive procedure\nof training a dense network and compressing it afterwards. Although sparse,\nweights are accessed as contiguous blocks of memory. In addition, enumerating\nthe paths using deterministic low discrepancy sequences, for example the Sobol'\nsequence, amounts to connecting the layers of neural units by progressive\npermutations, which naturally avoids bank conflicts in parallel computer\nhardware. We demonstrate that the artificial neural networks generated by low\ndiscrepancy sequences can achieve an accuracy within reach of their dense\ncounterparts at a much lower computational complexity.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.02687,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Lazy FSCA for Unsupervised Variable Selection\n\n  Various unsupervised greedy selection methods have been proposed as\ncomputationally tractable approximations to the NP-hard subset selection\nproblem. These methods rely on sequentially selecting the variables that best\nimprove performance with respect to a selection criterion. Theoretical results\nexist that provide performance bounds and enable \"lazy greedy\" efficient\nimplementations for selection criteria that satisfy a diminishing returns\nproperty known as submodularity. This has motivated the development of variable\nselection algorithms based on mutual information and frame potential. Recently,\nthe authors introduced Forward Selection Component Analysis (FSCA) which uses\nvariance explained as its selection criterion. While this criterion is not\nsubmodular, FSCA has been shown to be highly effective for applications such as\nmeasurement plan optimisation. In this paper a \"lazy\" implementation of the\nFSCA algorithm (L-FSCA) is proposed, which, although not equivalent to FSCA due\nto the absence of submodularity, has the potential to yield comparable\nperformance while being up to an order of magnitude faster to compute. The\nefficacy of L-FSCA is demonstrated by performing a systematic comparison with\nFSCA and five other unsupervised variable selection methods from the literature\nusing simulated and real-world case studies. Experimental results confirm that\nL-FSCA yields almost identical performance to FSCA while reducing computation\ntime by between 22% and 94% for the case studies considered.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.03892,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000100334,
      "text":"Set Representation Learning with Generalized Sliced-Wasserstein\n  Embeddings\n\n  An increasing number of machine learning tasks deal with learning\nrepresentations from set-structured data. Solutions to these problems involve\nthe composition of permutation-equivariant modules (e.g., self-attention, or\nindividual processing via feed-forward neural networks) and\npermutation-invariant modules (e.g., global average pooling, or pooling by\nmulti-head attention). In this paper, we propose a geometrically-interpretable\nframework for learning representations from set-structured data, which is\nrooted in the optimal mass transportation problem. In particular, we treat\nelements of a set as samples from a probability measure and propose an exact\nEuclidean embedding for Generalized Sliced Wasserstein (GSW) distances to learn\nfrom set-structured data effectively. We evaluate our proposed framework on\nmultiple supervised and unsupervised set learning tasks and demonstrate its\nsuperiority over state-of-the-art set representation learning approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.10367,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000121527,
      "text":"Neural Multi-Hop Reasoning With Logical Rules on Biomedical Knowledge\n  Graphs\n\n  Biomedical knowledge graphs permit an integrative computational approach to\nreasoning about biological systems. The nature of biological data leads to a\ngraph structure that differs from those typically encountered in benchmarking\ndatasets. To understand the implications this may have on the performance of\nreasoning algorithms, we conduct an empirical study based on the real-world\ntask of drug repurposing. We formulate this task as a link prediction problem\nwhere both compounds and diseases correspond to entities in a knowledge graph.\nTo overcome apparent weaknesses of existing algorithms, we propose a new\nmethod, PoLo, that combines policy-guided walks based on reinforcement learning\nwith logical rules. These rules are integrated into the algorithm by using a\nnovel reward function. We apply our method to Hetionet, which integrates\nbiomedical information from 29 prominent bioinformatics databases. Our\nexperiments show that our approach outperforms several state-of-the-art methods\nfor link prediction while providing interpretability.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.00755,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Adaptive Sampling for Minimax Fair Classification\n\n  Machine learning models trained on uncurated datasets can often end up\nadversely affecting inputs belonging to underrepresented groups. To address\nthis issue, we consider the problem of adaptively constructing training sets\nwhich allow us to learn classifiers that are fair in a minimax sense. We first\npropose an adaptive sampling algorithm based on the principle of optimism, and\nderive theoretical bounds on its performance. We also propose heuristic\nextensions of this algorithm suitable for application to large scale, practical\nproblems. Next, by deriving algorithm independent lower-bounds for a specific\nclass of problems, we show that the performance achieved by our adaptive scheme\ncannot be improved in general. We then validate the benefits of adaptively\nconstructing training sets via experiments on synthetic tasks with logistic\nregression classifiers, as well as on several real-world tasks using\nconvolutional neural networks (CNNs).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.13815,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Fast Approximate Spectral Normalization for Robust Deep Neural Networks\n\n  Deep neural networks (DNNs) play an important role in machine learning due to\nits outstanding performance compared to other alternatives. However, DNNs are\nnot suitable for safety-critical applications since DNNs can be easily fooled\nby well-crafted adversarial examples. One promising strategy to counter\nadversarial attacks is to utilize spectral normalization, which ensures that\nthe trained model has low sensitivity towards the disturbance of input samples.\nUnfortunately, this strategy requires exact computation of spectral norm, which\nis computation intensive and impractical for large-scale networks. In this\npaper, we introduce an approximate algorithm for spectral normalization based\non Fourier transform and layer separation. The primary contribution of our work\nis to effectively combine the sparsity of weight matrix and decomposability of\nconvolution layers. Extensive experimental evaluation demonstrates that our\nframework is able to significantly improve both time efficiency (up to 60\\%)\nand model robustness (61\\% on average) compared with the state-of-the-art\nspectral normalization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.06064,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Graph Neural Networks Inspired by Classical Iterative Algorithms\n\n  Despite the recent success of graph neural networks (GNN), common\narchitectures often exhibit significant limitations, including sensitivity to\noversmoothing, long-range dependencies, and spurious edges, e.g., as can occur\nas a result of graph heterophily or adversarial attacks. To at least partially\naddress these issues within a simple transparent framework, we consider a new\nfamily of GNN layers designed to mimic and integrate the update rules of two\nclassical iterative algorithms, namely, proximal gradient descent and iterative\nreweighted least squares (IRLS). The former defines an extensible base GNN\narchitecture that is immune to oversmoothing while nonetheless capturing\nlong-range dependencies by allowing arbitrary propagation steps. In contrast,\nthe latter produces a novel attention mechanism that is explicitly anchored to\nan underlying end-to-end energy function, contributing stability with respect\nto edge uncertainty. When combined we obtain an extremely simple yet robust\nmodel that we evaluate across disparate scenarios including standardized\nbenchmarks, adversarially-perturbated graphs, graphs with heterophily, and\ngraphs involving long-range dependencies. In doing so, we compare against SOTA\nGNN approaches that have been explicitly designed for the respective task,\nachieving competitive or superior node classification accuracy. Our code is\navailable at https:\/\/github.com\/FFTYYY\/TWIRLS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.08312,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Trainless Model Performance Estimation for Neural Architecture Search\n\n  Neural architecture search has become an indispensable part of the deep\nlearning field. Modern methods allow to find one of the best performing\narchitectures, or to build one from scratch, but they typically make decisions\nbased on the trained accuracy information. In the present article we explore\ninstead how the architectural component of a neural network affects its\nprediction power. We focus on relationships between the trained accuracy of an\narchitecture and its accuracy prior to training, by considering statistics over\nmultiple initialisations. We observe that minimising the coefficient of\nvariation of the untrained accuracy, $CV_{U}$, consistently leads to better\nperforming architectures. We test the $CV_{U}$ as a neural architecture search\nscoring metric using the NAS-Bench-201 database of trained neural\narchitectures. The architectures with the lowest $CV_{U}$ value have on average\nan accuracy of $91.90 \\pm 2.27$, $64.08 \\pm 5.63$ and $38.76 \\pm 6.62$ for\nCIFAR-10, CIFAR-100 and a downscaled version of ImageNet, respectively. Since\nthese values are statistically above the random baseline, we make a conclusion\nthat a good architecture should be stable against weights initialisations. It\ntakes about $190$ s for CIFAR-10 and CIFAR-100 and $133.9$ s for ImageNet16-120\nto process $100$ architectures, on a batch of $256$ images, with $100$\ninitialisations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.08348,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"Decorrelating Adversarial Nets for Clustering Mobile Network Data\n\n  Deep learning will play a crucial role in enabling cognitive automation for\nthe mobile networks of the future. Deep clustering, a subset of deep learning,\ncould be a valuable tool for many network automation use-cases. Unfortunately,\nmost state-of-the-art clustering algorithms target image datasets, which makes\nthem hard to apply to mobile network data due to their highly tuned nature and\nrelated assumptions about the data. In this paper, we propose a new algorithm,\nDANCE (Decorrelating Adversarial Nets for Clustering-friendly Encoding),\nintended to be a reliable deep clustering method which also performs well when\napplied to network automation use-cases. DANCE uses a reconstructive clustering\napproach, separating clustering-relevant from clustering-irrelevant features in\na latent representation. This separation removes unnecessary information from\nthe clustering, increasing consistency and peak performance. We comprehensively\nevaluate DANCE and other select state-of-the-art deep clustering algorithms,\nand show that DANCE outperforms these algorithms by a significant margin on a\nmobile network dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.01511,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000280804,
      "text":"Multi-label Classification via Adaptive Resonance Theory-based\n  Clustering\n\n  This paper proposes a multi-label classification algorithm capable of\ncontinual learning by applying an Adaptive Resonance Theory (ART)-based\nclustering algorithm and the Bayesian approach for label probability\ncomputation. The ART-based clustering algorithm adaptively and continually\ngenerates prototype nodes corresponding to given data, and the generated nodes\nare used as classifiers. The label probability computation independently counts\nthe number of label appearances for each class and calculates the Bayesian\nprobabilities. Thus, the label probability computation can cope with an\nincrease in the number of labels. Experimental results with synthetic and\nreal-world multi-label datasets show that the proposed algorithm has\ncompetitive classification performance to other well-known algorithms while\nrealizing continual learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.01124,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"Automated data-driven approach for gap filling in the time series using\n  evolutionary learning\n\n  In the paper, we propose an adaptive data-driven model-based approach for\nfilling the gaps in time series. The approach is based on the automated\nevolutionary identification of the optimal structure for a composite\ndata-driven model. It allows adapting the model for the effective gap-filling\nin a specific dataset without the involvement of the data scientist. As a case\nstudy, both synthetic and real datasets from different fields (environmental,\neconomic, etc) are used. The experiments confirm that the proposed approach\nallows achieving the higher quality of the gap restoration and improve the\neffectiveness of forecasting models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.15107,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000025498,
      "text":"Hierarchical Relationship Alignment Metric Learning\n\n  Most existing metric learning methods focus on learning a similarity or\ndistance measure relying on similar and dissimilar relations between sample\npairs. However, pairs of samples cannot be simply identified as similar or\ndissimilar in many real-world applications, e.g., multi-label learning, label\ndistribution learning. To this end, relation alignment metric learning (RAML)\nframework is proposed to handle the metric learning problem in those scenarios.\nBut RAML learn a linear metric, which can't model complex datasets. Combining\nwith deep learning and RAML framework, we propose a hierarchical relationship\nalignment metric leaning model HRAML, which uses the concept of relationship\nalignment to model metric learning problems under multiple learning tasks, and\nmakes full use of the consistency between the sample pair relationship in the\nfeature space and the sample pair relationship in the label space. Further we\norganize several experiment divided by learning tasks, and verified the better\nperformance of HRAML against many popular methods and RAML framework.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.06021,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000036756,
      "text":"Deep learning methods for screening patients' S-ICD implantation\n  eligibility\n\n  Subcutaneous Implantable Cardioverter-Defibrillators (S-ICDs) are used for\nprevention of sudden cardiac death triggered by ventricular arrhythmias. T Wave\nOver Sensing (TWOS) is an inherent risk with S-ICDs which can lead to\ninappropriate shocks. A major predictor of TWOS is a high T:R ratio (the ratio\nbetween the amplitudes of the T and R waves). Currently patients'\nElectrocardiograms (ECGs) are screened over 10 seconds to measure the T:R\nratio, determining the patients' eligibility for S-ICD implantation. Due to\ntemporal variations in the T:R ratio, 10 seconds is not long enough to reliably\ndetermine the normal values of a patient's T:R ratio. In this paper, we develop\na convolutional neural network (CNN) based model utilising phase space\nreconstruction matrices to predict T:R ratios from 10-second ECG segments\nwithout explicitly locating the R or T waves, thus avoiding the issue of TWOS.\nThis tool can be used to automatically screen patients over a much longer\nperiod and provide an in-depth description of the behaviour of the T:R ratio\nover that period. The tool can also enable much more reliable and descriptive\nscreenings to better assess patients' eligibility for S-ICD implantation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.15987,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"PLAN-B: Predicting Likely Alternative Next Best Sequences for Action\n  Prediction\n\n  Action prediction focuses on anticipating actions before they happen. Recent\nworks leverage probabilistic approaches to describe future uncertainties and\nsample future actions. However, these methods cannot easily find all\nalternative predictions, which are essential given the inherent\nunpredictability of the future, and current evaluation protocols do not measure\na system's ability to find such alternatives. We re-examine action prediction\nin terms of its ability to predict not only the top predictions, but also top\nalternatives with the accuracy@k metric. In addition, we propose Choice F1: a\nmetric inspired by F1 score which evaluates a prediction system's ability to\nfind all plausible futures while keeping only the most probable ones. To\nevaluate this problem, we present a novel method, Predicting the Likely\nAlternative Next Best, or PLAN-B, for action prediction which automatically\nfinds the set of most likely alternative futures. PLAN-B consists of two novel\ncomponents: (i) a Choice Table which ensures that all possible futures are\nfound, and (ii) a \"Collaborative\" RNN system which combines both action\nsequence and feature information. We demonstrate that our system outperforms\nstate-of-the-art results on benchmark datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.02523,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000035432,
      "text":"An Analysis of State-of-the-art Activation Functions For Supervised Deep\n  Neural Network\n\n  This paper provides an analysis of state-of-the-art activation functions with\nrespect to supervised classification of deep neural network. These activation\nfunctions comprise of Rectified Linear Units (ReLU), Exponential Linear Unit\n(ELU), Scaled Exponential Linear Unit (SELU), Gaussian Error Linear Unit\n(GELU), and the Inverse Square Root Linear Unit (ISRLU). To evaluate,\nexperiments over two deep learning network architectures integrating these\nactivation functions are conducted. The first model, basing on Multilayer\nPerceptron (MLP), is evaluated with MNIST dataset to perform these activation\nfunctions. Meanwhile, the second model, likely VGGish-based architecture, is\napplied for Acoustic Scene Classification (ASC) Task 1A in DCASE 2018\nchallenge, thus evaluate whether these activation functions work well in\ndifferent datasets as well as different network architectures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.06377,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Mitigating Adversarial Attack for Compute-in-Memory Accelerator\n  Utilizing On-chip Finetune\n\n  Compute-in-memory (CIM) has been proposed to accelerate the convolution\nneural network (CNN) computation by implementing parallel multiply and\naccumulation in analog domain. However, the subsequent processing is still\npreferred to be performed in digital domain. This makes the analog to digital\nconverter (ADC) critical in CIM architectures. One drawback is the ADC error\nintroduced by process variation. While research efforts are being made to\nimprove ADC design to reduce the offset, we find that the accuracy loss\nintroduced by the ADC error could be recovered by model weight finetune. In\naddition to compensate ADC offset, on-chip weight finetune could be leveraged\nto provide additional protection for adversarial attack that aims to fool the\ninference engine with manipulated input samples. Our evaluation results show\nthat by adapting the model weights to the specific ADC offset pattern to each\nchip, the transferability of the adversarial attack is suppressed. For a chip\nbeing attacked by the C&W method, the classification for CIFAR-10 dataset will\ndrop to almost 0%. However, when applying the similarly generated adversarial\nexamples to other chips, the accuracy could still maintain more than 62% and\n85% accuracy for VGG-8 and DenseNet-40, respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.10482,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000036756,
      "text":"GraphSVX: Shapley Value Explanations for Graph Neural Networks\n\n  Graph Neural Networks (GNNs) achieve significant performance for various\nlearning tasks on geometric data due to the incorporation of graph structure\ninto the learning of node representations, which renders their comprehension\nchallenging. In this paper, we first propose a unified framework satisfied by\nmost existing GNN explainers. Then, we introduce GraphSVX, a post hoc local\nmodel-agnostic explanation method specifically designed for GNNs. GraphSVX is a\ndecomposition technique that captures the \"fair\" contribution of each feature\nand node towards the explained prediction by constructing a surrogate model on\na perturbed dataset. It extends to graphs and ultimately provides as\nexplanation the Shapley Values from game theory. Experiments on real-world and\nsynthetic datasets demonstrate that GraphSVX achieves state-of-the-art\nperformance compared to baseline models while presenting core theoretical and\nhuman-centric properties.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.05997,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Tracking translation invariance in CNNs\n\n  Although Convolutional Neural Networks (CNNs) are widely used, their\ntranslation invariance (ability to deal with translated inputs) is still\nsubject to some controversy. We explore this question using\ntranslation-sensitivity maps to quantify how sensitive a standard CNN is to a\ntranslated input. We propose the use of Cosine Similarity as sensitivity metric\nover Euclidean Distance, and discuss the importance of restricting the\ndimensionality of either of these metrics when comparing architectures. Our\nmain focus is to investigate the effect of different architectural components\nof a standard CNN on that network's sensitivity to translation. By varying\nconvolutional kernel sizes and amounts of zero padding, we control the size of\nthe feature maps produced, allowing us to quantify the extent to which these\nelements influence translation invariance. We also measure translation\ninvariance at different locations within the CNN to determine the extent to\nwhich convolutional and fully connected layers, respectively, contribute to the\ntranslation invariance of a CNN as a whole. Our analysis indicates that both\nconvolutional kernel size and feature map size have a systematic influence on\ntranslation invariance. We also see that convolutional layers contribute less\nthan expected to translation invariance, when not specifically forced to do so.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.11893,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"LGD-GCN: Local and Global Disentangled Graph Convolutional Networks\n\n  Disentangled Graph Convolutional Network (DisenGCN) is an encouraging\nframework to disentangle the latent factors arising in a real-world graph.\nHowever, it relies on disentangling information heavily from a local range\n(i.e., a node and its 1-hop neighbors), while the local information in many\ncases can be uneven and incomplete, hindering the interpretabiliy power and\nmodel performance of DisenGCN. In this paper\\footnote{This paper is a lighter\nversion of \\href{https:\/\/jingweio.github.io\/assets\/pdf\/tnnls22.pdf}{\"Learning\nDisentangled Graph Convolutional Networks Locally and Globally\"} where the\nresults and analysis have been reworked substantially. Digital Object\nIdentifier \\url{https:\/\/doi.org\/10.1109\/TNNLS.2022.3195336}.}, we introduce a\nnovel Local and Global Disentangled Graph Convolutional Network (LGD-GCN) to\ncapture both local and global information for graph disentanglement. LGD-GCN\nperforms a statistical mixture modeling to derive a factor-aware latent\ncontinuous space, and then constructs different structures w.r.t. different\nfactors from the revealed space. In this way, the global factor-specific\ninformation can be efficiently and selectively encoded via a message passing\nalong these built structures, strengthening the intra-factor consistency. We\nalso propose a novel diversity promoting regularizer employed with the latent\nspace modeling, to encourage inter-factor diversity. Evaluations of the\nproposed LGD-GCN on the synthetic and real-world datasets show a better\ninterpretability and improved performance in node classification over the\nexisting competitive models. Code is available at\n\\url{https:\/\/github.com\/jingweio\/LGD-GCN}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.02899,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000073512,
      "text":"Recognizing and Verifying Mathematical Equations using Multiplicative\n  Differential Neural Units\n\n  Automated mathematical reasoning is a challenging problem that requires an\nagent to learn algebraic patterns that contain long-range dependencies. Two\nparticular tasks that test this type of reasoning are (1) mathematical equation\nverification, which requires determining whether trigonometric and linear\nalgebraic statements are valid identities or not, and (2) equation completion,\nwhich entails filling in a blank within an expression to make it true. Solving\nthese tasks with deep learning requires that the neural model learn how to\nmanipulate and compose various algebraic symbols, carrying this ability over to\npreviously unseen expressions. Artificial neural networks, including recurrent\nnetworks and transformers, struggle to generalize on these kinds of difficult\ncompositional problems, often exhibiting poor extrapolation performance. In\ncontrast, recursive neural networks (recursive-NNs) are, theoretically, capable\nof achieving better extrapolation due to their tree-like design but are\ndifficult to optimize as the depth of their underlying tree structure\nincreases. To overcome this issue, we extend recursive-NNs to utilize\nmultiplicative, higher-order synaptic connections and, furthermore, to learn to\ndynamically control and manipulate an external memory. We argue that this key\nmodification gives the neural system the ability to capture powerful transition\nfunctions for each possible input. We demonstrate the effectiveness of our\nproposed higher-order, memory-augmented recursive-NN models on two challenging\nmathematical equation tasks, showing improved extrapolation, stable\nperformance, and faster convergence. Our models achieve a 1.53% average\nimprovement over current state-of-the-art methods in equation verification and\nachieve a 2.22% Top-1 average accuracy and 2.96% Top-5 average accuracy for\nequation completion.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.09684,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"Suppressing simulation bias using multi-modal data\n\n  Many problems in science and engineering require making predictions based on\nfew observations. To build a robust predictive model, these sparse data may\nneed to be augmented with simulated data, especially when the design space is\nmulti-dimensional. Simulations, however, often suffer from an inherent bias.\nEstimation of this bias may be poorly constrained not only because of data\nsparsity, but also because traditional predictive models fit only one type of\nobserved outputs, such as scalars or images, instead of all available output\ndata modalities, which might have been acquired and simulated at great cost. To\nbreak this limitation and open up the path for multi-modal calibration, we\npropose to combine a novel, transfer learning technique for suppressing the\nbias with recent developments in deep learning, which allow building predictive\nmodels with multi-modal outputs. First, we train an initial neural network\nmodel on simulated data to learn important correlations between different\noutput modalities and between simulation inputs and outputs. Then, the model is\npartially retrained, or transfer learned, to fit the experiments; a method that\nhas never been implemented in this type of architecture. Using fewer than 10\ninertial confinement fusion experiments for training, transfer learning\nsystematically improves the simulation predictions while a simple output\ncalibration, which we design as a baseline, makes the predictions worse. We\nalso offer extensive cross-validation with real and carefully designed\nsynthetic data. The method described in this paper can be applied to a wide\nrange of problems that require transferring knowledge from simulations to the\ndomain of experiments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.00108,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"Explaining a Series of Models by Propagating Shapley Values\n\n  Local feature attribution methods are increasingly used to explain complex\nmachine learning models. However, current methods are limited because they are\nextremely expensive to compute or are not capable of explaining a distributed\nseries of models where each model is owned by a separate institution. The\nlatter is particularly important because it often arises in finance where\nexplanations are mandated. Here, we present DeepSHAP, a tractable method to\npropagate local feature attributions through complex series of models based on\na connection to the Shapley value. We evaluate DeepSHAP across biological,\nhealth, and financial datasets to show that it provides equally salient\nexplanations an order of magnitude faster than existing model-agnostic\nattribution techniques and demonstrate its use in an important distributed\nseries of models setting.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.12503,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000031789,
      "text":"Short-term forecast of EV charging stations occupancy probability using\n  big data streaming analysis\n\n  The widespread diffusion of electric mobility requires a contextual expansion\nof the charging infrastructure. An extended collection and processing of\ninformation regarding charging of electric vehicles may turn each electric\nvehicle charging station into a valuable source of streaming data. Charging\npoint operators may profit from all these data for optimizing their operation\nand planning activities. In such a scenario, big data and machine learning\ntechniques would allow valorizing real-time data coming from electric vehicle\ncharging stations. This paper presents an architecture able to deal with data\nstreams from a charging infrastructure, with the final aim to forecast electric\ncharging station availability after a set amount of minutes from present time.\nBoth batch data regarding past charges and real-time data streams are used to\ntrain a streaming logistic regression model, to take into account recurrent\npast situations and unexpected actual events. The streaming model performs\nbetter than a model trained only using historical data. The results highlight\nthe importance of constantly updating the predictive model parameters in order\nto adapt to changing conditions and always provide accurate forecasts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.01422,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"A Large-scale Study on Unsupervised Outlier Model Selection: Do Internal\n  Strategies Suffice?\n\n  Given an unsupervised outlier detection task, how should one select a\ndetection algorithm as well as its hyperparameters (jointly called a model)?\nUnsupervised model selection is notoriously difficult, in the absence of\nhold-out validation data with ground-truth labels. Therefore, the problem is\nvastly understudied. In this work, we study the feasibility of employing\ninternal model evaluation strategies for selecting a model for outlier\ndetection. These so-called internal strategies solely rely on the input data\n(without labels) and the output (outlier scores) of the candidate models. We\nsetup (and open-source) a large testbed with 39 detection tasks and 297\ncandidate models comprised of 8 detectors and various hyperparameter\nconfigurations. We evaluate 7 different strategies on their ability to\ndiscriminate between models w.r.t. detection performance, without using any\nlabels. Our study reveals room for progress -- we find that none would be\npractically useful, as they select models only comparable to a state-of-the-art\ndetector (with random configuration).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.03935,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"OGGN: A Novel Generalized Oracle Guided Generative Architecture for\n  Modelling Inverse Function of Artificial Neural Networks\n\n  This paper presents a novel Generative Neural Network Architecture for\nmodelling the inverse function of an Artificial Neural Network (ANN) either\ncompletely or partially. Modelling the complete inverse function of an ANN\ninvolves generating the values of all features that corresponds to a desired\noutput. On the other hand, partially modelling the inverse function means\ngenerating the values of a subset of features and fixing the remaining feature\nvalues. The feature set generation is a critical step for artificial neural\nnetworks, useful in several practical applications in engineering and science.\nThe proposed Oracle Guided Generative Neural Network, dubbed as OGGN, is\nflexible to handle a variety of feature generation problems. In general, an ANN\nis able to predict the target values based on given feature vectors. The OGGN\narchitecture enables to generate feature vectors given the predetermined target\nvalues of an ANN. When generated feature vectors are fed to the forward ANN,\nthe target value predicted by ANN will be close to the predetermined target\nvalues. Therefore, the OGGN architecture is able to map, inverse function of\nthe function represented by forward ANN. Besides, there is another important\ncontribution of this work. This paper also introduces a new class of functions,\ndefined as constraint functions. The constraint functions enable a neural\nnetwork to investigate a given local space for a longer period of time. Thus,\nenabling to find a local optimum of the loss function apart from just being\nable to find the global optimum. OGGN can also be adapted to solve a system of\npolynomial equations in many variables. The experiments on synthetic datasets\nvalidate the effectiveness of OGGN on various use cases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.0411,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"A Design Space Study for LISTA and Beyond\n\n  In recent years, great success has been witnessed in building\nproblem-specific deep networks from unrolling iterative algorithms, for solving\ninverse problems and beyond. Unrolling is believed to incorporate the\nmodel-based prior with the learning capacity of deep learning. This paper\nrevisits the role of unrolling as a design approach for deep networks: to what\nextent its resulting special architecture is superior, and can we find better?\nUsing LISTA for sparse recovery as a representative example, we conduct the\nfirst thorough design space study for the unrolled models. Among all possible\nvariations, we focus on extensively varying the connectivity patterns and\nneuron types, leading to a gigantic design space arising from LISTA. To\nefficiently explore this space and identify top performers, we leverage the\nemerging tool of neural architecture search (NAS). We carefully examine the\nsearched top architectures in a number of settings, and are able to discover\nnetworks that are consistently better than LISTA. We further present more\nvisualization and analysis to \"open the black box\", and find that the searched\ntop architectures demonstrate highly consistent and potentially transferable\npatterns. We hope our study to spark more reflections and explorations on how\nto better mingle model-based optimization prior and data-driven learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.06677,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000032451,
      "text":"Multi-Party Dual Learning\n\n  The performance of machine learning algorithms heavily relies on the\navailability of a large amount of training data. However, in reality, data\nusually reside in distributed parties such as different institutions and may\nnot be directly gathered and integrated due to various data policy constraints.\nAs a result, some parties may suffer from insufficient data available for\ntraining machine learning models. In this paper, we propose a multi-party dual\nlearning (MPDL) framework to alleviate the problem of limited data with poor\nquality in an isolated party. Since the knowledge sharing processes for\nmultiple parties always emerge in dual forms, we show that dual learning is\nnaturally suitable to handle the challenge of missing data, and explicitly\nexploits the probabilistic correlation and structural relationship between dual\ntasks to regularize the training process. We introduce a feature-oriented\ndifferential privacy with mathematical proof, in order to avoid possible\nprivacy leakage of raw features in the dual inference process. The approach\nrequires minimal modifications to the existing multi-party learning structure,\nand each party can build flexible and powerful models separately, whose\naccuracy is no less than non-distributed self-learning approaches. The MPDL\nframework achieves significant improvement compared with state-of-the-art\nmulti-party learning methods, as we demonstrated through simulations on\nreal-world datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.05641,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Generalization bounds via distillation\n\n  This paper theoretically investigates the following empirical phenomenon:\ngiven a high-complexity network with poor generalization bounds, one can\ndistill it into a network with nearly identical predictions but low complexity\nand vastly smaller generalization bounds. The main contribution is an analysis\nshowing that the original network inherits this good generalization bound from\nits distillation, assuming the use of well-behaved data augmentation. This\nbound is presented both in an abstract and in a concrete form, the latter\ncomplemented by a reduction technique to handle modern computation graphs\nfeaturing convolutional layers, fully-connected layers, and skip connections,\nto name a few. To round out the story, a (looser) classical uniform convergence\nanalysis of compression is also presented, as well as a variety of experiments\non cifar and mnist demonstrating similar generalization performance between the\noriginal network and its distillation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.13094,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000023511,
      "text":"Detection of Fake Users in SMPs Using NLP and Graph Embeddings\n\n  Social Media Platforms (SMPs) like Facebook, Twitter, Instagram etc. have\nlarge user base all around the world that generates huge amount of data every\nsecond. This includes a lot of posts by fake and spam users, typically used by\nmany organisations around the globe to have competitive edge over others. In\nthis work, we aim at detecting such user accounts in Twitter using a novel\napproach. We show how to distinguish between Genuine and Spam accounts in\nTwitter using a combination of Graph Representation Learning and Natural\nLanguage Processing techniques.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.1305,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Confined Gradient Descent: Privacy-preserving Optimization for Federated\n  Learning\n\n  Federated learning enables multiple participants to collaboratively train a\nmodel without aggregating the training data. Although the training data are\nkept within each participant and the local gradients can be securely\nsynthesized, recent studies have shown that such privacy protection is\ninsufficient. The global model parameters that have to be shared for\noptimization are susceptible to leak information about training data. In this\nwork, we propose Confined Gradient Descent (CGD) that enhances privacy of\nfederated learning by eliminating the sharing of global model parameters. CGD\nexploits the fact that a gradient descent optimization can start with a set of\ndiscrete points and converges to another set at the neighborhood of the global\nminimum of the objective function. It lets the participants independently train\non their local data, and securely share the sum of local gradients to benefit\neach other. We formally demonstrate CGD's privacy enhancement over traditional\nFL. We prove that less information is exposed in CGD compared to that of\ntraditional FL. CGD also guarantees desired model accuracy. We theoretically\nestablish a convergence rate for CGD. We prove that the loss of the proprietary\nmodels learned for each participant against a model learned by aggregated\ntraining data is bounded. Extensive experimental results on two real-world\ndatasets demonstrate the performance of CGD is comparable with the centralized\nlearning, with marginal differences on validation loss (mostly within 0.05) and\naccuracy (mostly within 1%).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.0988,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000023842,
      "text":"GMLP: Building Scalable and Flexible Graph Neural Networks with\n  Feature-Message Passing\n\n  In recent studies, neural message passing has proved to be an effective way\nto design graph neural networks (GNNs), which have achieved state-of-the-art\nperformance in many graph-based tasks. However, current neural-message passing\narchitectures typically need to perform an expensive recursive neighborhood\nexpansion in multiple rounds and consequently suffer from a scalability issue.\nMoreover, most existing neural-message passing schemes are inflexible since\nthey are restricted to fixed-hop neighborhoods and insensitive to the actual\ndemands of different nodes. We circumvent these limitations by a novel\nfeature-message passing framework, called Graph Multi-layer Perceptron (GMLP),\nwhich separates the neural update from the message passing. With such\nseparation, GMLP significantly improves the scalability and efficiency by\nperforming the message passing procedure in a pre-compute manner, and is\nflexible and adaptive in leveraging node feature messages over various levels\nof localities. We further derive novel variants of scalable GNNs under this\nframework to achieve the best of both worlds in terms of performance and\nefficiency. We conduct extensive evaluations on 11 benchmark datasets,\nincluding large-scale datasets like ogbn-products and an industrial dataset,\ndemonstrating that GMLP achieves not only the state-of-art performance, but\nalso high training scalability and efficiency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.02146,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000044372,
      "text":"Semi-Supervised Clustering with Inaccurate Pairwise Annotations\n\n  Pairwise relational information is a useful way of providing partial\nsupervision in domains where class labels are difficult to acquire. This work\npresents a clustering model that incorporates pairwise annotations in the form\nof must-link and cannot-link relations and considers possible annotation\ninaccuracies (i.e., a common setting when experts provide pairwise\nsupervision). We propose a generative model that assumes Gaussian-distributed\ndata samples along with must-link and cannot-link relations generated by\nstochastic block models. We adopt a maximum-likelihood approach and demonstrate\nthat, even when supervision is weak and inaccurate, accounting for relational\ninformation significantly improves clustering performance. Relational\ninformation also helps to detect meaningful groups in real-world datasets that\ndo not fit the original data-distribution assumptions. Additionally, we extend\nthe model to integrate prior knowledge of experts' accuracy and discuss\ncircumstances in which the use of this knowledge is beneficial.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.0715,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"When and Whom to Collaborate with in a Changing Environment: A\n  Collaborative Dynamic Bandit Solution\n\n  Collaborative bandit learning, i.e., bandit algorithms that utilize\ncollaborative filtering techniques to improve sample efficiency in online\ninteractive recommendation, has attracted much research attention as it enjoys\nthe best of both worlds. However, all existing collaborative bandit learning\nsolutions impose a stationary assumption about the environment, i.e., both user\npreferences and the dependency among users are assumed static over time.\nUnfortunately, this assumption hardly holds in practice due to users'\never-changing interests and dependence relations, which inevitably costs a\nrecommender system sub-optimal performance in practice.\n  In this work, we develop a collaborative dynamic bandit solution to handle a\nchanging environment for recommendation. We explicitly model the underlying\nchanges in both user preferences and their dependency relation as a stochastic\nprocess. Individual user's preference is modeled by a mixture of globally\nshared contextual bandit models with a Dirichlet Process prior. Collaboration\namong users is thus achieved via Bayesian inference over the global bandit\nmodels. Model selection and arm selection for each user are done via Thompson\nsampling to balance exploitation and exploration. Our solution is proved to\nmaintain a standard $\\tilde O(\\sqrt{T})$ sublinear regret even in such a\nchallenging environment. And extensive empirical evaluations on both synthetic\nand real-world datasets further confirmed the necessity of modeling a changing\nenvironment and our algorithm's practical advantages against several\nstate-of-the-art online learning solutions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.07072,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"Unsupervised low-rank representations for speech emotion recognition\n\n  We examine the use of linear and non-linear dimensionality reduction\nalgorithms for extracting low-rank feature representations for speech emotion\nrecognition. Two feature sets are used, one based on low-level descriptors and\ntheir aggregations (IS10) and one modeling recurrence dynamics of speech (RQA),\nas well as their fusion. We report speech emotion recognition (SER) results for\nlearned representations on two databases using different classification\nmethods. Classification with low-dimensional representations yields performance\nimprovement in a variety of settings. This indicates that dimensionality\nreduction is an effective way to combat the curse of dimensionality for SER.\nVisualization of features in two dimensions provides insight into\ndiscriminatory abilities of reduced feature sets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.03037,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"ConCAD: Contrastive Learning-based Cross Attention for Sleep Apnea\n  Detection\n\n  With recent advancements in deep learning methods, automatically learning\ndeep features from the original data is becoming an effective and widespread\napproach. However, the hand-crafted expert knowledge-based features are still\ninsightful. These expert-curated features can increase the model's\ngeneralization and remind the model of some data characteristics, such as the\ntime interval between two patterns. It is particularly advantageous in tasks\nwith the clinically-relevant data, where the data are usually limited and\ncomplex. To keep both implicit deep features and expert-curated explicit\nfeatures together, an effective fusion strategy is becoming indispensable. In\nthis work, we focus on a specific clinical application, i.e., sleep apnea\ndetection. In this context, we propose a contrastive learning-based cross\nattention framework for sleep apnea detection (named ConCAD). The cross\nattention mechanism can fuse the deep and expert features by automatically\nassigning attention weights based on their importance. Contrastive learning can\nlearn better representations by keeping the instances of each class closer and\npushing away instances from different classes in the embedding space\nconcurrently. Furthermore, a new hybrid loss is designed to simultaneously\nconduct contrastive learning and classification by integrating a supervised\ncontrastive loss with a cross-entropy loss. Our proposed framework can be\neasily integrated into standard deep learning models to utilize expert\nknowledge and contrastive learning to boost performance. As demonstrated on two\npublic ECG dataset with sleep apnea annotation, ConCAD significantly improves\nthe detection performance and outperforms state-of-art benchmark methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.11417,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Skew Orthogonal Convolutions\n\n  Training convolutional neural networks with a Lipschitz constraint under the\n$l_{2}$ norm is useful for provable adversarial robustness, interpretable\ngradients, stable training, etc. While 1-Lipschitz networks can be designed by\nimposing a 1-Lipschitz constraint on each layer, training such networks\nrequires each layer to be gradient norm preserving (GNP) to prevent gradients\nfrom vanishing. However, existing GNP convolutions suffer from slow training,\nlead to significant reduction in accuracy and provide no guarantees on their\napproximations. In this work, we propose a GNP convolution layer called Skew\nOrthogonal Convolution (SOC) that uses the following mathematical property:\nwhen a matrix is {\\it Skew-Symmetric}, its exponential function is an {\\it\northogonal} matrix. To use this property, we first construct a convolution\nfilter whose Jacobian is Skew-Symmetric. Then, we use the Taylor series\nexpansion of the Jacobian exponential to construct the SOC layer that is\northogonal. To efficiently implement SOC, we keep a finite number of terms from\nthe Taylor series and provide a provable guarantee on the approximation error.\nOur experiments on CIFAR-10 and CIFAR-100 show that SOC allows us to train\nprovably Lipschitz, large convolutional neural networks significantly faster\nthan prior works while achieving significant improvements for both standard and\ncertified robust accuracies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.08446,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Automatic Assessment of Alzheimer's Disease Diagnosis Based on Deep\n  Learning Techniques\n\n  Early detection is crucial to prevent the progression of Alzheimer's disease\n(AD). Thus, specialists can begin preventive treatment as soon as possible.\nThey demand fast and precise assessment in the diagnosis of AD in the earliest\nand hardest to detect stages. The main objective of this work is to develop a\nsystem that automatically detects the presence of the disease in sagittal\nmagnetic resonance images (MRI), which are not generally used. Sagittal MRIs\nfrom ADNI and OASIS data sets were employed. Experiments were conducted using\nTransfer Learning (TL) techniques in order to achieve more accurate results.\nThere are two main conclusions to be drawn from this work: first, the damages\nrelated to AD and its stages can be distinguished in sagittal MRI and, second,\nthe results obtained using DL models with sagittal MRIs are similar to the\nstate-of-the-art, which uses the horizontal-plane MRI. Although sagittal-plane\nMRIs are not commonly used, this work proved that they were, at least, as\neffective as MRI from other planes at identifying AD in early stages. This\ncould pave the way for further research. Finally, one should bear in mind that\nin certain fields, obtaining the examples for a data set can be very expensive.\nThis study proved that DL models could be built in these fields, whereas TL is\nan essential tool for completing the task with fewer examples.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.05944,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000086758,
      "text":"Slower is Better: Revisiting the Forgetting Mechanism in LSTM for Slower\n  Information Decay\n\n  Sequential information contains short- to long-range dependencies; however,\nlearning long-timescale information has been a challenge for recurrent neural\nnetworks. Despite improvements in long short-term memory networks (LSTMs), the\nforgetting mechanism results in the exponential decay of information, limiting\ntheir capacity to capture long-timescale information. Here, we propose a power\nlaw forget gate, which instead learns to forget information along a slower\npower law decay function. Specifically, the new gate learns to control the\npower law decay factor, p, allowing the network to adjust the information decay\nrate according to task demands. Our experiments show that an LSTM with power\nlaw forget gates (pLSTM) can effectively capture long-range dependencies beyond\nhundreds of elements on image classification, language modeling, and\ncategorization tasks, improving performance over the vanilla LSTM. We also\ninspected the revised forget gate by varying the initialization of p, setting p\nto a fixed value, and ablating cells in the pLSTM network. The results show\nthat the information decay can be controlled by the learnable decay factor p,\nwhich allows pLSTM to achieve its superior performance. Altogether, we found\nthat LSTM with the proposed forget gate can learn long-term dependencies,\noutperforming other recurrent networks in multiple domains; such gating\nmechanism can be integrated into other architectures for improving the learning\nof long timescale information in recurrent neural networks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.1199,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Optimal Sampling Density for Nonparametric Regression\n\n  We propose a novel active learning strategy for regression, which is\nmodel-agnostic, robust against model mismatch, and interpretable. Assuming that\na small number of initial samples are available, we derive the optimal training\ndensity that minimizes the generalization error of local polynomial smoothing\n(LPS) with its kernel bandwidth tuned locally: We adopt the mean integrated\nsquared error (MISE) as a generalization criterion, and use the asymptotic\nbehavior of the MISE as well as the locally optimal bandwidths (LOB) - the\nbandwidth function that minimizes MISE in the asymptotic limit. The asymptotic\nexpression of our objective then reveals the dependence of the MISE on the\ntraining density, enabling analytic minimization. As a result,we obtain the\noptimal training density in a closed-form. The almost model-free nature of our\napproach thus helps to encode the essential properties of the target problem,\nproviding a robust and model-agnostic active learning strategy. Furthermore,\nthe obtained training density factorizes the influence of local function\ncomplexity, noise level and test density in a transparent and interpretable\nway. We validate our theory in numerical simulations, and show that the\nproposed active learning method outperforms the existing state-of-the-art\nmodel-agnostic approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.01443,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"mil-benchmarks: Standardized Evaluation of Deep Multiple-Instance\n  Learning Techniques\n\n  Multiple-instance learning is a subset of weakly supervised learning where\nlabels are applied to sets of instances rather than the instances themselves.\nUnder the standard assumption, a set is positive only there is if at least one\ninstance in the set which is positive.\n  This paper introduces a series of multiple-instance learning benchmarks\ngenerated from MNIST, Fashion-MNIST, and CIFAR10. These benchmarks test the\nstandard, presence, absence, and complex assumptions and provide a framework\nfor future benchmarks to be distributed. I implement and evaluate several\nmultiple-instance learning techniques against the benchmarks. Further, I\nevaluate the Noisy-And method with label noise and find mixed results with\ndifferent datasets. The models are implemented in TensorFlow 2.4.1 and are\navailable on GitHub. The benchmarks are available from PyPi as mil-benchmarks\nand on GitHub.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.0403,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000017484,
      "text":"A Bit More Bayesian: Domain-Invariant Learning with Uncertainty\n\n  Domain generalization is challenging due to the domain shift and the\nuncertainty caused by the inaccessibility of target domain data. In this paper,\nwe address both challenges with a probabilistic framework based on variational\nBayesian inference, by incorporating uncertainty into neural network weights.\nWe couple domain invariance in a probabilistic formula with the variational\nBayesian inference. This enables us to explore domain-invariant learning in a\nprincipled way. Specifically, we derive domain-invariant representations and\nclassifiers, which are jointly established in a two-layer Bayesian neural\nnetwork. We empirically demonstrate the effectiveness of our proposal on four\nwidely used cross-domain visual recognition benchmarks. Ablation studies\nvalidate the synergistic benefits of our Bayesian treatment when jointly\nlearning domain-invariant representations and classifiers for domain\ngeneralization. Further, our method consistently delivers state-of-the-art mean\naccuracy on all benchmarks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.05553,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Principal Components Bias in Over-parameterized Linear Models, and its\n  Manifestation in Deep Neural Networks\n\n  Recent work suggests that convolutional neural networks of different\narchitectures learn to classify images in the same order. To understand this\nphenomenon, we revisit the over-parametrized deep linear network model. Our\nanalysis reveals that, when the hidden layers are wide enough, the convergence\nrate of this model's parameters is exponentially faster along the directions of\nthe larger principal components of the data, at a rate governed by the\ncorresponding singular values. We term this convergence pattern the Principal\nComponents bias (PC-bias). Empirically, we show how the PC-bias streamlines the\norder of learning of both linear and non-linear networks, more prominently at\nearlier stages of learning. We then compare our results to the simplicity bias,\nshowing that both biases can be seen independently, and affect the order of\nlearning in different ways. Finally, we discuss how the PC-bias may explain\nsome benefits of early stopping and its connection to PCA, and why deep\nnetworks converge more slowly with random labels.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.00127,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"Integer-Only Neural Network Quantization Scheme Based on\n  Shift-Batch-Normalization\n\n  Neural networks are very popular in many areas, but great computing\ncomplexity makes it hard to run neural networks on devices with limited\nresources. To address this problem, quantization methods are used to reduce\nmodel size and computation cost, making it possible to use neural networks on\nembedded platforms or mobile devices.\n  In this paper, an integer-only-quantization scheme is introduced. This scheme\nuses one layer that combines shift-based batch normalization and uniform\nquantization to implement 4-bit integer-only inference. Without big integer\nmultiplication(which is used in previous integer-only-quantization methods),\nthis scheme can achieve good power and latency efficiency, and is especially\nsuitable to be deployed on co-designed hardware platforms. Tests have proved\nthat this scheme works very well for easy tasks. And for tough tasks,\nperformance loss can be tolerated for its inference efficiency. Our work is\navailable on github: https:\/\/github.com\/hguq\/IntegerNet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.01119,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Iterated learning for emergent systematicity in VQA\n\n  Although neural module networks have an architectural bias towards\ncompositionality, they require gold standard layouts to generalize\nsystematically in practice. When instead learning layouts and modules jointly,\ncompositionality does not arise automatically and an explicit pressure is\nnecessary for the emergence of layouts exhibiting the right structure. We\npropose to address this problem using iterated learning, a cognitive science\ntheory of the emergence of compositional languages in nature that has primarily\nbeen applied to simple referential games in machine learning. Considering the\nlayouts of module networks as samples from an emergent language, we use\niterated learning to encourage the development of structure within this\nlanguage. We show that the resulting layouts support systematic generalization\nin neural agents solving the more complex task of visual question-answering.\nOur regularized iterated learning method can outperform baselines without\niterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a\nnew split of the SHAPES dataset we introduce to evaluate systematic\ngeneralization, and on CLOSURE, an extension of CLEVR also designed to test\nsystematic generalization. We demonstrate superior performance in recovering\nground-truth compositional program structure with limited supervision on both\nSHAPES-SyGeT and CLEVR.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.104,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Word-level Text Highlighting of Medical Texts for Telehealth Services\n\n  The medical domain is often subject to information overload. The digitization\nof healthcare, constant updates to online medical repositories, and increasing\navailability of biomedical datasets make it challenging to effectively analyze\nthe data. This creates additional work for medical professionals who are\nheavily dependent on medical data to complete their research and consult their\npatients. This paper aims to show how different text highlighting techniques\ncan capture relevant medical context. This would reduce the doctors' cognitive\nload and response time to patients by facilitating them in making faster\ndecisions, thus improving the overall quality of online medical services. Three\ndifferent word-level text highlighting methodologies are implemented and\nevaluated. The first method uses TF-IDF scores directly to highlight important\nparts of the text. The second method is a combination of TF-IDF scores and the\napplication of Local Interpretable Model-Agnostic Explanations to\nclassification models. The third method uses neural networks directly to make\npredictions on whether or not a word should be highlighted. The results of our\nexperiments show that the neural network approach is successful in highlighting\nmedically-relevant terms and its performance is improved as the size of the\ninput segment increases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.09974,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000128481,
      "text":"Wide & Deep neural network model for patch aggregation in CNN-based\n  prostate cancer detection systems\n\n  Prostate cancer (PCa) is one of the most commonly diagnosed cancer and one of\nthe leading causes of death among men, with almost 1.41 million new cases and\naround 375,000 deaths in 2020. Artificial Intelligence algorithms have had a\nhuge impact in medical image analysis, including digital histopathology, where\nConvolutional Neural Networks (CNNs) are used to provide a fast and accurate\ndiagnosis, supporting experts in this task. To perform an automatic diagnosis,\nprostate tissue samples are first digitized into gigapixel-resolution\nwhole-slide images. Due to the size of these images, neural networks cannot use\nthem as input and, therefore, small subimages called patches are extracted and\npredicted, obtaining a patch-level classification. In this work, a novel patch\naggregation method based on a custom Wide & Deep neural network model is\npresented, which performs a slide-level classification using the patch-level\nclasses obtained from a CNN. The malignant tissue ratio, a 10-bin malignant\nprobability histogram, the least squares regression line of the histogram, and\nthe number of malignant connected components are used by the proposed model to\nperform the classification. An accuracy of 94.24% and a sensitivity of 98.87%\nwere achieved, proving that the proposed system could aid pathologists by\nspeeding up the screening process and, thus, contribute to the fight against\nPCa.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.01209,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0,
      "text":"LaboRecommender: A crazy-easy to use Python-based recommender system for\n  laboratory tests\n\n  Laboratory tests play a major role in clinical decision making because they\nare essential for the confirmation of diagnostics suspicions and influence\nmedical decisions. The number of different laboratory tests available to\nphysicians in our age has been expanding very rapidly due to the rapid advances\nin laboratory technology. To find the correct desired tests within this\nexpanding plethora of elements, the Health Information System must provide a\npowerful search engine and the practitioner need to remember the exact name of\nthe laboratory test to correctly select the bag of tests to order. Recommender\nsystems are platforms which suggest appropriate items to a user after learning\nthe users' behaviour. A neighbourhood-based collaborative filtering method was\nused to model the recommender system, where similar bags, clustered using\nnearest neighbours algorithm, are used to make recommendations of tests for\neach other similar bag of laboratory tests. The recommender system developed in\nthis paper achieved 95.54 % in the mean average precision metric. A fully\ndocumented Python package named LaboRecommender was developed to implement the\nalgorithm proposed in this paper\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.00937,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000056293,
      "text":"LFI-CAM: Learning Feature Importance for Better Visual Explanation\n\n  Class Activation Mapping (CAM) is a powerful technique used to understand the\ndecision making of Convolutional Neural Network (CNN) in computer vision.\nRecently, there have been attempts not only to generate better visual\nexplanations, but also to improve classification performance using visual\nexplanations. However, the previous works still have their own drawbacks. In\nthis paper, we propose a novel architecture, LFI-CAM, which is trainable for\nimage classification and visual explanation in an end-to-end manner. LFI-CAM\ngenerates an attention map for visual explanation during forward propagation,\nat the same time, leverages the attention map to improve the classification\nperformance through the attention mechanism. Our Feature Importance Network\n(FIN) focuses on learning the feature importance instead of directly learning\nthe attention map to obtain a more reliable and consistent attention map. We\nconfirmed that LFI-CAM model is optimized not only by learning the feature\nimportance but also by enhancing the backbone feature representation to focus\nmore on important features of the input image. Experimental results show that\nLFI-CAM outperforms the baseline models's accuracy on the classification tasks\nas well as significantly improves on the previous works in terms of attention\nmap quality and stability over different hyper-parameters.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.02711,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000040068,
      "text":"SafeDrug: Dual Molecular Graph Encoders for Recommending Effective and\n  Safe Drug Combinations\n\n  Medication recommendation is an essential task of AI for healthcare. Existing\nworks focused on recommending drug combinations for patients with complex\nhealth conditions solely based on their electronic health records. Thus, they\nhave the following limitations: (1) some important data such as drug molecule\nstructures have not been utilized in the recommendation process. (2) drug-drug\ninteractions (DDI) are modeled implicitly, which can lead to sub-optimal\nresults. To address these limitations, we propose a DDI-controllable drug\nrecommendation model named SafeDrug to leverage drugs' molecule structures and\nmodel DDIs explicitly. SafeDrug is equipped with a global message passing\nneural network (MPNN) module and a local bipartite learning module to fully\nencode the connectivity and functionality of drug molecules. SafeDrug also has\na controllable loss function to control DDI levels in the recommended drug\ncombinations effectively. On a benchmark dataset, our SafeDrug is relatively\nshown to reduce DDI by 19.43% and improves 2.88% on Jaccard similarity between\nrecommended and actually prescribed drug combinations over previous approaches.\nMoreover, SafeDrug also requires much fewer parameters than previous deep\nlearning-based approaches, leading to faster training by about 14% and around\n2x speed-up in inference.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.10424,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"Low-Memory Implementations of Ridge Solutions for Broad Learning System\n  with Incremental Learning\n\n  The existing low-memory BLS implementation proposed recently avoids the need\nfor storing and inverting large matrices, to achieve efficient usage of\nmemories. However, the existing low-memory BLS implementation sacrifices the\ntesting accuracy as a price for efficient usage of memories, since it can no\nlonger obtain the generalized inverse or ridge solution for the output weights\nduring incremental learning, and it cannot work under the very small ridge\nparameter that is utilized in the original BLS. Accordingly, it is required to\ndevelop the low-memory BLS implementations, which can work under very small\nridge parameters and compute the generalized inverse or ridge solution for the\noutput weights in the process of incremental learning. In this paper, firstly\nwe propose the low-memory implementations for the recently proposed recursive\nand square-root BLS algorithms on added inputs and the recently proposed\nsquareroot BLS algorithm on added nodes, by simply processing a batch of inputs\nor nodes in each recursion. Since the recursive BLS implementation includes the\nrecursive updates of the inverse matrix that may introduce numerical\ninstabilities after a large number of iterations, and needs the extra\ncomputational load to decompose the inverse matrix into the Cholesky factor\nwhen cooperating with the proposed low-memory implementation of the square-root\nBLS algorithm on added nodes, we only improve the low-memory implementations of\nthe square-root BLS algorithms on added inputs and nodes, to propose the full\nlowmemory implementation of the square-root BLS algorithm. All the proposed\nlow-memory BLS implementations compute the ridge solution for the output\nweights in the process of incremental learning, and most of them can work under\nvery small ridge parameters.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.01571,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Effective Sparsification of Neural Networks with Global Sparsity\n  Constraint\n\n  Weight pruning is an effective technique to reduce the model size and\ninference time for deep neural networks in real-world deployments. However,\nsince magnitudes and relative importance of weights are very different for\ndifferent layers of a neural network, existing methods rely on either manual\ntuning or handcrafted heuristic rules to find appropriate pruning rates\nindividually for each layer. This approach generally leads to suboptimal\nperformance. In this paper, by directly working on the probability space, we\npropose an effective network sparsification method called {\\it probabilistic\nmasking} (ProbMask), which solves a natural sparsification formulation under\nglobal sparsity constraint. The key idea is to use probability as a global\ncriterion for all layers to measure the weight importance. An appealing feature\nof ProbMask is that the amounts of weight redundancy can be learned\nautomatically via our constraint and thus we avoid the problem of tuning\npruning rates individually for different layers in a network. Extensive\nexperimental results on CIFAR-10\/100 and ImageNet demonstrate that our method\nis highly effective, and can outperform previous state-of-the-art methods by a\nsignificant margin, especially in the high pruning rate situation. Notably, the\ngap of Top-1 accuracy between our ProbMask and existing methods can be up to\n10\\%. As a by-product, we show ProbMask is also highly effective in identifying\nsupermasks, which are subnetworks with high performance in a randomly weighted\ndense neural network.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.03819,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Evaluating Deep Neural Network Ensembles by Majority Voting cum\n  Meta-Learning scheme\n\n  Deep Neural Networks (DNNs) are prone to overfitting and hence have high\nvariance. Overfitted networks do not perform well for a new data instance. So\ninstead of using a single DNN as classifier we propose an ensemble of seven\nindependent DNN learners by varying only the input to these DNNs keeping their\narchitecture and intrinsic properties same. To induce variety in the training\ninput, for each of the seven DNNs, one-seventh of the data is deleted and\nreplenished by bootstrap sampling from the remaining samples. We have proposed\na novel technique for combining the prediction of the DNN learners in the\nensemble. Our method is called pre-filtering by majority voting coupled with\nstacked meta-learner which performs a two-step confi-dence check for the\npredictions before assigning the final class labels. All the algorithms in this\npaper have been tested on five benchmark datasets name-ly, Human Activity\nRecognition (HAR), Gas sensor array drift, Isolet, Spam-base and Internet\nadvertisements. Our ensemble approach achieves higher accuracy than a single\nDNN and the average individual accuracies of DNNs in the ensemble, as well as\nthe baseline approaches of plurality voting and meta-learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.07768,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Self-Learning for Received Signal Strength Map Reconstruction with\n  Neural Architecture Search\n\n  In this paper, we present a Neural Network (NN) model based on Neural\nArchitecture Search (NAS) and self-learning for received signal strength (RSS)\nmap reconstruction out of sparse single-snapshot input measurements, in the\ncase where data-augmentation by side deterministic simulations cannot be\nperformed. The approach first finds an optimal NN architecture and\nsimultaneously train the deduced model over some ground-truth measurements of a\ngiven (RSS) map. These ground-truth measurements along with the predictions of\nthe model over a set of randomly chosen points are then used to train a second\nNN model having the same architecture. Experimental results show that signal\npredictions of this second model outperforms non-learning based interpolation\nstate-of-the-art techniques and NN models with no architecture search on five\nlarge-scale maps of RSS measurements.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.0273,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Solve routing problems with a residual edge-graph attention neural\n  network\n\n  For NP-hard combinatorial optimization problems, it is usually difficult to\nfind high-quality solutions in polynomial time. The design of either an exact\nalgorithm or an approximate algorithm for these problems often requires\nsignificantly specialized knowledge. Recently, deep learning methods provide\nnew directions to solve such problems. In this paper, an end-to-end deep\nreinforcement learning framework is proposed to solve this type of\ncombinatorial optimization problems. This framework can be applied to different\nproblems with only slight changes of input (for example, for a traveling\nsalesman problem (TSP), the input is the two-dimensional coordinates of nodes;\nwhile for a capacity-constrained vehicle routing problem (CVRP), the input is\nsimply changed to three-dimensional vectors including the two-dimensional\ncoordinates and the customer demands of nodes), masks and decoder context\nvectors. The proposed framework is aiming to improve the models in literacy in\nterms of the neural network model and the training algorithm. The solution\nquality of TSP and the CVRP up to 100 nodes are significantly improved via our\nframework. Specifically, the average optimality gap is reduced from 4.53\\%\n(reported best \\cite{R22}) to 3.67\\% for TSP with 100 nodes and from 7.34\\%\n(reported best \\cite{R22}) to 6.68\\% for CVRP with 100 nodes when using the\ngreedy decoding strategy. Furthermore, our framework uses about 1\/3$\\sim$3\/4\ntraining samples compared with other existing learning methods while achieving\nbetter results. The results performed on randomly generated instances and the\nbenchmark instances from TSPLIB and CVRPLIB confirm that our framework has a\nlinear running time on the problem size (number of nodes) during the testing\nphase, and has a good generalization performance from random instance training\nto real-world instance testing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.15529,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000044703,
      "text":"On Graph Neural Network Ensembles for Large-Scale Molecular Property\n  Prediction\n\n  In order to advance large-scale graph machine learning, the Open Graph\nBenchmark Large Scale Challenge (OGB-LSC) was proposed at the KDD Cup 2021. The\nPCQM4M-LSC dataset defines a molecular HOMO-LUMO property prediction task on\nabout 3.8M graphs. In this short paper, we show our current work-in-progress\nsolution which builds an ensemble of three graph neural networks models based\non GIN, Bayesian Neural Networks and DiffPool. Our approach outperforms the\nprovided baseline by 7.6%. Moreover, using uncertainty in our ensemble's\nprediction, we can identify molecules whose HOMO-LUMO gaps are harder to\npredict (with Pearson's correlation of 0.5181). We anticipate that this will\nfacilitate active learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.03157,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000038081,
      "text":"Self-Supervision is All You Need for Solving Rubik's Cube\n\n  Existing combinatorial search methods are often complex and require some\nlevel of expertise. This work introduces a simple and efficient deep learning\nmethod for solving combinatorial problems with a predefined goal, represented\nby Rubik's Cube. We demonstrate that, for such problems, training a deep neural\nnetwork on random scrambles branching from the goal state is sufficient to\nachieve near-optimal solutions. When tested on Rubik's Cube, 15 Puzzle, and\n7$\\times$7 Lights Out, our method outperformed the previous state-of-the-art\nmethod DeepCubeA, improving the trade-off between solution optimality and\ncomputational cost, despite significantly less training data. Furthermore, we\ninvestigate the scaling law of our Rubik's Cube solver with respect to model\nsize and training data volume.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.04716,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Labeled Data Generation with Inexact Supervision\n\n  The recent advanced deep learning techniques have shown the promising results\nin various domains such as computer vision and natural language processing. The\nsuccess of deep neural networks in supervised learning heavily relies on a\nlarge amount of labeled data. However, obtaining labeled data with target\nlabels is often challenging due to various reasons such as cost of labeling and\nprivacy issues, which challenges existing deep models. In spite of that, it is\nrelatively easy to obtain data with \\textit{inexact supervision}, i.e., having\nlabels\/tags related to the target task. For example, social media platforms are\noverwhelmed with billions of posts and images with self-customized tags, which\nare not the exact labels for target classification tasks but are usually\nrelated to the target labels. It is promising to leverage these tags (inexact\nsupervision) and their relations with target classes to generate labeled data\nto facilitate the downstream classification tasks. However, the work on this is\nrather limited. Therefore, we study a novel problem of labeled data generation\nwith inexact supervision. We propose a novel generative framework named as\nADDES which can synthesize high-quality labeled data for target classification\ntasks by learning from data with inexact supervision and the relations between\ninexact supervision and target classes. Experimental results on image and text\ndatasets demonstrate the effectiveness of the proposed ADDES for generating\nrealistic labeled data from inexact supervision to facilitate the target\nclassification task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.03142,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000051326,
      "text":"A Physics-Informed Deep Learning Paradigm for Traffic State and\n  Fundamental Diagram Estimation\n\n  Traffic state estimation (TSE) bifurcates into two categories, model-driven\nand data-driven (e.g., machine learning, ML), while each suffers from either\ndeficient physics or small data. To mitigate these limitations, recent studies\nintroduced a hybrid paradigm, physics-informed deep learning (PIDL), which\ncontains both model-driven and data-driven components. This paper contributes\nan improved version, called physics-informed deep learning with a fundamental\ndiagram learner (PIDL+FDL), which integrates ML terms into the model-driven\ncomponent to learn a functional form of a fundamental diagram (FD), i.e., a\nmapping from traffic density to flow or velocity. The proposed PIDL+FDL has the\nadvantages of performing the TSE learning, model parameter identification, and\nFD estimation simultaneously. We demonstrate the use of PIDL+FDL to solve\npopular first-order and second-order traffic flow models and reconstruct the FD\nrelation as well as model parameters that are outside the FD terms. We then\nevaluate the PIDL+FDL-based TSE using the Next Generation SIMulation (NGSIM)\ndataset. The experimental results show the superiority of the PIDL+FDL in terms\nof improved estimation accuracy and data efficiency over advanced baseline TSE\nmethods, and additionally, the capacity to properly learn the unknown\nunderlying FD relation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.11059,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000026822,
      "text":"Improving Multi-Modal Learning with Uni-Modal Teachers\n\n  Learning multi-modal representations is an essential step towards real-world\nrobotic applications, and various multi-modal fusion models have been developed\nfor this purpose. However, we observe that existing models, whose objectives\nare mostly based on joint training, often suffer from learning inferior\nrepresentations of each modality. We name this problem Modality Failure, and\nhypothesize that the imbalance of modalities and the implicit bias of common\nobjectives in fusion method prevent encoders of each modality from sufficient\nfeature learning. To this end, we propose a new multi-modal learning method,\nUni-Modal Teacher, which combines the fusion objective and uni-modal\ndistillation to tackle the modality failure problem. We show that our method\nnot only drastically improves the representation of each modality, but also\nimproves the overall multi-modal task performance. Our method can be\neffectively generalized to most multi-modal fusion approaches. We achieve more\nthan 3% improvement on the VGGSound audio-visual classification task, as well\nas improving performance on the NYU depth V2 RGB-D image segmentation task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.16187,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Reinforcement Learning based Disease Progression Model for Alzheimer's\n  Disease\n\n  We model Alzheimer's disease (AD) progression by combining differential\nequations (DEs) and reinforcement learning (RL) with domain knowledge. DEs\nprovide relationships between some, but not all, factors relevant to AD. We\nassume that the missing relationships must satisfy general criteria about the\nworking of the brain, for e.g., maximizing cognition while minimizing the cost\nof supporting cognition. This allows us to extract the missing relationships by\nusing RL to optimize an objective (reward) function that captures the above\ncriteria. We use our model consisting of DEs (as a simulator) and the trained\nRL agent to predict individualized 10-year AD progression using baseline (year\n0) features on synthetic and real data. The model was comparable or better at\npredicting 10-year cognition trajectories than state-of-the-art learning-based\nmodels. Our interpretable model demonstrated, and provided insights into,\n\"recovery\/compensatory\" processes that mitigate the effect of AD, even though\nthose processes were not explicitly encoded in the model. Our framework\ncombines DEs with RL for modelling AD progression and has broad applicability\nfor understanding other neurological disorders.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.00502,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000094705,
      "text":"Automated Grading of Anatomical Objective Structured Practical Exams\n  Using Decision Trees\n\n  An Objective Structured Practical Examination (OSPE) is an effective and\nrobust, but resource-intensive, means of evaluating anatomical knowledge. Since\nmost OSPEs employ short answer or fill-in-the-blank style questions, the format\nrequires many people familiar with the content to mark the exams. However, the\nincreasing prevalence of online delivery for anatomy and physiology courses\ncould result in students losing the OSPE practice that they would receive in\nface-to-face learning sessions. The purpose of this study was to test the\naccuracy of Decision Trees (DTs) in marking OSPE questions as a potential first\nstep to creating an intelligent, online OSPE tutoring system. The study used\nthe results of the winter 2020 semester final OSPE from McMaster University's\nanatomy and physiology course in the Faculty of Health Sciences (HTHSCI\n2FF3\/2LL3\/1D06) as the data set. Ninety percent of the data set was used in a\n10-fold validation algorithm to train a DT for each of the 54 questions. Each\nDT was comprised of unique words that appeared in correct, student-written\nanswers. The remaining 10% of the data set was marked by the generated DTs.\nWhen the answers marked by the DT were compared to the answers marked by staff\nand faculty, the DT achieved an average accuracy of 94.49% across all 54\nquestions. This suggests that machine learning algorithms such as DTs are a\nhighly effective option for OSPE grading and are suitable for the development\nof an intelligent, online OSPE tutoring system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.15776,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Local Reweighting for Adversarial Training\n\n  Instances-reweighted adversarial training (IRAT) can significantly boost the\nrobustness of trained models, where data being less\/more vulnerable to the\ngiven attack are assigned smaller\/larger weights during training. However, when\ntested on attacks different from the given attack simulated in training, the\nrobustness may drop significantly (e.g., even worse than no reweighting). In\nthis paper, we study this problem and propose our solution--locally reweighted\nadversarial training (LRAT). The rationale behind IRAT is that we do not need\nto pay much attention to an instance that is already safe under the attack. We\nargue that the safeness should be attack-dependent, so that for the same\ninstance, its weight can change given different attacks based on the same\nmodel. Thus, if the attack simulated in training is mis-specified, the weights\nof IRAT are misleading. To this end, LRAT pairs each instance with its\nadversarial variants and performs local reweighting inside each pair, while\nperforming no global reweighting--the rationale is to fit the instance itself\nif it is immune to the attack, but not to skip the pair, in order to passively\ndefend different attacks in future. Experiments show that LRAT works better\nthan both IRAT (i.e., global reweighting) and the standard AT (i.e., no\nreweighting) when trained with an attack and tested on different attacks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.06983,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000108613,
      "text":"Two-way Spectrum Pursuit for CUR Decomposition and Its Application in\n  Joint Column\/Row Subset Selection\n\n  The problem of simultaneous column and row subset selection is addressed in\nthis paper. The column space and row space of a matrix are spanned by its left\nand right singular vectors, respectively. However, the singular vectors are not\nwithin actual columns\/rows of the matrix. In this paper, an iterative approach\nis proposed to capture the most structural information of columns\/rows via\nselecting a subset of actual columns\/rows. This algorithm is referred to as\ntwo-way spectrum pursuit (TWSP) which provides us with an accurate solution for\nthe CUR matrix decomposition. TWSP is applicable in a wide range of\napplications since it enjoys a linear complexity w.r.t. number of original\ncolumns\/rows. We demonstrated the application of TWSP for joint channel and\nsensor selection in cognitive radio networks, informative users and contents\ndetection, and efficient supervised data reduction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.09547,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"Machine Learning for Postprocessing Ensemble Streamflow Forecasts\n\n  Skillful streamflow forecasts can inform decisions in various areas of water\npolicy and management. We integrate numerical weather prediction ensembles,\ndistributed hydrological model and machine learning to generate ensemble\nstreamflow forecasts at medium-range lead times (1 - 7 days). We demonstrate a\ncase study for machine learning applications in postprocessing ensemble\nstreamflow forecasts in the Upper Susquehanna River basin in the eastern United\nStates. Our results show that the machine learning postprocessor can improve\nstreamflow forecasts relative to low complexity forecasts (e.g., climatological\nand temporal persistence) as well as standalone hydrometeorological modeling\nand neural network. The relative gain in forecast skill from postprocessor is\ngenerally higher at medium-range timescales compared to shorter lead times;\nhigh flows compared to low-moderate flows, and warm-season compared to cool\nones. Overall, our results highlight the benefits of machine learning in many\naspects for improving both the skill and reliability of streamflow forecasts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.02734,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000078148,
      "text":"Revisiting Hilbert-Schmidt Information Bottleneck for Adversarial\n  Robustness\n\n  We investigate the HSIC (Hilbert-Schmidt independence criterion) bottleneck\nas a regularizer for learning an adversarially robust deep neural network\nclassifier. In addition to the usual cross-entropy loss, we add regularization\nterms for every intermediate layer to ensure that the latent representations\nretain useful information for output prediction while reducing redundant\ninformation. We show that the HSIC bottleneck enhances robustness to\nadversarial attacks both theoretically and experimentally. In particular, we\nprove that the HSIC bottleneck regularizer reduces the sensitivity of the\nclassifier to adversarial examples. Our experiments on multiple benchmark\ndatasets and architectures demonstrate that incorporating an HSIC bottleneck\nregularizer attains competitive natural accuracy and improves adversarial\nrobustness, both with and without adversarial examples during training. Our\ncode and adversarially robust models are publicly available.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.08279,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000140402,
      "text":"First Place Solution of KDD Cup 2021 & OGB Large-Scale Challenge Graph\n  Prediction Track\n\n  In this technical report, we present our solution of KDD Cup 2021 OGB\nLarge-Scale Challenge - PCQM4M-LSC Track. We adopt Graphormer and ExpC as our\nbasic models. We train each model by 8-fold cross-validation, and additionally\ntrain two Graphormer models on the union of training and validation sets with\ndifferent random seeds. For final submission, we use a naive ensemble for these\n18 models by taking average of their outputs. Using our method, our team\nMachineLearning achieved 0.1200 MAE on test set, which won the first place in\nKDD Cup graph prediction track.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.03764,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"On the Expressive Power of Self-Attention Matrices\n\n  Transformer networks are able to capture patterns in data coming from many\ndomains (text, images, videos, proteins, etc.) with little or no change to\narchitecture components. We perform a theoretical analysis of the core\ncomponent responsible for signal propagation between elements, i.e. the\nself-attention matrix. In practice, this matrix typically exhibits two\nproperties: (1) it is sparse, meaning that each token only attends to a small\nsubset of other tokens; and (2) it changes dynamically depending on the input\nto the module. With these considerations in mind, we ask the following\nquestion: Can a fixed self-attention module approximate arbitrary sparse\npatterns depending on the input? How small is the hidden size $d$ required for\nsuch approximation? We make progress in answering this question and show that\nthe self-attention matrix can provably approximate sparse matrices, where\nsparsity is in terms of a bounded number of nonzero elements in each row and\ncolumn. While the parameters of self-attention are fixed, various sparse\nmatrices can be approximated by only modifying the inputs. Our proof is based\non the random projection technique and uses the seminal Johnson-Lindenstrauss\nlemma. Our proof is constructive, enabling us to propose an algorithm for\nfinding adaptive inputs and fixed self-attention parameters in order to\napproximate a given matrix. In particular, we show that, in order to\napproximate any sparse matrix up to a given precision defined in terms of\npreserving matrix element ratios, $d$ grows only logarithmically with the\nsequence length $L$ (i.e. $d = O(\\log L)$).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.05863,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Learning Functional Priors and Posteriors from Data and Physics\n\n  We develop a new Bayesian framework based on deep neural networks to be able\nto extrapolate in space-time using historical data and to quantify\nuncertainties arising from both noisy and gappy data in physical problems.\nSpecifically, the proposed approach has two stages: (1) prior learning and (2)\nposterior estimation. At the first stage, we employ the physics-informed\nGenerative Adversarial Networks (PI-GAN) to learn a functional prior either\nfrom a prescribed function distribution, e.g., Gaussian process, or from\nhistorical data and physics. At the second stage, we employ the Hamiltonian\nMonte Carlo (HMC) method to estimate the posterior in the latent space of\nPI-GANs. In addition, we use two different approaches to encode the physics:\n(1) automatic differentiation, used in the physics-informed neural networks\n(PINNs) for scenarios with explicitly known partial differential equations\n(PDEs), and (2) operator regression using the deep operator network (DeepONet)\nfor PDE-agnostic scenarios. We then test the proposed method for (1)\nmeta-learning for one-dimensional regression, and forward\/inverse PDE problems\n(combined with PINNs); (2) PDE-agnostic physical problems (combined with\nDeepONet), e.g., fractional diffusion as well as saturated stochastic\n(100-dimensional) flows in heterogeneous porous media; and (3) spatial-temporal\nregression problems, i.e., inference of a marine riser displacement field. The\nresults demonstrate that the proposed approach can provide accurate predictions\nas well as uncertainty quantification given very limited scattered and noisy\ndata, since historical data could be available to provide informative priors.\nIn summary, the proposed method is capable of learning flexible functional\npriors, and can be extended to big data problems using stochastic HMC or\nnormalizing flows since the latent space is generally characterized as low\ndimensional.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.06536,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Unsupervised Neural Hidden Markov Models with a Continuous latent state\n  space\n\n  We introduce a new procedure to neuralize unsupervised Hidden Markov Models\nin the continuous case. This provides higher flexibility to solve problems with\nunderlying latent variables. This approach is evaluated on both synthetic and\nreal data. On top of generating likely model parameters with comparable\nperformances to off-the-shelf neural architecture (LSTMs, GRUs,..), the\nobtained results are easily interpretable.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.0305,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000060929,
      "text":"Efficient Continuous Control with Double Actors and Regularized Critics\n\n  How to obtain good value estimation is one of the key problems in\nReinforcement Learning (RL). Current value estimation methods, such as DDPG and\nTD3, suffer from unnecessary over- or underestimation bias. In this paper, we\nexplore the potential of double actors, which has been neglected for a long\ntime, for better value function estimation in continuous setting. First, we\nuncover and demonstrate the bias alleviation property of double actors by\nbuilding double actors upon single critic and double critics to handle\noverestimation bias in DDPG and underestimation bias in TD3 respectively. Next,\nwe interestingly find that double actors help improve the exploration ability\nof the agent. Finally, to mitigate the uncertainty of value estimate from\ndouble critics, we further propose to regularize the critic networks under\ndouble actors architecture, which gives rise to Double Actors Regularized\nCritics (DARC) algorithm. Extensive experimental results on challenging\ncontinuous control tasks show that DARC significantly outperforms\nstate-of-the-art methods with higher sample efficiency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.08641,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"Best of both worlds: local and global explanations with\n  human-understandable concepts\n\n  Interpretability techniques aim to provide the rationale behind a model's\ndecision, typically by explaining either an individual prediction (local\nexplanation, e.g. 'why is this patient diagnosed with this condition') or a\nclass of predictions (global explanation, e.g. 'why is this set of patients\ndiagnosed with this condition in general'). While there are many methods\nfocused on either one, few frameworks can provide both local and global\nexplanations in a consistent manner. In this work, we combine two powerful\nexisting techniques, one local (Integrated Gradients, IG) and one global\n(Testing with Concept Activation Vectors), to provide local and global\nconcept-based explanations. We first sanity check our idea using two synthetic\ndatasets with a known ground truth, and further demonstrate with a benchmark\nnatural image dataset. We test our method with various concepts, target\nclasses, model architectures and IG parameters (e.g. baselines). We show that\nour method improves global explanations over vanilla TCAV when compared to\nground truth, and provides useful local insights. Finally, a user study\ndemonstrates the usefulness of the method compared to no or global explanations\nonly. We hope our work provides a step towards building bridges between many\nexisting local and global methods to get the best of both worlds.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.10189,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000014239,
      "text":"Adversarial Training Helps Transfer Learning via Better Representations\n\n  Transfer learning aims to leverage models pre-trained on source data to\nefficiently adapt to target setting, where only limited data are available for\nmodel fine-tuning. Recent works empirically demonstrate that adversarial\ntraining in the source data can improve the ability of models to transfer to\nnew domains. However, why this happens is not known. In this paper, we provide\na theoretical model to rigorously analyze how adversarial training helps\ntransfer learning. We show that adversarial training in the source data\ngenerates provably better representations, so fine-tuning on top of this\nrepresentation leads to a more accurate predictor of the target data. We\nfurther demonstrate both theoretically and empirically that semi-supervised\nlearning in the source data can also improve transfer learning by similarly\nimproving the representation. Moreover, performing adversarial training on top\nof semi-supervised learning can further improve transferability, suggesting\nthat the two approaches have complementary benefits on representations. We\nsupport our theories with experiments on popular data sets and deep learning\narchitectures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.01613,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"NODE-GAM: Neural Generalized Additive Model for Interpretable Deep\n  Learning\n\n  Deployment of machine learning models in real high-risk settings (e.g.\nhealthcare) often depends not only on the model's accuracy but also on its\nfairness, robustness, and interpretability. Generalized Additive Models (GAMs)\nare a class of interpretable models with a long history of use in these\nhigh-risk domains, but they lack desirable features of deep learning such as\ndifferentiability and scalability. In this work, we propose a neural GAM\n(NODE-GAM) and neural GA$^2$M (NODE-GA$^2$M) that scale well and perform better\nthan other GAMs on large datasets, while remaining interpretable compared to\nother ensemble and deep learning models. We demonstrate that our models find\ninteresting patterns in the data. Lastly, we show that we improve model\naccuracy via self-supervised pre-training, an improvement that is not possible\nfor non-differentiable GAMs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.03004,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000036094,
      "text":"Exploring the Limits of Out-of-Distribution Detection\n\n  Near out-of-distribution detection (OOD) is a major challenge for deep neural\nnetworks. We demonstrate that large-scale pre-trained transformers can\nsignificantly improve the state-of-the-art (SOTA) on a range of near OOD tasks\nacross different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD\ndetection, we improve the AUROC from 85% (current SOTA) to more than 96% using\nVision Transformers pre-trained on ImageNet-21k. On a challenging genomics OOD\ndetection benchmark, we improve the AUROC from 66% to 77% using transformers\nand unsupervised pre-training. To further improve performance, we explore the\nfew-shot outlier exposure setting where a few examples from outlier classes may\nbe available; we show that pre-trained transformers are particularly\nwell-suited for outlier exposure, and that the AUROC of OOD detection on\nCIFAR-100 vs CIFAR-10 can be improved to 98.7% with just 1 image per OOD class,\nand 99.46% with 10 images per OOD class. For multi-modal image-text pre-trained\ntransformers such as CLIP, we explore a new way of using just the names of\noutlier classes as a sole source of information without any accompanying\nimages, and show that this outperforms previous SOTA on standard vision OOD\nbenchmark tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.02566,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000016325,
      "text":"Missingness Augmentation: A General Approach for Improving Generative\n  Imputation Models\n\n  Missing data imputation is a fundamental problem in data analysis, and many\nstudies have been conducted to improve its performance by exploring model\nstructures and learning procedures. However, data augmentation, as a simple yet\neffective method, has not received enough attention in this area. In this\npaper, we propose a novel data augmentation method called Missingness\nAugmentation (MisA) for generative imputation models. Our approach dynamically\nproduces incomplete samples at each epoch by utilizing the generator's output,\nconstraining the augmented samples using a simple reconstruction loss, and\ncombining this loss with the original loss to form the final optimization\nobjective. As a general augmentation technique, MisA can be easily integrated\ninto generative imputation frameworks, providing a simple yet effective way to\nenhance their performance. Experimental results demonstrate that MisA\nsignificantly improves the performance of many recently proposed generative\nimputation models on a variety of tabular and image datasets. The code is\navailable at \\url{https:\/\/github.com\/WYu-Feng\/Missingness-Augmentation}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.05039,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Learning from Crowds with Sparse and Imbalanced Annotations\n\n  Traditional supervised learning requires ground truth labels for the training\ndata, whose collection can be difficult in many cases. Recently, crowdsourcing\nhas established itself as an efficient labeling solution through resorting to\nnon-expert crowds. To reduce the labeling error effects, one common practice is\nto distribute each instance to multiple workers, whereas each worker only\nannotates a subset of data, resulting in the {\\it sparse annotation}\nphenomenon. In this paper, we note that when meeting with class-imbalance,\ni.e., when the ground truth labels are {\\it class-imbalanced}, the sparse\nannotations are prone to be skewly distributed, which thus can severely bias\nthe learning algorithm. To combat this issue, we propose one self-training\nbased approach named {\\it Self-Crowd} by progressively adding confident\npseudo-annotations and rebalancing the annotation distribution. Specifically,\nwe propose one distribution aware confidence measure to select confident\npseudo-annotations, which adopts the resampling strategy to oversample the\nminority annotations and undersample the majority annotations. On one\nreal-world crowdsourcing image classification task, we show that the proposed\nmethod yields more balanced annotations throughout training than the\ndistribution agnostic methods and substantially improves the learning\nperformance at different annotation sparsity levels.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.02425,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"GradDiv: Adversarial Robustness of Randomized Neural Networks via\n  Gradient Diversity Regularization\n\n  Deep learning is vulnerable to adversarial examples. Many defenses based on\nrandomized neural networks have been proposed to solve the problem, but fail to\nachieve robustness against attacks using proxy gradients such as the\nExpectation over Transformation (EOT) attack. We investigate the effect of the\nadversarial attacks using proxy gradients on randomized neural networks and\ndemonstrate that it highly relies on the directional distribution of the loss\ngradients of the randomized neural network. We show in particular that proxy\ngradients are less effective when the gradients are more scattered. To this\nend, we propose Gradient Diversity (GradDiv) regularizations that minimize the\nconcentration of the gradients to build a robust randomized neural network. Our\nexperiments on MNIST, CIFAR10, and STL10 show that our proposed GradDiv\nregularizations improve the adversarial robustness of randomized neural\nnetworks against a variety of state-of-the-art attack methods. Moreover, our\nmethod efficiently reduces the transferability among sample models of\nrandomized neural networks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.09786,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"Communication and Computation Reduction for Split Learning using\n  Asynchronous Training\n\n  Split learning is a promising privacy-preserving distributed learning scheme\nthat has low computation requirement at the edge device but has the\ndisadvantage of high communication overhead between edge device and server. To\nreduce the communication overhead, this paper proposes a loss-based\nasynchronous training scheme that updates the client-side model less frequently\nand only sends\/receives activations\/gradients in selected epochs. To further\nreduce the communication overhead, the activations\/gradients are quantized\nusing 8-bit floating point prior to transmission. An added benefit of the\nproposed communication reduction method is that the computations at the client\nside are reduced due to reduction in the number of client model updates.\nFurthermore, the privacy of the proposed communication reduction based split\nlearning method is almost the same as traditional split learning. Simulation\nresults on VGG11, VGG13 and ResNet18 models on CIFAR-10 show that the\ncommunication cost is reduced by 1.64x-106.7x and the computations in the\nclient are reduced by 2.86x-32.1x when the accuracy degradation is less than\n0.5% for the single-client case. For 5 and 10-client cases, the communication\ncost reduction is 11.9x and 11.3x on VGG11 for 0.5% loss in accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.01296,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Subspace Clustering Based Analysis of Neural Networks\n\n  Tools to analyze the latent space of deep neural networks provide a step\ntowards better understanding them. In this work, we motivate sparse subspace\nclustering (SSC) with an aim to learn affinity graphs from the latent structure\nof a given neural network layer trained over a set of inputs. We then use tools\nfrom Community Detection to quantify structures present in the input. These\nexperiments reveal that as we go deeper in a network, inputs tend to have an\nincreasing affinity to other inputs of the same class. Subsequently, we utilise\nmatrix similarity measures to perform layer-wise comparisons between affinity\ngraphs. In doing so we first demonstrate that when comparing a given layer\ncurrently under training to its final state, the shallower the layer of the\nnetwork, the quicker it is to converge than the deeper layers. When performing\na pairwise analysis of the entire network architecture, we observe that, as the\nnetwork increases in size, it reorganises from a state where each layer is\nmoderately similar to its neighbours, to a state where layers within a block\nhave high similarity than to layers in other blocks. Finally, we analyze the\nlearned affinity graphs of the final convolutional layer of the network and\ndemonstrate how an input's local neighbourhood affects its classification by\nthe network.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.04163,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000319547,
      "text":"Towards Robust Active Feature Acquisition\n\n  Truly intelligent systems are expected to make critical decisions with\nincomplete and uncertain data. Active feature acquisition (AFA), where features\nare sequentially acquired to improve the prediction, is a step towards this\ngoal. However, current AFA models all deal with a small set of candidate\nfeatures and have difficulty scaling to a large feature space. Moreover, they\nare ignorant about the valid domains where they can predict confidently, thus\nthey can be vulnerable to out-of-distribution (OOD) inputs. In order to remedy\nthese deficiencies and bring AFA models closer to practical use, we propose\nseveral techniques to advance the current AFA approaches. Our framework can\neasily handle a large number of features using a hierarchical acquisition\npolicy and is more robust to OOD inputs with the help of an OOD detector for\npartially observed data. Extensive experiments demonstrate the efficacy of our\nframework over strong baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.06665,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Disparity Between Batches as a Signal for Early Stopping\n\n  We propose a metric for evaluating the generalization ability of deep neural\nnetworks trained with mini-batch gradient descent. Our metric, called gradient\ndisparity, is the $\\ell_2$ norm distance between the gradient vectors of two\nmini-batches drawn from the training set. It is derived from a probabilistic\nupper bound on the difference between the classification errors over a given\nmini-batch, when the network is trained on this mini-batch and when the network\nis trained on another mini-batch of points sampled from the same dataset. We\nempirically show that gradient disparity is a very promising early-stopping\ncriterion (i) when data is limited, as it uses all the samples for training and\n(ii) when available data has noisy labels, as it signals overfitting better\nthan the validation data. Furthermore, we show in a wide range of experimental\nsettings that gradient disparity is strongly related to the generalization\nerror between the training and test sets, and that it is also very informative\nabout the level of label noise.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.01757,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000035101,
      "text":"The Least Restriction for Offline Reinforcement Learning\n\n  Many practical applications of reinforcement learning (RL) constrain the\nagent to learn from a fixed offline dataset of logged interactions, which has\nalready been gathered, without offering further possibility for data\ncollection. However, commonly used off-policy RL algorithms, such as the Deep Q\nNetwork and the Deep Deterministic Policy Gradient, are incapable of learning\nwithout data correlated to the distribution under the current policy, making\nthem ineffective for this offline setting. As the first step towards useful\noffline RL algorithms, we analysis the reason of instability in standard\noff-policy RL algorithms. It is due to the bootstrapping error. The key to\navoiding this error, is ensuring that the agent's action space does not go out\nof the fixed offline dataset. Based on our consideration, a creative offline RL\nframework, the Least Restriction (LR), is proposed in this paper. The LR\nregards selecting an action as taking a sample from the probability\ndistribution. It merely set a little limit for action selection, which not only\navoid the action being out of the offline dataset but also remove all the\nunreasonable restrictions in earlier approaches (e.g. Batch-Constrained Deep\nQ-Learning). In the further, we will demonstrate that the LR, is able to learn\nrobustly from different offline datasets, including random and suboptimal\ndemonstrations, on a range of practical control tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.11625,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000038412,
      "text":"Discrete Denoising Flows\n\n  Discrete flow-based models are a recently proposed class of generative models\nthat learn invertible transformations for discrete random variables. Since they\ndo not require data dequantization and maximize an exact likelihood objective,\nthey can be used in a straight-forward manner for lossless compression. In this\npaper, we introduce a new discrete flow-based model for categorical random\nvariables: Discrete Denoising Flows (DDFs). In contrast with other discrete\nflow-based models, our model can be locally trained without introducing\ngradient bias. We show that DDFs outperform Discrete Flows on modeling a toy\nexample, binary MNIST and Cityscapes segmentation maps, measured in\nlog-likelihood.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.13171,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Learning with Multiclass AUC: Theory and Algorithms\n\n  The Area under the ROC curve (AUC) is a well-known ranking metric for\nproblems such as imbalanced learning and recommender systems. The vast majority\nof existing AUC-optimization-based machine learning methods only focus on\nbinary-class cases, while leaving the multiclass cases unconsidered. In this\npaper, we start an early trial to consider the problem of learning multiclass\nscoring functions via optimizing multiclass AUC metrics. Our foundation is\nbased on the M metric, which is a well-known multiclass extension of AUC. We\nfirst pay a revisit to this metric, showing that it could eliminate the\nimbalance issue from the minority class pairs. Motivated by this, we propose an\nempirical surrogate risk minimization framework to approximately optimize the M\nmetric. Theoretically, we show that: (i) optimizing most of the popular\ndifferentiable surrogate losses suffices to reach the Bayes optimal scoring\nfunction asymptotically; (ii) the training framework enjoys an imbalance-aware\ngeneralization error bound, which pays more attention to the bottleneck samples\nof minority classes compared with the traditional $O(\\sqrt{1\/N})$ result.\nPractically, to deal with the low scalability of the computational operations,\nwe propose acceleration methods for three popular surrogate loss functions,\nincluding the exponential loss, squared loss, and hinge loss, to speed up loss\nand gradient evaluations. Finally, experimental results on 11 real-world\ndatasets demonstrate the effectiveness of our proposed framework.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.10804,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000020199,
      "text":"Active Learning in Incomplete Label Multiple Instance Multiple Label\n  Learning\n\n  In multiple instance multiple label learning, each sample, a bag, consists of\nmultiple instances. To alleviate labeling complexity, each sample is associated\nwith a set of bag-level labels leaving instances within the bag unlabeled. This\nsetting is more convenient and natural for representing complicated objects,\nwhich have multiple semantic meanings. Compared to single instance labeling,\nthis approach allows for labeling larger datasets at an equivalent labeling\ncost. However, for sufficiently large datasets, labeling all bags may become\nprohibitively costly. Active learning uses an iterative labeling and retraining\napproach aiming to provide reasonable classification performance using a small\nnumber of labeled samples. To our knowledge, only a few works in the area of\nactive learning in the MIML setting are available. These approaches can provide\npractical solutions to reduce labeling cost but their efficacy remains unclear.\nIn this paper, we propose a novel bag-class pair based approach for active\nlearning in the MIML setting. Due to the partial availability of bag-level\nlabels, we focus on the incomplete-label MIML setting for the proposed active\nlearning approach. Our approach is based on a discriminative graphical model\nwith efficient and exact inference. For the query process, we adapt active\nlearning criteria to the novel bag-class pair selection strategy. Additionally,\nwe introduce an online stochastic gradient descent algorithm to provide an\nefficient model update after each query. Numerical experiments on benchmark\ndatasets illustrate the robustness of the proposed approach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.13653,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000103646,
      "text":"Demand Forecasting in Smart Grid Using Long Short-Term Memory\n\n  Demand forecasting in power sector has become an important part of modern\ndemand management and response systems with the rise of smart metering enabled\ngrids. Long Short-Term Memory (LSTM) shows promising results in predicting time\nseries data which can also be applied to power load demand in smart grids. In\nthis paper, an LSTM based model using neural network architecture is proposed\nto forecast power demand. The model is trained with hourly energy and power\nusage data of four years from a smart grid. After training and prediction, the\naccuracy of the model is compared against the traditional statistical time\nseries analysis algorithms, such as Auto-Regressive (AR), to determine the\nefficiency. The mean absolute percentile error is found to be 1.22 in the\nproposed LSTM model, which is the lowest among the other models. From the\nfindings, it is clear that the inclusion of neural network in predicting power\ndemand reduces the error of prediction significantly. Thus, the application of\nLSTM can enable a more efficient demand response system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.11921,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000131461,
      "text":"Compensation Learning\n\n  Weighting strategy prevails in machine learning. For example, a common\napproach in robust machine learning is to exert lower weights on samples which\nare likely to be noisy or quite hard. This study reveals another undiscovered\nstrategy, namely, compensating. Various incarnations of compensating have been\nutilized but it has not been explicitly revealed. Learning with compensating is\ncalled compensation learning and a systematic taxonomy is constructed for it in\nthis study. In our taxonomy, compensation learning is divided on the basis of\nthe compensation targets, directions, inference manners, and granularity\nlevels. Many existing learning algorithms including some classical ones can be\nviewed or understood at least partially as compensation techniques.\nFurthermore, a family of new learning algorithms can be obtained by plugging\nthe compensation learning into existing learning algorithms. Specifically, two\nconcrete new learning algorithms are proposed for robust machine learning.\nExtensive experiments on image classification and text sentiment analysis\nverify the effectiveness of the two new algorithms. Compensation learning can\nalso be used in other various learning scenarios, such as imbalance learning,\nclustering, regression, and so on.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.06755,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"DIT4BEARs Smart Roads Internship\n\n  The research internship at UiT - The Arctic University of Norway was offered\nfor our team being the winner of the 'Smart Roads - Winter Road Maintenance\n2021' Hackathon. The internship commenced on 3 May 2021 and ended on 21 May\n2021 with meetings happening twice each week. In spite of having different\nnationalities and educational backgrounds, we both interns tried to collaborate\nas a team as much as possible. The most alluring part was working on this\nproject made us realize the critical conditions faced by the arctic people,\nwhere it was hard to gain such a unique experience from our residence. We\ndeveloped and implemented several deep learning models to classify the states\n(dry, moist, wet, icy, snowy, slushy). Depending upon the best model, the\nweather forecast app will predict the state taking the Ta, Tsurf, Height,\nSpeed, Water, etc. into consideration. The crucial part was to define a safety\nmetric which is the product of the accident rates based on friction and the\naccident rates based on states. We developed a regressor that will predict the\nsafety metric depending upon the state obtained from the classifier and the\nfriction obtained from the sensor data. A pathfinding algorithm has been\ndesigned using the sensor data, open street map data, weather data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.14171,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000163582,
      "text":"Tianshou: a Highly Modularized Deep Reinforcement Learning Library\n\n  In this paper, we present Tianshou, a highly modularized Python library for\ndeep reinforcement learning (DRL) that uses PyTorch as its backend. Tianshou\nintends to be research-friendly by providing a flexible and reliable\ninfrastructure of DRL algorithms. It supports online and offline training with\nmore than 20 classic algorithms through a unified interface. To facilitate\nrelated research and prove Tianshou's reliability, we have released Tianshou's\nbenchmark of MuJoCo environments, covering eight classic algorithms with\nstate-of-the-art performance. We open-sourced Tianshou at\nhttps:\/\/github.com\/thu-ml\/tianshou\/.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.0663,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000039736,
      "text":"Online Evaluation Methods for the Causal Effect of Recommendations\n\n  Evaluating the causal effect of recommendations is an important objective\nbecause the causal effect on user interactions can directly leads to an\nincrease in sales and user engagement. To select an optimal recommendation\nmodel, it is common to conduct A\/B testing to compare model performance.\nHowever, A\/B testing of causal effects requires a large number of users, making\nsuch experiments costly and risky. We therefore propose the first interleaving\nmethods that can efficiently compare recommendation models in terms of causal\neffects. In contrast to conventional interleaving methods, we measure the\noutcomes of both items on an interleaved list and items not on the interleaved\nlist, since the causal effect is the difference between outcomes with and\nwithout recommendations. To ensure that the evaluations are unbiased, we either\nselect items with equal probability or weight the outcomes using inverse\npropensity scores. We then verify the unbiasedness and efficiency of online\nevaluation methods through simulated online experiments. The results indicate\nthat our proposed methods are unbiased and that they have superior efficiency\nto A\/B testing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.01689,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Restless and Uncertain: Robust Policies for Restless Bandits via Deep\n  Multi-Agent Reinforcement Learning\n\n  We introduce robustness in \\textit{restless multi-armed bandits} (RMABs), a\npopular model for constrained resource allocation among independent stochastic\nprocesses (arms). Nearly all RMAB techniques assume stochastic dynamics are\nprecisely known. However, in many real-world settings, dynamics are estimated\nwith significant \\emph{uncertainty}, e.g., via historical data, which can lead\nto bad outcomes if ignored. To address this, we develop an algorithm to compute\nminimax regret -- robust policies for RMABs. Our approach uses a double oracle\nframework (oracles for \\textit{agent} and \\textit{nature}), which is often used\nfor single-process robust planning but requires significant new techniques to\naccommodate the combinatorial nature of RMABs. Specifically, we design a deep\nreinforcement learning (RL) algorithm, DDLPO, which tackles the combinatorial\nchallenge by learning an auxiliary \"$\\lambda$-network\" in tandem with policy\nnetworks per arm, greatly reducing sample complexity, with guarantees on\nconvergence. DDLPO, of general interest, implements our reward-maximizing agent\noracle. We then tackle the challenging regret-maximizing nature oracle, a\nnon-stationary RL challenge, by formulating it as a multi-agent RL problem\nbetween a policy optimizer and adversarial nature. This formulation is of\ngeneral interest -- we solve it for RMABs by creating a multi-agent extension\nof DDLPO with a shared critic. We show our approaches work well in three\nexperimental domains.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.06908,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000027816,
      "text":"Understanding Failures in Out-of-Distribution Detection with Deep\n  Generative Models\n\n  Deep generative models (DGMs) seem a natural fit for detecting\nout-of-distribution (OOD) inputs, but such models have been shown to assign\nhigher probabilities or densities to OOD images than images from the training\ndistribution. In this work, we explain why this behavior should be attributed\nto model misestimation. We first prove that no method can guarantee performance\nbeyond random chance without assumptions on which out-distributions are\nrelevant. We then interrogate the typical set hypothesis, the claim that\nrelevant out-distributions can lie in high likelihood regions of the data\ndistribution, and that OOD detection should be defined based on the data\ndistribution's typical set. We highlight the consequences implied by assuming\nsupport overlap between in- and out-distributions, as well as the arbitrariness\nof the typical set for OOD detection. Our results suggest that estimation error\nis a more plausible explanation than the misalignment between likelihood-based\nOOD detection and out-distributions of interest, and we illustrate how even\nminimal estimation error can lead to OOD detection failures, yielding\nimplications for future work in deep generative modeling and OOD detection.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.13191,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Neural Network Approximation of Refinable Functions\n\n  In the desire to quantify the success of neural networks in deep learning and\nother applications, there is a great interest in understanding which functions\nare efficiently approximated by the outputs of neural networks. By now, there\nexists a variety of results which show that a wide range of functions can be\napproximated with sometimes surprising accuracy by these outputs. For example,\nit is known that the set of functions that can be approximated with exponential\naccuracy (in terms of the number of parameters used) includes, on one hand,\nvery smooth functions such as polynomials and analytic functions (see e.g.\n\\cite{E,S,Y}) and, on the other hand, very rough functions such as the\nWeierstrass function (see e.g. \\cite{EPGB,DDFHP}), which is nowhere\ndifferentiable. In this paper, we add to the latter class of rough functions by\nshowing that it also includes refinable functions. Namely, we show that\nrefinable functions are approximated by the outputs of deep ReLU networks with\na fixed width and increasing depth with accuracy exponential in terms of their\nnumber of parameters. Our results apply to functions used in the standard\nconstruction of wavelets as well as to functions constructed via subdivision\nalgorithms in Computer Aided Geometric Design.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.14194,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"On the combined effect of class imbalance and concept complexity in deep\n  learning\n\n  Structural concept complexity, class overlap, and data scarcity are some of\nthe most important factors influencing the performance of classifiers under\nclass imbalance conditions. When these effects were uncovered in the early\n2000s, understandably, the classifiers on which they were demonstrated belonged\nto the classical rather than Deep Learning categories of approaches. As Deep\nLearning is gaining ground over classical machine learning and is beginning to\nbe used in critical applied settings, it is important to assess systematically\nhow well they respond to the kind of challenges their classical counterparts\nhave struggled with in the past two decades. The purpose of this paper is to\nstudy the behavior of deep learning systems in settings that have previously\nbeen deemed challenging to classical machine learning systems to find out\nwhether the depth of the systems is an asset in such settings. The results in\nboth artificial and real-world image datasets (MNIST Fashion, CIFAR-10) show\nthat these settings remain mostly challenging for Deep Learning systems and\nthat deeper architectures seem to help with structural concept complexity but\nnot with overlap challenges in simple artificial domains. Data scarcity is not\novercome by deeper layers, either. In the real-world image domains, where\noverfitting is a greater concern than in the artificial domains, the advantage\nof deeper architectures is less obvious: while it is observed in certain cases,\nit is quickly cancelled as models get deeper and perform worse than their\nshallower counterparts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.05338,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000024835,
      "text":"Truncated Emphatic Temporal Difference Methods for Prediction and\n  Control\n\n  Emphatic Temporal Difference (TD) methods are a class of off-policy\nReinforcement Learning (RL) methods involving the use of followon traces.\nDespite the theoretical success of emphatic TD methods in addressing the\nnotorious deadly triad of off-policy RL, there are still two open problems.\nFirst, followon traces typically suffer from large variance, making them hard\nto use in practice. Second, though Yu (2015) confirms the asymptotic\nconvergence of some emphatic TD methods for prediction problems, there is still\nno finite sample analysis for any emphatic TD method for prediction, much less\ncontrol. In this paper, we address those two open problems simultaneously via\nusing truncated followon traces in emphatic TD methods. Unlike the original\nfollowon traces, which depend on all previous history, truncated followon\ntraces depend on only finite history, reducing variance and enabling the finite\nsample analysis of our proposed emphatic TD methods for both prediction and\ncontrol.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.07396,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000014239,
      "text":"Diagnosis of Acute Myeloid Leukaemia Using Machine Learning\n\n  We train a machine learning model on a dataset of 2177 individuals using as\nfeatures 26 probe sets and their age in order to classify if someone has acute\nmyeloid leukaemia or is healthy. The dataset is multicentric and consists of\ndata from 27 organisations, 25 cities, 15 countries and 4 continents. The\naccuracy or our model is 99.94\\% and its F1-score 0.9996. To the best of our\nknowledge the performance of our model is the best one in the literature, as\nregards the prediction of AML using similar or not data. Moreover, there has\nnot been any bibliographic reference associated with acute myeloid leukaemia\nfor the 26 probe sets we used as features in our model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.0638,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000062585,
      "text":"Detecting OODs as datapoints with High Uncertainty\n\n  Deep neural networks (DNNs) are known to produce incorrect predictions with\nvery high confidence on out-of-distribution inputs (OODs). This limitation is\none of the key challenges in the adoption of DNNs in high-assurance systems\nsuch as autonomous driving, air traffic management, and medical diagnosis. This\nchallenge has received significant attention recently, and several techniques\nhave been developed to detect inputs where the model's prediction cannot be\ntrusted. These techniques detect OODs as datapoints with either high epistemic\nuncertainty or high aleatoric uncertainty. We demonstrate the difference in the\ndetection ability of these techniques and propose an ensemble approach for\ndetection of OODs as datapoints with high uncertainty (epistemic or aleatoric).\nWe perform experiments on vision datasets with multiple DNN architectures,\nachieving state-of-the-art results in most cases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.06504,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"LinkTeller: Recovering Private Edges from Graph Neural Networks via\n  Influence Analysis\n\n  Graph structured data have enabled several successful applications such as\nrecommendation systems and traffic prediction, given the rich node features and\nedges information. However, these high-dimensional features and high-order\nadjacency information are usually heterogeneous and held by different data\nholders in practice. Given such vertical data partition (e.g., one data holder\nwill only own either the node features or edge information), different data\nholders have to develop efficient joint training protocols rather than directly\ntransfer data to each other due to privacy concerns. In this paper, we focus on\nthe edge privacy, and consider a training scenario where Bob with node features\nwill first send training node features to Alice who owns the adjacency\ninformation. Alice will then train a graph neural network (GNN) with the joint\ninformation and release an inference API. During inference, Bob is able to\nprovide test node features and query the API to obtain the predictions for test\nnodes. Under this setting, we first propose a privacy attack LinkTeller via\ninfluence analysis to infer the private edge information held by Alice via\ndesigning adversarial queries for Bob. We then empirically show that LinkTeller\nis able to recover a significant amount of private edges, outperforming\nexisting baselines. To further evaluate the privacy leakage, we adapt an\nexisting algorithm for differentially private graph convolutional network (DP\nGCN) training and propose a new DP GCN mechanism LapGraph. We show that these\nDP GCN mechanisms are not always resilient against LinkTeller empirically under\nmild privacy guarantees ($\\varepsilon>5$). Our studies will shed light on\nfuture research towards designing more resilient privacy-preserving GCN models;\nin the meantime, provide an in-depth understanding of the tradeoff between GCN\nmodel utility and robustness against potential privacy attacks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.03449,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000068214,
      "text":"Self-learning sparse PCA for multimode process monitoring\n\n  This paper proposes a novel sparse principal component analysis algorithm\nwith self-learning ability for successive modes, where synaptic intelligence is\nemployed to measure the importance of variables and a regularization term is\nadded to preserve the learned knowledge of previous modes. Different from\ntraditional multimode monitoring methods, the monitoring model is updated based\non the current model and new data when a new mode arrives, thus delivering\nprominent performance for sequential modes. Besides, the computation and\nstorage resources are saved in the long run, because it is not necessary to\nretrain the model from scratch frequently and store data from previous modes.\nMore importantly, the model furnishes excellent interpretability owing to the\nsparsity of parameters. Finally, a numerical case and a practical pulverizing\nsystem are adopted to illustrate the effectiveness of the proposed algorithm.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.02834,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"Efficient recurrent neural network methods for anomalously diffusing\n  single particle short and noisy trajectories\n\n  Anomalous diffusion occurs at very different scales in nature, from atomic\nsystems to motions in cell organelles, biological tissues or ecology, and also\nin artificial materials, such as cement. Being able to accurately measure the\nanomalous exponent associated with a given particle trajectory, thus\ndetermining whether the particle subdiffuses, superdiffuses or performs normal\ndiffusion is of key importance to understand the diffusion process. Also, it is\noften important to trustingly identify the model behind the trajectory, as this\ngives a large amount of information on the system dynamics. Both aspects are\nparticularly difficult when the input data are short and noisy trajectories. It\nis even more difficult if one cannot guarantee that the trajectories output in\nexperiments is homogeneous, hindering the statistical methods based on\nensembles of trajectories. We present a data-driven method able to infer the\nanomalous exponent and to identify the type of anomalous diffusion process\nbehind single, noisy and short trajectories, with good accuracy. This model was\nused in our participation in the Anomalous Diffusion (AnDi) Challenge. A\ncombination of convolutional and recurrent neural networks were used to achieve\nstate-of-the-art results when compared to methods participating in the AnDi\nChallenge, ranking top 4 in both classification and diffusion exponent\nregression.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.05233,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000014239,
      "text":"EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks\n\n  Graph Neural Networks (GNNs) have shown superior performance in analyzing\nattributed networks in various web-based applications such as social\nrecommendation and web search. Nevertheless, in high-stake decision-making\nscenarios such as online fraud detection, there is an increasing societal\nconcern that GNNs could make discriminatory decisions towards certain\ndemographic groups. Despite recent explorations on fair GNNs, these works are\ntailored for a specific GNN model. However, myriads of GNN variants have been\nproposed for different applications, and it is costly to fine-tune existing\ndebiasing algorithms for each specific GNN architecture. Different from\nexisting works that debias GNN models, we aim to debias the input attributed\nnetwork to achieve fairer GNNs through feeding GNNs with less biased data.\nSpecifically, we propose novel definitions and metrics to measure the bias in\nan attributed network, which leads to the optimization objective to mitigate\nbias. We then develop a framework EDITS to mitigate the bias in attributed\nnetworks while maintaining the performance of GNNs in downstream tasks. EDITS\nworks in a model-agnostic manner, i.e., it is independent of any specific GNN.\nExperiments demonstrate the validity of the proposed bias metrics and the\nsuperiority of EDITS on both bias mitigation and utility maintenance.\nOpen-source implementation: https:\/\/github.com\/yushundong\/EDITS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.02551,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"Ensemble Consensus-based Representation Deep Reinforcement Learning for\n  Hybrid FSO\/RF Communication Systems\n\n  Hybrid FSO\/RF system requires an efficient FSO and RF link switching\nmechanism to improve the system capacity by realizing the complementary\nbenefits of both the links. The dynamics of network conditions, such as fog,\ndust, and sand storms compound the link switching problem and control\ncomplexity. To address this problem, we initiate the study of deep\nreinforcement learning (DRL) for link switching of hybrid FSO\/RF systems.\nSpecifically, in this work, we focus on actor-critic called Actor\/Critic-FSO\/RF\nand Deep-Q network (DQN) called DQN-FSO\/RF for FSO\/RF link switching under\natmospheric turbulences. To formulate the problem, we define the state, action,\nand reward function of a hybrid FSO\/RF system. DQN-FSO\/RF frequently updates\nthe deployed policy that interacts with the environment in a hybrid FSO\/RF\nsystem, resulting in high switching costs. To overcome this, we lift this\nproblem to ensemble consensus-based representation learning for deep\nreinforcement called DQNEnsemble-FSO\/RF. The proposed novel DQNEnsemble-FSO\/RF\nDRL approach uses consensus learned features representations based on an\nensemble of asynchronous threads to update the deployed policy. Experimental\nresults corroborate that the proposed DQNEnsemble-FSO\/RF's consensus-learned\nfeatures switching achieves better performance than Actor\/Critic-FSO\/RF,\nDQN-FSO\/RF, and MyOpic for FSO\/RF link switching while keeping the switching\ncost significantly low.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.04126,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Improved Feature Importance Computations for Tree Models: Shapley vs.\n  Banzhaf\n\n  Shapley values are one of the main tools used to explain predictions of tree\nensemble models. The main alternative to Shapley values are Banzhaf values that\nhave not been understood equally well. In this paper we make a step towards\nfilling this gap, providing both experimental and theoretical comparison of\nthese model explanation methods. Surprisingly, we show that Banzhaf values\noffer several advantages over Shapley values while providing essentially the\nsame explanations. We verify that Banzhaf values: (1) have a more intuitive\ninterpretation, (2) allow for more efficient algorithms, and (3) are much more\nnumerically robust. We provide an experimental evaluation of these theses. In\nparticular, we show that on real world instances.\n  Additionally, from a theoretical perspective we provide new and improved\nalgorithm computing the same Shapley value based explanations as the algorithm\nof Lundberg et al. [Nat. Mach. Intell. 2020]. Our algorithm runs in $O(TLD+n)$\ntime, whereas the previous algorithm had $O(TLD^2+n)$ running time bound. Here,\n$T$ is the number of trees, $L$ is the maximum number of leaves in a tree, and\n$D$ denotes the maximum depth of a tree in the ensemble. Using the\ncomputational techniques developed for Shapley values we deliver an optimal\n$O(TL+n)$ time algorithm for computing Banzhaf values based explanations. In\nour experiments these algorithms give running times smaller even by an order of\nmagnitude.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.02479,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000047684,
      "text":"HyperJump: Accelerating HyperBand via Risk Modelling\n\n  In the literature on hyper-parameter tuning, a number of recent solutions\nrely on low-fidelity observations (e.g., training with sub-sampled datasets) in\norder to efficiently identify promising configurations to be then tested via\nhigh-fidelity observations (e.g., using the full dataset). Among these,\nHyperBand is arguably one of the most popular solutions, due to its efficiency\nand theoretically provable robustness. In this work, we introduce HyperJump, a\nnew approach that builds on HyperBand's robust search strategy and complements\nit with novel model-based risk analysis techniques that accelerate the search\nby skipping the evaluation of low risk configurations, i.e., configurations\nthat are likely to be eventually discarded by HyperBand. We evaluate HyperJump\non a suite of hyper-parameter optimization problems and show that it provides\nover one-order of magnitude speed-ups, both in sequential and parallel\ndeployments, on a variety of deep-learning, kernel-based learning, and neural\narchitectural search problems when compared to HyperBand and to several\nstate-of-the-art optimizers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.11673,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Why Adversarial Reprogramming Works, When It Fails, and How to Tell the\n  Difference\n\n  Adversarial reprogramming allows repurposing a machine-learning model to\nperform a different task. For example, a model trained to recognize animals can\nbe reprogrammed to recognize digits by embedding an adversarial program in the\ndigit images provided as input. Recent work has shown that adversarial\nreprogramming may not only be used to abuse machine-learning models provided as\na service, but also beneficially, to improve transfer learning when training\ndata is scarce. However, the factors affecting its success are still largely\nunexplained. In this work, we develop a first-order linear model of adversarial\nreprogramming to show that its success inherently depends on the size of the\naverage input gradient, which grows when input gradients are more aligned, and\nwhen inputs have higher dimensionality. The results of our experimental\nanalysis, involving fourteen distinct reprogramming tasks, show that the above\nfactors are correlated with the success and the failure of adversarial\nreprogramming.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.12298,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Reinforcement Learning based Condition-oriented Maintenance Scheduling\n  for Flow Line Systems\n\n  Maintenance scheduling is a complex decision-making problem in the production\ndomain, where a number of maintenance tasks and resources has to be assigned\nand scheduled to production entities in order to prevent unplanned production\ndowntime. Intelligent maintenance strategies are required that are able to\nadapt to the dynamics and different conditions of production systems. The paper\nintroduces a deep reinforcement learning approach for condition-oriented\nmaintenance scheduling in flow line systems. Different policies are learned,\nanalyzed and evaluated against a benchmark scheduling heuristic based on reward\nmodelling. The evaluation of the learned policies shows that reinforcement\nlearning based maintenance strategies meet the requirements of the presented\nuse case and are suitable for maintenance scheduling in the shop floor.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.0887,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Topo2vec: Topography Embedding Using the Fractal Effect\n\n  Recent advances in deep learning have transformed many fields by introducing\ngeneric embedding spaces, capable of achieving great predictive performance\nwith minimal labeling effort. The geology field has not yet met such success.\nIn this work, we introduce an extension for self-supervised learning techniques\ntailored for exploiting the fractal-effect in remote-sensing images. The\nfractal-effect assumes that the same structures (for example rivers, peaks and\nsaddles) will appear in all scales. We demonstrate our method's effectiveness\non elevation data, we also use the effect in inference. We perform an extensive\nanalysis on several classification tasks and emphasize its effectiveness in\ndetecting the same class on different scales. To the best of our knowledge, it\nis the first attempt to build a generic representation for topographic images.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.13581,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000070863,
      "text":"DoGR: Disaggregated Gaussian Regression for Reproducible Analysis of\n  Heterogeneous Data\n\n  Quantitative analysis of large-scale data is often complicated by the\npresence of diverse subgroups, which reduce the accuracy of inferences they\nmake on held-out data. To address the challenge of heterogeneous data analysis,\nwe introduce DoGR, a method that discovers latent confounders by simultaneously\npartitioning the data into overlapping clusters (disaggregation) and modeling\nthe behavior within them (regression). When applied to real-world data, our\nmethod discovers meaningful clusters and their characteristic behaviors, thus\ngiving insight into group differences and their impact on the outcome of\ninterest. By accounting for latent confounders, our framework facilitates\nexploratory analysis of noisy, heterogeneous data and can be used to learn\npredictive models that better generalize to new data. We provide the code to\nenable others to use DoGR within their data analytic workflows.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.07743,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Incremental cluster validity index-guided online learning for\n  performance and robustness to presentation order\n\n  In streaming data applications incoming samples are processed and discarded,\ntherefore, intelligent decision-making is crucial for the performance of\nlifelong learning systems. In addition, the order in which samples arrive may\nheavily affect the performance of online (and offline) incremental learners.\nThe recently introduced incremental cluster validity indices (iCVIs) provide\nvaluable aid in addressing such class of problems. Their primary use-case has\nbeen cluster quality monitoring; nonetheless, they have been very recently\nintegrated in a streaming clustering method to assist the clustering task\nitself. In this context, the work presented here introduces the first adaptive\nresonance theory (ART)-based model that uses iCVIs for unsupervised and\nsemi-supervised online learning. Moreover, it shows for the first time how to\nuse iCVIs to regulate ART vigilance via an iCVI-based match tracking mechanism.\nThe model achieves improved accuracy and robustness to ordering effects by\nintegrating an online iCVI framework as module B of a topological adaptive\nresonance theory predictive mapping (TopoARTMAP) -- thereby being named\niCVI-TopoARTMAP -- and by employing iCVI-driven post-processing heuristics at\nthe end of each learning step. The online iCVI framework provides assignments\nof input samples to clusters at each iteration in accordance to any of several\niCVIs. The iCVI-TopoARTMAP maintains useful properties shared by ARTMAP models,\nsuch as stability, immunity to catastrophic forgetting, and the many-to-one\nmapping capability via the map field module. The performance (unsupervised and\nsemi-supervised) and robustness to presentation order (unsupervised) of\niCVI-TopoARTMAP were evaluated via experiments with a synthetic data set and\ndeep embeddings of a real-world face image data set.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.09976,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Revealing the Distributional Vulnerability of Discriminators by Implicit\n  Generators\n\n  In deep neural learning, a discriminator trained on in-distribution (ID)\nsamples may make high-confidence predictions on out-of-distribution (OOD)\nsamples. This triggers a significant matter for robust, trustworthy and safe\ndeep learning. The issue is primarily caused by the limited ID samples\nobservable in training the discriminator when OOD samples are unavailable. We\npropose a general approach for \\textit{fine-tuning discriminators by implicit\ngenerators} (FIG). FIG is grounded on information theory and applicable to\nstandard discriminators without retraining. It improves the ability of a\nstandard discriminator in distinguishing ID and OOD samples by generating and\npenalizing its specific OOD samples. According to the Shannon entropy, an\nenergy-based implicit generator is inferred from a discriminator without extra\ntraining costs. Then, a Langevin dynamic sampler draws specific OOD samples for\nthe implicit generator. Lastly, we design a regularizer fitting the design\nprinciple of the implicit generator to induce high entropy on those generated\nOOD samples. The experiments on different networks and datasets demonstrate\nthat FIG achieves the state-of-the-art OOD detection performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.0349,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000036756,
      "text":"Clustering Algorithms to Analyze the Road Traffic Crashes\n\n  Selecting an appropriate clustering method as well as an optimal number of\nclusters in road accident data is at times confusing and difficult. This paper\nanalyzes shortcomings of different existing techniques applied to cluster\naccident-prone areas and recommends using Density-Based Spatial Clustering of\nApplications with Noise (DBSCAN) and Ordering Points To Identify the Clustering\nStructure (OPTICS) to overcome them. Comparative performance analysis based on\nreal-life data on the recorded cases of road accidents in North Carolina also\nshow more effectiveness and efficiency achieved by these algorithms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.07247,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Robust Hierarchical Clustering for Directed Networks: An Axiomatic\n  Approach\n\n  We provide a complete taxonomic characterization of robust hierarchical\nclustering methods for directed networks following an axiomatic approach. We\nbegin by introducing three practical properties associated with the notion of\nrobustness in hierarchical clustering: linear scale preservation, stability,\nand excisiveness. Linear scale preservation enforces imperviousness to change\nin units of measure whereas stability ensures that a bounded perturbation in\nthe input network entails a bounded perturbation in the clustering output.\nExcisiveness refers to the local consistency of the clustering outcome.\nAlgorithmically, excisiveness implies that we can reduce computational\ncomplexity by only clustering a subset of our data while theoretically\nguaranteeing that the same hierarchical outcome would be observed when\nclustering the whole dataset. In parallel to these three properties, we\nintroduce the concept of representability, a generative model for describing\nclustering methods through the specification of their action on a collection of\nnetworks. Our main result is to leverage this generative model to give a\nprecise characterization of all robust -- i.e., excisive, linear scale\npreserving, and stable -- hierarchical clustering methods for directed\nnetworks. We also address the implementation of our methods and describe an\napplication to real data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.03214,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000102984,
      "text":"Simple Modifications to Improve Tabular Neural Networks\n\n  There is growing interest in neural network architectures for tabular data.\nMany general-purpose tabular deep learning models have been introduced\nrecently, with performance sometimes rivaling gradient boosted decision trees\n(GBDTs). These recent models draw inspiration from various sources, including\nGBDTs, factorization machines, and neural networks from other application\ndomains. Previous tabular neural networks are also drawn upon, but are possibly\nunder-considered, especially models associated with specific tabular problems.\nThis paper focuses on several such models, and proposes modifications for\nimproving their performance. When modified, these models are shown to be\ncompetitive with leading general-purpose tabular models, including GBDTs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.06758,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000014239,
      "text":"An Investigation of Replay-based Approaches for Continual Learning\n\n  Continual learning (CL) is a major challenge of machine learning (ML) and\ndescribes the ability to learn several tasks sequentially without catastrophic\nforgetting (CF). Recent works indicate that CL is a complex topic, even more so\nwhen real-world scenarios with multiple constraints are involved. Several\nsolution classes have been proposed, of which so-called replay-based approaches\nseem very promising due to their simplicity and robustness. Such approaches\nstore a subset of past samples in a dedicated memory for later processing:\nwhile this does not solve all problems, good results have been obtained. In\nthis article, we empirically investigate replay-based approaches of continual\nlearning and assess their potential for applications. Selected recent\napproaches as well as own proposals are compared on a common set of benchmarks,\nwith a particular focus on assessing the performance of different sample\nselection strategies. We find that the impact of sample selection increases\nwhen a smaller number of samples is stored. Nevertheless, performance varies\nstrongly between different replay approaches. Surprisingly, we find that the\nmost naive rehearsal-based approaches that we propose here can outperform\nrecent state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.04015,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000076824,
      "text":"Generation, augmentation, and alignment: A pseudo-source domain based\n  method for source-free domain adaptation\n\n  Conventional unsupervised domain adaptation (UDA) methods need to access both\nlabeled source samples and unlabeled target samples simultaneously to train the\nmodel. While in some scenarios, the source samples are not available for the\ntarget domain due to data privacy and safety. To overcome this challenge,\nrecently, source-free domain adaptation (SFDA) has attracted the attention of\nresearchers, where both a trained source model and unlabeled target samples are\ngiven. Existing SFDA methods either adopt a pseudo-label based strategy or\ngenerate more samples. However, these methods do not explicitly reduce the\ndistribution shift across domains, which is the key to a good adaptation.\nAlthough there are no source samples available, fortunately, we find that some\ntarget samples are very similar to the source domain and can be used to\napproximate the source domain. This approximated domain is denoted as the\npseudo-source domain. In this paper, inspired by this observation, we propose a\nnovel method based on the pseudo-source domain. The proposed method firstly\ngenerates and augments the pseudo-source domain, and then employs distribution\nalignment with four novel losses based on pseudo-label based strategy. Among\nthem, a domain adversarial loss is introduced between the pseudo-source domain\nthe remaining target domain to reduce the distribution shift. The results on\nthree real-world datasets verify the effectiveness of the proposed method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.07815,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000041061,
      "text":"Probability-driven scoring functions in combining linear classifiers\n\n  Although linear classifiers are one of the oldest methods in machine\nlearning, they are still very popular in the machine learning community. This\nis due to their low computational complexity and robustness to overfitting.\nConsequently, linear classifiers are often used as base classifiers of multiple\nensemble classification systems. This research is aimed at building a new\nfusion method dedicated to the ensemble of linear classifiers. The fusion\nscheme uses both measurement space and geometrical space. Namely, we proposed a\nprobability-driven scoring function which shape depends on the orientation of\nthe decision hyperplanes generated by the base classifiers. The proposed fusion\nmethod is compared with the reference method using multiple benchmark datasets\ntaken from the KEEL repository. The comparison is done using multiple quality\ncriteria. The statistical analysis of the obtained results is also performed.\nThe experimental study shows that, under certain conditions, some improvement\nmay be obtained.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.10797,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000047684,
      "text":"Improved Multi-label Classification with Frequent Label-set Mining and\n  Association\n\n  Multi-label (ML) data deals with multiple classes associated with individual\nsamples at the same time. This leads to the co-occurrence of several classes\nrepeatedly, which indicates some existing correlation among them. In this\narticle, the correlation among classes has been explored to improve the\nclassification performance of existing ML classifiers. A novel approach of\nfrequent label-set mining has been proposed to extract these correlated classes\nfrom the label-sets of the data. Both co-presence (CP) and co-absence (CA) of\nclasses have been taken into consideration. The rules mined from the ML data\nhas been further used to incorporate class correlation information into\nexisting ML classifiers. The soft scores generated by an ML classifier are\nmodified through a novel approach using the CP-CA rules. A concept of certain\nand uncertain scores has been defined here, where the proposed method aims to\nimprove the uncertain scores with the help of the certain scores and their\ncorresponding CP-CA rules. This has been experimentally analysed on ten ML\ndatasets for three ML existing classifiers which shows substantial improvement\nin their overall performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.10596,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000015563,
      "text":"Fully probabilistic design for knowledge fusion between Bayesian filters\n  under uniform disturbances\n\n  This paper considers the problem of Bayesian transfer learning-based\nknowledge fusion between linear state-space processes driven by uniform state\nand observation noise processes. The target task conditions on probabilistic\nstate predictor(s) supplied by the source filtering task(s) to improve its own\nstate estimate. A joint model of the target and source(s) is not required and\nis not elicited. The resulting decision-making problem for choosing the optimal\nconditional target filtering distribution under incomplete modelling is solved\nvia fully probabilistic design (FPD), i.e. via appropriate minimization of\nKullback-Leibler divergence (KLD). The resulting FPD-optimal target learner is\nrobust, in the sense that it can reject poor-quality source knowledge. In\naddition, the fact that this Bayesian transfer learning (BTL) scheme does not\ndepend on a model of interaction between the source and target tasks ensures\nrobustness to the misspecification of such a model. The latter is a problem\nthat affects conventional transfer learning methods. The properties of the\nproposed BTL scheme are demonstrated via extensive simulations, and in\ncomparison with two contemporary alternatives.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.14648,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000320541,
      "text":"A Study of Feature Selection and Extraction Algorithms for Cancer\n  Subtype Prediction\n\n  In this work, we study and analyze different feature selection algorithms\nthat can be used to classify cancer subtypes in case of highly varying\nhigh-dimensional data. We apply three different feature selection methods on\nfive different types of cancers having two separate omics each. We show that\nthe existing feature selection methods are computationally expensive when\napplied individually. Instead, we apply these algorithms sequentially which\nhelps in lowering the computational cost and improving the predictive\nperformance. We further show that reducing the number of features using some\ndimension reduction techniques can improve the performance of machine learning\nmodels in some cases. We support our findings through comprehensive data\nanalysis and visualization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.01538,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Investigate the Correlation of Breast Cancer Dataset using Different\n  Clustering Technique\n\n  The objectives of this paper are to explore ways to analyze breast cancer\ndataset in the context of unsupervised learning without prior training model.\nThe paper investigates different ways of clustering techniques as well as\npreprocessing. This in-depth analysis builds the footprint which can further\nuse for designing a most robust and accurate medical prognosis system. This\npaper also give emphasis on correlations of data points with different standard\nbenchmark techniques. Keywords: Breast cancer dataset, Clustering Technique\nHopkins Statistic, K-means Clustering, k-medoids or partitioning around medoids\n(PAM)\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.04566,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000162257,
      "text":"SanitAIs: Unsupervised Data Augmentation to Sanitize Trojaned Neural\n  Networks\n\n  Self-supervised learning (SSL) methods have resulted in broad improvements to\nneural network performance by leveraging large, untapped collections of\nunlabeled data to learn generalized underlying structure. In this work, we\nharness unsupervised data augmentation (UDA), an SSL technique, to mitigate\nbackdoor or Trojan attacks on deep neural networks. We show that UDA is more\neffective at removing trojans than current state-of-the-art methods for both\nfeature space and point triggers, over a range of model architectures, trojans,\nand data quantities provided for trojan removal. These results demonstrate that\nUDA is both an effective and practical approach to mitigating the effects of\nbackdoors on neural networks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.01394,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"Mulberry Leaf Yield Prediction Using Machine Learning Techniques\n\n  Soil nutrients are essential for the growth of healthy crops. India produces\na humungous quantity of Mulberry leaves which in turn produces the raw silk.\nSince the climatic conditions in India is favourable, Mulberry is grown\nthroughout the year. Majority of the farmers hardly pay attention to the nature\nof soil and abiotic factors due to which leaves become malnutritious and thus\nwhen they are consumed by the silkworm, desired quality end-product, raw silk,\nwill not be produced. It is beneficial for the farmers to know the amount of\nyield that their land can produce so that they can plan in advance. In this\npaper, different Machine Learning techniques are used in predicting the yield\nof the Mulberry crops based on the soil parameters. Three advanced\nmachine-learning models are selected and compared, namely, Multiple linear\nregression, Ridge regression and Random Forest Regression (RF). The\nexperimental results show that Random Forest Regression outperforms other\nalgorithms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.11043,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Learning Predictive and Interpretable Timeseries Summaries from ICU Data\n\n  Machine learning models that utilize patient data across time (rather than\njust the most recent measurements) have increased performance for many risk\nstratification tasks in the intensive care unit. However, many of these models\nand their learned representations are complex and therefore difficult for\nclinicians to interpret, creating challenges for validation. Our work proposes\na new procedure to learn summaries of clinical time-series that are both\npredictive and easily understood by humans. Specifically, our summaries consist\nof simple and intuitive functions of clinical data (e.g. falling mean arterial\npressure). Our learned summaries outperform traditional interpretable model\nclasses and achieve performance comparable to state-of-the-art deep learning\nmodels on an in-hospital mortality classification task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.05635,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Mixing between the Cross Entropy and the Expectation Loss Terms\n\n  The cross entropy loss is widely used due to its effectiveness and solid\ntheoretical grounding. However, as training progresses, the loss tends to focus\non hard to classify samples, which may prevent the network from obtaining gains\nin performance. While most work in the field suggest ways to classify hard\nnegatives, we suggest to strategically leave hard negatives behind, in order to\nfocus on misclassified samples with higher probabilities. We show that adding\nto the optimization goal the expectation loss, which is a better approximation\nof the zero-one loss, helps the network to achieve better accuracy. We,\ntherefore, propose to shift between the two losses during training, focusing\nmore on the expectation loss gradually during the later stages of training. Our\nexperiments show that the new training protocol improves performance across a\ndiverse set of classification domains, including computer vision, natural\nlanguage processing, tabular data, and sequences. Our code and scripts are\navailable at supplementary.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.04255,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000064903,
      "text":"Optimal Reservoir Operations using Long Short-Term Memory Network\n\n  A reliable forecast of inflows to the reservoir is a key factor in the\noptimal operation of reservoirs. Real-time operation of the reservoir based on\nforecasts of inflows can lead to substantial economic gains. However, the\nforecast of inflow is an intricate task as it has to incorporate the impacts of\nclimate and hydrological changes. Therefore, the major objective of the present\nwork is to develop a novel approach based on long short-term memory (LSTM) for\nthe forecast of inflows. Real-time inflow forecast, in other words, daily\ninflow at the reservoir helps in efficient operation of water resources. Also,\ndaily variations in the release can be monitored efficiently and the\nreliability of operation is improved. This work proposes a naive anomaly\ndetection algorithm baseline based on LSTM. In other words, a strong baseline\nto forecast flood and drought for any deep learning-based prediction model. The\npracticality of the approach has been demonstrated using the observed daily\ndata of the past 20 years from Bhakra Dam in India. The results of the\nsimulations conducted herein clearly indicate the supremacy of the LSTM\napproach over the traditional methods of forecasting. Although, experiments are\nrun on data from Bhakra Dam Reservoir in India, LSTM model, and anomaly\ndetection algorithm are general purpose and can be applied to any basin with\nminimal changes. A distinct practical advantage of the LSTM method presented\nherein is that it can adequately simulate non-stationarity and non-linearity in\nthe historical data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.06786,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Multiple shooting for training neural differential equations on time\n  series\n\n  Neural differential equations have recently emerged as a flexible\ndata-driven\/hybrid approach to model time-series data. This work experimentally\ndemonstrates that if the data contains oscillations, then standard fitting of a\nneural differential equation may result in a flattened out trajectory that\nfails to describe the data. We then introduce the multiple shooting method and\npresent successful demonstrations of this method for the fitting of a neural\ndifferential equation to two datasets (synthetic and experimental) that the\nstandard approach fails to fit. Constraints introduced by multiple shooting can\nbe satisfied using a penalty or augmented Lagrangian method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.03048,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Early ICU Mortality Prediction and Survival Analysis for Respiratory\n  Failure\n\n  Respiratory failure is the one of major causes of death in critical care\nunit. During the outbreak of COVID-19, critical care units experienced an\nextreme shortage of mechanical ventilation because of respiratory failure\nrelated syndromes. To help this, the early mortality risk prediction in\npatients who suffer respiratory failure can provide timely support for clinical\ntreatment and resource management. In the study, we propose a dynamic modeling\napproach for early mortality risk prediction of the respiratory failure\npatients based on the first 24 hours ICU physiological data. Our proposed model\nis validated on the eICU collaborate database. We achieved a high AUROC\nperformance (80-83%) and significantly improved AUCPR 4% on Day 5 since ICU\nadmission, compared to the state-of-art prediction models. In addition, we\nillustrated that the survival curve includes the time-varying information for\nthe early ICU admission survival analysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.0588,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Explaining Deep Learning Representations by Tracing the Training Process\n\n  We propose a novel explanation method that explains the decisions of a deep\nneural network by investigating how the intermediate representations at each\nlayer of the deep network were refined during the training process. This way we\ncan a) find the most influential training examples during training and b)\nanalyze which classes attributed most to the final representation. Our method\nis general: it can be wrapped around any iterative optimization procedure and\ncovers a variety of neural network architectures, including feed-forward\nnetworks and convolutional neural networks. We first propose a method for\nstochastic training with single training instances, but continue to also derive\na variant for the common mini-batch training. In experimental evaluations, we\nshow that our method identifies highly representative training instances that\ncan be used as an explanation. Additionally, we propose a visualization that\nprovides explanations in the form of aggregated statistics over the whole\ntraining process.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.10061,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Graph Neural Networks for Graph Drawing\n\n  Graph Drawing techniques have been developed in the last few years with the\npurpose of producing aesthetically pleasing node-link layouts. Recently, the\nemployment of differentiable loss functions has paved the road to the massive\nusage of Gradient Descent and related optimization algorithms. In this paper,\nwe propose a novel framework for the development of Graph Neural Drawers (GND),\nmachines that rely on neural computation for constructing efficient and complex\nmaps. GNDs are Graph Neural Networks (GNNs) whose learning process can be\ndriven by any provided loss function, such as the ones commonly employed in\nGraph Drawing. Moreover, we prove that this mechanism can be guided by loss\nfunctions computed by means of Feedforward Neural Networks, on the basis of\nsupervision hints that express beauty properties, like the minimization of\ncrossing edges. In this context, we show that GNNs can nicely be enriched by\npositional features to deal also with unlabelled vertexes. We provide a\nproof-of-concept by constructing a loss function for the edge-crossing and\nprovide quantitative and qualitative comparisons among different GNN models\nworking under the proposed framework.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.00415,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"Optimization Networks for Integrated Machine Learning\n\n  Optimization networks are a new methodology for holistically solving\ninterrelated problems that have been developed with combinatorial optimization\nproblems in mind. In this contribution we revisit the core principles of\noptimization networks and demonstrate their suitability for solving machine\nlearning problems. We use feature selection in combination with linear model\ncreation as a benchmark application and compare the results of optimization\nnetworks to ordinary least squares with optional elastic net regularization.\nBased on this example we justify the advantages of optimization networks by\nadapting the network to solve other machine learning problems. Finally,\noptimization analysis is presented, where optimal input values of a system have\nto be found to achieve desired output values. Optimization analysis can be\ndivided into three subproblems: model creation to describe the system, model\nselection to choose the most appropriate one and parameter optimization to\nobtain the input values. Therefore, optimization networks are an obvious choice\nfor handling optimization analysis tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.14117,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000015563,
      "text":"Neural Network Ensembles: Theory, Training, and the Importance of\n  Explicit Diversity\n\n  Ensemble learning is a process by which multiple base learners are\nstrategically generated and combined into one composite learner. There are two\nfeatures that are essential to an ensemble's performance, the individual\naccuracies of the component learners and the overall diversity in the ensemble.\nThe right balance of learner accuracy and ensemble diversity can improve the\nperformance of machine learning tasks on benchmark and real-world data sets,\nand recent theoretical and practical work has demonstrated the subtle trade-off\nbetween accuracy and diversity in an ensemble. In this paper, we extend the\nextant literature by providing a deeper theoretical understanding for assessing\nand improving the optimality of any given ensemble, including random forests\nand deep neural network ensembles. We also propose a training algorithm for\nneural network ensembles and demonstrate that our approach provides improved\nperformance when compared to both state-of-the-art individual learners and\nensembles of state-of-the-art learners trained using standard loss functions.\nOur key insight is that it is better to explicitly encourage diversity in an\nensemble, rather than merely allowing diversity to occur by happenstance, and\nthat rigorous theoretical bounds on the trade-off between diversity and learner\naccuracy allow one to know when an optimal arrangement has been achieved.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.07747,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000031127,
      "text":"Neural-network acceleration of projection-based model-order-reduction\n  for finite plasticity: Application to RVEs\n\n  Compared to conventional projection-based model-order-reduction, its\nneural-network acceleration has the advantage that the online simulations are\nequation-free, meaning that no system of equations needs to be solved\niteratively. Consequently, no stiffness matrix needs to be constructed and the\nstress update needs to be computed only once per increment. In this\ncontribution, a recurrent neural network is developed to accelerate a\nprojection-based model-order-reduction of the elastoplastic mechanical\nbehaviour of an RVE. In contrast to a neural network that merely emulates the\nrelation between the macroscopic deformation (path) and the macroscopic stress,\nthe neural network acceleration of projection-based model-order-reduction\npreserves all microstructural information, at the price of computing this\ninformation once per increment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.0947,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"ACReL: Adversarial Conditional value-at-risk Reinforcement Learning\n\n  In the classical Reinforcement Learning (RL) setting, one aims to find a\npolicy that maximizes its expected return. This objective may be inappropriate\nin safety-critical domains such as healthcare or autonomous driving, where\nintrinsic uncertainties due to stochastic policies and environment variability\nmay lead to catastrophic failures. This can be addressed by using the\nConditional-Value-at-Risk (CVaR) objective to instill risk-aversion in learned\npolicies. In this paper, we propose Adversarial Cvar Reinforcement Learning\n(ACReL), a novel adversarial meta-algorithm to optimize the CVaR objective in\nRL. ACReL is based on a max-min between a policy player and a learned adversary\nthat perturbs the policy player's state transitions given a finite budget. We\nprove that, the closer the players are to the game's equilibrium point, the\ncloser the learned policy is to the CVaR-optimal one with a risk tolerance\nexplicitly related to the adversary's budget. We provide a gradient-based\ntraining procedure to solve the proposed game by formulating it as a\nStackelberg game, enabling the use of deep RL architectures and training\nalgorithms. Empirical experiments show that ACReL matches a CVaR RL\nstate-of-the-art baseline for retrieving CVaR optimal policies, while also\nbenefiting from theoretical guarantees.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.09948,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"Neural networks with trainable matrix activation functions\n\n  The training process of neural networks usually optimize weights and bias\nparameters of linear transformations, while nonlinear activation functions are\npre-specified and fixed. This work develops a systematic approach to\nconstructing matrix-valued activation functions whose entries are generalized\nfrom ReLU. The activation is based on matrix-vector multiplications using only\nscalar multiplications and comparisons. The proposed activation functions\ndepend on parameters that are trained along with the weights and bias vectors.\nNeural networks based on this approach are simple and efficient and are shown\nto be robust in numerical experiments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.15072,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000032451,
      "text":"Leveraging Recursive Gumbel-Max Trick for Approximate Inference in\n  Combinatorial Spaces\n\n  Structured latent variables allow incorporating meaningful prior knowledge\ninto deep learning models. However, learning with such variables remains\nchallenging because of their discrete nature. Nowadays, the standard learning\napproach is to define a latent variable as a perturbed algorithm output and to\nuse a differentiable surrogate for training. In general, the surrogate puts\nadditional constraints on the model and inevitably leads to biased gradients.\nTo alleviate these shortcomings, we extend the Gumbel-Max trick to define\ndistributions over structured domains. We avoid the differentiable surrogates\nby leveraging the score function estimators for optimization. In particular, we\nhighlight a family of recursive algorithms with a common feature we call\nstochastic invariant. The feature allows us to construct reliable gradient\nestimates and control variates without additional constraints on the model. In\nour experiments, we consider various structured latent variable models and\nachieve results competitive with relaxation-based counterparts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.13578,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Distributional Reinforcement Learning for Multi-Dimensional Reward\n  Functions\n\n  A growing trend for value-based reinforcement learning (RL) algorithms is to\ncapture more information than scalar value functions in the value network. One\nof the most well-known methods in this branch is distributional RL, which\nmodels return distribution instead of scalar value. In another line of work,\nhybrid reward architectures (HRA) in RL have studied to model source-specific\nvalue functions for each source of reward, which is also shown to be beneficial\nin performance. To fully inherit the benefits of distributional RL and hybrid\nreward architectures, we introduce Multi-Dimensional Distributional DQN\n(MD3QN), which extends distributional RL to model the joint return distribution\nfrom multiple reward sources. As a by-product of joint distribution modeling,\nMD3QN can capture not only the randomness in returns for each source of reward,\nbut also the rich reward correlation between the randomness of different\nsources. We prove the convergence for the joint distributional Bellman operator\nand build our empirical algorithm by minimizing the Maximum Mean Discrepancy\nbetween joint return distribution and its Bellman target. In experiments, our\nmethod accurately models the joint return distribution in environments with\nrichly correlated reward functions, and outperforms previous RL methods\nutilizing multi-dimensional reward functions in the control setting.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.02048,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000037418,
      "text":"Graph Coloring: Comparing Cluster Graphs to Factor Graphs\n\n  We present a means of formulating and solving graph coloring problems with\nprobabilistic graphical models. In contrast to the prevalent literature that\nuses factor graphs for this purpose, we instead approach it from a cluster\ngraph perspective. Since there seems to be a lack of algorithms to\nautomatically construct valid cluster graphs, we provide such an algorithm\n(termed LTRIP). Our experiments indicate a significant advantage for preferring\ncluster graphs over factor graphs, both in terms of accuracy as well as\ncomputational efficiency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.09606,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Efficient Analysis of COVID-19 Clinical Data using Machine Learning\n  Models\n\n  Because of the rapid spread of COVID-19 to almost every part of the globe,\nhuge volumes of data and case studies have been made available, providing\nresearchers with a unique opportunity to find trends and make discoveries like\nnever before, by leveraging such big data. This data is of many different\nvarieties, and can be of different levels of veracity e.g., precise, imprecise,\nuncertain, and missing, making it challenging to extract important information\nfrom such data. Yet, efficient analyses of this continuously growing and\nevolving COVID-19 data is crucial to inform -- often in real-time -- the\nrelevant measures needed for controlling, mitigating, and ultimately avoiding\nviral spread. Applying machine learning based algorithms to this big data is a\nnatural approach to take to this aim, since they can quickly scale to such\ndata, and extract the relevant information in the presence of variety and\ndifferent levels of veracity. This is important for COVID-19, and for potential\nfuture pandemics in general.\n  In this paper, we design a straightforward encoding of clinical data (on\ncategorical attributes) into a fixed-length feature vector representation, and\nthen propose a model that first performs efficient feature selection from such\nrepresentation. We apply this approach on two clinical datasets of the COVID-19\npatients and then apply different machine learning algorithms downstream for\nclassification purposes. We show that with the efficient feature selection\nalgorithm, we can achieve a prediction accuracy of more than 90\\% in most\ncases. We also computed the importance of different attributes in the dataset\nusing information gain. This can help the policy makers to focus on only\ncertain attributes for the purposes of studying this disease rather than\nfocusing on multiple random factors that may not be very informative to patient\noutcomes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.12231,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"Learning curves for Gaussian process regression with power-law priors\n  and targets\n\n  We characterize the power-law asymptotics of learning curves for Gaussian\nprocess regression (GPR) under the assumption that the eigenspectrum of the\nprior and the eigenexpansion coefficients of the target function follow a power\nlaw. Under similar assumptions, we leverage the equivalence between GPR and\nkernel ridge regression (KRR) to show the generalization error of KRR.\nInfinitely wide neural networks can be related to GPR with respect to the\nneural network GP kernel and the neural tangent kernel, which in several cases\nis known to have a power-law spectrum. Hence our methods can be applied to\nstudy the generalization error of infinitely wide neural networks. We present\ntoy experiments demonstrating the theory.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.00841,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Transfer Learning Approaches for Knowledge Discovery in Grid-based\n  Geo-Spatiotemporal Data\n\n  Extracting and meticulously analyzing geo-spatiotemporal features is crucial\nto recognize intricate underlying causes of natural events, such as floods.\nLimited evidence about hidden factors leading to climate change makes it\nchallenging to predict regional water discharge accurately. In addition, the\nexplosive growth in complex geo-spatiotemporal environment data that requires\nrepeated learning by the state-of-the-art neural networks for every new region\nemphasizes the need for new computationally efficient methods, advanced\ncomputational resources, and extensive training on a massive amount of\navailable monitored data. We, therefore, propose HydroDeep, an effectively\nreusable pretrained model to address this problem of transferring knowledge\nfrom one region to another by effectively capturing their intrinsic\ngeo-spatiotemporal variance. Further, we present four transfer learning\napproaches on HydroDeep for spatiotemporal interpretability that improve\nNash-Sutcliffe efficiency by 9% to 108% in new regions with a 95% reduction in\ntime.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.03403,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Disentangling deep neural networks with rectified linear units using\n  duality\n\n  Despite their success deep neural networks (DNNs) are still largely\nconsidered as black boxes. The main issue is that the linear and non-linear\noperations are entangled in every layer, making it hard to interpret the hidden\nlayer outputs. In this paper, we look at DNNs with rectified linear units\n(ReLUs), and focus on the gating property (`on\/off' states) of the ReLUs. We\nextend the recently developed dual view in which the computation is broken\npath-wise to show that learning in the gates is more crucial, and learning the\nweights given the gates is characterised analytically via the so called neural\npath kernel (NPK) which depends on inputs and gates. In this paper, we present\nnovel results to show that convolution with global pooling and skip connection\nprovide respectively rotational invariance and ensemble structure to the NPK.\nTo address `black box'-ness, we propose a novel interpretable counterpart of\nDNNs with ReLUs namely deep linearly gated networks (DLGN): the pre-activations\nto the gates are generated by a deep linear network, and the gates are then\napplied as external masks to learn the weights in a different network. The DLGN\nis not an alternative architecture per se, but a disentanglement and an\ninterpretable re-arrangement of the computations in a DNN with ReLUs. The DLGN\ndisentangles the computations into two `mathematically' interpretable\nlinearities (i) the `primal' linearity between the input and the\npre-activations in the gating network and (ii) the `dual' linearity in the path\nspace in the weights network characterised by the NPK. We compare the\nperformance of DNN, DGN and DLGN on CIFAR-10 and CIFAR-100 to show that, the\nDLGN recovers more than $83.5\\%$ of the performance of state-of-the-art DNNs.\nThis brings us to an interesting question: `Is DLGN a universal spectral\napproximator?'\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.00532,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000054306,
      "text":"Layer-wise and Dimension-wise Locally Adaptive Federated Learning\n\n  In the emerging paradigm of Federated Learning (FL), large amount of clients\nsuch as mobile devices are used to train possibly high-dimensional models on\ntheir respective data. Combining (dimension-wise) adaptive gradient methods\n(e.g. Adam, AMSGrad) with FL has been an active direction, which is shown to\noutperform traditional SGD based FL in many cases. In this paper, we focus on\nthe problem of training federated deep neural networks, and propose a novel FL\nframework which further introduces layer-wise adaptivity to the local model\nupdates. Our framework can be applied to locally adaptive FL methods including\ntwo recent algorithms, Mime and Fed-AMS. Theoretically, we provide a\nconvergence analysis of our layer-wise FL methods, coined Fed-LAMB and\nMime-LAMB, which matches the convergence rate of state-of-the-art results in FL\nand exhibits linear speedup in terms of the number of workers. Experimental\nresults on various datasets and models, under both IID and non-IID local data\nsettings, show that both Fed-LAMB and Mime-LAMB achieve faster convergence\nspeed and better generalization performance, compared to the various recent\nadaptive FL methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.12187,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000048015,
      "text":"AFEC: Active Forgetting of Negative Transfer in Continual Learning\n\n  Continual learning aims to learn a sequence of tasks from dynamic data\ndistributions. Without accessing to the old training samples, knowledge\ntransfer from the old tasks to each new task is difficult to determine, which\nmight be either positive or negative. If the old knowledge interferes with the\nlearning of a new task, i.e., the forward knowledge transfer is negative, then\nprecisely remembering the old tasks will further aggravate the interference,\nthus decreasing the performance of continual learning. By contrast, biological\nneural networks can actively forget the old knowledge that conflicts with the\nlearning of a new experience, through regulating the learning-triggered\nsynaptic expansion and synaptic convergence. Inspired by the biological active\nforgetting, we propose to actively forget the old knowledge that limits the\nlearning of new tasks to benefit continual learning. Under the framework of\nBayesian continual learning, we develop a novel approach named Active\nForgetting with synaptic Expansion-Convergence (AFEC). Our method dynamically\nexpands parameters to learn each new task and then selectively combines them,\nwhich is formally consistent with the underlying mechanism of biological active\nforgetting. We extensively evaluate AFEC on a variety of continual learning\nbenchmarks, including CIFAR-10 regression tasks, visual classification tasks\nand Atari reinforcement tasks, where AFEC effectively improves the learning of\nnew tasks and achieves the state-of-the-art performance in a plug-and-play way.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.10249,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Neural Stochastic PDEs: Resolution-Invariant Learning of Continuous\n  Spatiotemporal Dynamics\n\n  Stochastic partial differential equations (SPDEs) are the mathematical tool\nof choice for modelling spatiotemporal PDE-dynamics under the influence of\nrandomness. Based on the notion of mild solution of an SPDE, we introduce a\nnovel neural architecture to learn solution operators of PDEs with (possibly\nstochastic) forcing from partially observed data. The proposed Neural SPDE\nmodel provides an extension to two popular classes of physics-inspired\narchitectures. On the one hand, it extends Neural CDEs and variants --\ncontinuous-time analogues of RNNs -- in that it is capable of processing\nincoming sequential information arriving at arbitrary spatial resolutions. On\nthe other hand, it extends Neural Operators -- generalizations of neural\nnetworks to model mappings between spaces of functions -- in that it can\nparameterize solution operators of SPDEs depending simultaneously on the\ninitial condition and a realization of the driving noise. By performing\noperations in the spectral domain, we show how a Neural SPDE can be evaluated\nin two ways, either by calling an ODE solver (emulating a spectral Galerkin\nscheme), or by solving a fixed point problem. Experiments on various semilinear\nSPDEs, including the stochastic Navier-Stokes equations, demonstrate how the\nNeural SPDE model is capable of learning complex spatiotemporal dynamics in a\nresolution-invariant way, with better accuracy and lighter training data\nrequirements compared to alternative models, and up to 3 orders of magnitude\nfaster than traditional solvers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.09133,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Online Sign Identification: Minimization of the Number of Errors in\n  Thresholding Bandits\n\n  In the fixed budget thresholding bandit problem, an algorithm sequentially\nallocates a budgeted number of samples to different distributions. It then\npredicts whether the mean of each distribution is larger or lower than a given\nthreshold. We introduce a large family of algorithms (containing most existing\nrelevant ones), inspired by the Frank-Wolfe algorithm, and provide a thorough\nyet generic analysis of their performance. This allowed us to construct new\nexplicit algorithms, for a broad class of problems, whose losses are within a\nsmall constant factor of the non-adaptive oracle ones. Quite interestingly, we\nobserved that adaptive methods empirically greatly out-perform non-adaptive\noracles, an uncommon behavior in standard online learning settings, such as\nregret minimization. We explain this surprising phenomenon on an insightful toy\nproblem.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.06851,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Fast Posterior Estimation of Cardiac Electrophysiological Model\n  Parameters via Bayesian Active Learning\n\n  Probabilistic estimation of cardiac electrophysiological model parameters\nserves an important step towards model personalization and uncertain\nquantification. The expensive computation associated with these model\nsimulations, however, makes direct Markov Chain Monte Carlo (MCMC) sampling of\nthe posterior probability density function (pdf) of model parameters\ncomputationally intensive. Approximated posterior pdfs resulting from replacing\nthe simulation model with a computationally efficient surrogate, on the other\nhand, have seen limited accuracy. In this paper, we present a Bayesian active\nlearning method to directly approximate the posterior pdf function of cardiac\nmodel parameters, in which we intelligently select training points to query the\nsimulation model in order to learn the posterior pdf using a small number of\nsamples. We integrate a generative model into Bayesian active learning to allow\napproximating posterior pdf of high-dimensional model parameters at the\nresolution of the cardiac mesh. We further introduce new acquisition functions\nto focus the selection of training points on better approximating the shape\nrather than the modes of the posterior pdf of interest. We evaluated the\npresented method in estimating tissue excitability in a 3D cardiac\nelectrophysiological model in a range of synthetic and real-data experiments.\nWe demonstrated its improved accuracy in approximating the posterior pdf\ncompared to Bayesian active learning using regular acquisition functions, and\nsubstantially reduced computational cost in comparison to existing standard or\naccelerated MCMC sampling.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.1003,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000201994,
      "text":"Accelerating Framework of Transformer by Hardware Design and Model\n  Compression Co-Optimization\n\n  State-of-the-art Transformer-based models, with gigantic parameters, are\ndifficult to be accommodated on resource constrained embedded devices.\nMoreover, with the development of technology, more and more embedded devices\nare available to run a Transformer model. For a Transformer model with\ndifferent constraints (tight or loose), it can be deployed onto devices with\ndifferent computing power. However, in previous work, designers did not choose\nthe best device among multiple devices. Instead, they just used an existing\ndevice to deploy model, which was not necessarily the best fit and may lead to\nunderutilization of resources. To address the deployment challenge of\nTransformer and the problem to select the best device, we propose an algorithm\n& hardware closed-loop acceleration framework. Given a dataset, a model,\nlatency constraint LC and accuracy constraint AC, our framework can provide a\nbest device satisfying both constraints. In order to generate a compressed\nmodel with high sparsity ratio, we propose a novel pruning technique,\nhierarchical pruning (HP). We optimize the sparse matrix storage format for HP\nmatrix to further reduce memory usage for FPGA implementation. We design a\naccelerator that takes advantage of HP to solve the problem of concurrent\nrandom access. Experiments on Transformer and TinyBert model show that our\nframework can find different devices for various LC and AC, covering from\nlow-end devices to high-end devices. Our HP can achieve higher sparsity ratio\nand is more flexible than other sparsity pattern. Our framework can achieve\n37x, 1.9x, 1.7x speedup compared to CPU, GPU and FPGA, respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.13511,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"AutoDEUQ: Automated Deep Ensemble with Uncertainty Quantification\n\n  Deep neural networks are powerful predictors for a variety of tasks. However,\nthey do not capture uncertainty directly. Using neural network ensembles to\nquantify uncertainty is competitive with approaches based on Bayesian neural\nnetworks while benefiting from better computational scalability. However,\nbuilding ensembles of neural networks is a challenging task because, in\naddition to choosing the right neural architecture or hyperparameters for each\nmember of the ensemble, there is an added cost of training each model. We\npropose AutoDEUQ, an automated approach for generating an ensemble of deep\nneural networks. Our approach leverages joint neural architecture and\nhyperparameter search to generate ensembles. We use the law of total variance\nto decompose the predictive variance of deep ensembles into aleatoric (data)\nand epistemic (model) uncertainties. We show that AutoDEUQ outperforms\nprobabilistic backpropagation, Monte Carlo dropout, deep ensemble,\ndistribution-free ensembles, and hyper ensemble methods on a number of\nregression benchmarks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.03802,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Hitting the Target: Stopping Active Learning at the Cost-Based Optimum\n\n  Active learning allows machine learning models to be trained using fewer\nlabels while retaining similar performance to traditional supervised learning.\nAn active learner selects the most informative data points, requests their\nlabels, and retrains itself. While this approach is promising, it raises the\nquestion of how to determine when the model is `good enough' without the\nadditional labels required for traditional evaluation. Previously, different\nstopping criteria have been proposed aiming to identify the optimal stopping\npoint. Yet, optimality can only be expressed as a domain-dependent trade-off\nbetween accuracy and the number of labels, and no criterion is superior in all\napplications. As a further complication, a comparison of criteria for a\nparticular real-world application would require practitioners to collect\nadditional labelled data they are aiming to avoid by using active learning in\nthe first place. This work enables practitioners to employ active learning by\nproviding actionable recommendations for which stopping criteria are best for a\ngiven real-world scenario. We contribute the first large-scale comparison of\nstopping criteria for pool-based active learning, using a cost measure to\nquantify the accuracy\/label trade-off, public implementations of all stopping\ncriteria we evaluate, and an open-source framework for evaluating stopping\ncriteria. Our research enables practitioners to substantially reduce labelling\ncosts by utilizing the stopping criterion which best suits their domain.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.14056,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"How to transfer algorithmic reasoning knowledge to learn new algorithms?\n\n  Learning to execute algorithms is a fundamental problem that has been widely\nstudied. Prior work~\\cite{veli19neural} has shown that to enable systematic\ngeneralisation on graph algorithms it is critical to have access to the\nintermediate steps of the program\/algorithm. In many reasoning tasks, where\nalgorithmic-style reasoning is important, we only have access to the input and\noutput examples. Thus, inspired by the success of pre-training on similar tasks\nor data in Natural Language Processing (NLP) and Computer Vision, we set out to\nstudy how we can transfer algorithmic reasoning knowledge. Specifically, we\ninvestigate how we can use algorithms for which we have access to the execution\ntrace to learn to solve similar tasks for which we do not. We investigate two\nmajor classes of graph algorithms, parallel algorithms such as breadth-first\nsearch and Bellman-Ford and sequential greedy algorithms such as Prim and\nDijkstra. Due to the fundamental differences between algorithmic reasoning\nknowledge and feature extractors such as used in Computer Vision or NLP, we\nhypothesise that standard transfer techniques will not be sufficient to achieve\nsystematic generalisation. To investigate this empirically we create a dataset\nincluding 9 algorithms and 3 different graph types. We validate this\nempirically and show how instead multi-task learning can be used to achieve the\ntransfer of algorithmic reasoning knowledge.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.02858,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Distribution Preserving Multiple Hypotheses Prediction for Uncertainty\n  Modeling\n\n  Many supervised machine learning tasks, such as future state prediction in\ndynamical systems, require precise modeling of a forecast's uncertainty. The\nMultiple Hypotheses Prediction (MHP) approach addresses this problem by\nproviding several hypotheses that represent possible outcomes. Unfortunately,\nwith the common $l_2$ loss function, these hypotheses do not preserve the data\ndistribution's characteristics. We propose an alternative loss for distribution\npreserving MHP and review relevant theorems supporting our claims. Furthermore,\nwe empirically show that our approach yields more representative hypotheses on\na synthetic and a real-world motion prediction data set. The outputs of the\nproposed method can directly be used in sampling-based Monte-Carlo methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.13799,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000047684,
      "text":"Neural PPO-Clip Attains Global Optimality: A Hinge Loss Perspective\n\n  Policy optimization is a fundamental principle for designing reinforcement\nlearning algorithms, and one example is the proximal policy optimization\nalgorithm with a clipped surrogate objective (PPO-Clip), which has been\npopularly used in deep reinforcement learning due to its simplicity and\neffectiveness. Despite its superior empirical performance, PPO-Clip has not\nbeen justified via theoretical proof up to date. In this paper, we establish\nthe first global convergence rate of PPO-Clip under neural function\napproximation. We identify the fundamental challenges of analyzing PPO-Clip and\naddress them with the two core ideas: (i) We reinterpret PPO-Clip from the\nperspective of hinge loss, which connects policy improvement with solving a\nlarge-margin classification problem with hinge loss and offers a generalized\nversion of the PPO-Clip objective. (ii) Based on the above viewpoint, we\npropose a two-step policy improvement scheme, which facilitates the convergence\nanalysis by decoupling policy search from the complex neural policy\nparameterization with the help of entropic mirror descent and a\nregression-based policy update scheme. Moreover, our theoretical results\nprovide the first characterization of the effect of the clipping mechanism on\nthe convergence of PPO-Clip. Through experiments, we empirically validate the\nreinterpretation of PPO-Clip and the generalized objective with various\nclassifiers on various RL benchmark tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.08255,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Yformer: U-Net Inspired Transformer Architecture for Far Horizon Time\n  Series Forecasting\n\n  Time series data is ubiquitous in research as well as in a wide variety of\nindustrial applications. Effectively analyzing the available historical data\nand providing insights into the far future allows us to make effective\ndecisions. Recent research has witnessed the superior performance of\ntransformer-based architectures, especially in the regime of far horizon time\nseries forecasting. However, the current state of the art sparse Transformer\narchitectures fail to couple down- and upsampling procedures to produce outputs\nin a similar resolution as the input. We propose the Yformer model, based on a\nnovel Y-shaped encoder-decoder architecture that (1) uses direct connection\nfrom the downscaled encoder layer to the corresponding upsampled decoder layer\nin a U-Net inspired architecture, (2) Combines the downscaling\/upsampling with\nsparse attention to capture long-range effects, and (3) stabilizes the\nencoder-decoder stacks with the addition of an auxiliary reconstruction loss.\nExtensive experiments have been conducted with relevant baselines on four\nbenchmark datasets, demonstrating an average improvement of 19.82, 18.41\npercentage MSE and 13.62, 11.85 percentage MAE in comparison to the current\nstate of the art for the univariate and the multivariate settings respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.00581,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000005828,
      "text":"Classification of Time-Series Data Using Boosted Decision Trees\n\n  Time-series data classification is central to the analysis and control of\nautonomous systems, such as robots and self-driving cars. Temporal logic-based\nlearning algorithms have been proposed recently as classifiers of such data.\nHowever, current frameworks are either inaccurate for real-world applications,\nsuch as autonomous driving, or they generate long and complicated formulae that\nlack interpretability. To address these limitations, we introduce a novel\nlearning method, called Boosted Concise Decision Trees (BCDTs), to generate\nbinary classifiers that are represented as Signal Temporal Logic (STL)\nformulae. Our algorithm leverages an ensemble of Concise Decision Trees (CDTs)\nto improve the classification performance, where each CDT is a decision tree\nthat is empowered by a set of techniques to generate simpler formulae and\nimprove interpretability. The effectiveness and classification performance of\nour algorithm are evaluated on naval surveillance and urban-driving case\nstudies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.13293,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"KNAS: Green Neural Architecture Search\n\n  Many existing neural architecture search (NAS) solutions rely on downstream\ntraining for architecture evaluation, which takes enormous computations.\nConsidering that these computations bring a large carbon footprint, this paper\naims to explore a green (namely environmental-friendly) NAS solution that\nevaluates architectures without training. Intuitively, gradients, induced by\nthe architecture itself, directly decide the convergence and generalization\nresults. It motivates us to propose the gradient kernel hypothesis: Gradients\ncan be used as a coarse-grained proxy of downstream training to evaluate\nrandom-initialized networks. To support the hypothesis, we conduct a\ntheoretical analysis and find a practical gradient kernel that has good\ncorrelations with training loss and validation performance. According to this\nhypothesis, we propose a new kernel based architecture search approach KNAS.\nExperiments show that KNAS achieves competitive results with orders of\nmagnitude faster than \"train-then-test\" paradigms on image classification\ntasks. Furthermore, the extremely low search cost enables its wide\napplications. The searched network also outperforms strong baseline\nRoBERTA-large on two text classification tasks. Codes are available at\n\\url{https:\/\/github.com\/Jingjing-NLP\/KNAS} .\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.01996,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Pareto Adversarial Robustness: Balancing Spatial Robustness and\n  Sensitivity-based Robustness\n\n  Adversarial robustness, which primarily comprises sensitivity-based\nrobustness and spatial robustness, plays an integral part in achieving robust\ngeneralization. In this paper, we endeavor to design strategies to achieve\nuniversal adversarial robustness. To achieve this, we first investigate the\nrelatively less-explored realm of spatial robustness. Then, we integrate the\nexisting spatial robustness methods by incorporating both local and global\nspatial vulnerability into a unified spatial attack and adversarial training\napproach. Furthermore, we present a comprehensive relationship between natural\naccuracy, sensitivity-based robustness, and spatial robustness, supported by\nstrong evidence from the perspective of robust representation. Crucially, to\nreconcile the interplay between the mutual impacts of various robustness\ncomponents into one unified framework, we incorporate the \\textit{Pareto\ncriterion} into the adversarial robustness analysis, yielding a novel strategy\ncalled Pareto Adversarial Training for achieving universal robustness. The\nresulting Pareto front, which delineates the set of optimal solutions, provides\nan optimal balance between natural accuracy and various adversarial robustness.\nThis sheds light on solutions for achieving universal robustness in the future.\nTo the best of our knowledge, we are the first to consider universal\nadversarial robustness via multi-objective optimization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.089,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000070863,
      "text":"A GNN-RNN Approach for Harnessing Geospatial and Temporal Information:\n  Application to Crop Yield Prediction\n\n  Climate change is posing new challenges to crop-related concerns including\nfood insecurity, supply stability and economic planning. As one of the central\nchallenges, crop yield prediction has become a pressing task in the machine\nlearning field. Despite its importance, the prediction task is exceptionally\ncomplicated since crop yields depend on various factors such as weather, land\nsurface, soil quality as well as their interactions. In recent years, machine\nlearning models have been successfully applied in this domain. However, these\nmodels either restrict their tasks to a relatively small region, or only study\nover a single or few years, which makes them hard to generalize spatially and\ntemporally. In this paper, we introduce a novel graph-based recurrent neural\nnetwork for crop yield prediction, to incorporate both geographical and\ntemporal knowledge in the model, and further boost predictive power. Our method\nis trained, validated, and tested on over 2000 counties from 41 states in the\nUS mainland, covering years from 1981 to 2019. As far as we know, this is the\nfirst machine learning method that embeds geographical knowledge in crop yield\nprediction and predicts the crop yields at county level nationwide. We also\nlaid a solid foundation for the comparison with other machine learning\nbaselines by applying well-known linear models, tree-based models, deep\nlearning methods and comparing their performance. Experiments show that our\nproposed method consistently outperforms the existing state-of-the-art methods\non various metrics, validating the effectiveness of geospatial and temporal\ninformation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.05968,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Linear Speedup in Personalized Collaborative Learning\n\n  Collaborative training can improve the accuracy of a model for a user by\ntrading off the model's bias (introduced by using data from other users who are\npotentially different) against its variance (due to the limited amount of data\non any single user). In this work, we formalize the personalized collaborative\nlearning problem as a stochastic optimization of a task 0 while giving access\nto N related but different tasks 1,..., N. We provide convergence guarantees\nfor two algorithms in this setting -- a popular collaboration method known as\nweighted gradient averaging, and a novel bias correction method -- and explore\nconditions under which we can achieve linear speedup w.r.t. the number of\nauxiliary tasks N. Further, we also empirically study their performance\nconfirming our theoretical insights.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.15432,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"TiWS-iForest: Isolation Forest in Weakly Supervised and Tiny ML\n  scenarios\n\n  Unsupervised anomaly detection tackles the problem of finding anomalies\ninside datasets without the labels availability; since data tagging is\ntypically hard or expensive to obtain, such approaches have seen huge\napplicability in recent years. In this context, Isolation Forest is a popular\nalgorithm able to define an anomaly score by means of an ensemble of peculiar\ntrees called isolation trees. These are built using a random partitioning\nprocedure that is extremely fast and cheap to train. However, we find that the\nstandard algorithm might be improved in terms of memory requirements, latency\nand performances; this is of particular importance in low resources scenarios\nand in TinyML implementations on ultra-constrained microprocessors. Moreover,\nAnomaly Detection approaches currently do not take advantage of weak\nsupervisions: being typically consumed in Decision Support Systems, feedback\nfrom the users, even if rare, can be a valuable source of information that is\ncurrently unexplored. Beside showing iForest training limitations, we propose\nhere TiWS-iForest, an approach that, by leveraging weak supervision is able to\nreduce Isolation Forest complexity and to enhance detection performances. We\nshowed the effectiveness of TiWS-iForest on real word datasets and we share the\ncode in a public repository to enhance reproducibility.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.05894,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"Graph Neural Network Training with Data Tiering\n\n  Graph Neural Networks (GNNs) have shown success in learning from\ngraph-structured data, with applications to fraud detection, recommendation,\nand knowledge graph reasoning. However, training GNN efficiently is challenging\nbecause: 1) GPU memory capacity is limited and can be insufficient for large\ndatasets, and 2) the graph-based data structure causes irregular data access\npatterns. In this work, we provide a method to statistical analyze and identify\nmore frequently accessed data ahead of GNN training. Our data tiering method\nnot only utilizes the structure of input graph, but also an insight gained from\nactual GNN training process to achieve a higher prediction result. With our\ndata tiering method, we additionally provide a new data placement and access\nstrategy to further minimize the CPU-GPU communication overhead. We also take\ninto account of multi-GPU GNN training as well and we demonstrate the\neffectiveness of our strategy in a multi-GPU system. The evaluation results\nshow that our work reduces CPU-GPU traffic by 87-95% and improves the training\nspeed of GNN over the existing solutions by 1.6-2.1x on graphs with hundreds of\nmillions of nodes and billions of edges.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.02014,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000008444,
      "text":"Neural network is heterogeneous: Phase matters more\n\n  We find a heterogeneity in both complex and real valued neural networks with\nthe insight from wave optics, claiming a much more important role of phase than\nits amplitude counterpart in the weight matrix. In complex-valued neural\nnetworks, we show that among different types of pruning, the weight matrix with\nonly phase information preserved achieves the best accuracy, which holds\nrobustly under various settings of depth and width. The conclusion can be\ngeneralized to real-valued neural networks, where signs take the place of\nphases. These inspiring findings enrich the techniques of network pruning and\nbinary computation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.05323,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000218219,
      "text":"Variational Multi-Task Learning with Gumbel-Softmax Priors\n\n  Multi-task learning aims to explore task relatedness to improve individual\ntasks, which is of particular significance in the challenging scenario that\nonly limited data is available for each task. To tackle this challenge, we\npropose variational multi-task learning (VMTL), a general probabilistic\ninference framework for learning multiple related tasks. We cast multi-task\nlearning as a variational Bayesian inference problem, in which task relatedness\nis explored in a unified manner by specifying priors. To incorporate shared\nknowledge into each task, we design the prior of a task to be a learnable\nmixture of the variational posteriors of other related tasks, which is learned\nby the Gumbel-Softmax technique. In contrast to previous methods, our VMTL can\nexploit task relatedness for both representations and classifiers in a\nprincipled way by jointly inferring their posteriors. This enables individual\ntasks to fully leverage inductive biases provided by related tasks, therefore\nimproving the overall performance of all tasks. Experimental results\ndemonstrate that the proposed VMTL is able to effectively tackle a variety of\nchallenging multi-task learning settings with limited training data for both\nclassification and regression. Our method consistently surpasses previous\nmethods, including strong Bayesian approaches, and achieves state-of-the-art\nperformance on five benchmark datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.1184,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Subgraph Permutation Equivariant Networks\n\n  In this work we develop a new method, named Sub-graph Permutation Equivariant\nNetworks (SPEN), which provides a framework for building graph neural networks\nthat operate on sub-graphs, while using a base update function that is\npermutation equivariant, that are equivariant to a novel choice of automorphism\ngroup. Message passing neural networks have been shown to be limited in their\nexpressive power and recent approaches to over come this either lack\nscalability or require structural information to be encoded into the feature\nspace. The general framework presented here overcomes the scalability issues\nassociated with global permutation equivariance by operating more locally on\nsub-graphs. In addition, through operating on sub-graphs the expressive power\nof higher-dimensional global permutation equivariant networks is improved; this\nis due to fact that two non-distinguishable graphs often contain\ndistinguishable sub-graphs. Furthermore, the proposed framework only requires a\nchoice of $k$-hops for creating ego-network sub-graphs and a choice of\nrepresentation space to be used for each layer, which makes the method easily\napplicable across a range of graph based domains. We experimentally validate\nthe method on a range of graph benchmark classification tasks, demonstrating\nstatistically indistinguishable results from the state-of-the-art on six out of\nseven benchmarks. Further, we demonstrate that the use of local update\nfunctions offers a significant improvement in GPU memory over global methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.01276,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Multi network InfoMax: A pre-training method involving graph\n  convolutional networks\n\n  Discovering distinct features and their relations from data can help us\nuncover valuable knowledge crucial for various tasks, e.g., classification. In\nneuroimaging, these features could help to understand, classify, and possibly\nprevent brain disorders. Model introspection of highly performant\noverparameterized deep learning (DL) models could help find these features and\nrelations. However, to achieve high-performance level DL models require\nnumerous labeled training samples ($n$) rarely available in many fields. This\npaper presents a pre-training method involving graph convolutional\/neural\nnetworks (GCNs\/GNNs), based on maximizing mutual information between two\nhigh-level embeddings of an input sample. Many of the recently proposed\npre-training methods pre-train one of many possible networks of an\narchitecture. Since almost every DL model is an ensemble of multiple networks,\nwe take our high-level embeddings from two different networks of a model --a\nconvolutional and a graph network--. The learned high-level graph latent\nrepresentations help increase performance for downstream graph classification\ntasks and bypass the need for a high number of labeled data samples. We apply\nour method to a neuroimaging dataset for classifying subjects into healthy\ncontrol (HC) and schizophrenia (SZ) groups. Our experiments show that the\npre-trained model significantly outperforms the non-pre-trained model and\nrequires $50\\%$ less data for similar performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.13526,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000093049,
      "text":"An Optimization Framework for Federated Edge Learning\n\n  The optimal design of federated learning (FL) algorithms for solving general\nmachine learning (ML) problems in practical edge computing systems with\nquantized message passing remains an open problem. This paper considers an edge\ncomputing system where the server and workers have possibly different computing\nand communication capabilities and employ quantization before transmitting\nmessages. To explore the full potential of FL in such an edge computing system,\nwe first present a general FL algorithm, namely GenQSGD, parameterized by the\nnumbers of global and local iterations, mini-batch size, and step size\nsequence. Then, we analyze its convergence for an arbitrary step size sequence\nand specify the convergence results under three commonly adopted step size\nrules, namely the constant, exponential, and diminishing step size rules. Next,\nwe optimize the algorithm parameters to minimize the energy cost under the time\nconstraint and convergence error constraint, with the focus on the overall\nimplementing process of FL. Specifically, for any given step size sequence\nunder each considered step size rule, we optimize the numbers of global and\nlocal iterations and mini-batch size to optimally implement FL for applications\nwith preset step size sequences. We also optimize the step size sequence along\nwith these algorithm parameters to explore the full potential of FL. The\nresulting optimization problems are challenging non-convex problems with\nnon-differentiable constraint functions. We propose iterative algorithms to\nobtain KKT points using general inner approximation (GIA) and tricks for\nsolving complementary geometric programming (CGP). Finally, we numerically\ndemonstrate the remarkable gains of GenQSGD with optimized algorithm parameters\nover existing FL algorithms and reveal the significance of optimally designing\ngeneral FL algorithms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.01915,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"Decision Support Models for Predicting and Explaining Airport Passenger\n  Connectivity from Data\n\n  Predicting if passengers in a connecting flight will lose their connection is\nparamount for airline profitability. We present novel machine learning-based\ndecision support models for the different stages of connection flight\nmanagement, namely for strategic, pre-tactical, tactical and post-operations.\nWe predict missed flight connections in an airline's hub airport using\nhistorical data on flights and passengers, and analyse the factors that\ncontribute additively to the predicted outcome for each decision horizon. Our\ndata is high-dimensional, heterogeneous, imbalanced and noisy, and does not\ninform about passenger arrival\/departure transit time. We employ probabilistic\nencoding of categorical classes, data balancing with Gaussian Mixture Models,\nand boosting. For all planning horizons, our models attain an AUC of the ROC\nhigher than 0.93. SHAP value explanations of our models indicate that\nscheduled\/perceived connection times contribute the most to the prediction,\nfollowed by passenger age and whether border controls are required.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.02994,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Towards an Understanding of Default Policies in Multitask Policy\n  Optimization\n\n  Much of the recent success of deep reinforcement learning has been driven by\nregularized policy optimization (RPO) algorithms with strong performance across\nmultiple domains. In this family of methods, agents are trained to maximize\ncumulative reward while penalizing deviation in behavior from some reference,\nor default policy. In addition to empirical success, there is a strong\ntheoretical foundation for understanding RPO methods applied to single tasks,\nwith connections to natural gradient, trust region, and variational approaches.\nHowever, there is limited formal understanding of desirable properties for\ndefault policies in the multitask setting, an increasingly important domain as\nthe field shifts towards training more generally capable agents. Here, we take\na first step towards filling this gap by formally linking the quality of the\ndefault policy to its effect on optimization. Using these results, we then\nderive a principled RPO algorithm for multitask learning with strong\nperformance guarantees.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.03476,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000309613,
      "text":"A Variational U-Net for Weather Forecasting\n\n  Not only can discovering patterns and insights from atmospheric data enable\nmore accurate weather predictions, but it may also provide valuable information\nto help tackle climate change. Weather4cast is an open competition that aims to\nevaluate machine learning algorithms' capability to predict future atmospheric\nstates. Here, we describe our third-place solution to Weather4cast. We present\na novel Variational U-Net that combines a Variational Autoencoder's ability to\nconsider the probabilistic nature of data with a U-Net's ability to recover\nfine-grained details. This solution is an evolution from our fourth-place\nsolution to Traffic4cast 2020 with many commonalities, suggesting its\napplicability to vastly different domains, such as weather and traffic.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.08164,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.4028320312,
      "text":"A Survey on Neural-symbolic Learning Systems\n\n  In recent years, neural systems have demonstrated highly effective learning\nability and superior perception intelligence. However, they have been found to\nlack effective reasoning and cognitive ability. On the other hand, symbolic\nsystems exhibit exceptional cognitive intelligence but suffer from poor\nlearning capabilities when compared to neural systems. Recognizing the\nadvantages and disadvantages of both methodologies, an ideal solution emerges:\ncombining neural systems and symbolic systems to create neural-symbolic\nlearning systems that possess powerful perception and cognition. The purpose of\nthis paper is to survey the advancements in neural-symbolic learning systems\nfrom four distinct perspectives: challenges, methods, applications, and future\ndirections. By doing so, this research aims to propel this emerging field\nforward, offering researchers a comprehensive and holistic overview. This\noverview will not only highlight the current state-of-the-art but also identify\npromising avenues for future research.\n",
      "prediction":"Possibly AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.07046,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Iterative Training: Finding Binary Weight Deep Neural Networks with\n  Layer Binarization\n\n  In low-latency or mobile applications, lower computation complexity, lower\nmemory footprint and better energy efficiency are desired. Many prior works\naddress this need by removing redundant parameters. Parameter quantization\nreplaces floating-point arithmetic with lower precision fixed-point arithmetic,\nfurther reducing complexity.\n  Typical training of quantized weight neural networks starts from fully\nquantized weights. Quantization creates random noise. As a way to compensate\nfor this noise, during training, we propose to quantize some weights while\nkeeping others in floating-point precision. A deep neural network has many\nlayers. To arrive at a fully quantized weight network, we start from one\nquantized layer and then quantize more and more layers. We show that the order\nof layer quantization affects accuracies. Order count is large for deep neural\nnetworks. A sensitivity pre-training is proposed to guide the layer\nquantization order.\n  Recent work in weight binarization replaces weight-input matrix\nmultiplication with additions. We apply the proposed iterative training to\nweight binarization. Our experiments cover fully connected and convolutional\nnetworks on MNIST, CIFAR-10 and ImageNet datasets. We show empirically that,\nstarting from partial binary weights instead of from fully binary ones,\ntraining reaches fully binary weight networks with better accuracies for larger\nand deeper networks. Layer binarization in the forward order results in better\naccuracies. Guided layer binarization can further improve that. The\nimprovements come at a cost of longer training time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.12922,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000085433,
      "text":"Clustering Effect of (Linearized) Adversarial Robust Models\n\n  Adversarial robustness has received increasing attention along with the study\nof adversarial examples. So far, existing works show that robust models not\nonly obtain robustness against various adversarial attacks but also boost the\nperformance in some downstream tasks. However, the underlying mechanism of\nadversarial robustness is still not clear. In this paper, we interpret\nadversarial robustness from the perspective of linear components, and find that\nthere exist some statistical properties for comprehensively robust models.\nSpecifically, robust models show obvious hierarchical clustering effect on\ntheir linearized sub-networks, when removing or replacing all non-linear\ncomponents (e.g., batch normalization, maximum pooling, or activation layers).\nBased on these observations, we propose a novel understanding of adversarial\nrobustness and apply it on more tasks including domain adaption and robustness\nboosting. Experimental evaluations demonstrate the rationality and superiority\nof our proposed clustering strategy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.13297,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Latent Space based Memory Replay for Continual Learning in Artificial\n  Neural Networks\n\n  Memory replay may be key to learning in biological brains, which manage to\nlearn new tasks continually without catastrophically interfering with previous\nknowledge. On the other hand, artificial neural networks suffer from\ncatastrophic forgetting and tend to only perform well on tasks that they were\nrecently trained on. In this work we explore the application of latent space\nbased memory replay for classification using artificial neural networks. We are\nable to preserve good performance in previous tasks by storing only a small\npercentage of the original data in a compressed latent space version.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.09666,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000023511,
      "text":"CCSL: A Causal Structure Learning Method from Multiple Unknown\n  Environments\n\n  Most existing causal structure learning methods assume data collected from\none environment and independent and identically distributed (i.i.d.). In some\ncases, data are collected from different subjects from multiple environments,\nwhich provides more information but might make the data non-identical or\nnon-independent distribution. Some previous efforts try to learn causal\nstructure from this type of data in two independent stages, i.e., first\ndiscovering i.i.d. groups from non-i.i.d. samples, then learning the causal\nstructures from different groups. This straightforward solution ignores the\nintrinsic connections between the two stages, that is both the clustering stage\nand the learning stage should be guided by the same causal mechanism. Towards\nthis end, we propose a unified Causal Cluster Structures Learning (named CCSL)\nmethod for causal discovery from non-i.i.d. data. This method simultaneously\nintegrates the following two tasks: 1) clustering samples of the subjects with\nthe same causal mechanism into different groups; 2) learning causal structures\nfrom the samples within the group. Specifically, for the former, we provide a\nCausality-related Chinese Restaurant Process to cluster samples based on the\nsimilarity of the causal structure; for the latter, we introduce a\nvariational-inference-based approach to learn the causal structures.\nTheoretical results provide identification of the causal model and the\nclustering model under the linear non-Gaussian assumption. Experimental results\non both simulated and real-world data further validate the correctness and\neffectiveness of the proposed method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.15139,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Breaking the Linear Iteration Cost Barrier for Some Well-known\n  Conditional Gradient Methods Using MaxIP Data-structures\n\n  Conditional gradient methods (CGM) are widely used in modern machine\nlearning. CGM's overall running time usually consists of two parts: the number\nof iterations and the cost of each iteration. Most efforts focus on reducing\nthe number of iterations as a means to reduce the overall running time. In this\nwork, we focus on improving the per iteration cost of CGM. The bottleneck step\nin most CGM is maximum inner product search (MaxIP), which requires a linear\nscan over the parameters. In practice, approximate MaxIP data-structures are\nfound to be helpful heuristics. However, theoretically, nothing is known about\nthe combination of approximate MaxIP data-structures and CGM. In this work, we\nanswer this question positively by providing a formal framework to combine the\nlocality sensitive hashing type approximate MaxIP data-structures with CGM\nalgorithms. As a result, we show the first algorithm, where the cost per\niteration is sublinear in the number of parameters, for many fundamental\noptimization algorithms, e.g., Frank-Wolfe, Herding algorithm, and policy\ngradient.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.04728,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Reducing Catastrophic Forgetting in Self Organizing Maps with\n  Internally-Induced Generative Replay\n\n  A lifelong learning agent is able to continually learn from potentially\ninfinite streams of pattern sensory data. One major historic difficulty in\nbuilding agents that adapt in this way is that neural systems struggle to\nretain previously-acquired knowledge when learning from new samples. This\nproblem is known as catastrophic forgetting (interference) and remains an\nunsolved problem in the domain of machine learning to this day. While\nforgetting in the context of feedforward networks has been examined extensively\nover the decades, far less has been done in the context of alternative\narchitectures such as the venerable self-organizing map (SOM), an unsupervised\nneural model that is often used in tasks such as clustering and dimensionality\nreduction. Although the competition among its internal neurons might carry the\npotential to improve memory retention, we observe that a fixed-sized SOM\ntrained on task incremental data, i.e., it receives data points related to\nspecific classes at certain temporal increments, experiences significant\nforgetting. In this study, we propose the continual SOM (c-SOM), a model that\nis capable of reducing its own forgetting when processing information.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.01998,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"Application of Machine Learning in understanding plant virus\n  pathogenesis: Trends and perspectives on emergence, diagnosis, host-virus\n  interplay and management\n\n  Inclusion of high throughput technologies in the field of biology has\ngenerated massive amounts of biological data in the recent years. Now,\ntransforming these huge volumes of data into knowledge is the primary challenge\nin computational biology. The traditional methods of data analysis have failed\nto carry out the task. Hence, researchers are turning to machine learning based\napproaches for the analysis of high-dimensional big data. In machine learning,\nonce a model is trained with a training dataset, it can be applied on a testing\ndataset which is independent. In current times, deep learning algorithms\nfurther promote the application of machine learning in several field of biology\nincluding plant virology. Considering a significant progress in the application\nof machine learning in understanding plant virology, this review highlights an\nintroductory note on machine learning and comprehensively discusses the trends\nand prospects of machine learning in diagnosis of viral diseases, understanding\nhost-virus interplay and emergence of plant viruses.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.12549,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"Combining Minkowski and Chebyshev: New distance proposal and survey of distance metrics using k-nearest neighbours classifier\n\nThis work proposes a distance that combines Minkowski and Chebyshev distances and can be seen as an intermediary distance. This combination not only achieves efficient run times in neighbourhood iteration tasks in Z^2, but also obtains good accuracies when coupled with the k-Nearest Neighbours (k-NN) classifier. The proposed distance is approximately 1.3 times faster than Manhattan distance and 329.5 times faster than Euclidean distance in discrete neighbourhood iterations. An accuracy analysis of the k-NN classifier using a total of 33 datasets from the UCI repository, 15 distances and values assigned to k that vary from 1 to 200 is presented. In this experiment, the proposed distance obtained accuracies that were better than the average more often than its counterparts (in 26 cases out of 33), and also obtained the best accuracy more frequently (in 9 out of 33 cases).",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.05393,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000053644,
      "text":"A Self-supervised Mixed-curvature Graph Neural Network\n\n  Graph representation learning received increasing attentions in recent years.\nMost of existing methods ignore the complexity of the graph structures and\nrestrict graphs in a single constant-curvature representation space, which is\nonly suitable to particular kinds of graph structure indeed. Additionally,\nthese methods follow the supervised or semi-supervised learning paradigm, and\nthereby notably limit their deployment on the unlabeled graphs in real\napplications. To address these aforementioned limitations, we take the first\nattempt to study the self-supervised graph representation learning in the\nmixed-curvature spaces. In this paper, we present a novel Self-supervised\nMixed-curvature Graph Neural Network (SelfMGNN). Instead of working on one\nsingle constant-curvature space, we construct a mixed-curvature space via the\nCartesian product of multiple Riemannian component spaces and design\nhierarchical attention mechanisms for learning and fusing the representations\nacross these component spaces. To enable the self-supervisd learning, we\npropose a novel dual contrastive approach. The mixed-curvature Riemannian space\nactually provides multiple Riemannian views for the contrastive learning. We\nintroduce a Riemannian projector to reveal these views, and utilize a\nwell-designed Riemannian discriminator for the single-view and cross-view\ncontrastive learning within and across the Riemannian views. Finally, extensive\nexperiments show that SelfMGNN captures the complicated graph structures in\nreality and outperforms state-of-the-art baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.01021,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000142058,
      "text":"Fighting Fire with Fire: Contrastive Debiasing without Bias-free Data\n  via Generative Bias-transformation\n\n  Deep neural networks (DNNs), despite their impressive ability to generalize\nover-capacity networks, often rely heavily on malignant bias as shortcuts\ninstead of task-related information for discriminative tasks. To address this\nproblem, recent studies utilize auxiliary information related to the bias,\nwhich is rarely obtainable in practice, or sift through a handful of bias-free\nsamples for debiasing. However, the success of these methods is not always\nguaranteed due to the unfulfilled presumptions. In this paper, we propose a\nnovel method, Contrastive Debiasing via Generative Bias-transformation (CDvG),\nwhich works without explicit bias labels or bias-free samples. Motivated by our\nobservation that not only discriminative models but also image translation\nmodels tend to focus on the malignant bias, CDvG employs an image translation\nmodel to transform one bias mode into another while preserving the\ntask-relevant information. Additionally, the bias-transformed views are set\nagainst each other through contrastive learning to learn bias-invariant\nrepresentations. Our method demonstrates superior performance compared to prior\napproaches, especially when bias-free samples are scarce or absent.\nFurthermore, CDvG can be integrated with the methods that focus on bias-free\nsamples in a plug-and-play manner for additional enhancements, as demonstrated\nby diverse experimental results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.06672,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"Tree-Based Dynamic Classifier Chains\n\n  Classifier chains are an effective technique for modeling label dependencies\nin multi-label classification. However, the method requires a fixed, static\norder of the labels. While in theory, any order is sufficient, in practice,\nthis order has a substantial impact on the quality of the final prediction.\nDynamic classifier chains denote the idea that for each instance to classify,\nthe order in which the labels are predicted is dynamically chosen. The\ncomplexity of a naive implementation of such an approach is prohibitive,\nbecause it would require to train a sequence of classifiers for every possible\npermutation of the labels. To tackle this problem efficiently, we propose a new\napproach based on random decision trees which can dynamically select the label\nordering for each prediction. We show empirically that a dynamic selection of\nthe next label improves over the use of a static ordering under an otherwise\nunchanged random decision tree model. % and experimental environment. In\naddition, we also demonstrate an alternative approach based on extreme gradient\nboosted trees, which allows for a more target-oriented training of dynamic\nclassifier chains. Our results show that this variant outperforms random\ndecision trees and other tree-based multi-label classification methods. More\nimportantly, the dynamic selection strategy allows to considerably speed up\ntraining and prediction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.00275,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000045035,
      "text":"Learning from Mistakes based on Class Weighting with Application to\n  Neural Architecture Search\n\n  Learning from mistakes is an effective learning approach widely used in human\nlearning, where a learner pays greater focus on mistakes to circumvent them in\nthe future to improve the overall learning outcomes. In this work, we aim to\ninvestigate how effectively we can leverage this exceptional learning ability\nto improve machine learning models. We propose a simple and effective\nmulti-level optimization framework called learning from mistakes using class\nweighting (LFM-CW), inspired by mistake-driven learning to train better machine\nlearning models. In this formulation, the primary objective is to train a model\nto perform effectively on target tasks by using a re-weighting technique. We\nlearn the class weights by minimizing the validation loss of the model and\nre-train the model with the synthetic data from the image generator weighted by\nclass-wise performance and real data. We apply our LFM-CW framework with\ndifferential architecture search methods on image classification datasets such\nas CIFAR and ImageNet, where the results show that our proposed strategy\nachieves lower error rate than the baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.01254,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000025332,
      "text":"Hierarchical Learning to Solve Partial Differential Equations Using\n  Physics-Informed Neural Networks\n\n  The neural network-based approach to solving partial differential equations\nhas attracted considerable attention due to its simplicity and flexibility in\nrepresenting the solution of the partial differential equation. In training a\nneural network, the network learns global features corresponding to\nlow-frequency components while high-frequency components are approximated at a\nmuch slower rate. For a class of equations in which the solution contains a\nwide range of scales, the network training process can suffer from slow\nconvergence and low accuracy due to its inability to capture the high-frequency\ncomponents. In this work, we propose a hierarchical approach to improve the\nconvergence rate and accuracy of the neural network solution to partial\ndifferential equations. The proposed method comprises multi-training levels in\nwhich a newly introduced neural network is guided to learn the residual of the\nprevious level approximation. By the nature of neural networks' training\nprocess, the high-level correction is inclined to capture the high-frequency\ncomponents. We validate the efficiency and robustness of the proposed\nhierarchical approach through a suite of linear and nonlinear partial\ndifferential equations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.0296,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000007285,
      "text":"Two Wrongs Don't Make a Right: Combating Confirmation Bias in Learning\n  with Label Noise\n\n  Noisy labels damage the performance of deep networks. For robust learning, a\nprominent two-stage pipeline alternates between eliminating possible incorrect\nlabels and semi-supervised training. However, discarding part of noisy labels\ncould result in a loss of information, especially when the corruption has a\ndependency on data, e.g., class-dependent or instance-dependent. Moreover, from\nthe training dynamics of a representative two-stage method DivideMix, we\nidentify the domination of confirmation bias: pseudo-labels fail to correct a\nconsiderable amount of noisy labels, and consequently, the errors accumulate.\nTo sufficiently exploit information from noisy labels and mitigate wrong\ncorrections, we propose Robust Label Refurbishment (Robust LR) a new hybrid\nmethod that integrates pseudo-labeling and confidence estimation techniques to\nrefurbish noisy labels. We show that our method successfully alleviates the\ndamage of both label noise and confirmation bias. As a result, it achieves\nstate-of-the-art performance across datasets and noise types, namely CIFAR\nunder different levels of synthetic noise and Mini-WebVision and ANIMAL-10N\nwith real-world noise.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.00362,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Dimensionality Reduction for Categorical Data\n\n  Categorical attributes are those that can take a discrete set of values,\ne.g., colours. This work is about compressing vectors over categorical\nattributes to low-dimension discrete vectors. The current hash-based methods\ncompressing vectors over categorical attributes to low-dimension discrete\nvectors do not provide any guarantee on the Hamming distances between the\ncompressed representations. Here we present FSketch to create sketches for\nsparse categorical data and an estimator to estimate the pairwise Hamming\ndistances among the uncompressed data only from their sketches. We claim that\nthese sketches can be used in the usual data mining tasks in place of the\noriginal data without compromising the quality of the task. For that, we ensure\nthat the sketches also are categorical, sparse, and the Hamming distance\nestimates are reasonably precise. Both the sketch construction and the Hamming\ndistance estimation algorithms require just a single-pass; furthermore, changes\nto a data point can be incorporated into its sketch in an efficient manner. The\ncompressibility depends upon how sparse the data is and is independent of the\noriginal dimension -- making our algorithm attractive for many real-life\nscenarios. Our claims are backed by rigorous theoretical analysis of the\nproperties of FSketch and supplemented by extensive comparative evaluations\nwith related algorithms on some real-world datasets. We show that FSketch is\nsignificantly faster, and the accuracy obtained by using its sketches are among\nthe top for the standard unsupervised tasks of RMSE, clustering and similarity\nsearch.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.10878,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000039736,
      "text":"Enabling NAS with Automated Super-Network Generation\n\n  Recent Neural Architecture Search (NAS) solutions have produced impressive\nresults training super-networks and then deriving subnetworks, a.k.a. child\nmodels that outperform expert-crafted models from a pre-defined search space.\nEfficient and robust subnetworks can be selected for resource-constrained edge\ndevices, allowing them to perform well in the wild. However, constructing\nsuper-networks for arbitrary architectures is still a challenge that often\nprevents the adoption of these approaches. To address this challenge, we\npresent BootstrapNAS, a software framework for automatic generation of\nsuper-networks for NAS. BootstrapNAS takes a pre-trained model from a popular\narchitecture, e.g., ResNet- 50, or from a valid custom design, and\nautomatically creates a super-network out of it, then uses state-of-the-art NAS\ntechniques to train the super-network, resulting in subnetworks that\nsignificantly outperform the given pre-trained model. We demonstrate the\nsolution by generating super-networks from arbitrary model repositories and\nmake available the resulting super-networks for reproducibility of the results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.0666,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"Subspace Decomposition based DNN algorithm for elliptic type multi-scale\n  PDEs\n\n  While deep learning algorithms demonstrate a great potential in scientific\ncomputing, its application to multi-scale problems remains to be a big\nchallenge. This is manifested by the \"frequency principle\" that neural networks\ntend to learn low frequency components first. Novel architectures such as\nmulti-scale deep neural network (MscaleDNN) were proposed to alleviate this\nproblem to some extent. In this paper, we construct a subspace decomposition\nbased DNN (dubbed SD$^2$NN) architecture for a class of multi-scale problems by\ncombining traditional numerical analysis ideas and MscaleDNN algorithms. The\nproposed architecture includes one low frequency normal DNN submodule, and one\n(or a few) high frequency MscaleDNN submodule(s), which are designed to capture\nthe smooth part and the oscillatory part of the multi-scale solutions,\nrespectively. In addition, a novel trigonometric activation function is\nincorporated in the SD$^2$NN model. We demonstrate the performance of the\nSD$^2$NN architecture through several benchmark multi-scale problems in regular\nor irregular geometric domains. Numerical results show that the SD$^2$NN model\nis superior to existing models such as MscaleDNN.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.13922,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Predicting Breakdown Risk Based on Historical Maintenance Data for Air\n  Force Ground Vehicles\n\n  Unscheduled maintenance has contributed to longer downtime for vehicles and\nincreased costs for Logistic Readiness Squadrons (LRSs) in the Air Force. When\nvehicles are in need of repair outside of their scheduled time, depending on\ntheir priority level, the entire squadron's slated repair schedule is\ntransformed negatively. The repercussions of unscheduled maintenance are\nspecifically seen in the increase of man hours required to maintain vehicles\nthat should have been working well: this can include more man hours spent on\nmaintenance itself, waiting for parts to arrive, hours spent re-organizing the\nrepair schedule, and more. The dominant trend in the current maintenance system\nat LRSs is that they do not have predictive maintenance infrastructure to\ncounteract the influx of unscheduled repairs they experience currently, and as\na result, their readiness and performance levels are lower than desired.\n  We use data pulled from the Defense Property and Accountability System\n(DPAS), that the LRSs currently use to store their vehicle maintenance\ninformation. Using historical vehicle maintenance data we receive from DPAS, we\napply three different algorithms independently to construct an accurate\npredictive system to optimize maintenance schedules at any given time. Through\nthe application of Logistics Regression, Random Forest, and Gradient Boosted\nTrees algorithms, we found that a Logistic Regression algorithm, fitted to our\ndata, produced the most accurate results. Our findings indicate that not only\nwould continuing the use of Logistic Regression be prudent for our research\npurposes, but that there is opportunity to further tune and optimize our\nLogistic Regression model for higher accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.10588,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"CGAN-EB: A Non-parametric Empirical Bayes Method for Crash Hotspot\n  Identification Using Conditional Generative Adversarial Networks: A\n  Real-world Crash Data Study\n\n  The empirical Bayes (EB) method based on parametric statistical models such\nas the negative binomial (NB) has been widely used for ranking sites in road\nnetwork safety screening process. This paper is the continuation of the authors\nprevious research, where a novel non-parametric EB method for modelling crash\nfrequency data data based on Conditional Generative Adversarial Networks (CGAN)\nwas proposed and evaluated over several simulated crash data sets. Unlike\nparametric approaches, there is no need for a pre-specified underlying\nrelationship between dependent and independent variables in the proposed\nCGAN-EB and they are able to model any types of distributions. The proposed\nmethodology is now applied to a real-world data set collected for road segments\nfrom 2012 to 2017 in Washington State. The performance of CGAN-EB in terms of\nmodel fit, predictive performance and network screening outcomes is compared\nwith the conventional approach (NB-EB) as a benchmark. The results indicate\nthat the proposed CGAN-EB approach outperforms NB-EB in terms of prediction\npower and hotspot identification tests.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.13502,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Deep Treatment-Adaptive Network for Causal Inference\n\n  Causal inference is capable of estimating the treatment effect (i.e., the\ncausal effect of treatment on the outcome) to benefit the decision making in\nvarious domains. One fundamental challenge in this research is that the\ntreatment assignment bias in observational data. To increase the validity of\nobservational studies on causal inference, representation based methods as the\nstate-of-the-art have demonstrated the superior performance of treatment effect\nestimation. Most representation based methods assume all observed covariates\nare pre-treatment (i.e., not affected by the treatment), and learn a balanced\nrepresentation from these observed covariates for estimating treatment effect.\nUnfortunately, this assumption is often too strict a requirement in practice,\nas some covariates are changed by doing an intervention on treatment (i.e.,\npost-treatment). By contrast, the balanced representation learned from\nunchanged covariates thus biases the treatment effect estimation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.03461,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000005232,
      "text":"GraphPAS: Parallel Architecture Search for Graph Neural Networks\n\n  Graph neural architecture search has received a lot of attention as Graph\nNeural Networks (GNNs) has been successfully applied on the non-Euclidean data\nrecently. However, exploring all possible GNNs architectures in the huge search\nspace is too time-consuming or impossible for big graph data. In this paper, we\npropose a parallel graph architecture search (GraphPAS) framework for graph\nneural networks. In GraphPAS, we explore the search space in parallel by\ndesigning a sharing-based evolution learning, which can improve the search\nefficiency without losing the accuracy. Additionally, architecture information\nentropy is adopted dynamically for mutation selection probability, which can\nreduce space exploration. The experimental result shows that GraphPAS\noutperforms state-of-art models with efficiency and accuracy simultaneously.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.006,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Towards Futuristic Autonomous Experimentation--A Surprise-Reacting\n  Sequential Experiment Policy\n\n  An autonomous experimentation platform in manufacturing is supposedly capable\nof conducting a sequential search for finding suitable manufacturing conditions\nby itself or even for discovering new materials with minimal human\nintervention. The core of the intelligent control of such platforms is a policy\nto decide where to conduct the next experiment based on what has been done thus\nfar. Such policy inevitably trades off between exploitation and exploration.\nCurrently, the prevailing approach is to use various acquisition functions in\nthe Bayesian optimization framework. We discuss whether it is beneficial to\ntrade off exploitation versus exploration by measuring the element and degree\nof surprise associated with the immediate past observation. We devise a\nsurprise-reacting policy using two existing surprise metrics, known as the\nShannon surprise and Bayesian surprise. Our analysis shows that the\nsurprise-reacting policy appears to be better suited for quickly characterizing\nthe overall landscape of a response surface under resource constraints. We do\nnot claim that we have a fully autonomous experimentation system but believe\nthat the surprise-reacting capability benefits the automation of sequential\ndecisions in autonomous experimentation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.0089,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000001457,
      "text":"Counterfactual Explanations via Latent Space Projection and\n  Interpolation\n\n  Counterfactual explanations represent the minimal change to a data sample\nthat alters its predicted classification, typically from an unfavorable initial\nclass to a desired target class. Counterfactuals help answer questions such as\n\"what needs to change for this application to get accepted for a loan?\". A\nnumber of recently proposed approaches to counterfactual generation give\nvarying definitions of \"plausible\" counterfactuals and methods to generate\nthem. However, many of these methods are computationally intensive and provide\nunconvincing explanations. Here we introduce SharpShooter, a method for binary\nclassification that starts by creating a projected version of the input that\nclassifies as the target class. Counterfactual candidates are then generated in\nlatent space on the interpolation line between the input and its projection. We\nthen demonstrate that our framework translates core characteristics of a sample\nto its counterfactual through the use of learned representations. Furthermore,\nwe show that SharpShooter is competitive across common quality metrics on\ntabular and image datasets while being orders of magnitude faster than two\ncomparable methods and excels at measures of realism, making it well-suited for\nhigh velocity machine learning applications which require timely explanations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.15072,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Empirical Evaluation of Deep Learning Models for Knowledge Tracing: Of\n  Hyperparameters and Metrics on Performance and Replicability\n\n  We review and evaluate a body of deep learning knowledge tracing (DLKT)\nmodels with openly available and widely-used data sets, and with a novel data\nset of students learning to program. The evaluated knowledge tracing models\ninclude Vanilla-DKT, two Long Short-Term Memory Deep Knowledge Tracing\n(LSTM-DKT) variants, two Dynamic Key-Value Memory Network (DKVMN) variants, and\nSelf-Attentive Knowledge Tracing (SAKT). As baselines, we evaluate simple\nnon-learning models, logistic regression and Bayesian Knowledge Tracing (BKT).\nTo evaluate how different aspects of DLKT models influence model performance,\nwe test input and output layer variations found in the compared models that are\nindependent of the main architectures. We study maximum attempt count options,\nincluding filtering out long attempt sequences, that have been implicitly and\nexplicitly used in prior studies. We contrast the observed performance\nvariations against variations from non-model properties such as randomness and\nhardware. Performance of models is assessed using multiple metrics, whereby we\nalso contrast the impact of the choice of metric on model performance. The key\ncontributions of this work are: Evidence that DLKT models generally outperform\nmore traditional models, but not necessarily by much and not always; Evidence\nthat even simple baselines with little to no predictive value may outperform\nDLKT models, especially in terms of accuracy -- highlighting importance of\nselecting proper baselines for comparison; Disambiguation of properties that\naffect performance in DLKT models including metric choice, input and output\nlayer variations, common hyperparameters, random seeding and hardware;\nDiscussion of issues in replicability when evaluating DLKT models, including\ndiscrepancies in prior reported results and methodology. Model implementations,\nevaluation code, and data are published as a part of this work.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.02301,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Adaptive label thresholding methods for online multi-label\n  classification\n\n  Existing online multi-label classification works cannot well handle the\nonline label thresholding problem and lack the regret analysis for their online\nalgorithms. This paper proposes a novel framework of adaptive label\nthresholding algorithms for online multi-label classification, with the aim to\novercome the drawbacks of existing methods. The key feature of our framework is\nthat both scoring and thresholding models are included as important components\nof the online multi-label classifier and are incorporated into one online\noptimization problem. Further, in order to establish the relationship between\nscoring and thresholding models, a novel multi-label classification loss\nfunction is derived, which measures to what an extent the multi-label\nclassifier can distinguish between relevant labels and irrelevant ones for an\nincoming instance. Based on this new framework and loss function, we present a\nfirst-order linear algorithm and a second-order one, which both enjoy closed\nform update, but rely on different techniques for updating the multi-label\nclassifier. Both algorithms are proved to achieve a sub-linear regret. Using\nMercer kernels, our first-order algorithm has been extended to deal with\nnonlinear multi-label prediction tasks. Experiments show the advantage of our\nlinear and nonlinear algorithms, in terms of various multi-label performance\nmetrics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.09069,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Neuronal Correlation: a Central Concept in Neural Network\n\n  This paper proposes to study neural networks through neuronal correlation, a\nstatistical measure of correlated neuronal activity on the penultimate layer.\nWe show that neuronal correlation can be efficiently estimated via weight\nmatrix, can be effectively enforced through layer structure, and is a strong\nindicator of generalisation ability of the network. More importantly, we show\nthat neuronal correlation significantly impacts on the accuracy of entropy\nestimation in high-dimensional hidden spaces. While previous estimation methods\nmay be subject to significant inaccuracy due to implicit assumption on neuronal\nindependence, we present a novel computational method to have an efficient and\nauthentic computation of entropy, by taking into consideration the neuronal\ncorrelation. In doing so, we install neuronal correlation as a central concept\nof neural network.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.08186,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Conditional Generation of Medical Time Series for Extrapolation to\n  Underrepresented Populations\n\n  The widespread adoption of electronic health records (EHRs) and subsequent\nincreased availability of longitudinal healthcare data has led to significant\nadvances in our understanding of health and disease with direct and immediate\nimpact on the development of new diagnostics and therapeutic treatment options.\nHowever, access to EHRs is often restricted due to their perceived sensitive\nnature and associated legal concerns, and the cohorts therein typically are\nthose seen at a specific hospital or network of hospitals and therefore not\nrepresentative of the wider population of patients. Here, we present HealthGen,\na new approach for the conditional generation of synthetic EHRs that maintains\nan accurate representation of real patient characteristics, temporal\ninformation and missingness patterns. We demonstrate experimentally that\nHealthGen generates synthetic cohorts that are significantly more faithful to\nreal patient EHRs than the current state-of-the-art, and that augmenting real\ndata sets with conditionally generated cohorts of underrepresented\nsubpopulations of patients can significantly enhance the generalisability of\nmodels derived from these data sets to different patient populations. Synthetic\nconditionally generated EHRs could help increase the accessibility of\nlongitudinal healthcare data sets and improve the generalisability of\ninferences made from these data sets to underrepresented populations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.05336,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000070201,
      "text":"IDEA: Interpretable Dynamic Ensemble Architecture for Time Series\n  Prediction\n\n  We enhance the accuracy and generalization of univariate time series point\nprediction by an explainable ensemble on the fly. We propose an Interpretable\nDynamic Ensemble Architecture (IDEA), in which interpretable base learners give\npredictions independently with sparse communication as a group. The model is\ncomposed of several sequentially stacked groups connected by group backcast\nresiduals and recurrent input competition. Ensemble driven by end-to-end\ntraining both horizontally and vertically brings state-of-the-art (SOTA)\nperformances. Forecast accuracy improves by 2.6% over the best statistical\nbenchmark on the TOURISM dataset and 2% over the best deep learning benchmark\non the M4 dataset. The architecture enjoys several advantages, being applicable\nto time series from various domains, explainable to users with specialized\nmodular structure and robust to changes in task distribution.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.12204,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"From data to functa: Your data point is a function and you can treat it\n  like one\n\n  It is common practice in deep learning to represent a measurement of the\nworld on a discrete grid, e.g. a 2D grid of pixels. However, the underlying\nsignal represented by these measurements is often continuous, e.g. the scene\ndepicted in an image. A powerful continuous alternative is then to represent\nthese measurements using an implicit neural representation, a neural function\ntrained to output the appropriate measurement value for any input spatial\nlocation. In this paper, we take this idea to its next level: what would it\ntake to perform deep learning on these functions instead, treating them as\ndata? In this context we refer to the data as functa, and propose a framework\nfor deep learning on functa. This view presents a number of challenges around\nefficient conversion from data to functa, compact representation of functa, and\neffectively solving downstream tasks on functa. We outline a recipe to overcome\nthese challenges and apply it to a wide range of data modalities including\nimages, 3D shapes, neural radiance fields (NeRF) and data on manifolds. We\ndemonstrate that this approach has various compelling properties across data\nmodalities, in particular on the canonical tasks of generative modeling, data\nimputation, novel view synthesis and classification. Code:\nhttps:\/\/github.com\/deepmind\/functa\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.13357,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000028147,
      "text":"DNS: Determinantal Point Process Based Neural Network Sampler for\n  Ensemble Reinforcement Learning\n\n  Application of ensemble of neural networks is becoming an imminent tool for\nadvancing the state-of-the-art in deep reinforcement learning algorithms.\nHowever, training these large numbers of neural networks in the ensemble has an\nexceedingly high computation cost which may become a hindrance in training\nlarge-scale systems. In this paper, we propose DNS: a Determinantal Point\nProcess based Neural Network Sampler that specifically uses k-dpp to sample a\nsubset of neural networks for backpropagation at every training step thus\nsignificantly reducing the training time and computation cost. We integrated\nDNS in REDQ for continuous control tasks and evaluated on MuJoCo environments.\nOur experiments show that DNS augmented REDQ outperforms baseline REDQ in terms\nof average cumulative reward and achieves this using less than 50% computation\nwhen measured in FLOPS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.13403,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Vibration Fault Diagnosis in Wind Turbines based on Automated Feature\n  Learning\n\n  A growing number of wind turbines are equipped with vibration measurement\nsystems to enable a close monitoring and early detection of developing fault\nconditions. The vibration measurements are analyzed to continuously assess the\ncomponent health and prevent failures that can result in downtimes. This study\nfocuses on gearbox monitoring but is applicable also to other subsystems. The\ncurrent state-of-the-art gearbox fault diagnosis algorithms rely on statistical\nor machine learning methods based on fault signatures that have been defined by\nhuman analysts. This has multiple disadvantages. Defining the fault signatures\nby human analysts is a time-intensive process that requires highly detailed\nknowledge of the gearbox composition. This effort needs to be repeated for\nevery new turbine, so it does not scale well with the increasing number of\nmonitored turbines, especially in fast growing portfolios. Moreover, fault\nsignatures defined by human analysts can result in biased and imprecise\ndecision boundaries that lead to imprecise and uncertain fault diagnosis\ndecisions. We present a novel accurate fault diagnosis method for\nvibration-monitored wind turbine components that overcomes these disadvantages.\nOur approach combines autonomous data-driven learning of fault signatures and\nhealth state classification based on convolutional neural networks and\nisolation forests. We demonstrate its performance with vibration measurements\nfrom two wind turbine gearboxes. Unlike the state-of-the-art methods, our\napproach does not require gearbox-type specific diagnosis expertise and is not\nrestricted to predefined frequencies or spectral ranges but can monitor the\nfull spectrum at once.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.0015,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"Learning Infinite-Horizon Average-Reward Markov Decision Processes with\n  Constraints\n\n  We study regret minimization for infinite-horizon average-reward Markov\nDecision Processes (MDPs) under cost constraints. We start by designing a\npolicy optimization algorithm with carefully designed action-value estimator\nand bonus term, and show that for ergodic MDPs, our algorithm ensures\n$\\widetilde{O}(\\sqrt{T})$ regret and constant constraint violation, where $T$\nis the total number of time steps. This strictly improves over the algorithm of\n(Singh et al., 2020), whose regret and constraint violation are both\n$\\widetilde{O}(T^{2\/3})$. Next, we consider the most general class of weakly\ncommunicating MDPs. Through a finite-horizon approximation, we develop another\nalgorithm with $\\widetilde{O}(T^{2\/3})$ regret and constraint violation, which\ncan be further improved to $\\widetilde{O}(\\sqrt{T})$ via a simple modification,\nalbeit making the algorithm computationally inefficient. As far as we know,\nthese are the first set of provable algorithms for weakly communicating MDPs\nwith cost constraints.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.0699,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Knock Detection in Combustion Engine Time Series Using a Theory-Guided\n  1D Convolutional Neural Network Approach\n\n  This paper introduces a method for the detection of knock occurrences in an\ninternal combustion engine (ICE) using a 1D convolutional neural network\ntrained on in-cylinder pressure data. The model architecture was based on\nconsiderations regarding the expected frequency characteristics of knocking\ncombustion. To aid the feature extraction, all cycles were reduced to 60{\\deg}\nCA long windows, with no further processing applied to the pressure traces. The\nneural networks were trained exclusively on in-cylinder pressure traces from\nmultiple conditions and labels provided by human experts. The best-performing\nmodel architecture achieves an accuracy of above 92% on all test sets in a\ntenfold cross-validation when distinguishing between knocking and non-knocking\ncycles. In a multi-class problem where each cycle was labeled by the number of\nexperts who rated it as knocking, 78% of cycles were labeled perfectly, while\n90% of cycles were classified at most one class from ground truth. They thus\nconsiderably outperform the broadly applied MAPO (Maximum Amplitude of Pressure\nOscillation) detection method, as well as other references reconstructed from\nprevious works. Our analysis indicates that the neural network learned\nphysically meaningful features connected to engine-characteristic resonance\nfrequencies, thus verifying the intended theory-guided data science approach.\nDeeper performance investigation further shows remarkable generalization\nability to unseen operating points. In addition, the model proved to classify\nknocking cycles in unseen engines with increased accuracy of 89% after adapting\nto their features via training on a small number of exclusively non-knocking\ncycles. The algorithm takes below 1 ms (on CPU) to classify individual cycles,\neffectively making it suitable for real-time engine control.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.13019,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000045035,
      "text":"On the Robustness of Quality Measures for GANs\n\n  This work evaluates the robustness of quality measures of generative models\nsuch as Inception Score (IS) and Fr\\'echet Inception Distance (FID). Analogous\nto the vulnerability of deep models against a variety of adversarial attacks,\nwe show that such metrics can also be manipulated by additive pixel\nperturbations. Our experiments indicate that one can generate a distribution of\nimages with very high scores but low perceptual quality. Conversely, one can\noptimize for small imperceptible perturbations that, when added to real world\nimages, deteriorate their scores. We further extend our evaluation to\ngenerative models themselves, including the state of the art network\nStyleGANv2. We show the vulnerability of both the generative model and the FID\nagainst additive perturbations in the latent space. Finally, we show that the\nFID can be robustified by simply replacing the standard Inception with a robust\nInception. We validate the effectiveness of the robustified metric through\nextensive experiments, showing it is more robust against manipulation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.0204,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Deep Fusion of Lead-lag Graphs: Application to Cryptocurrencies\n\n  The study of time series has motivated many researchers, particularly on the\narea of multivariate-analysis. The study of co-movements and dependency between\nrandom variables leads us to develop metrics to describe existing connection\nbetween assets. The most commonly used are correlation and causality. Despite\nthe growing literature, some connections remained still undetected. The\nobjective of this paper is to propose a new representation learning algorithm\ncapable to integrate synchronous and asynchronous relationships.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.12674,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Rewiring with Positional Encodings for Graph Neural Networks\n\n  Several recent works use positional encodings to extend the receptive fields\nof graph neural network (GNN) layers equipped with attention mechanisms. These\ntechniques, however, extend receptive fields to the complete graph, at\nsubstantial computational cost and risking a change in the inductive biases of\nconventional GNNs, or require complex architecture adjustments. As a\nconservative alternative, we use positional encodings to expand receptive\nfields to $r$-hop neighborhoods. More specifically, our method augments the\ninput graph with additional nodes\/edges and uses positional encodings as node\nand\/or edge features. We thus modify graphs before inputting them to a\ndownstream GNN model, instead of modifying the model itself. This makes our\nmethod model-agnostic, i.e., compatible with any of the existing GNN\narchitectures. We also provide examples of positional encodings that are\nlossless with a one-to-one map between the original and the modified graphs. We\ndemonstrate that extending receptive fields via positional encodings and a\nvirtual fully-connected node significantly improves GNN performance and\nalleviates over-squashing using small $r$. We obtain improvements on a variety\nof models and datasets and reach competitive performance using traditional GNNs\nor graph Transformers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.01825,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Planted Dense Subgraphs in Dense Random Graphs Can Be Recovered using\n  Graph-based Machine Learning\n\n  Multiple methods of finding the vertices belonging to a planted dense\nsubgraph in a random dense $G(n, p)$ graph have been proposed, with an emphasis\non planted cliques. Such methods can identify the planted subgraph in\npolynomial time, but are all limited to several subgraph structures. Here, we\npresent PYGON, a graph neural network-based algorithm, which is insensitive to\nthe structure of the planted subgraph. This is the first algorithm that uses\nadvanced learning tools for recovering dense subgraphs. We show that PYGON can\nrecover cliques of sizes $\\Theta\\left(\\sqrt{n}\\right)$, where $n$ is the size\nof the background graph, comparable with the state of the art. We also show\nthat the same algorithm can recover multiple other planted subgraphs of size\n$\\Theta\\left(\\sqrt{n}\\right)$, in both directed and undirected graphs. We\nsuggest a conjecture that no polynomial time PAC-learning algorithm can detect\nplanted dense subgraphs with size smaller than $O\\left(\\sqrt{n}\\right)$, even\nif in principle one could find dense subgraphs of logarithmic size.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.06534,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Logarithmic Continual Learning\n\n  We introduce a neural network architecture that logarithmically reduces the\nnumber of self-rehearsal steps in the generative rehearsal of continually\nlearned models. In continual learning (CL), training samples come in subsequent\ntasks, and the trained model can access only a single task at a time. To replay\nprevious samples, contemporary CL methods bootstrap generative models and train\nthem recursively with a combination of current and regenerated past data. This\nrecurrence leads to superfluous computations as the same past samples are\nregenerated after each task, and the reconstruction quality successively\ndegrades. In this work, we address these limitations and propose a new\ngenerative rehearsal architecture that requires at most logarithmic number of\nretraining for each sample. Our approach leverages allocation of past data in\na~set of generative models such that most of them do not require retraining\nafter a~task. The experimental evaluation of our logarithmic continual learning\napproach shows the superiority of our method with respect to the\nstate-of-the-art generative rehearsal methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.08987,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000352992,
      "text":"Survival Prediction of Children Undergoing Hematopoietic Stem Cell\n  Transplantation Using Different Machine Learning Classifiers by Performing\n  Chi-squared Test and Hyper-parameter Optimization: A Retrospective Analysis\n\n  Bone Marrow Transplant, a gradational rescue for a wide range of disorders\nemanating from the bone marrow, is an efficacious surgical treatment. Several\nrisk factors, such as post-transplant illnesses, new malignancies, and even\norgan damage, can impair long-term survival. Therefore, technologies like\nMachine Learning are deployed for investigating the survival prediction of BMT\nreceivers along with the influences that limit their resilience. In this study,\nan efficient survival classification model is presented in a comprehensive\nmanner, incorporating the Chi-squared feature selection method to address the\ndimensionality problem and Hyper Parameter Optimization (HPO) to increase\naccuracy. A synthetic dataset is generated by imputing the missing values,\ntransforming the data using dummy variable encoding, and compressing the\ndataset from 59 features to the 11 most correlated features using Chi-squared\nfeature selection. The dataset was split into train and test sets at a ratio of\n80:20, and the hyperparameters were optimized using Grid Search\nCross-Validation. Several supervised ML methods were trained in this regard,\nlike Decision Tree, Random Forest, Logistic Regression, K-Nearest Neighbors,\nGradient Boosting Classifier, Ada Boost, and XG Boost. The simulations have\nbeen performed for both the default and optimized hyperparameters by using the\noriginal and reduced synthetic dataset. After ranking the features using the\nChi-squared test, it was observed that the top 11 features with HPO, resulted\nin the same accuracy of prediction (94.73%) as the entire dataset with default\nparameters. Moreover, this approach requires less time and resources for\npredicting the survivability of children undergoing BMT. Hence, the proposed\napproach may aid in the development of a computer-aided diagnostic system with\nsatisfactory accuracy and minimal computation time by utilizing medical data\nrecords.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.1215,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Learning Curves for Decision Making in Supervised Machine Learning: A\n  Survey\n\n  Learning curves are a concept from social sciences that has been adopted in\nthe context of machine learning to assess the performance of a learning\nalgorithm with respect to a certain resource, e.g., the number of training\nexamples or the number of training iterations. Learning curves have important\napplications in several machine learning contexts, most notably in data\nacquisition, early stopping of model training, and model selection. For\ninstance, learning curves can be used to model the performance of the\ncombination of an algorithm and its hyperparameter configuration, providing\ninsights into their potential suitability at an early stage and often\nexpediting the algorithm selection process. Various learning curve models have\nbeen proposed to use learning curves for decision making. Some of these models\nanswer the binary decision question of whether a given algorithm at a certain\nbudget will outperform a certain reference performance, whereas more complex\nmodels predict the entire learning curve of an algorithm. We contribute a\nframework that categorises learning curve approaches using three criteria: the\ndecision-making situation they address, the intrinsic learning curve question\nthey answer and the type of resources they use. We survey papers from the\nliterature and classify them into this framework.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.12843,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000285771,
      "text":"Graph Representation Learning via Aggregation Enhancement\n\n  Graph neural networks (GNNs) have become a powerful tool for processing\ngraph-structured data but still face challenges in effectively aggregating and\npropagating information between layers, which limits their performance. We\ntackle this problem with the kernel regression (KR) approach, using KR loss as\nthe primary loss in self-supervised settings or as a regularization term in\nsupervised settings. We show substantial performance improvements compared to\nstate-of-the-art in both scenarios on multiple transductive and inductive node\nclassification datasets, especially for deep networks. As opposed to mutual\ninformation (MI), KR loss is convex and easy to estimate in high-dimensional\ncases, even though it indirectly maximizes the MI between its inputs. Our work\nhighlights the potential of KR to advance the field of graph representation\nlearning and enhance the performance of GNNs. The code to reproduce our\nexperiments is available at https:\/\/github.com\/Anonymous1252022\/KR_for_GNNs\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.09965,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"Decentralized EM to Learn Gaussian Mixtures from Datasets Distributed by\n  Features\n\n  Expectation Maximization (EM) is the standard method to learn Gaussian\nmixtures. Yet its classic, centralized form is often infeasible, due to privacy\nconcerns and computational and communication bottlenecks. Prior work dealt with\ndata distributed by examples, horizontal partitioning, but we lack a\ncounterpart for data scattered by features, an increasingly common scheme (e.g.\nuser profiling with data from multiple entities). To fill this gap, we provide\nan EM-based algorithm to fit Gaussian mixtures to Vertically Partitioned data\n(VP-EM). In federated learning setups, our algorithm matches the centralized EM\nfitting of Gaussian mixtures constrained to a subspace. In arbitrary\ncommunication graphs, consensus averaging allows VP-EM to run on large\npeer-to-peer networks as an EM approximation. This mismatch comes from\nconsensus error only, which vanishes exponentially fast with the number of\nconsensus rounds. We demonstrate VP-EM on various topologies for both synthetic\nand real data, evaluating its approximation of centralized EM and seeing that\nit outperforms the available benchmark.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.03029,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Discriminant Analysis in Contrasting Dimensions for Polycystic Ovary\n  Syndrome Prognostication\n\n  A lot of prognostication methodologies have been formulated for early\ndetection of Polycystic Ovary Syndrome also known as PCOS using Machine\nLearning. PCOS is a binary classification problem. Dimensionality Reduction\nmethods impact the performance of Machine Learning to a greater extent and\nusing a Supervised Dimensionality Reduction method can give us a new edge to\ntackle this problem. In this paper we present Discriminant Analysis in\ndifferent dimensions with Linear and Quadratic form for binary classification\nalong with metrics. We were able to achieve good accuracy and less variation\nwith Discriminant Analysis as compared to many commonly used classification\nalgorithms with training accuracy reaching 97.37% and testing accuracy of\n95.92% using Quadratic Discriminant Analysis. Paper also gives the analysis of\ndata with visualizations for deeper understanding of problem.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.01702,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000002318,
      "text":"Bringing Your Own View: Graph Contrastive Learning without Prefabricated\n  Data Augmentations\n\n  Self-supervision is recently surging at its new frontier of graph learning.\nIt facilitates graph representations beneficial to downstream tasks; but its\nsuccess could hinge on domain knowledge for handcraft or the often expensive\ntrials and errors. Even its state-of-the-art representative, graph contrastive\nlearning (GraphCL), is not completely free of those needs as GraphCL uses a\nprefabricated prior reflected by the ad-hoc manual selection of graph data\naugmentations. Our work aims at advancing GraphCL by answering the following\nquestions: How to represent the space of graph augmented views? What principle\ncan be relied upon to learn a prior in that space? And what framework can be\nconstructed to learn the prior in tandem with contrastive learning?\nAccordingly, we have extended the prefabricated discrete prior in the\naugmentation set, to a learnable continuous prior in the parameter space of\ngraph generators, assuming that graph priors per se, similar to the concept of\nimage manifolds, can be learned by data generation. Furthermore, to form\ncontrastive views without collapsing to trivial solutions due to the prior\nlearnability, we have leveraged both principles of information minimization\n(InfoMin) and information bottleneck (InfoBN) to regularize the learned priors.\nEventually, contrastive learning, InfoMin, and InfoBN are incorporated\norganically into one framework of bi-level optimization. Our principled and\nautomated approach has proven to be competitive against the state-of-the-art\ngraph self-supervision methods, including GraphCL, on benchmarks of small\ngraphs; and shown even better generalizability on large-scale graphs, without\nresorting to human expertise or downstream validation. Our code is publicly\nreleased at https:\/\/github.com\/Shen-Lab\/GraphCL_Automated.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.13025,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000008146,
      "text":"Learning Robust Representation through Graph Adversarial Contrastive\n  Learning\n\n  Existing studies show that node representations generated by graph neural\nnetworks (GNNs) are vulnerable to adversarial attacks, such as unnoticeable\nperturbations of adjacent matrix and node features. Thus, it is requisite to\nlearn robust representations in graph neural networks. To improve the\nrobustness of graph representation learning, we propose a novel Graph\nAdversarial Contrastive Learning framework (GraphACL) by introducing\nadversarial augmentations into graph self-supervised learning. In this\nframework, we maximize the mutual information between local and global\nrepresentations of a perturbed graph and its adversarial augmentations, where\nthe adversarial graphs can be generated in either supervised or unsupervised\napproaches. Based on the Information Bottleneck Principle, we theoretically\nprove that our method could obtain a much tighter bound, thus improving the\nrobustness of graph representation learning. Empirically, we evaluate several\nmethods on a range of node classification benchmarks and the results\ndemonstrate GraphACL could achieve comparable accuracy over previous supervised\nmethods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.02427,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Lightweight Compositional Embeddings for Incremental Streaming\n  Recommendation\n\n  Most work in graph-based recommender systems considers a {\\em static} setting\nwhere all information about test nodes (i.e., users and items) is available\nupfront at training time. However, this static setting makes little sense for\nmany real-world applications where data comes in continuously as a stream of\nnew edges and nodes, and one has to update model predictions incrementally to\nreflect the latest state. To fully capitalize on the newly available data in\nthe stream, recent graph-based recommendation models would need to be\nrepeatedly retrained, which is infeasible in practice.\n  In this paper, we study the graph-based streaming recommendation setting and\npropose a compositional recommendation model -- Lightweight Compositional\nEmbedding (LCE) -- that supports incremental updates under low computational\ncost. Instead of learning explicit embeddings for the full set of nodes, LCE\nlearns explicit embeddings for only a subset of nodes and represents the other\nnodes {\\em implicitly}, through a composition function based on their\ninteractions in the graph. This provides an effective, yet efficient, means to\nleverage streaming graph data when one node type (e.g., items) is more amenable\nto static representation. We conduct an extensive empirical study to compare\nLCE to a set of competitive baselines on three large-scale user-item\nrecommendation datasets with interactions under a streaming setting. The\nresults demonstrate the superior performance of LCE, showing that it achieves\nnearly skyline performance with significantly fewer parameters than alternative\ngraph-based models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.01381,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"ETSformer: Exponential Smoothing Transformers for Time-series\n  Forecasting\n\n  Transformers have been actively studied for time-series forecasting in recent\nyears. While often showing promising results in various scenarios, traditional\nTransformers are not designed to fully exploit the characteristics of\ntime-series data and thus suffer some fundamental limitations, e.g., they\ngenerally lack of decomposition capability and interpretability, and are\nneither effective nor efficient for long-term forecasting. In this paper, we\npropose ETSFormer, a novel time-series Transformer architecture, which exploits\nthe principle of exponential smoothing in improving Transformers for\ntime-series forecasting. In particular, inspired by the classical exponential\nsmoothing methods in time-series forecasting, we propose the novel exponential\nsmoothing attention (ESA) and frequency attention (FA) to replace the\nself-attention mechanism in vanilla Transformers, thus improving both accuracy\nand efficiency. Based on these, we redesign the Transformer architecture with\nmodular decomposition blocks such that it can learn to decompose the\ntime-series data into interpretable time-series components such as level,\ngrowth and seasonality. Extensive experiments on various time-series benchmarks\nvalidate the efficacy and advantages of the proposed method. Code is available\nat https:\/\/github.com\/salesforce\/ETSformer.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.07987,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000105302,
      "text":"Out-Of-Distribution Generalization on Graphs: A Survey\n\n  Graph machine learning has been extensively studied in both academia and\nindustry. Although booming with a vast number of emerging methods and\ntechniques, most of the literature is built on the in-distribution hypothesis,\ni.e., testing and training graph data are identically distributed. However,\nthis in-distribution hypothesis can hardly be satisfied in many real-world\ngraph scenarios where the model performance substantially degrades when there\nexist distribution shifts between testing and training graph data. To solve\nthis critical problem, out-of-distribution (OOD) generalization on graphs,\nwhich goes beyond the in-distribution hypothesis, has made great progress and\nattracted ever-increasing attention from the research community. In this paper,\nwe comprehensively survey OOD generalization on graphs and present a detailed\nreview of recent advances in this area. First, we provide a formal problem\ndefinition of OOD generalization on graphs. Second, we categorize existing\nmethods into three classes from conceptually different perspectives, i.e.,\ndata, model, and learning strategy, based on their positions in the graph\nmachine learning pipeline, followed by detailed discussions for each category.\nWe also review the theories related to OOD generalization on graphs and\nintroduce the commonly used graph datasets for thorough evaluations. Finally,\nwe share our insights on future research directions. This paper is the first\nsystematic and comprehensive review of OOD generalization on graphs, to the\nbest of our knowledge.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.08137,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000014239,
      "text":"A data-driven approach for learning to control computers\n\n  It would be useful for machines to use computers as humans do so that they\ncan aid us in everyday tasks. This is a setting in which there is also the\npotential to leverage large-scale expert demonstrations and human judgements of\ninteractive behaviour, which are two ingredients that have driven much recent\nsuccess in AI. Here we investigate the setting of computer control using\nkeyboard and mouse, with goals specified via natural language. Instead of\nfocusing on hand-designed curricula and specialized action spaces, we focus on\ndeveloping a scalable method centered on reinforcement learning combined with\nbehavioural priors informed by actual human-computer interactions. We achieve\nstate-of-the-art and human-level mean performance across all tasks within the\nMiniWob++ benchmark, a challenging suite of computer control problems, and find\nstrong evidence of cross-task transfer. These results demonstrate the\nusefulness of a unified human-agent interface when training machines to use\ncomputers. Altogether our results suggest a formula for achieving competency\nbeyond MiniWob++ and towards controlling computers, in general, as a human\nwould.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.01319,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000255307,
      "text":"Deep Learning for Epidemiologists: An Introduction to Neural Networks\n\n  Deep learning methods are increasingly being applied to problems in medicine\nand healthcare. However, few epidemiologists have received formal training in\nthese methods. To bridge this gap, this article introduces to the fundamentals\nof deep learning from an epidemiological perspective. Specifically, this\narticle reviews core concepts in machine learning (overfitting, regularization,\nhyperparameters), explains several fundamental deep learning architectures\n(convolutional neural networks, recurrent neural networks), and summarizes\ntraining, evaluation, and deployment of models. We aim to enable the reader to\nengage with and critically evaluate medical applications of deep learning,\nfacilitating a dialogue between computer scientists and epidemiologists that\nwill improve the safety and efficacy of applications of this technology.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.08395,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000030465,
      "text":"SWIM: Selective Write-Verify for Computing-in-Memory Neural Accelerators\n\n  Computing-in-Memory architectures based on non-volatile emerging memories\nhave demonstrated great potential for deep neural network (DNN) acceleration\nthanks to their high energy efficiency. However, these emerging devices can\nsuffer from significant variations during the mapping process i.e., programming\nweights to the devices), and if left undealt with, can cause significant\naccuracy degradation. The non-ideality of weight mapping can be compensated by\niterative programming with a write-verify scheme, i.e., reading the conductance\nand rewriting if necessary. In all existing works, such a practice is applied\nto every single weight of a DNN as it is being mapped, which requires extensive\nprogramming time. In this work, we show that it is only necessary to select a\nsmall portion of the weights for write-verify to maintain the DNN accuracy,\nthus achieving significant speedup. We further introduce a second derivative\nbased technique SWIM, which only requires a single pass of forward and\nbackpropagation, to efficiently select the weights that need write-verify.\nExperimental results on various DNN architectures for different datasets show\nthat SWIM can achieve up to 10x programming speedup compared with conventional\nfull-blown write-verify while attaining a comparable accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.08516,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000310275,
      "text":"SAITS: Self-Attention-based Imputation for Time Series\n\n  Missing data in time series is a pervasive problem that puts obstacles in the\nway of advanced analysis. A popular solution is imputation, where the\nfundamental challenge is to determine what values should be filled in. This\npaper proposes SAITS, a novel method based on the self-attention mechanism for\nmissing value imputation in multivariate time series. Trained by a\njoint-optimization approach, SAITS learns missing values from a weighted\ncombination of two diagonally-masked self-attention (DMSA) blocks. DMSA\nexplicitly captures both the temporal dependencies and feature correlations\nbetween time steps, which improves imputation accuracy and training speed.\nMeanwhile, the weighted-combination design enables SAITS to dynamically assign\nweights to the learned representations from two DMSA blocks according to the\nattention map and the missingness information. Extensive experiments\nquantitatively and qualitatively demonstrate that SAITS outperforms the\nstate-of-the-art methods on the time-series imputation task efficiently and\nreveal SAITS' potential to improve the learning performance of pattern\nrecognition models on incomplete time-series data from the real world. The code\nis open source on GitHub at https:\/\/github.com\/WenjieDu\/SAITS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.04332,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000003212,
      "text":"Imitation Learning by State-Only Distribution Matching\n\n  Imitation Learning from observation describes policy learning in a similar\nway to human learning. An agent's policy is trained by observing an expert\nperforming a task. While many state-only imitation learning approaches are\nbased on adversarial imitation learning, one main drawback is that adversarial\ntraining is often unstable and lacks a reliable convergence estimator. If the\ntrue environment reward is unknown and cannot be used to select the\nbest-performing model, this can result in bad real-world policy performance. We\npropose a non-adversarial learning-from-observations approach, together with an\ninterpretable convergence and performance metric.\n  Our training objective minimizes the Kulback-Leibler divergence (KLD) between\nthe policy and expert state transition trajectories which can be optimized in a\nnon-adversarial fashion. Such methods demonstrate improved robustness when\nlearned density models guide the optimization. We further improve the sample\nefficiency by rewriting the KLD minimization as the Soft Actor Critic objective\nbased on a modified reward using additional density models that estimate the\nenvironment's forward and backward dynamics. Finally, we evaluate the\neffectiveness of our approach on well-known continuous control environments and\nshow state-of-the-art performance while having a reliable performance estimator\ncompared to several recent learning-from-observation methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.02898,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000296036,
      "text":"Gradient boosting machines and careful pre-processing work best: ASHRAE\n  Great Energy Predictor III lessons learned\n\n  The ASHRAE Great Energy Predictor III (GEPIII) competition was held in late\n2019 as one of the largest machine learning competitions ever held focused on\nbuilding performance. It was hosted on the Kaggle platform and resulted in\n39,402 prediction submissions, with the top five teams splitting $25,000 in\nprize money. This paper outlines lessons learned from participants, mainly from\nteams who scored in the top 5% of the competition. Various insights were gained\nfrom their experience through an online survey, analysis of publicly shared\nsubmissions and notebooks, and the documentation of the winning teams. The\ntop-performing solutions mostly used ensembles of Gradient Boosting Machine\n(GBM) tree-based models, with the LightGBM package being the most popular. The\nsurvey participants indicated that the preprocessing and feature extraction\nphases were the most important aspects of creating the best modeling approach.\nAll the survey respondents used Python as their primary modeling tool, and it\nwas common to use Jupyter-style Notebooks as development environments. These\nconclusions are essential to help steer the research and practical\nimplementation of building energy meter prediction in the future.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.05068,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"Controlling the Complexity and Lipschitz Constant improves polynomial\n  nets\n\n  While the class of Polynomial Nets demonstrates comparable performance to\nneural networks (NN), it currently has neither theoretical generalization\ncharacterization nor robustness guarantees. To this end, we derive new\ncomplexity bounds for the set of Coupled CP-Decomposition (CCP) and Nested\nCoupled CP-decomposition (NCP) models of Polynomial Nets in terms of the\n$\\ell_\\infty$-operator-norm and the $\\ell_2$-operator norm. In addition, we\nderive bounds on the Lipschitz constant for both models to establish a\ntheoretical certificate for their robustness. The theoretical results enable us\nto propose a principled regularization scheme that we also evaluate\nexperimentally in six datasets and show that it improves the accuracy as well\nas the robustness of the models to adversarial perturbations. We showcase how\nthis regularization can be combined with adversarial training, resulting in\nfurther improvements.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.12166,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000026822,
      "text":"Attention Enables Zero Approximation Error\n\n  Deep learning models have been widely applied in various aspects of daily\nlife. Many variant models based on deep learning structures have achieved even\nbetter performances. Attention-based architectures have become almost\nubiquitous in deep learning structures. Especially, the transformer model has\nnow defeated the convolutional neural network in image classification tasks to\nbecome the most widely used tool. However, the theoretical properties of\nattention-based models are seldom considered. In this work, we show that with\nsuitable adaptations, the single-head self-attention transformer with a fixed\nnumber of transformer encoder blocks and free parameters is able to generate\nany desired polynomial of the input with no error. The number of transformer\nencoder blocks is the same as the degree of the target polynomial. Even more\nexciting, we find that these transformer encoder blocks in this model do not\nneed to be trained. As a direct consequence, we show that the single-head\nself-attention transformer with increasing numbers of free parameters is\nuniversal. These surprising theoretical results clearly explain the outstanding\nperformances of the transformer model and may shed light on future\nmodifications in real applications. We also provide some experiments to verify\nour theoretical result.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.08388,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"Generalizable Information Theoretic Causal Representation\n\n  It is evidence that representation learning can improve model's performance\nover multiple downstream tasks in many real-world scenarios, such as image\nclassification and recommender systems. Existing learning approaches rely on\nestablishing the correlation (or its proxy) between features and the downstream\ntask (labels), which typically results in a representation containing cause,\neffect and spurious correlated variables of the label. Its generalizability may\ndeteriorate because of the unstability of the non-causal parts. In this paper,\nwe propose to learn causal representation from observational data by\nregularizing the learning procedure with mutual information measures according\nto our hypothetical causal graph. The optimization involves a counterfactual\nloss, based on which we deduce a theoretical guarantee that the\ncausality-inspired learning is with reduced sample complexity and better\ngeneralization ability. Extensive experiments show that the models trained on\ncausal representations learned by our approach is robust under adversarial\nattacks and distribution shift.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.12316,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"AutoIP: A United Framework to Integrate Physics into Gaussian Processes\n\n  Physical modeling is critical for many modern science and engineering\napplications. From a data science or machine learning perspective, where more\ndomain-agnostic, data-driven models are pervasive, physical knowledge -- often\nexpressed as differential equations -- is valuable in that it is complementary\nto data, and it can potentially help overcome issues such as data sparsity,\nnoise, and inaccuracy. In this work, we propose a simple, yet powerful and\ngeneral framework -- AutoIP, for Automatically Incorporating Physics -- that\ncan integrate all kinds of differential equations into Gaussian Processes (GPs)\nto enhance prediction accuracy and uncertainty quantification. These equations\ncan be linear or nonlinear, spatial, temporal, or spatio-temporal, complete or\nincomplete with unknown source terms, and so on. Based on kernel\ndifferentiation, we construct a GP prior to sample the values of the target\nfunction, equation-related derivatives, and latent source functions, which are\nall jointly from a multivariate Gaussian distribution. The sampled values are\nfed to two likelihoods: one to fit the observations, and the other to conform\nto the equation. We use the whitening method to evade the strong dependency\nbetween the sampled function values and kernel parameters, and we develop a\nstochastic variational learning algorithm. AutoIP shows improvement upon\nvanilla GPs in both simulation and several real-world applications, even using\nrough, incomplete equations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.08335,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000018875,
      "text":"Task-Agnostic Graph Explanations\n\n  Graph Neural Networks (GNNs) have emerged as powerful tools to encode\ngraph-structured data. Due to their broad applications, there is an increasing\nneed to develop tools to explain how GNNs make decisions given graph-structured\ndata. Existing learning-based GNN explanation approaches are task-specific in\ntraining and hence suffer from crucial drawbacks. Specifically, they are\nincapable of producing explanations for a multitask prediction model with a\nsingle explainer. They are also unable to provide explanations in cases where\nthe GNN is trained in a self-supervised manner, and the resulting\nrepresentations are used in future downstream tasks. To address these\nlimitations, we propose a Task-Agnostic GNN Explainer (TAGE) that is\nindependent of downstream models and trained under self-supervision with no\nknowledge of downstream tasks. TAGE enables the explanation of GNN embedding\nmodels with unseen downstream tasks and allows efficient explanation of\nmultitask models. Our extensive experiments show that TAGE can significantly\nspeed up the explanation efficiency by using the same model to explain\npredictions for multiple downstream tasks while achieving explanation quality\nas good as or even better than current state-of-the-art GNN explanation\napproaches. Our code is pubicly available as part of the DIG library at\nhttps:\/\/github.com\/divelab\/DIG\/tree\/main\/dig\/xgraph\/TAGE\/.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.12064,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000601345,
      "text":"Interfering Paths in Decision Trees: A Note on Deodata Predictors\n\n  A technique for improving the prediction accuracy of decision trees is\nproposed. It consists in evaluating the tree's branches in parallel over\nmultiple paths. The technique enables predictions that are more aligned with\nthe ones generated by the nearest neighborhood variant of the deodata\nalgorithms. The technique also enables the hybridization of the decision tree\nalgorithm with the nearest neighborhood variant.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.10816,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000023511,
      "text":"Why Fair Labels Can Yield Unfair Predictions: Graphical Conditions for\n  Introduced Unfairness\n\n  In addition to reproducing discriminatory relationships in the training data,\nmachine learning systems can also introduce or amplify discriminatory effects.\nWe refer to this as introduced unfairness, and investigate the conditions under\nwhich it may arise. To this end, we propose introduced total variation as a\nmeasure of introduced unfairness, and establish graphical conditions under\nwhich it may be incentivised to occur. These criteria imply that adding the\nsensitive attribute as a feature removes the incentive for introduced variation\nunder well-behaved loss functions. Additionally, taking a causal perspective,\nintroduced path-specific effects shed light on the issue of when specific paths\nshould be considered fair.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.01341,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000054969,
      "text":"Robust Binary Models by Pruning Randomly-initialized Networks\n\n  Robustness to adversarial attacks was shown to require a larger model\ncapacity, and thus a larger memory footprint. In this paper, we introduce an\napproach to obtain robust yet compact models by pruning randomly-initialized\nbinary networks. Unlike adversarial training, which learns the model\nparameters, we initialize the model parameters as either +1 or -1, keep them\nfixed, and find a subnetwork structure that is robust to attacks. Our method\nconfirms the Strong Lottery Ticket Hypothesis in the presence of adversarial\nattacks, and extends this to binary networks. Furthermore, it yields more\ncompact networks with competitive performance than existing works by 1)\nadaptively pruning different network layers; 2) exploiting an effective binary\ninitialization scheme; 3) incorporating a last batch normalization layer to\nimprove training stability. Our experiments demonstrate that our approach not\nonly always outperforms the state-of-the-art robust binary networks, but also\ncan achieve accuracy better than full-precision ones on some datasets. Finally,\nwe show the structured patterns of our pruned binary networks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.06828,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"On the Convergence of SARSA with Linear Function Approximation\n\n  SARSA, a classical on-policy control algorithm for reinforcement learning, is\nknown to chatter when combined with linear function approximation: SARSA does\nnot diverge but oscillates in a bounded region. However, little is known about\nhow fast SARSA converges to that region and how large the region is. In this\npaper, we make progress towards this open problem by showing the convergence\nrate of projected SARSA to a bounded region. Importantly, the region is much\nsmaller than the region that we project into, provided that the magnitude of\nthe reward is not too large. Existing works regarding the convergence of linear\nSARSA to a fixed point all require the Lipschitz constant of SARSA's policy\nimprovement operator to be sufficiently small; our analysis instead applies to\narbitrary Lipschitz constants and thus characterizes the behavior of linear\nSARSA for a new regime.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.02894,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Effects of Parametric and Non-Parametric Methods on High Dimensional\n  Sparse Matrix Representations\n\n  The semantics are derived from textual data that provide representations for\nMachine Learning algorithms. These representations are interpretable form of\nhigh dimensional sparse matrix that are given as an input to the machine\nlearning algorithms. Since learning methods are broadly classified as\nparametric and non-parametric learning methods, in this paper we provide the\neffects of these type of algorithms on the high dimensional sparse matrix\nrepresentations. In order to derive the representations from the text data, we\nhave considered TF-IDF representation with valid reason in the paper. We have\nformed representations of 50, 100, 500, 1000 and 5000 dimensions respectively\nover which we have performed classification using Linear Discriminant Analysis\nand Naive Bayes as parametric learning method, Decision Tree and Support Vector\nMachines as non-parametric learning method. We have later provided the metrics\non every single dimension of the representation and effect of every single\nalgorithm detailed in this paper.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.138,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000041723,
      "text":"Differential equation and probability inspired graph neural networks for\n  latent variable learning\n\n  Probabilistic theory and differential equation are powerful tools for the\ninterpretability and guidance of the design of machine learning models,\nespecially for illuminating the mathematical motivation of learning latent\nvariable from observation. Subspace learning maps high-dimensional features on\nlow-dimensional subspace to capture efficient representation. Graphs are widely\napplied for modeling latent variable learning problems, and graph neural\nnetworks implement deep learning architectures on graphs. Inspired by\nprobabilistic theory and differential equations, this paper conducts notes and\nproposals about graph neural networks to solve subspace learning problems by\nvariational inference and differential equation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.02088,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Second-order Symmetric Non-negative Latent Factor Analysis\n\n  Precise representation of large-scale undirected network is the basis for\nunderstanding relations within a massive entity set. The undirected network\nrepresentation task can be efficiently addressed by a symmetry non-negative\nlatent factor (SNLF) model, whose objective is clearly non-convex. However,\nexisting SNLF models commonly adopt a first-order optimizer that cannot well\nhandle the non-convex objective, thereby resulting in inaccurate representation\nresults. On the other hand, higher-order learning algorithms are expected to\nmake a breakthrough, but their computation efficiency are greatly limited due\nto the direct manipulation of the Hessian matrix, which can be huge in\nundirected network representation tasks. Aiming at addressing this issue, this\nstudy proposes to incorporate an efficient second-order method into SNLF,\nthereby establishing a second-order symmetric non-negative latent factor\nanalysis model for undirected network with two-fold ideas: a) incorporating a\nmapping strategy into SNLF model to form an unconstrained model, and b)\ntraining the unconstrained model with a specially designed second order method\nto acquire a proper second-order step efficiently. Empirical studies indicate\nthat proposed model outperforms state-of-the-art models in representation\naccuracy with affordable computational burden.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.01641,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"On generating parametrised structural data using conditional generative\n  adversarial networks\n\n  A powerful approach, and one of the most common ones in structural health\nmonitoring (SHM), is to use data-driven models to make predictions and\ninferences about structures and their condition. Such methods almost\nexclusively rely on the quality of the data. Within the SHM discipline, data do\nnot always suffice to build models with satisfactory accuracy for given tasks.\nEven worse, data may be completely missing from one's dataset, regarding the\nbehaviour of a structure under different environmental conditions. In the\ncurrent work, with a view to confronting such issues, the generation of\nartificial data using a variation of the generative adversarial network (GAN)\nalgorithm, is used. The aforementioned variation is that of the conditional GAN\nor cGAN. The algorithm is not only used to generate artificial data, but also\nto learn transformations of manifolds according to some known parameters.\nAssuming that the structure's response is represented by points in a manifold,\npart of the space will be formed due to variations in external conditions\naffecting the structure. This idea proves efficient in SHM, as it is exploited\nto generate structural data for specific values of environmental coefficients.\nThe scheme is applied here on a simulated structure which operates under\ndifferent temperature and humidity conditions. The cGAN is trained on data for\nsome discrete values of the temperature within some range, and is able to\ngenerate data for every temperature in this range with satisfactory accuracy.\nThe novelty, compared to classic regression in similar problems, is that the\ncGAN allows unknown environmental parameters to affect the structure and can\ngenerate whole manifolds of data for every value of the known parameters, while\nthe unknown ones vary within the generated manifolds.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.11805,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000074175,
      "text":"Robust Classification using Contractive Hamiltonian Neural ODEs\n\n  Deep neural networks can be fragile and sensitive to small input\nperturbations that might cause a significant change in the output. In this\npaper, we employ contraction theory to improve the robustness of neural ODEs\n(NODEs). A dynamical system is contractive if all solutions with different\ninitial conditions converge to each other exponentially fast. As a consequence,\nperturbations in initial conditions become less and less relevant over time.\nSince in NODEs the input data corresponds to the initial condition of dynamical\nsystems, we show contractivity can mitigate the effect of input perturbations.\nMore precisely, inspired by NODEs with Hamiltonian dynamics, we propose a class\nof contractive Hamiltonian NODEs (CH-NODEs). By properly tuning a scalar\nparameter, CH-NODEs ensure contractivity by design and can be trained using\nstandard backpropagation. Moreover, CH-NODEs enjoy built-in guarantees of\nnon-exploding gradients, which ensure a well-posed training process. Finally,\nwe demonstrate the robustness of CH-NODEs on the MNIST image classification\nproblem with noisy test data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.09637,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Investigating Compounding Prediction Errors in Learned Dynamics Models\n\n  Accurately predicting the consequences of agents' actions is a key\nprerequisite for planning in robotic control. Model-based reinforcement\nlearning (MBRL) is one paradigm which relies on the iterative learning and\nprediction of state-action transitions to solve a task. Deep MBRL has become a\npopular candidate, using a neural network to learn a dynamics model that\npredicts with each pass from high-dimensional states to actions. These\n\"one-step\" predictions are known to become inaccurate over longer horizons of\ncomposed prediction - called the compounding error problem. Given the\nprevalence of the compounding error problem in MBRL and related fields of\ndata-driven control, we set out to understand the properties of and conditions\ncausing these long-horizon errors. In this paper, we explore the effects of\nsubcomponents of a control problem on long term prediction error: including\nchoosing a system, collecting data, and training a model. These detailed\nquantitative studies on simulated and real-world data show that the underlying\ndynamics of a system are the strongest factor determining the shape and\nmagnitude of prediction error. Given a clearer understanding of compounding\nprediction error, researchers can implement new types of models beyond\n\"one-step\" that are more useful for control.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.03423,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Multivariate Time Series Forecasting with Latent Graph Inference\n\n  This paper introduces a new approach for Multivariate Time Series forecasting\nthat jointly infers and leverages relations among time series. Its modularity\nallows it to be integrated with current univariate methods. Our approach allows\nto trade-off accuracy and computational efficiency gradually via offering on\none extreme inference of a potentially fully-connected graph or on another\nextreme a bipartite graph. In the potentially fully-connected case we consider\nall pair-wise interactions among time-series which yields the best forecasting\naccuracy. Conversely, the bipartite case leverages the dependency structure by\ninter-communicating the N time series through a small set of K auxiliary nodes\nthat we introduce. This reduces the time and memory complexity w.r.t. previous\ngraph inference methods from O(N^2) to O(NK) with a small trade-off in\naccuracy. We demonstrate the effectiveness of our model in a variety of\ndatasets where both of its variants perform better or very competitively to\nprevious graph inference methods in terms of forecasting accuracy and time\nefficiency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.04419,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000029471,
      "text":"Survival Prediction of Brain Cancer with Incomplete Radiology,\n  Pathology, Genomics, and Demographic Data\n\n  Integrating cross-department multi-modal data (e.g., radiological,\npathological, genomic, and clinical data) is ubiquitous in brain cancer\ndiagnosis and survival prediction. To date, such an integration is typically\nconducted by human physicians (and panels of experts), which can be subjective\nand semi-quantitative. Recent advances in multi-modal deep learning, however,\nhave opened a door to leverage such a process to a more objective and\nquantitative manner. Unfortunately, the prior arts of using four modalities on\nbrain cancer survival prediction are limited by a \"complete modalities\" setting\n(i.e., with all modalities available). Thus, there are still open questions on\nhow to effectively predict brain cancer survival from the incomplete\nradiological, pathological, genomic, and demographic data (e.g., one or more\nmodalities might not be collected for a patient). For instance, should we use\nboth complete and incomplete data, and more importantly, how to use those data?\nTo answer the preceding questions, we generalize the multi-modal learning on\ncross-department multi-modal data to a missing data setting. Our contribution\nis three-fold: 1) We introduce optimal multi-modal learning with missing data\n(MMD) pipeline with optimized hardware consumption and computational\nefficiency; 2) We extend multi-modal learning on radiological, pathological,\ngenomic, and demographic data into missing data scenarios; 3) a large-scale\npublic dataset (with 962 patients) is collected to systematically evaluate\nglioma tumor survival prediction using four modalities. The proposed method\nimproved the C-index of survival prediction from 0.7624 to 0.8053.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.09879,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000050664,
      "text":"Class-wise Classifier Design Capable of Continual Learning using\n  Adaptive Resonance Theory-based Topological Clustering\n\n  This paper proposes a supervised classification algorithm capable of\ncontinual learning by utilizing an Adaptive Resonance Theory (ART)-based\ngrowing self-organizing clustering algorithm. The ART-based clustering\nalgorithm is theoretically capable of continual learning, and the proposed\nalgorithm independently applies it to each class of training data for\ngenerating classifiers. Whenever an additional training data set from a new\nclass is given, a new ART-based clustering will be defined in a different\nlearning space. Thanks to the above-mentioned features, the proposed algorithm\nrealizes continual learning capability. Simulation experiments showed that the\nproposed algorithm has superior classification performance compared with\nstate-of-the-art clustering-based classification algorithms capable of\ncontinual learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.1345,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000005232,
      "text":"A Comparative Survey of Deep Active Learning\n\n  While deep learning (DL) is data-hungry and usually relies on extensive\nlabeled data to deliver good performance, Active Learning (AL) reduces labeling\ncosts by selecting a small proportion of samples from unlabeled data for\nlabeling and training. Therefore, Deep Active Learning (DAL) has risen as a\nfeasible solution for maximizing model performance under a limited labeling\ncost\/budget in recent years. Although abundant methods of DAL have been\ndeveloped and various literature reviews conducted, the performance evaluation\nof DAL methods under fair comparison settings is not yet available. Our work\nintends to fill this gap. In this work, We construct a DAL toolkit, DeepAL+, by\nre-implementing 19 highly-cited DAL methods. We survey and categorize\nDAL-related works and construct comparative experiments across frequently used\ndatasets and DAL algorithms. Additionally, we explore some factors (e.g., batch\nsize, number of epochs in the training process) that influence the efficacy of\nDAL, which provides better references for researchers to design their DAL\nexperiments or carry out DAL-related applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.03009,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000179807,
      "text":"Coresets for Data Discretization and Sine Wave Fitting\n\n  In the \\emph{monitoring} problem, the input is an unbounded stream\n$P={p_1,p_2\\cdots}$ of integers in $[N]:=\\{1,\\cdots,N\\}$, that are obtained\nfrom a sensor (such as GPS or heart beats of a human). The goal (e.g., for\nanomaly detection) is to approximate the $n$ points received so far in $P$ by a\nsingle frequency $\\sin$, e.g. $\\min_{c\\in C}cost(P,c)+\\lambda(c)$, where\n$cost(P,c)=\\sum_{i=1}^n \\sin^2(\\frac{2\\pi}{N} p_ic)$, $C\\subseteq [N]$ is a\nfeasible set of solutions, and $\\lambda$ is a given regularization function.\nFor any approximation error $\\varepsilon>0$, we prove that \\emph{every} set $P$\nof $n$ integers has a weighted subset $S\\subseteq P$ (sometimes called\ncore-set) of cardinality $|S|\\in O(\\log(N)^{O(1)})$ that approximates\n$cost(P,c)$ (for every $c\\in [N]$) up to a multiplicative factor of\n$1\\pm\\varepsilon$. Using known coreset techniques, this implies streaming\nalgorithms using only $O((\\log(N)\\log(n))^{O(1)})$ memory. Our results hold for\na large family of functions. Experimental results and open source code are\nprovided.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.16214,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Adaptive Divergence-based Non-negative Latent Factor Analysis\n\n  High-Dimensional and Incomplete (HDI) data are frequently found in various\nindustrial applications with complex interactions among numerous nodes, which\nare commonly non-negative for representing the inherent non-negativity of node\ninteractions. A Non-negative Latent Factor (NLF) model is able to extract\nintrinsic features from such data efficiently. However, existing NLF models all\nadopt a static divergence metric like Euclidean distance or {\\alpha}-\\b{eta}\ndivergence to build its learning objective, which greatly restricts its\nscalability of accurately representing HDI data from different domains. Aiming\nat addressing this issue, this study presents an Adaptive Divergence-based\nNon-negative Latent Factor (ADNLF) model with three-fold ideas: a) generalizing\nthe objective function with the {\\alpha}-\\b{eta}-divergence to expand its\npotential of representing various HDI data; b) adopting a non-negative bridging\nfunction to connect the optimization variables with output latent factors for\nfulfilling the non-negativity constraints constantly; and c) making the\ndivergence parameters adaptive through particle swarm optimization, thereby\nfacilitating adaptive divergence in the learning objective to achieve high\nscalability. Empirical studies are conducted on four HDI datasets from real\napplications, whose results demonstrate that in comparison with\nstate-of-the-art NLF models, an ADNLF model achieves significantly higher\nestimation accuracy for missing data of an HDI dataset with high computational\nefficiency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.15932,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000050002,
      "text":"Self-Contrastive Learning based Semi-Supervised Radio Modulation\n  Classification\n\n  This paper presents a semi-supervised learning framework that is new in being\ndesigned for automatic modulation classification (AMC). By carefully utilizing\nunlabeled signal data with a self-supervised contrastive-learning pre-training\nstep, our framework achieves higher performance given smaller amounts of\nlabeled data, thereby largely reducing the labeling burden of deep learning. We\nevaluate the performance of our semi-supervised framework on a public dataset.\nThe evaluation results demonstrate that our semi-supervised approach\nsignificantly outperforms supervised frameworks thereby substantially enhancing\nour ability to train deep neural networks for automatic modulation\nclassification in a manner that leverages unlabeled data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.16282,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"The Weak Supervision Landscape\n\n  Many ways of annotating a dataset for machine learning classification tasks\nthat go beyond the usual class labels exist in practice. These are of interest\nas they can simplify or facilitate the collection of annotations, while not\ngreatly affecting the resulting machine learning model. Many of these fall\nunder the umbrella term of weak labels or annotations. However, it is not\nalways clear how different alternatives are related. In this paper we propose a\nframework for categorising weak supervision settings with the aim of: (1)\nhelping the dataset owner or annotator navigate through the available options\nwithin weak supervision when prescribing an annotation process, and (2)\ndescribing existing annotations for a dataset to machine learning practitioners\nso that we allow them to understand the implications for the learning process.\nTo this end, we identify the key elements that characterise weak supervision\nand devise a series of dimensions that categorise most of the existing\napproaches. We show how common settings in the literature fit within the\nframework and discuss its possible uses in practice.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.11683,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000069539,
      "text":"Twin Weisfeiler-Lehman: High Expressive GNNs for Graph Classification\n\n  The expressive power of message passing GNNs is upper-bounded by\nWeisfeiler-Lehman (WL) test. To achieve high expressive GNNs beyond WL test, we\npropose a novel graph isomorphism test method, namely Twin-WL, which\nsimultaneously passes node labels and node identities rather than only passes\nnode label as WL. The identity-passing mechanism encodes complete structure\ninformation of rooted subgraph, and thus Twin-WL can offer extra power beyond\nWL at distinguishing graph structures. Based on Twin-WL, we implement two\nTwin-GNNs for graph classification via defining readout function over rooted\nsubgraph: one simply readouts the size of rooted subgraph and the other\nreadouts rich structure information of subgraph following a GNN-style. We prove\nthat the two Twin-GNNs both have higher expressive power than traditional\nmessage passing GNNs. Experiments also demonstrate the Twin-GNNs significantly\noutperform state-of-the-art methods at the task of graph classification.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.08008,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000080466,
      "text":"Beyond Explaining: Opportunities and Challenges of XAI-Based Model\n  Improvement\n\n  Explainable Artificial Intelligence (XAI) is an emerging research field\nbringing transparency to highly complex and opaque machine learning (ML)\nmodels. Despite the development of a multitude of methods to explain the\ndecisions of black-box classifiers in recent years, these tools are seldomly\nused beyond visualization purposes. Only recently, researchers have started to\nemploy explanations in practice to actually improve models. This paper offers a\ncomprehensive overview over techniques that apply XAI practically for improving\nvarious properties of ML models, and systematically categorizes these\napproaches, comparing their respective strengths and weaknesses. We provide a\ntheoretical perspective on these methods, and show empirically through\nexperiments on toy and realistic settings how explanations can help improve\nproperties such as model generalization ability or reasoning, among others. We\nfurther discuss potential caveats and drawbacks of these methods. We conclude\nthat while model improvement based on XAI can have significant beneficial\neffects even on complex and not easily quantifyable model properties, these\nmethods need to be applied carefully, since their success can vary depending on\na multitude of factors, such as the model and dataset used, or the employed\nexplanation method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.16801,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"Robust Meta-Reinforcement Learning with Curriculum-Based Task Sampling\n\n  Meta-reinforcement learning (meta-RL) acquires meta-policies that show good\nperformance for tasks in a wide task distribution. However, conventional\nmeta-RL, which learns meta-policies by randomly sampling tasks, has been\nreported to show meta-overfitting for certain tasks, especially for easy tasks\nwhere an agent can easily get high scores. To reduce effects of the\nmeta-overfitting, we considered meta-RL with curriculum-based task sampling.\nOur method is Robust Meta Reinforcement Learning with Guided Task Sampling\n(RMRL-GTS), which is an effective method that restricts task sampling based on\nscores and epochs. We show that in order to achieve robust meta-RL, it is\nnecessary not only to intensively sample tasks with poor scores, but also to\nrestrict and expand the task regions of the tasks to be sampled.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.07691,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.00000553,
      "text":"Supervised Contrastive Learning with Structure Inference for Graph\n  Classification\n\n  Advanced graph neural networks have shown great potentials in graph\nclassification tasks recently. Different from node classification where node\nembeddings aggregated from local neighbors can be directly used to learn node\nlabels, graph classification requires a hierarchical accumulation of different\nlevels of topological information to generate discriminative graph embeddings.\nStill, how to fully explore graph structures and formulate an effective graph\nclassification pipeline remains rudimentary. In this paper, we propose a novel\ngraph neural network based on supervised contrastive learning with structure\ninference for graph classification. First, we propose a data-driven graph\naugmentation strategy that can discover additional connections to enhance the\nexisting edge set. Concretely, we resort to a structure inference stage based\non diffusion cascades to recover possible connections with high node\nsimilarities. Second, to improve the contrastive power of graph neural\nnetworks, we propose to use a supervised contrastive loss for graph\nclassification. With the integration of label information, the one-vs-many\ncontrastive learning can be extended to a many-vs-many setting, so that the\ngraph-level embeddings with higher topological similarities will be pulled\ncloser. The supervised contrastive loss and structure inference can be\nnaturally incorporated within the hierarchical graph neural networks where the\ntopological patterns can be fully explored to produce discriminative graph\nembeddings. Experiment results show the effectiveness of the proposed method\ncompared with recent state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.09141,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000087089,
      "text":"Graph Representation Learning with Individualization and Refinement\n\n  Graph Neural Networks (GNNs) have emerged as prominent models for\nrepresentation learning on graph structured data. GNNs follow an approach of\nmessage passing analogous to 1-dimensional Weisfeiler Lehman (1-WL) test for\ngraph isomorphism and consequently are limited by the distinguishing power of\n1-WL. More expressive higher-order GNNs which operate on k-tuples of nodes need\nincreased computational resources in order to process higher-order tensors.\nInstead of the WL approach, in this work, we follow the classical approach of\nIndividualization and Refinement (IR), a technique followed by most practical\nisomorphism solvers. Individualization refers to artificially distinguishing a\nnode in the graph and refinement is the propagation of this information to\nother nodes through message passing. We learn to adaptively select nodes to\nindividualize and to aggregate the resulting graphs after refinement to help\nhandle the complexity. Our technique lets us learn richer node embeddings while\nkeeping the computational complexity manageable. Theoretically, we show that\nour procedure is more expressive than the 1-WL test. Experiments show that our\nmethod outperforms prominent 1-WL GNN models as well as competitive\nhigher-order baselines on several benchmark synthetic and real datasets.\nFurthermore, our method opens new doors for exploring the paradigm of learning\non graph structures with individualization and refinement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.01016,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"The Theoretical Expressiveness of Maxpooling\n\n  Over the decade since deep neural networks became state of the art image\nclassifiers there has been a tendency towards less use of max pooling: the\nfunction that takes the largest of nearby pixels in an image. Since max pooling\nfeatured prominently in earlier generations of image classifiers, we wish to\nunderstand this trend, and whether it is justified. We develop a theoretical\nframework analyzing ReLU based approximations to max pooling, and prove a sense\nin which max pooling cannot be efficiently replicated using ReLU activations.\nWe analyze the error of a class of optimal approximations, and find that whilst\nthe error can be made exponentially small in the kernel size, doing so requires\nan exponentially complex approximation.\n  Our work gives a theoretical basis for understanding the trend away from max\npooling in newer architectures. We conclude that the main cause of a difference\nbetween max pooling and an optimal approximation, a prevalent large difference\nbetween the max and other values within pools, can be overcome with other\narchitectural decisions, or is not prevalent in natural images.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.13421,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Learning Losses for Strategic Classification\n\n  Strategic classification, i.e. classification under possible strategic\nmanipulations of features, has received a lot of attention from both the\nmachine learning and the game theory community. Most works focus on analysing\nproperties of the optimal decision rule under such manipulations. In our work\nwe take a learning theoretic perspective, focusing on the sample complexity\nneeded to learn a good decision rule which is robust to strategic manipulation.\nWe perform this analysis by introducing a novel loss function, the\n\\emph{strategic manipulation loss}, which takes into account both the accuracy\nof the final decision rule and its vulnerability to manipulation. We analyse\nthe sample complexity for a known graph of possible manipulations in terms of\nthe complexity of the function class and the manipulation graph. Additionally,\nwe initialize the study of learning under unknown manipulation capabilities of\nthe involved agents. Using techniques from transfer learning theory, we define\na similarity measure for manipulation graphs and show that learning outcomes\nare robust with respect to small changes in the manipulation graph. Lastly, we\nanalyse the (sample complexity of) learning of the manipulation capability of\nagents with respect to this similarity measure, providing novel guarantees for\nstrategic classification with respect to an unknown manipulation graph.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.0098,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Boosted Ensemble Learning based on Randomized NNs for Time Series\n  Forecasting\n\n  Time series forecasting is a challenging problem particularly when a time\nseries expresses multiple seasonality, nonlinear trend and varying variance. In\nthis work, to forecast complex time series, we propose ensemble learning which\nis based on randomized neural networks, and boosted in three ways. These\ncomprise ensemble learning based on residuals, corrected targets and opposed\nresponse. The latter two methods are employed to ensure similar forecasting\ntasks are solved by all ensemble members, which justifies the use of exactly\nthe same base models at all stages of ensembling. Unification of the tasks for\nall members simplifies ensemble learning and leads to increased forecasting\naccuracy. This was confirmed in an experimental study involving forecasting\ntime series with triple seasonality, in which we compare our three variants of\nensemble boosting. The strong points of the proposed ensembles based on RandNNs\nare extremely rapid training and pattern-based time series representation,\nwhich extracts relevant information from time series.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.01618,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Deep-Ensemble-Based Uncertainty Quantification in Spatiotemporal Graph\n  Neural Networks for Traffic Forecasting\n\n  Deep-learning-based data-driven forecasting methods have produced impressive\nresults for traffic forecasting. A major limitation of these methods, however,\nis that they provide forecasts without estimates of uncertainty, which are\ncritical for real-time deployments. We focus on a diffusion convolutional\nrecurrent neural network (DCRNN), a state-of-the-art method for short-term\ntraffic forecasting. We develop a scalable deep ensemble approach to quantify\nuncertainties for DCRNN. Our approach uses a scalable Bayesian optimization\nmethod to perform hyperparameter optimization, selects a set of high-performing\nconfigurations, fits a generative model to capture the joint distributions of\nthe hyperparameter configurations, and trains an ensemble of models by sampling\na new set of hyperparameter configurations from the generative model. We\ndemonstrate the efficacy of the proposed methods by comparing them with other\nuncertainty estimation techniques. We show that our generic and scalable\napproach outperforms the current state-of-the-art Bayesian and a number of\nother commonly used frequentist techniques.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.01715,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"BigDL 2.0: Seamless Scaling of AI Pipelines from Laptops to Distributed\n  Cluster\n\n  Most AI projects start with a Python notebook running on a single laptop;\nhowever, one usually needs to go through a mountain of pains to scale it to\nhandle larger dataset (for both experimentation and production deployment).\nThese usually entail many manual and error-prone steps for the data scientists\nto fully take advantage of the available hardware resources (e.g., SIMD\ninstructions, multi-processing, quantization, memory allocation optimization,\ndata partitioning, distributed computing, etc.). To address this challenge, we\nhave open sourced BigDL 2.0 at https:\/\/github.com\/intel-analytics\/BigDL\/ under\nApache 2.0 license (combining the original BigDL and Analytics Zoo projects);\nusing BigDL 2.0, users can simply build conventional Python notebooks on their\nlaptops (with possible AutoML support), which can then be transparently\naccelerated on a single node (with up-to 9.6x speedup in our experiments), and\nseamlessly scaled out to a large cluster (across several hundreds servers in\nreal-world use cases). BigDL 2.0 has already been adopted by many real-world\nusers (such as Mastercard, Burger King, Inspur, etc.) in production.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.11326,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Beyond the Quadratic Approximation: the Multiscale Structure of Neural\n  Network Loss Landscapes\n\n  A quadratic approximation of neural network loss landscapes has been\nextensively used to study the optimization process of these networks. Though,\nit usually holds in a very small neighborhood of the minimum, it cannot explain\nmany phenomena observed during the optimization process. In this work, we study\nthe structure of neural network loss functions and its implication on\noptimization in a region beyond the reach of a good quadratic approximation.\nNumerically, we observe that neural network loss functions possesses a\nmultiscale structure, manifested in two ways: (1) in a neighborhood of minima,\nthe loss mixes a continuum of scales and grows subquadratically, and (2) in a\nlarger region, the loss shows several separate scales clearly. Using the\nsubquadratic growth, we are able to explain the Edge of Stability phenomenon\n[5] observed for the gradient descent (GD) method. Using the separate scales,\nwe explain the working mechanism of learning rate decay by simple examples.\nFinally, we study the origin of the multiscale structure and propose that the\nnon-convexity of the models and the non-uniformity of training data is one of\nthe causes. By constructing a two-layer neural network problem we show that\ntraining data with different magnitudes give rise to different scales of the\nloss function, producing subquadratic growth and multiple separate scales.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.11164,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000029802,
      "text":"Are Your Reviewers Being Treated Equally? Discovering Subgroup\n  Structures to Improve Fairness in Spam Detection\n\n  User-generated reviews of products are vital assets of online commerce, such\nas Amazon and Yelp, while fake reviews are prevalent to mislead customers. GNN\nis the state-of-the-art method that detects suspicious reviewers by exploiting\nthe topologies of the graph connecting reviewers, reviews, and target products.\nHowever, the discrepancy in the detection accuracy over different groups of\nreviewers can degrade reviewer engagement and customer trust in the review\nwebsites. Unlike the previous belief that the difference between the groups\ncauses unfairness, we study the subgroup structures within the groups that can\nalso cause discrepancies in treating different groups. This paper addresses the\nchallenges of defining, approximating, and utilizing a new subgroup structure\nfor fair spam detection. We first identify subgroup structures in the review\ngraph that lead to discrepant accuracy in the groups. The complex dependencies\nover the review graph create difficulties in teasing out subgroups hidden\nwithin larger groups. We design a model that can be trained to jointly infer\nthe hidden subgroup memberships and exploits the membership for calibrating the\ndetection accuracy across groups. Comprehensive comparisons against baselines\non three large Yelp review datasets demonstrate that the subgroup membership\ncan be identified and exploited for group fairness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.04292,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Evolving Pareto-Optimal Actor-Critic Algorithms for Generalizability and\n  Stability\n\n  Generalizability and stability are two key objectives for operating\nreinforcement learning (RL) agents in the real world. Designing RL algorithms\nthat optimize these objectives can be a costly and painstaking process. This\npaper presents MetaPG, an evolutionary method for automated design of\nactor-critic loss functions. MetaPG explicitly optimizes for generalizability\nand performance, and implicitly optimizes the stability of both metrics. We\ninitialize our loss function population with Soft Actor-Critic (SAC) and\nperform multi-objective optimization using fitness metrics encoding single-task\nperformance, zero-shot generalizability to unseen environment configurations,\nand stability across independent runs with different random seeds. On a set of\ncontinuous control tasks from the Real-World RL Benchmark Suite, we find that\nour method, using a single environment during evolution, evolves algorithms\nthat improve upon SAC's performance and generalizability by 4% and 20%,\nrespectively, and reduce instability up to 67%. Then, we scale up to more\ncomplex environments from the Brax physics simulator and replicate\ngeneralizability tests encountered in practical settings, such as different\nfriction coefficients. MetaPG evolves algorithms that can obtain 10% better\ngeneralizability without loss of performance within the same meta-training\nenvironment and obtain similar results to SAC when doing cross-domain\nevaluations in other Brax environments. The evolution results are\ninterpretable; by analyzing the structure of the best algorithms we identify\nelements that help optimizing certain objectives, such as regularization terms\nfor the critic loss.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.07485,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"How to Use K-means for Big Data Clustering?\n\n  K-means plays a vital role in data mining and is the simplest and most widely\nused algorithm under the Euclidean Minimum Sum-of-Squares Clustering (MSSC)\nmodel. However, its performance drastically drops when applied to vast amounts\nof data. Therefore, it is crucial to improve K-means by scaling it to big data\nusing as few of the following computational resources as possible: data, time,\nand algorithmic ingredients. We propose a new parallel scheme of using K-means\nand K-means++ algorithms for big data clustering that satisfies the properties\nof a ``true big data'' algorithm and outperforms the classical and recent\nstate-of-the-art MSSC approaches in terms of solution quality and runtime. The\nnew approach naturally implements global search by decomposing the MSSC problem\nwithout using additional metaheuristics. This work shows that data\ndecomposition is the basic approach to solve the big data clustering problem.\nThe empirical success of the new algorithm allowed us to challenge the common\nbelief that more data is required to obtain a good clustering solution.\nMoreover, the present work questions the established trend that more\nsophisticated hybrid approaches and algorithms are required to obtain a better\nclustering solution.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.01732,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000030796,
      "text":"A high-order tensor completion algorithm based on Fully-Connected Tensor\n  Network weighted optimization\n\n  Tensor completion aimes at recovering missing data, and it is one of the\npopular concerns in deep learning and signal processing. Among the higher-order\ntensor decomposition algorithms, the recently proposed fully-connected tensor\nnetwork decomposition (FCTN) algorithm is the most advanced. In this paper, by\nleveraging the superior expression of the fully-connected tensor network (FCTN)\ndecomposition, we propose a new tensor completion method named the fully\nconnected tensor network weighted optization(FCTN-WOPT). The algorithm performs\na composition of the completed tensor by initialising the factors from the FCTN\ndecomposition. We build a loss function with the weight tensor, the completed\ntensor and the incomplete tensor together, and then update the completed tensor\nusing the lbfgs gradient descent algorithm to reduce the spatial memory\noccupation and speed up iterations. Finally we test the completion with\nsynthetic data and real data (both image data and video data) and the results\nshow the advanced performance of our FCTN-WOPT when it is applied to\nhigher-order tensor completion.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.00861,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"A Differential Evolution-Enhanced Latent Factor Analysis Model for\n  High-dimensional and Sparse Data\n\n  High-dimensional and sparse (HiDS) matrices are frequently adopted to\ndescribe the complex relationships in various big data-related systems and\napplications. A Position-transitional Latent Factor Analysis (PLFA) model can\naccurately and efficiently represent an HiDS matrix. However, its involved\nlatent factors are optimized by stochastic gradient descent with the specific\ngradient direction step-by-step, which may cause a suboptimal solution. To\naddress this issue, this paper proposes a Sequential-Group-Differential-\nEvolution (SGDE) algorithm to refine the latent factors optimized by a PLFA\nmodel, thereby achieving a highly-accurate SGDE-PLFA model to HiDS matrices. As\ndemonstrated by the experiments on four HiDS matrices, a SGDE-PLFA model\noutperforms the state-of-the-art models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.13467,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"COSTI: a New Classifier for Sequences of Temporal Intervals\n\n  Classification of sequences of temporal intervals is a part of time series\nanalysis which concerns series of events. We propose a new method of\ntransforming the problem to a task of multivariate series classification. We\nuse one of the state-of-the-art algorithms from the latter domain on the new\nrepresentation to obtain significantly better accuracy than the\nstate-of-the-art methods from the former field. We discuss limitations of this\nworkflow and address them by developing a novel method for classification\ntermed COSTI (short for Classification of Sequences of Temporal Intervals)\noperating directly on sequences of temporal intervals. The proposed method\nremains at a high level of accuracy and obtains better performance while\navoiding shortcomings connected to operating on transformed data. We propose a\ngeneralized version of the problem of classification of temporal intervals,\nwhere each event is supplemented with information about its intensity. We also\nprovide two new data sets where this information is of substantial value.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.02735,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000048346,
      "text":"Distilling Robust and Non-Robust Features in Adversarial Examples by\n  Information Bottleneck\n\n  Adversarial examples, generated by carefully crafted perturbation, have\nattracted considerable attention in research fields. Recent works have argued\nthat the existence of the robust and non-robust features is a primary cause of\nthe adversarial examples, and investigated their internal interactions in the\nfeature space. In this paper, we propose a way of explicitly distilling feature\nrepresentation into the robust and non-robust features, using Information\nBottleneck. Specifically, we inject noise variation to each feature unit and\nevaluate the information flow in the feature representation to dichotomize\nfeature units either robust or non-robust, based on the noise variation\nmagnitude. Through comprehensive experiments, we demonstrate that the distilled\nfeatures are highly correlated with adversarial prediction, and they have\nhuman-perceptible semantic information by themselves. Furthermore, we present\nan attack mechanism intensifying the gradient of non-robust features that is\ndirectly related to the model prediction, and validate its effectiveness of\nbreaking model robustness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.11423,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000033114,
      "text":"Trusted Multi-View Classification with Dynamic Evidential Fusion\n\n  Existing multi-view classification algorithms focus on promoting accuracy by\nexploiting different views, typically integrating them into common\nrepresentations for follow-up tasks. Although effective, it is also crucial to\nensure the reliability of both the multi-view integration and the final\ndecision, especially for noisy, corrupted and out-of-distribution data.\nDynamically assessing the trustworthiness of each view for different samples\ncould provide reliable integration. This can be achieved through uncertainty\nestimation. With this in mind, we propose a novel multi-view classification\nalgorithm, termed trusted multi-view classification (TMC), providing a new\nparadigm for multi-view learning by dynamically integrating different views at\nan evidence level. The proposed TMC can promote classification reliability by\nconsidering evidence from each view. Specifically, we introduce the variational\nDirichlet to characterize the distribution of the class probabilities,\nparameterized with evidence from different views and integrated with the\nDempster-Shafer theory. The unified learning framework induces accurate\nuncertainty and accordingly endows the model with both reliability and\nrobustness against possible noise or corruption. Both theoretical and\nexperimental results validate the effectiveness of the proposed model in\naccuracy, robustness and trustworthiness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.07071,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"A Unified Analysis of Dynamic Interactive Learning\n\n  In this paper we investigate the problem of learning evolving concepts over a\ncombinatorial structure. Previous work by Emamjomeh-Zadeh et al. [2020]\nintroduced dynamics into interactive learning as a way to model non-static user\npreferences in clustering problems or recommender systems. We provide many\nuseful contributions to this problem. First, we give a framework that captures\nboth of the models analyzed by [Emamjomeh-Zadeh et al., 2020], which allows us\nto study any type of concept evolution and matches the same query complexity\nbounds and running time guarantees of the previous models. Using this general\nmodel we solve the open problem of closing the gap between the upper and lower\nbounds on query complexity. Finally, we study an efficient algorithm where the\nlearner simply follows the feedback at each round, and we provide mistake\nbounds for low diameter graphs such as cliques, stars, and general o(log n)\ndiameter graphs by using a Markov Chain model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.11115,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Time Series Forecasting (TSF) Using Various Deep Learning Models\n\n  Time Series Forecasting (TSF) is used to predict the target variables at a\nfuture time point based on the learning from previous time points. To keep the\nproblem tractable, learning methods use data from a fixed length window in the\npast as an explicit input. In this paper, we study how the performance of\npredictive models change as a function of different look-back window sizes and\ndifferent amounts of time to predict into the future. We also consider the\nperformance of the recent attention-based Transformer models, which has had\ngood success in the image processing and natural language processing domains.\nIn all, we compare four different deep learning methods (RNN, LSTM, GRU, and\nTransformer) along with a baseline method. The dataset (hourly) we used is the\nBeijing Air Quality Dataset from the UCI website, which includes a multivariate\ntime series of many factors measured on an hourly basis for a period of 5 years\n(2010-14). For each model, we also report on the relationship between the\nperformance and the look-back window sizes and the number of predicted time\npoints into the future. Our experiments suggest that Transformer models have\nthe best performance with the lowest Mean Average Errors (MAE = 14.599, 23.273)\nand Root Mean Square Errors (RSME = 23.573, 38.131) for most of our single-step\nand multi-steps predictions. The best size for the look-back window to predict\n1 hour into the future appears to be one day, while 2 or 4 days perform the\nbest to predict 3 hours into the future.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.05351,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Graph Ordering Attention Networks\n\n  Graph Neural Networks (GNNs) have been successfully used in many problems\ninvolving graph-structured data, achieving state-of-the-art performance. GNNs\ntypically employ a message-passing scheme, in which every node aggregates\ninformation from its neighbors using a permutation-invariant aggregation\nfunction. Standard well-examined choices such as the mean or sum aggregation\nfunctions have limited capabilities, as they are not able to capture\ninteractions among neighbors. In this work, we formalize these interactions\nusing an information-theoretic framework that notably includes synergistic\ninformation. Driven by this definition, we introduce the Graph Ordering\nAttention (GOAT) layer, a novel GNN component that captures interactions\nbetween nodes in a neighborhood. This is achieved by learning local node\norderings via an attention mechanism and processing the ordered representations\nusing a recurrent neural network aggregator. This design allows us to make use\nof a permutation-sensitive aggregator while maintaining the\npermutation-equivariance of the proposed GOAT layer. The GOAT model\ndemonstrates its increased performance in modeling graph metrics that capture\ncomplex information, such as the betweenness centrality and the effective size\nof a node. In practical use-cases, its superior modeling capability is\nconfirmed through its success in several real-world node classification\nbenchmarks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.04511,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"FuNNscope: Visual microscope for interactively exploring the loss\n  landscape of fully connected neural networks\n\n  Despite their effective use in various fields, many aspects of neural\nnetworks are poorly understood. One important way to investigate the\ncharacteristics of neural networks is to explore the loss landscape. However,\nmost models produce a high-dimensional non-convex landscape which is difficult\nto visualize. We discuss and extend existing visualization methods based on 1D-\nand 2D slicing with a novel method that approximates the actual loss landscape\ngeometry by using charts with interpretable axes. Based on the assumption that\nobservations on small neural networks can generalize to more complex systems\nand provide us with helpful insights, we focus on small models in the range of\na few dozen weights, which enables computationally cheap experiments and the\nuse of an interactive dashboard. We observe symmetries around the zero vector,\nthe influence of different layers on the global landscape, the different weight\nsensitivities around a minimizer, and how gradient descent navigates high-loss\nobstacles. The user study resulted in an average SUS (System Usability Scale)\nscore with suggestions for improvement and opened up a number of possible\napplication scenarios, such as autoencoders and ensemble networks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.13607,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"Process-BERT: A Framework for Representation Learning on Educational\n  Process Data\n\n  Educational process data, i.e., logs of detailed student activities in\ncomputerized or online learning platforms, has the potential to offer deep\ninsights into how students learn. One can use process data for many downstream\ntasks such as learning outcome prediction and automatically delivering\npersonalized intervention. However, analyzing process data is challenging since\nthe specific format of process data varies a lot depending on different\nlearning\/testing scenarios. In this paper, we propose a framework for learning\nrepresentations of educational process data that is applicable across many\ndifferent learning scenarios. Our framework consists of a pre-training step\nthat uses BERT-type objectives to learn representations from sequential process\ndata and a fine-tuning step that further adjusts these representations on\ndownstream prediction tasks. We apply our framework to the 2019 nation's report\ncard data mining competition dataset that consists of student problem-solving\nprocess data and detail the specific models we use in this scenario. We conduct\nboth quantitative and qualitative experiments to show that our framework\nresults in process data representations that are both predictive and\ninformative.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.00846,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural\n  Networks\n\n  Lipschitz constants of neural networks allow for guarantees of robustness in\nimage classification, safety in controller design, and generalizability beyond\nthe training data. As calculating Lipschitz constants is NP-hard, techniques\nfor estimating Lipschitz constants must navigate the trade-off between\nscalability and accuracy. In this work, we significantly push the scalability\nfrontier of a semidefinite programming technique known as LipSDP while\nachieving zero accuracy loss. We first show that LipSDP has chordal sparsity,\nwhich allows us to derive a chordally sparse formulation that we call\nChordal-LipSDP. The key benefit is that the main computational bottleneck of\nLipSDP, a large semidefinite constraint, is now decomposed into an equivalent\ncollection of smaller ones: allowing Chordal-LipSDP to outperform LipSDP\nparticularly as the network depth grows. Moreover, our formulation uses a\ntunable sparsity parameter that enables one to gain tighter estimates without\nincurring a significant computational cost. We illustrate the scalability of\nour approach through extensive numerical experiments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.09975,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000004073,
      "text":"Eliminating Backdoor Triggers for Deep Neural Networks Using Attention\n  Relation Graph Distillation\n\n  Due to the prosperity of Artificial Intelligence (AI) techniques, more and\nmore backdoors are designed by adversaries to attack Deep Neural Networks\n(DNNs).Although the state-of-the-art method Neural Attention Distillation (NAD)\ncan effectively erase backdoor triggers from DNNs, it still suffers from\nnon-negligible Attack Success Rate (ASR) together with lowered classification\nACCuracy (ACC), since NAD focuses on backdoor defense using attention features\n(i.e., attention maps) of the same order. In this paper, we introduce a novel\nbackdoor defense framework named Attention Relation Graph Distillation (ARGD),\nwhich fully explores the correlation among attention features with different\norders using our proposed Attention Relation Graphs (ARGs). Based on the\nalignment of ARGs between both teacher and student models during knowledge\ndistillation, ARGD can eradicate more backdoor triggers than NAD. Comprehensive\nexperimental results show that, against six latest backdoor attacks, ARGD\noutperforms NAD by up to 94.85% reduction in ASR, while ACC can be improved by\nup to 3.23%.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.13372,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000236101,
      "text":"Phase Shift Design in RIS Empowered Wireless Networks: From Optimization\n  to AI-Based Methods\n\n  Reconfigurable intelligent surfaces (RISs) have a revolutionary capability to\ncustomize the radio propagation environment for wireless networks. To fully\nexploit the advantages of RISs in wireless systems, the phases of the\nreflecting elements must be jointly designed with conventional communication\nresources, such as beamformers, transmit power, and computation time. However,\ndue to the unique constraints on the phase shift, and massive numbers of\nreflecting units and users in large-scale networks, the resulting optimization\nproblems are challenging to solve. This paper provides a review of current\noptimization methods and artificial intelligence-based methods for handling the\nconstraints imposed by RIS and compares them in terms of solution quality and\ncomputational complexity. Future challenges in phase shift optimization\ninvolving RISs are also described and potential solutions are discussed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.09583,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000023842,
      "text":"Improved Group Robustness via Classifier Retraining on Independent\n  Splits\n\n  Deep neural networks trained by minimizing the average risk can achieve\nstrong average performance. Still, their performance for a subgroup may degrade\nif the subgroup is underrepresented in the overall data population. Group\ndistributionally robust optimization (Sagawa et al., 2020a), or group DRO in\nshort, is a widely used baseline for learning models with strong worst-group\nperformance. We note that this method requires group labels for every example\nat training time and can overfit to small groups, requiring strong\nregularization. Given a limited amount of group labels at training time, Just\nTrain Twice (Liu et al., 2021), or JTT in short, is a two-stage method that\ninfers a pseudo group label for every unlabeled example first, then applies\ngroup DRO based on the inferred group labels. The inference process is also\nsensitive to overfitting, sometimes involving additional hyperparameters. This\npaper designs a simple method based on the idea of classifier retraining on\nindependent splits of the training data. We find that using a novel\nsample-splitting procedure achieves robust worst-group performance in the\nfine-tuning step. When evaluated on benchmark image and text classification\ntasks, our approach consistently performs favorably to group DRO, JTT, and\nother strong baselines when either group labels are available during training\nor are only given in validation sets. Importantly, our method only relies on a\nsingle hyperparameter, which adjusts the fraction of labels used for training\nfeature extractors vs. training classification layers. We justify the rationale\nof our splitting scheme with a generalization-bound analysis of the worst-group\nloss.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.15653,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000007583,
      "text":"Label-Enhanced Graph Neural Network for Semi-supervised Node\n  Classification\n\n  Graph Neural Networks (GNNs) have been widely applied in the semi-supervised\nnode classification task, where a key point lies in how to sufficiently\nleverage the limited but valuable label information. Most of the classical GNNs\nsolely use the known labels for computing the classification loss at the\noutput. In recent years, several methods have been designed to additionally\nutilize the labels at the input. One part of the methods augment the node\nfeatures via concatenating or adding them with the one-hot encodings of labels,\nwhile other methods optimize the graph structure by assuming neighboring nodes\ntend to have the same label. To bring into full play the rich information of\nlabels, in this paper, we present a label-enhanced learning framework for GNNs,\nwhich first models each label as a virtual center for intra-class nodes and\nthen jointly learns the representations of both nodes and labels. Our approach\ncould not only smooth the representations of nodes belonging to the same class,\nbut also explicitly encode the label semantics into the learning process of\nGNNs. Moreover, a training node selection technique is provided to eliminate\nthe potential label leakage issue and guarantee the model generalization\nability. Finally, an adaptive self-training strategy is proposed to iteratively\nenlarge the training set with more reliable pseudo labels and distinguish the\nimportance of each pseudo-labeled node during the model training process.\nExperimental results on both real-world and synthetic datasets demonstrate our\napproach can not only consistently outperform the state-of-the-arts, but also\neffectively smooth the representations of intra-class nodes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.1376,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Tranception: protein fitness prediction with autoregressive transformers\n  and inference-time retrieval\n\n  The ability to accurately model the fitness landscape of protein sequences is\ncritical to a wide range of applications, from quantifying the effects of human\nvariants on disease likelihood, to predicting immune-escape mutations in\nviruses and designing novel biotherapeutic proteins. Deep generative models of\nprotein sequences trained on multiple sequence alignments have been the most\nsuccessful approaches so far to address these tasks. The performance of these\nmethods is however contingent on the availability of sufficiently deep and\ndiverse alignments for reliable training. Their potential scope is thus limited\nby the fact many protein families are hard, if not impossible, to align. Large\nlanguage models trained on massive quantities of non-aligned protein sequences\nfrom diverse families address these problems and show potential to eventually\nbridge the performance gap. We introduce Tranception, a novel transformer\narchitecture leveraging autoregressive predictions and retrieval of homologous\nsequences at inference to achieve state-of-the-art fitness prediction\nperformance. Given its markedly higher performance on multiple mutants,\nrobustness to shallow alignments and ability to score indels, our approach\noffers significant gain of scope over existing approaches. To enable more\nrigorous model testing across a broader range of protein families, we develop\nProteinGym -- an extensive set of multiplexed assays of variant effects,\nsubstantially increasing both the number and diversity of assays compared to\nexisting benchmarks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.15733,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Template based Graph Neural Network with Optimal Transport Distances\n\n  Current Graph Neural Networks (GNN) architectures generally rely on two\nimportant components: node features embedding through message passing, and\naggregation with a specialized form of pooling. The structural (or topological)\ninformation is implicitly taken into account in these two steps. We propose in\nthis work a novel point of view, which places distances to some learnable graph\ntemplates at the core of the graph representation. This distance embedding is\nconstructed thanks to an optimal transport distance: the Fused\nGromov-Wasserstein (FGW) distance, which encodes simultaneously feature and\nstructure dissimilarities by solving a soft graph-matching problem. We\npostulate that the vector of FGW distances to a set of template graphs has a\nstrong discriminative power, which is then fed to a non-linear classifier for\nfinal predictions. Distance embedding can be seen as a new layer, and can\nleverage on existing message passing techniques to promote sensible feature\nrepresentations. Interestingly enough, in our work the optimal set of template\ngraphs is also learnt in an end-to-end fashion by differentiating through this\nlayer. After describing the corresponding learning procedure, we empirically\nvalidate our claim on several synthetic and real life graph classification\ndatasets, where our method is competitive or surpasses kernel and GNN\nstate-of-the-art approaches. We complete our experiments by an ablation study\nand a sensitivity analysis to parameters.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.03005,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Design Target Achievement Index: A Differentiable Metric to Enhance Deep\n  Generative Models in Multi-Objective Inverse Design\n\n  Deep Generative Machine Learning Models have been growing in popularity\nacross the design community thanks to their ability to learn and mimic complex\ndata distributions. While early works are promising, further advancement will\ndepend on addressing several critical considerations such as design quality,\nfeasibility, novelty, and targeted inverse design. We propose the Design Target\nAchievement Index (DTAI), a differentiable, tunable metric that scores a\ndesign's ability to achieve designer-specified minimum performance targets. We\ndemonstrate that DTAI can drastically improve the performance of generated\ndesigns when directly used as a training loss in Deep Generative Models. We\napply the DTAI loss to a Performance-Augmented Diverse GAN (PaDGAN) and\ndemonstrate superior generative performance compared to a set of baseline Deep\nGenerative Models including a Multi-Objective PaDGAN and specialized tabular\ngeneration algorithms like the Conditional Tabular GAN (CTGAN). We further\nenhance PaDGAN with an auxiliary feasibility classifier to encourage feasible\ndesigns. To evaluate methods, we propose a comprehensive set of evaluation\nmetrics for generative methods that focus on feasibility, diversity, and\nsatisfaction of design performance targets. Methods are tested on a challenging\nbenchmarking problem: the FRAMED bicycle frame design dataset featuring\nmixed-datatype parametric data, heavily skewed and multimodal distributions,\nand ten competing performance objectives.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.14403,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000078811,
      "text":"Rethinking the Setting of Semi-supervised Learning on Graphs\n\n  We argue that the present setting of semisupervised learning on graphs may\nresult in unfair comparisons, due to its potential risk of over-tuning\nhyper-parameters for models. In this paper, we highlight the significant\ninfluence of tuning hyper-parameters, which leverages the label information in\nthe validation set to improve the performance. To explore the limit of\nover-tuning hyperparameters, we propose ValidUtil, an approach to fully utilize\nthe label information in the validation set through an extra group of\nhyper-parameters. With ValidUtil, even GCN can easily get high accuracy of\n85.8% on Cora.\n  To avoid over-tuning, we merge the training set and the validation set and\nconstruct an i.i.d. graph benchmark (IGB) consisting of 4 datasets. Each\ndataset contains 100 i.i.d. graphs sampled from a large graph to reduce the\nevaluation variance. Our experiments suggest that IGB is a more stable\nbenchmark than previous datasets for semisupervised learning on graphs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.09573,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"Jacobian Granger Causal Neural Networks for Analysis of Stationary and\n  Nonstationary Data\n\n  Granger causality is a commonly used method for uncovering information flow\nand dependencies in a time series. Here we introduce JGC (Jacobian Granger\nCausality), a neural network-based approach to Granger causality using the\nJacobian as a measure of variable importance, and propose a thresholding\nprocedure for inferring Granger causal variables using this measure. The\nresulting approach performs consistently well compared to other approaches in\nidentifying Granger causal variables, the associated time lags, as well as\ninteraction signs. Lastly, through the inclusion of a time variable, we show\nthat this approach is able to learn the temporal dependencies for nonstationary\nsystems whose Granger causal structures change in time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.11412,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000048015,
      "text":"Instance-Based Uncertainty Estimation for Gradient-Boosted Regression\n  Trees\n\n  Gradient-boosted regression trees (GBRTs) are hugely popular for solving\ntabular regression problems, but provide no estimate of uncertainty. We propose\nInstance-Based Uncertainty estimation for Gradient-boosted regression trees\n(IBUG), a simple method for extending any GBRT point predictor to produce\nprobabilistic predictions. IBUG computes a non-parametric distribution around a\nprediction using the $k$-nearest training instances, where distance is measured\nwith a tree-ensemble kernel. The runtime of IBUG depends on the number of\ntraining examples at each leaf in the ensemble, and can be improved by sampling\ntrees or training instances. Empirically, we find that IBUG achieves similar or\nbetter performance than the previous state-of-the-art across 22 benchmark\nregression datasets. We also find that IBUG can achieve improved probabilistic\nperformance by using different base GBRT models, and can more flexibly model\nthe posterior distribution of a prediction than competing methods. We also find\nthat previous methods suffer from poor probabilistic calibration on some\ndatasets, which can be mitigated using a scalar factor tuned on the validation\ndata. Source code is available at https:\/\/www.github.com\/jjbrophy47\/ibug.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.01358,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000188086,
      "text":"Learning Label Initialization for Time-Dependent Harmonic Extension\n\n  Node classification on graphs can be formulated as the Dirichlet problem on\ngraphs where the signal is given at the labeled nodes, and the harmonic\nextension is done on the unlabeled nodes. This paper considers a time-dependent\nversion of the Dirichlet problem on graphs and shows how to improve its\nsolution by learning the proper initialization vector on the unlabeled nodes.\nFurther, we show that the improved solution is at par with state-of-the-art\nmethods used for node classification. Finally, we conclude this paper by\ndiscussing the importance of parameter t, pros, and future directions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.03257,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000030133,
      "text":"Synthetic Data -- what, why and how?\n\n  This explainer document aims to provide an overview of the current state of\nthe rapidly expanding work on synthetic data technologies, with a particular\nfocus on privacy. The article is intended for a non-technical audience, though\nsome formal definitions have been given to provide clarity to specialists. This\narticle is intended to enable the reader to quickly become familiar with the\nnotion of synthetic data, as well as understand some of the subtle intricacies\nthat come with it. We do believe that synthetic data is a very useful tool, and\nour hope is that this report highlights that, while drawing attention to\nnuances that can easily be overlooked in its deployment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.13988,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000057949,
      "text":"Deep Ensembles for Graphs with Higher-order Dependencies\n\n  Graph neural networks (GNNs) continue to achieve state-of-the-art performance\non many graph learning tasks, but rely on the assumption that a given graph is\na sufficient approximation of the true neighborhood structure. When a system\ncontains higher-order sequential dependencies, we show that the tendency of\ntraditional graph representations to underfit each node's neighborhood causes\nexisting GNNs to generalize poorly. To address this, we propose a novel Deep\nGraph Ensemble (DGE), which captures neighborhood variance by training an\nensemble of GNNs on different neighborhood subspaces of the same node within a\nhigher-order network structure. We show that DGE consistently outperforms\nexisting GNNs on semisupervised and supervised tasks on six real-world data\nsets with known higher-order dependencies, even under a similar parameter\nbudget. We demonstrate that learning diverse and accurate base classifiers is\ncentral to DGE's success, and discuss the implications of these findings for\nfuture work on ensembles of GNNs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.09332,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000002318,
      "text":"Accelerated Training of Physics-Informed Neural Networks (PINNs) using\n  Meshless Discretizations\n\n  We present a new technique for the accelerated training of physics-informed\nneural networks (PINNs): discretely-trained PINNs (DT-PINNs). The repeated\ncomputation of partial derivative terms in the PINN loss functions via\nautomatic differentiation during training is known to be computationally\nexpensive, especially for higher-order derivatives. DT-PINNs are trained by\nreplacing these exact spatial derivatives with high-order accurate numerical\ndiscretizations computed using meshless radial basis function-finite\ndifferences (RBF-FD) and applied via sparse-matrix vector multiplication. The\nuse of RBF-FD allows for DT-PINNs to be trained even on point cloud samples\nplaced on irregular domain geometries. Additionally, though traditional PINNs\n(vanilla-PINNs) are typically stored and trained in 32-bit floating-point\n(fp32) on the GPU, we show that for DT-PINNs, using fp64 on the GPU leads to\nsignificantly faster training times than fp32 vanilla-PINNs with comparable\naccuracy. We demonstrate the efficiency and accuracy of DT-PINNs via a series\nof experiments. First, we explore the effect of network depth on both numerical\nand automatic differentiation of a neural network with random weights and show\nthat RBF-FD approximations of third-order accuracy and above are more efficient\nwhile being sufficiently accurate. We then compare the DT-PINNs to\nvanilla-PINNs on both linear and nonlinear Poisson equations and show that\nDT-PINNs achieve similar losses with 2-4x faster training times on a consumer\nGPU. Finally, we also demonstrate that similar results can be obtained for the\nPINN solution to the heat equation (a space-time problem) by discretizing the\nspatial derivatives using RBF-FD and using automatic differentiation for the\ntemporal derivative. Our results show that fp64 DT-PINNs offer a superior\ncost-accuracy profile to fp32 vanilla-PINNs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.10264,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"DEMAND: Deep Matrix Approximately Nonlinear Decomposition to Identify\n  Meta, Canonical, and Sub-Spatial Pattern of functional Magnetic Resonance\n  Imaging in the Human Brain\n\n  Deep Neural Networks (DNNs) have already become a crucial computational\napproach to revealing the spatial patterns in the human brain; however, there\nare three major shortcomings in utilizing DNNs to detect the spatial patterns\nin functional Magnetic Resonance Signals: 1). It is a fully connected\narchitecture that increases the complexity of network structures that is\ndifficult to optimize and vulnerable to overfitting; 2). The requirement of\nlarge training samples results in erasing the individual\/minor patterns in\nfeature extraction; 3). The hyperparameters are required to be tuned manually,\nwhich is time-consuming. Therefore, we propose a novel deep nonlinear matrix\nfactorization named Deep Matrix Approximately Nonlinear Decomposition (DEMAND)\nin this work to take advantage of the shallow linear model, e.g., Sparse\nDictionary Learning (SDL) and DNNs. At first, the proposed DEMAND employs a\nnon-fully connected and multilayer-stacked architecture that is easier to be\noptimized compared with canonical DNNs; furthermore, due to the efficient\narchitecture, training DEMAND can avoid overfitting and enables the recognition\nof individual\/minor features based on a small dataset such as an individual\ndata; finally, a novel rank estimator technique is introduced to tune all\nhyperparameters of DEMAND automatically. Moreover, the proposed DEMAND is\nvalidated by four other peer methodologies via real functional Magnetic\nResonance Imaging data in the human brain. In short, the validation results\ndemonstrate that DEMAND can reveal the reproducible meta, canonical, and\nsub-spatial features of the human brain more efficiently than other peer\nmethodologies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.15623,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000019537,
      "text":"k-Means Maximum Entropy Exploration\n\n  Exploration in high-dimensional, continuous spaces with sparse rewards is an\nopen problem in reinforcement learning. Artificial curiosity algorithms address\nthis by creating rewards that lead to exploration. Given a reinforcement\nlearning algorithm capable of maximizing rewards, the problem reduces to\nfinding an optimization objective consistent with exploration. Maximum entropy\nexploration uses the entropy of the state visitation distribution as such an\nobjective. However, efficiently estimating the entropy of the state visitation\ndistribution is challenging in high-dimensional, continuous spaces. We\nintroduce an artificial curiosity algorithm based on lower bounding an\napproximation to the entropy of the state visitation distribution. The bound\nrelies on a result we prove for non-parametric density estimation in arbitrary\ndimensions using k-means. We show that our approach is both computationally\nefficient and competitive on benchmarks for exploration in high-dimensional,\ncontinuous spaces, especially on tasks where reinforcement learning algorithms\nare unable to find rewards.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.03169,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000031789,
      "text":"The NT-Xent loss upper bound\n\n  Self-supervised learning is a growing paradigm in deep representation\nlearning, showing great generalization capabilities and competitive performance\nin low-labeled data regimes. The SimCLR framework proposes the NT-Xent loss for\ncontrastive representation learning. The objective of the loss function is to\nmaximize agreement, similarity, between sampled positive pairs. This short\npaper derives and proposes an upper bound for the loss and average similarity.\nAn analysis of the implications is however not provided, but we strongly\nencourage anyone in the field to conduct this.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.14276,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"So3krates: Equivariant attention for interactions on arbitrary\n  length-scales in molecular systems\n\n  The application of machine learning methods in quantum chemistry has enabled\nthe study of numerous chemical phenomena, which are computationally intractable\nwith traditional ab-initio methods. However, some quantum mechanical properties\nof molecules and materials depend on non-local electronic effects, which are\noften neglected due to the difficulty of modeling them efficiently. This work\nproposes a modified attention mechanism adapted to the underlying physics,\nwhich allows to recover the relevant non-local effects. Namely, we introduce\nspherical harmonic coordinates (SPHCs) to reflect higher-order geometric\ninformation for each atom in a molecule, enabling a non-local formulation of\nattention in the SPHC space. Our proposed model So3krates - a self-attention\nbased message passing neural network - uncouples geometric information from\natomic features, making them independently amenable to attention mechanisms.\nThereby we construct spherical filters, which extend the concept of continuous\nfilters in Euclidean space to SPHC space and serve as foundation for a\nspherical self-attention mechanism. We show that in contrast to other published\nmethods, So3krates is able to describe non-local quantum mechanical effects\nover arbitrary length scales. Further, we find evidence that the inclusion of\nhigher-order geometric correlations increases data efficiency and improves\ngeneralization. So3krates matches or exceeds state-of-the-art performance on\npopular benchmarks, notably, requiring a significantly lower number of\nparameters (0.25 - 0.4x) while at the same time giving a substantial speedup (6\n- 14x for training and 2 - 11x for inference) compared to other models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.0267,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000038081,
      "text":"Unsupervised Mismatch Localization in Cross-Modal Sequential Data with\n  Application to Mispronunciations Localization\n\n  Content mismatch usually occurs when data from one modality is translated to\nanother, e.g. language learners producing mispronunciations (errors in speech)\nwhen reading a sentence (target text) aloud. However, most existing alignment\nalgorithms assume that the content involved in the two modalities is perfectly\nmatched, thus leading to difficulty in locating such mismatch between speech\nand text. In this work, we develop an unsupervised learning algorithm that can\ninfer the relationship between content-mismatched cross-modal sequential data,\nespecially for speech-text sequences. More specifically, we propose a\nhierarchical Bayesian deep learning model, dubbed mismatch localization\nvariational autoencoder (ML-VAE), which decomposes the generative process of\nthe speech into hierarchically structured latent variables, indicating the\nrelationship between the two modalities. Training such a model is very\nchallenging due to the discrete latent variables with complex dependencies\ninvolved. To address this challenge, we propose a novel and effective training\nprocedure that alternates between estimating the hard assignments of the\ndiscrete latent variables over a specifically designed mismatch localization\nfinite-state acceptor (ML-FSA) and updating the parameters of neural networks.\nIn this work, we focus on the mismatch localization problem for speech and\ntext, and our experimental results show that ML-VAE successfully locates the\nmismatch between text and speech, without the need for human annotations for\nmodel training.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.15763,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"Exact Feature Collisions in Neural Networks\n\n  Predictions made by deep neural networks were shown to be highly sensitive to\nsmall changes made in the input space where such maliciously crafted data\npoints containing small perturbations are being referred to as adversarial\nexamples. On the other hand, recent research suggests that the same networks\ncan also be extremely insensitive to changes of large magnitude, where\npredictions of two largely different data points can be mapped to approximately\nthe same output. In such cases, features of two data points are said to\napproximately collide, thus leading to the largely similar predictions. Our\nresults improve and extend the work of Li et al.(2019), laying out theoretical\ngrounds for the data points that have colluding features from the perspective\nof weights of neural networks, revealing that neural networks not only suffer\nfrom features that approximately collide but also suffer from features that\nexactly collide. We identify the necessary conditions for the existence of such\nscenarios, hereby investigating a large number of DNNs that have been used to\nsolve various computer vision problems. Furthermore, we propose the Null-space\nsearch, a numerical approach that does not rely on heuristics, to create data\npoints with colliding features for any input and for any task, including, but\nnot limited to, classification, localization, and segmentation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.10678,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"On the problem of entity matching and its application in automated\n  settlement of receivables\n\n  This paper covers automated settlement of receivables in non-governmental\norganizations. We tackle the problem with entity matching techniques. We\nconsider setup, where base algorithm is used for preliminary ranking of\nmatches, then we apply several novel methods to increase matching quality of\nbase algorithm: score post processing, cascade model and chain model. The\nmethods presented here contribute to automated settlement of receivables,\nentity matching and multilabel classification in open-world scenario. We\nevaluate our approach on real world operational data which come from company\nproviding settlement of receivables as a service: proposed methods boost recall\nfrom 78% (base model) to >90% at precision 99%.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.03168,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Defending against Reconstruction Attacks through Differentially Private\n  Federated Learning for Classification of Heterogeneous Chest X-Ray Data\n\n  Privacy regulations and the physical distribution of heterogeneous data are\noften primary concerns for the development of deep learning models in a medical\ncontext. This paper evaluates the feasibility of differentially private\nfederated learning for chest X-ray classification as a defense against data\nprivacy attacks. To the best of our knowledge, we are the first to directly\ncompare the impact of differentially private training on two different neural\nnetwork architectures, DenseNet121 and ResNet50. Extending the federated\nlearning environments previously analyzed in terms of privacy, we simulated a\nheterogeneous and imbalanced federated setting by distributing images from the\npublic CheXpert and Mendeley chest X-ray datasets unevenly among 36 clients.\nBoth non-private baseline models achieved an area under the receiver operating\ncharacteristic curve (AUC) of $0.94$ on the binary classification task of\ndetecting the presence of a medical finding. We demonstrate that both model\narchitectures are vulnerable to privacy violation by applying image\nreconstruction attacks to local model updates from individual clients. The\nattack was particularly successful during later training stages. To mitigate\nthe risk of privacy breach, we integrated R\\'enyi differential privacy with a\nGaussian noise mechanism into local model training. We evaluate model\nperformance and attack vulnerability for privacy budgets $\\epsilon \\in$ {1, 3,\n6, 10}. The DenseNet121 achieved the best utility-privacy trade-off with an AUC\nof $0.94$ for $\\epsilon$ = 6. Model performance deteriorated slightly for\nindividual clients compared to the non-private baseline. The ResNet50 only\nreached an AUC of $0.76$ in the same privacy setting. Its performance was\ninferior to that of the DenseNet121 for all considered privacy constraints,\nsuggesting that the DenseNet121 architecture is more robust to differentially\nprivate training.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.13481,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"DeepJoint: Robust Survival Modelling Under Clinical Presence Shift\n\n  Observational data in medicine arise as a result of the complex interaction\nbetween patients and the healthcare system. The sampling process is often\nhighly irregular and itself constitutes an informative process. When using such\ndata to develop prediction models, this phenomenon is often ignored, leading to\nsub-optimal performance and generalisability of models when practices evolve.\nWe propose a multi-task recurrent neural network which models three clinical\npresence dimensions -- namely the longitudinal, the inter-observation and the\nmissingness processes -- in parallel to the survival outcome. On a prediction\ntask using MIMIC III laboratory tests, explicit modelling of these three\nprocesses showed improved performance in comparison to state-of-the-art\npredictive models (C-index at 1 day horizon: 0.878). More importantly, the\nproposed approach was more robust to change in the clinical presence setting,\ndemonstrated by performance comparison between patients admitted on weekdays\nand weekends. This analysis demonstrates the importance of studying and\nleveraging clinical presence to improve performance and create more\ntransportable clinical models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.00796,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Stabilizing Q-learning with Linear Architectures for Provably Efficient\n  Learning\n\n  The $Q$-learning algorithm is a simple and widely-used stochastic\napproximation scheme for reinforcement learning, but the basic protocol can\nexhibit instability in conjunction with function approximation. Such\ninstability can be observed even with linear function approximation. In\npractice, tools such as target networks and experience replay appear to be\nessential, but the individual contribution of each of these mechanisms is not\nwell understood theoretically. This work proposes an exploration variant of the\nbasic $Q$-learning protocol with linear function approximation. Our modular\nanalysis illustrates the role played by each algorithmic tool that we adopt: a\nsecond order update rule, a set of target networks, and a mechanism akin to\nexperience replay. Together, they enable state of the art regret bounds on\nlinear MDPs while preserving the most prominent feature of the algorithm,\nnamely a space complexity independent of the number of step elapsed. We show\nthat the performance of the algorithm degrades very gracefully under a novel\nand more permissive notion of approximation error. The algorithm also exhibits\na form of instance-dependence, in that its performance depends on the\n\"effective\" feature dimension.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.07756,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Hybrid thermal modeling of additive manufacturing processes using\n  physics-informed neural networks for temperature prediction and parameter\n  identification\n\n  Understanding the thermal behavior of additive manufacturing (AM) processes\nis crucial for enhancing the quality control and enabling customized process\ndesign. Most purely physics-based computational models suffer from intensive\ncomputational costs and the need of calibrating unknown parameters, thus not\nsuitable for online control and iterative design application. Data-driven\nmodels taking advantage of the latest developed computational tools can serve\nas a more efficient surrogate, but they are usually trained over a large amount\nof simulation data and often fail to effectively use small but high-quality\nexperimental data. In this work, we developed a hybrid physics-based\ndata-driven thermal modeling approach of AM processes using physics-informed\nneural networks. Specifically, partially observed temperature data measured\nfrom an infrared camera is combined with the physics laws to predict full-field\ntemperature history and to discover unknown material and process parameters. In\nthe numerical and experimental examples, the effectiveness of adding auxiliary\ntraining data and using the pretrained model on training efficiency and\nprediction accuracy, as well as the ability to identify unknown parameters with\npartially observed data, are demonstrated. The results show that the hybrid\nthermal model can effectively identify unknown parameters and capture the\nfull-field temperature accurately, and thus it has the potential to be used in\niterative process design and real-time process control of AM.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.13888,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Explaining Any ML Model? -- On Goals and Capabilities of XAI\n\n  An increasing ubiquity of machine learning (ML) motivates research on\nalgorithms to explain ML models and their predictions -- so-called eXplainable\nArtificial Intelligence (XAI). Despite many survey papers and discussions, the\ngoals and capabilities of XAI algorithms are far from being well understood. We\nargue that this is because of a problematic reasoning scheme in XAI literature:\nXAI algorithms are said to complement ML models with desired properties, such\nas \"interpretability\", or \"explainability\". These properties are in turn\nassumed to contribute to a goal, like \"trust\" in an ML system. But most\nproperties lack precise definitions and their relationship to such goals is far\nfrom obvious. The result is a reasoning scheme that obfuscates research results\nand leaves an important question unanswered: What can one expect from XAI\nalgorithms? In this article, we clarify the goals and capabilities of XAI\nalgorithms from a concrete perspective: that of their users. Explaining ML\nmodels is only necessary if users have questions about them. We show that users\ncan ask diverse questions, but that only one of them can be answered by current\nXAI algorithms. Answering this core question can be trivial, difficult or even\nimpossible, depending on the ML application. Based on these insights, we\noutline which capabilities policymakers, researchers and society can reasonably\nexpect from XAI algorithms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.14418,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000234776,
      "text":"Optimization-Induced Graph Implicit Nonlinear Diffusion\n\n  Due to the over-smoothing issue, most existing graph neural networks can only\ncapture limited dependencies with their inherently finite aggregation layers.\nTo overcome this limitation, we propose a new kind of graph convolution, called\nGraph Implicit Nonlinear Diffusion (GIND), which implicitly has access to\ninfinite hops of neighbors while adaptively aggregating features with nonlinear\ndiffusion to prevent over-smoothing. Notably, we show that the learned\nrepresentation can be formalized as the minimizer of an explicit convex\noptimization objective. With this property, we can theoretically characterize\nthe equilibrium of our GIND from an optimization perspective. More\ninterestingly, we can induce new structural variants by modifying the\ncorresponding optimization objective. To be specific, we can embed prior\nproperties to the equilibrium, as well as introducing skip connections to\npromote training stability. Extensive experiments show that GIND is good at\ncapturing long-range dependencies, and performs well on both homophilic and\nheterophilic graphs with nonlinear diffusion. Moreover, we show that the\noptimization-induced variants of our models can boost the performance and\nimprove training stability and efficiency as well. As a result, our GIND\nobtains significant improvements on both node-level and graph-level tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.02126,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"Learning Dynamics and Generalization in Reinforcement Learning\n\n  Solving a reinforcement learning (RL) problem poses two competing challenges:\nfitting a potentially discontinuous value function, and generalizing well to\nnew observations. In this paper, we analyze the learning dynamics of temporal\ndifference algorithms to gain novel insight into the tension between these two\nobjectives. We show theoretically that temporal difference learning encourages\nagents to fit non-smooth components of the value function early in training,\nand at the same time induces the second-order effect of discouraging\ngeneralization. We corroborate these findings in deep RL agents trained on a\nrange of environments, finding that neural networks trained using temporal\ndifference algorithms on dense reward tasks exhibit weaker generalization\nbetween states than randomly initialized networks and networks trained with\npolicy gradient methods. Finally, we investigate how post-training policy\ndistillation may avoid this pitfall, and show that this approach improves\ngeneralization to novel environments in the ProcGen suite and improves\nrobustness to input perturbations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.05507,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000195371,
      "text":"Federated Learning with GAN-based Data Synthesis for Non-IID Clients\n\n  Federated learning (FL) has recently emerged as a popular privacy-preserving\ncollaborative learning paradigm. However, it suffers from the non-independent\nand identically distributed (non-IID) data among clients. In this paper, we\npropose a novel framework, named Synthetic Data Aided Federated Learning\n(SDA-FL), to resolve this non-IID challenge by sharing synthetic data.\nSpecifically, each client pretrains a local generative adversarial network\n(GAN) to generate differentially private synthetic data, which are uploaded to\nthe parameter server (PS) to construct a global shared synthetic dataset. To\ngenerate confident pseudo labels for the synthetic dataset, we also propose an\niterative pseudo labeling mechanism performed by the PS. A combination of the\nlocal private dataset and synthetic dataset with confident pseudo labels leads\nto nearly identical data distributions among clients, which improves the\nconsistency among local models and benefits the global aggregation. Extensive\nexperiments evidence that the proposed framework outperforms the baseline\nmethods by a large margin in several benchmark datasets under both the\nsupervised and semi-supervised settings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.0553,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000002914,
      "text":"Memorization-Dilation: Modeling Neural Collapse Under Label Noise\n\n  The notion of neural collapse refers to several emergent phenomena that have\nbeen empirically observed across various canonical classification problems.\nDuring the terminal phase of training a deep neural network, the feature\nembedding of all examples of the same class tend to collapse to a single\nrepresentation, and the features of different classes tend to separate as much\nas possible. Neural collapse is often studied through a simplified model,\ncalled the unconstrained feature representation, in which the model is assumed\nto have \"infinite expressivity\" and can map each data point to any arbitrary\nrepresentation. In this work, we propose a more realistic variant of the\nunconstrained feature representation that takes the limited expressivity of the\nnetwork into account. Empirical evidence suggests that the memorization of\nnoisy data points leads to a degradation (dilation) of the neural collapse.\nUsing a model of the memorization-dilation (M-D) phenomenon, we show one\nmechanism by which different losses lead to different performances of the\ntrained network on noisy data. Our proofs reveal why label smoothing, a\nmodification of cross-entropy empirically observed to produce a regularization\neffect, leads to improved generalization in classification tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.08738,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Detecting Adversarial Examples in Batches -- a geometrical approach\n\n  Many deep learning methods have successfully solved complex tasks in computer\nvision and speech recognition applications. Nonetheless, the robustness of\nthese models has been found to be vulnerable to perturbed inputs or adversarial\nexamples, which are imperceptible to the human eye, but lead the model to\nerroneous output decisions. In this study, we adapt and introduce two geometric\nmetrics, density and coverage, and evaluate their use in detecting adversarial\nsamples in batches of unseen data. We empirically study these metrics using\nMNIST and two real-world biomedical datasets from MedMNIST, subjected to two\ndifferent adversarial attacks. Our experiments show promising results for both\nmetrics to detect adversarial examples. We believe that his work can lay the\nground for further study on these metrics' use in deployed machine learning\nsystems to monitor for possible attacks by adversarial examples or related\npathologies such as dataset shift.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.02618,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"Generalized Federated Learning via Sharpness Aware Minimization\n\n  Federated Learning (FL) is a promising framework for performing\nprivacy-preserving, distributed learning with a set of clients. However, the\ndata distribution among clients often exhibits non-IID, i.e., distribution\nshift, which makes efficient optimization difficult. To tackle this problem,\nmany FL algorithms focus on mitigating the effects of data heterogeneity across\nclients by increasing the performance of the global model. However, almost all\nalgorithms leverage Empirical Risk Minimization (ERM) to be the local\noptimizer, which is easy to make the global model fall into a sharp valley and\nincrease a large deviation of parts of local clients. Therefore, in this paper,\nwe revisit the solutions to the distribution shift problem in FL with a focus\non local learning generality. To this end, we propose a general, effective\nalgorithm, \\texttt{FedSAM}, based on Sharpness Aware Minimization (SAM) local\noptimizer, and develop a momentum FL algorithm to bridge local and global\nmodels, \\texttt{MoFedSAM}. Theoretically, we show the convergence analysis of\nthese two algorithms and demonstrate the generalization bound of\n\\texttt{FedSAM}. Empirically, our proposed algorithms substantially outperform\nexisting FL studies and significantly decrease the learning deviation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.12848,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000020199,
      "text":"Analysis of Stochastic Processes through Replay Buffers\n\n  Replay buffers are a key component in many reinforcement learning schemes.\nYet, their theoretical properties are not fully understood. In this paper we\nanalyze a system where a stochastic process X is pushed into a replay buffer\nand then randomly sampled to generate a stochastic process Y from the replay\nbuffer. We provide an analysis of the properties of the sampled process such as\nstationarity, Markovity and autocorrelation in terms of the properties of the\noriginal process. Our theoretical analysis sheds light on why replay buffer may\nbe a good de-correlator. Our analysis provides theoretical tools for proving\nthe convergence of replay buffer based algorithms which are prevalent in\nreinforcement learning schemes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.148,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"Understanding Generalization via Leave-One-Out Conditional Mutual\n  Information\n\n  We study the mutual information between (certain summaries of) the output of\na learning algorithm and its $n$ training data, conditional on a supersample of\n$n+1$ i.i.d. data from which the training data is chosen at random without\nreplacement. These leave-one-out variants of the conditional mutual information\n(CMI) of an algorithm (Steinke and Zakynthinou, 2020) are also seen to control\nthe mean generalization error of learning algorithms with bounded loss\nfunctions. For learning algorithms achieving zero empirical risk under 0-1 loss\n(i.e., interpolating algorithms), we provide an explicit connection between\nleave-one-out CMI and the classical leave-one-out error estimate of the risk.\nUsing this connection, we obtain upper and lower bounds on risk in terms of the\n(evaluated) leave-one-out CMI. When the limiting risk is constant or decays\npolynomially, the bounds converge to within a constant factor of two. As an\napplication, we analyze the population risk of the one-inclusion graph\nalgorithm, a general-purpose transductive learning algorithm for VC classes in\nthe realizable setting. Using leave-one-out CMI, we match the optimal bound for\nlearning VC classes in the realizable setting, answering an open challenge\nraised by Steinke and Zakynthinou (2020). Finally, in order to understand the\nrole of leave-one-out CMI in studying generalization, we place leave-one-out\nCMI in a hierarchy of measures, with a novel unconditional mutual information\nat the root. For 0-1 loss and interpolating learning algorithms, this mutual\ninformation is observed to be precisely the risk.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.08175,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Not All Lotteries Are Made Equal\n\n  The Lottery Ticket Hypothesis (LTH) states that for a reasonably sized neural\nnetwork, a sub-network within the same network yields no less performance than\nthe dense counterpart when trained from the same initialization. This work\ninvestigates the relation between model size and the ease of finding these\nsparse sub-networks. We show through experiments that, surprisingly, under a\nfinite budget, smaller models benefit more from Ticket Search (TS).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.01131,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Predictive Multiplicity in Probabilistic Classification\n\n  Machine learning models are often used to inform real world risk assessment\ntasks: predicting consumer default risk, predicting whether a person suffers\nfrom a serious illness, or predicting a person's risk to appear in court. Given\nmultiple models that perform almost equally well for a prediction task, to what\nextent do predictions vary across these models? If predictions are relatively\nconsistent for similar models, then the standard approach of choosing the model\nthat optimizes a penalized loss suffices. But what if predictions vary\nsignificantly for similar models? In machine learning, this is referred to as\npredictive multiplicity i.e. the prevalence of conflicting predictions assigned\nby near-optimal competing models. In this paper, we present a framework for\nmeasuring predictive multiplicity in probabilistic classification (predicting\nthe probability of a positive outcome). We introduce measures that capture the\nvariation in risk estimates over the set of competing models, and develop\noptimization-based methods to compute these measures efficiently and reliably\nfor convex empirical risk minimization problems. We demonstrate the incidence\nand prevalence of predictive multiplicity in real-world tasks. Further, we\nprovide insight into how predictive multiplicity arises by analyzing the\nrelationship between predictive multiplicity and data set characteristics\n(outliers, separability, and majority-minority structure). Our results\nemphasize the need to report predictive multiplicity more widely.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.0833,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"Compressed-VFL: Communication-Efficient Learning with Vertically\n  Partitioned Data\n\n  We propose Compressed Vertical Federated Learning (C-VFL) for\ncommunication-efficient training on vertically partitioned data. In C-VFL, a\nserver and multiple parties collaboratively train a model on their respective\nfeatures utilizing several local iterations and sharing compressed intermediate\nresults periodically. Our work provides the first theoretical analysis of the\neffect message compression has on distributed training over vertically\npartitioned data. We prove convergence of non-convex objectives at a rate of\n$O(\\frac{1}{\\sqrt{T}})$ when the compression error is bounded over the course\nof training. We provide specific requirements for convergence with common\ncompression techniques, such as quantization and top-$k$ sparsification.\nFinally, we experimentally show compression can reduce communication by over\n$90\\%$ without a significant decrease in accuracy over VFL without compression.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.13507,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000038081,
      "text":"Envelope imbalanced ensemble model with deep sample learning and\n  local-global structure consistency\n\n  The class imbalance problem is important and challenging. Ensemble approaches\nare widely used to tackle this problem because of their effectiveness. However,\nexisting ensemble methods are always applied into original samples, while not\nconsidering the structure information among original samples. The limitation\nwill prevent the imbalanced learning from being better. Besides, research shows\nthat the structure information among samples includes local and global\nstructure information. Based on the analysis above, an imbalanced ensemble\nalgorithm with the deep sample pre-envelope network (DSEN) and local-global\nstructure consistency mechanism (LGSCM) is proposed here to solve the\nproblem.This algorithm can guarantee high-quality deep envelope samples for\nconsidering the local manifold and global structures information, which is\nhelpful for imbalance learning. First, the deep sample envelope pre-network\n(DSEN) is designed to mine structure information among samples.Then, the local\nmanifold structure metric (LMSM) and global structure distribution metric\n(GSDM) are designed to construct LGSCM to enhance distribution consistency of\ninterlayer samples. Next, the DSEN and LGSCM are put together to form the final\ndeep sample envelope network (DSEN-LG). After that, base classifiers are\napplied on the layers of deep samples respectively.Finally, the predictive\nresults from base classifiers are fused through bagging ensemble learning\nmechanism. To demonstrate the effectiveness of the proposed method, forty-four\npublic datasets and more than ten representative relevant algorithms are chosen\nfor verification. The experimental results show that the algorithm is\nsignificantly better than other imbalanced ensemble algorithms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.1114,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Understanding and Extending Subgraph GNNs by Rethinking Their Symmetries\n\n  Subgraph GNNs are a recent class of expressive Graph Neural Networks (GNNs)\nwhich model graphs as collections of subgraphs. So far, the design space of\npossible Subgraph GNN architectures as well as their basic theoretical\nproperties are still largely unexplored. In this paper, we study the most\nprominent form of subgraph methods, which employs node-based subgraph selection\npolicies such as ego-networks or node marking and deletion. We address two\ncentral questions: (1) What is the upper-bound of the expressive power of these\nmethods? and (2) What is the family of equivariant message passing layers on\nthese sets of subgraphs?. Our first step in answering these questions is a\nnovel symmetry analysis which shows that modelling the symmetries of node-based\nsubgraph collections requires a significantly smaller symmetry group than the\none adopted in previous works. This analysis is then used to establish a link\nbetween Subgraph GNNs and Invariant Graph Networks (IGNs). We answer the\nquestions above by first bounding the expressive power of subgraph methods by\n3-WL, and then proposing a general family of message-passing layers for\nsubgraph methods that generalises all previous node-based Subgraph GNNs.\nFinally, we design a novel Subgraph GNN dubbed SUN, which theoretically unifies\nprevious architectures while providing better empirical performance on multiple\nbenchmarks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.05852,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000151992,
      "text":"ChordMixer: A Scalable Neural Attention Model for Sequences with\n  Different Lengths\n\n  Sequential data naturally have different lengths in many domains, with some\nvery long sequences. As an important modeling tool, neural attention should\ncapture long-range interaction in such sequences. However, most existing neural\nattention models admit only short sequences, or they have to employ chunking or\npadding to enforce a constant input length. Here we propose a simple neural\nnetwork building block called ChordMixer which can model the attention for long\nsequences with variable lengths. Each ChordMixer block consists of a\nposition-wise rotation layer without learnable parameters and an element-wise\nMLP layer. Repeatedly applying such blocks forms an effective network backbone\nthat mixes the input signals towards the learning targets. We have tested\nChordMixer on the synthetic adding problem, long document classification, and\nDNA sequence-based taxonomy classification. The experiment results show that\nour method substantially outperforms other neural attention models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.10057,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Robust Deep Reinforcement Learning through Bootstrapped Opportunistic\n  Curriculum\n\n  Despite considerable advances in deep reinforcement learning, it has been\nshown to be highly vulnerable to adversarial perturbations to state\nobservations. Recent efforts that have attempted to improve adversarial\nrobustness of reinforcement learning can nevertheless tolerate only very small\nperturbations, and remain fragile as perturbation size increases. We propose\nBootstrapped Opportunistic Adversarial Curriculum Learning (BCL), a novel\nflexible adversarial curriculum learning framework for robust reinforcement\nlearning. Our framework combines two ideas: conservatively bootstrapping each\ncurriculum phase with highest quality solutions obtained from multiple runs of\nthe previous phase, and opportunistically skipping forward in the curriculum.\nIn our experiments we show that the proposed BCL framework enables dramatic\nimprovements in robustness of learned policies to adversarial perturbations.\nThe greatest improvement is for Pong, where our framework yields robustness to\nperturbations of up to 25\/255; in contrast, the best existing approach can only\ntolerate adversarial noise up to 5\/255. Our code is available at:\nhttps:\/\/github.com\/jlwu002\/BCL.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.01715,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Towards Evading the Limits of Randomized Smoothing: A Theoretical\n  Analysis\n\n  Randomized smoothing is the dominant standard for provable defenses against\nadversarial examples. Nevertheless, this method has recently been proven to\nsuffer from important information theoretic limitations. In this paper, we\nargue that these limitations are not intrinsic, but merely a byproduct of\ncurrent certification methods. We first show that these certificates use too\nlittle information about the classifier, and are in particular blind to the\nlocal curvature of the decision boundary. This leads to severely sub-optimal\nrobustness guarantees as the dimension of the problem increases. We then show\nthat it is theoretically possible to bypass this issue by collecting more\ninformation about the classifier. More precisely, we show that it is possible\nto approximate the optimal certificate with arbitrary precision, by probing the\ndecision boundary with several noise distributions. Since this process is\nexecuted at certification time rather than at test time, it entails no loss in\nnatural accuracy while enhancing the quality of the certificates. This result\nfosters further research on classifier-specific certification and demonstrates\nthat randomized smoothing is still worth investigating. Although\nclassifier-specific certification may induce more computational cost, we also\nprovide some theoretical insight on how to mitigate it.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.11489,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.00000553,
      "text":"Nearly Minimax Optimal Reinforcement Learning with Linear Function\n  Approximation\n\n  We study reinforcement learning with linear function approximation where the\ntransition probability and reward functions are linear with respect to a\nfeature mapping $\\boldsymbol{\\phi}(s,a)$. Specifically, we consider the\nepisodic inhomogeneous linear Markov Decision Process (MDP), and propose a\nnovel computation-efficient algorithm, LSVI-UCB$^+$, which achieves an\n$\\widetilde{O}(Hd\\sqrt{T})$ regret bound where $H$ is the episode length, $d$\nis the feature dimension, and $T$ is the number of steps. LSVI-UCB$^+$ builds\non weighted ridge regression and upper confidence value iteration with a\nBernstein-type exploration bonus. Our statistical results are obtained with\nnovel analytical tools, including a new Bernstein self-normalized bound with\nconservatism on elliptical potentials, and refined analysis of the correction\nterm. This is a minimax optimal algorithm for linear MDPs up to logarithmic\nfactors, which closes the $\\sqrt{Hd}$ gap between the upper bound of\n$\\widetilde{O}(\\sqrt{H^3d^3T})$ in (Jin et al., 2020) and lower bound of\n$\\Omega(Hd\\sqrt{T})$ for linear MDPs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.12782,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000061591,
      "text":"An Explainable Decision Support System for Predictive Process Analytics\n\n  Predictive Process Analytics is becoming an essential aid for organizations,\nproviding online operational support of their processes. However, process\nstakeholders need to be provided with an explanation of the reasons why a given\nprocess execution is predicted to behave in a certain way. Otherwise, they will\nbe unlikely to trust the predictive monitoring technology and, hence, adopt it.\nThis paper proposes a predictive analytics framework that is also equipped with\nexplanation capabilities based on the game theory of Shapley Values. The\nframework has been implemented in the IBM Process Mining suite and\ncommercialized for business users. The framework has been tested on real-life\nevent data to assess the quality of the predictions and the corresponding\nevaluations. In particular, a user evaluation has been performed in order to\nunderstand if the explanations provided by the system were intelligible to\nprocess stakeholders.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.00335,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Functional Rule Extraction Method for Artificial Neural Networks\n\n  The idea I propose in this paper is a method that is based on comprehensive\nfunctions for directed and undirected rule extraction from artificial neural\nnetwork operations. Firstly, I defined comprehensive functions, then\nconstructed a comprehensive multilayer network (denoted as $N$). Each\nactivation function of $N$ is parametrized to a comprehensive function.\nFollowing $N$ construction, I extracted rules from the network by observing\nthat the network output depends on probabilities of composite functions that\nare comprehensive functions. This functional rule extraction method applies to\nthe perceptron and multilayer neural network. For any $N$ model that is trained\nto predict some outcome given some event, that model behaviour can be expressed\n\u00e2\u0080\u0093 using the functional rule extraction method \u00e2\u0080\u0093 as a formal rule or informal\nrule obeyed by the network to predict that outcome. As example, figure 1\nconsist of a comprehensive physics function that is parameter for one of the\nnetwork hidden activation functions. Using the functional rule extraction\nmethod, I deduced that the comprehensive multilayer network prediction depends\non probability of that physics function and probabilities of other composite\ncomprehensive functions in $N$. Additionally, functional rule extraction method\ncan aid in applied settings for generation of equations of learned phenomena.\nThis generation can be achieved by first training an $N$ model toward\npredicting outcome of a phenomenon, then extracting the rules and assuming that\nprobability values of the network comprehensive functions are constants.\nFinally, to simplify the generated equation, comprehensive functions with\nprobability $p = 0$ can be omitted.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.12208,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Series2Graph: Graph-based Subsequence Anomaly Detection for Time Series\n\n  Subsequence anomaly detection in long sequences is an important problem with\napplications in a wide range of domains. However, the approaches proposed so\nfar in the literature have severe limitations: they either require prior domain\nknowledge used to design the anomaly discovery algorithms, or become cumbersome\nand expensive to use in situations with recurrent anomalies of the same type.\nIn this work, we address these problems, and propose an unsupervised method\nsuitable for domain agnostic subsequence anomaly detection. Our method,\nSeries2Graph, is based on a graph representation of a novel low-dimensionality\nembedding of subsequences. Series2Graph needs neither labeled instances (like\nsupervised techniques) nor anomaly-free data (like zero-positive learning\ntechniques), and identifies anomalies of varying lengths. The experimental\nresults, on the largest set of synthetic and real datasets used to date,\ndemonstrate that the proposed approach correctly identifies single and\nrecurrent anomalies without any prior knowledge of their characteristics,\noutperforming by a large margin several competing approaches in accuracy, while\nbeing up to orders of magnitude faster. This paper has appeared in VLDB 2020.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.01575,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"De-Biasing Generative Models using Counterfactual Methods\n\n  Variational autoencoders (VAEs) and other generative methods have garnered\ngrowing interest not just for their generative properties but also for the\nability to dis-entangle a low-dimensional latent variable space. However, few\nexisting generative models take causality into account. We propose a new\ndecoder based framework named the Causal Counterfactual Generative Model\n(CCGM), which includes a partially trainable causal layer in which a part of a\ncausal model can be learned without significantly impacting reconstruction\nfidelity. By learning the causal relationships between image semantic labels or\ntabular variables, we can analyze biases, intervene on the generative model,\nand simulate new scenarios. Furthermore, by modifying the causal structure, we\ncan generate samples outside the domain of the original training data and use\nsuch counterfactual models to de-bias datasets. Thus, datasets with known\nbiases can still be used to train the causal generative model and learn the\ncausal relationships, but we can produce de-biased datasets on the generative\nside. Our proposed method combines a causal latent space VAE model with\nspecific modification to emphasize causal fidelity, enabling finer control over\nthe causal layer and the ability to learn a robust intervention framework. We\nexplore how better disentanglement of causal learning and encoding\/decoding\ngenerates higher causal intervention quality. We also compare our model against\nsimilar research to demonstrate the need for explicit generative de-biasing\nbeyond interventions. Our initial experiments show that our model can generate\nimages and tabular data with high fidelity to the causal framework and\naccommodate explicit de-biasing to ignore undesired relationships in the causal\ndata compared to the baseline.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.05679,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000033776,
      "text":"Using Machine Learning to Reduce Observational Biases When Detecting New\n  Impacts on Mars\n\n  The current inventory of recent (fresh) impacts on Mars shows a strong bias\ntowards areas of low thermal inertia. These areas are generally visually\nbright, and impacts create dark scours and rays that make them easier to\ndetect. It is expected that impacts occur at a similar rate in areas of higher\nthermal inertia, but those impacts are under-detected. This study investigates\nthe use of a trained machine learning classifier to increase the detection of\nfresh impacts on Mars using CTX data. This approach discovered 69 new fresh\nimpacts that have been confirmed with follow-up HiRISE images. We found that\nexamining candidates partitioned by thermal inertia (TI) values, which is only\npossible due to the large number of machine learning candidates, helps reduce\nthe observational bias and increase the number of known high-TI impacts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.02773,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000383125,
      "text":"DIWIFT: Discovering Instance-wise Influential Features for Tabular Data\n\n  Tabular data is one of the most common data storage formats behind many\nreal-world web applications such as retail, banking, and e-commerce. The\nsuccess of these web applications largely depends on the ability of the\nemployed machine learning model to accurately distinguish influential features\nfrom all the predetermined features in tabular data. Intuitively, in practical\nbusiness scenarios, different instances should correspond to different sets of\ninfluential features, and the set of influential features of the same instance\nmay vary in different scenarios. However, most existing methods focus on global\nfeature selection assuming that all instances have the same set of influential\nfeatures, and few methods considering instance-wise feature selection ignore\nthe variability of influential features in different scenarios. In this paper,\nwe first introduce a new perspective based on the influence function for\ninstance-wise feature selection, and give some corresponding theoretical\ninsights, the core of which is to use the influence function as an indicator to\nmeasure the importance of an instance-wise feature. We then propose a new\nsolution for discovering instance-wise influential features in tabular data\n(DIWIFT), where a self-attention network is used as a feature selection model\nand the value of the corresponding influence function is used as an\noptimization objective to guide the model. Benefiting from the advantage of the\ninfluence function, i.e., its computation does not depend on a specific\narchitecture and can also take into account the data distribution in different\nscenarios, our DIWIFT has better flexibility and robustness. Finally, we\nconduct extensive experiments on both synthetic and real-world datasets to\nvalidate the effectiveness of our DIWIFT.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.1235,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"Energy-efficient DNN Inference on Approximate Accelerators Through\n  Formal Property Exploration\n\n  Deep Neural Networks (DNNs) are being heavily utilized in modern applications\nand are putting energy-constraint devices to the test. To bypass high energy\nconsumption issues, approximate computing has been employed in DNN accelerators\nto balance out the accuracy-energy reduction trade-off. However, the\napproximation-induced accuracy loss can be very high and drastically degrade\nthe performance of the DNN. Therefore, there is a need for a fine-grain\nmechanism that would assign specific DNN operations to approximation in order\nto maintain acceptable DNN accuracy, while also achieving low energy\nconsumption. In this paper, we present an automated framework for\nweight-to-approximation mapping enabling formal property exploration for\napproximate DNN accelerators. At the MAC unit level, our experimental\nevaluation surpassed already energy-efficient mappings by more than $\\times2$\nin terms of energy gains, while also supporting significantly more fine-grain\ncontrol over the introduced approximation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.0718,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Contrastive Adapters for Foundation Model Group Robustness\n\n  While large pretrained foundation models (FMs) have shown remarkable\nzero-shot classification robustness to dataset-level distribution shifts, their\nrobustness to subpopulation or group shifts is relatively underexplored. We\nstudy this problem, and find that FMs such as CLIP may not be robust to various\ngroup shifts. Across 9 robustness benchmarks, zero-shot classification with\ntheir embeddings results in gaps of up to 80.7 percentage points (pp) between\naverage and worst-group accuracy. Unfortunately, existing methods to improve\nrobustness require retraining, which can be prohibitively expensive on large\nfoundation models. We also find that efficient ways to improve model inference\n(e.g., via adapters, lightweight networks with FM embeddings as inputs) do not\nconsistently improve and can sometimes hurt group robustness compared to\nzero-shot (e.g., increasing the accuracy gap by 50.1 pp on CelebA). We thus\ndevelop an adapter training strategy to effectively and efficiently improve FM\ngroup robustness. Our motivating observation is that while poor robustness\nresults from groups in the same class being embedded far apart in the\nfoundation model \"embedding space,\" standard adapter training may not bring\nthese points closer together. We thus propose contrastive adapting, which\ntrains adapters with contrastive learning to bring sample embeddings close to\nboth their ground-truth class embeddings and other sample embeddings in the\nsame class. Across the 9 benchmarks, our approach consistently improves group\nrobustness, raising worst-group accuracy by 8.5 to 56.0 pp over zero-shot. Our\napproach is also efficient, doing so without any FM finetuning and only a fixed\nset of frozen FM embeddings. On benchmarks such as Waterbirds and CelebA, this\nleads to worst-group accuracy comparable to state-of-the-art methods that\nretrain entire models, while only training $\\leq$1% of the model parameters.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.1271,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Active Learning of Ordinal Embeddings: A User Study on Football Data\n\n  Humans innately measure distance between instances in an unlabeled dataset\nusing an unknown similarity function. Distance metrics can only serve as proxy\nfor similarity in information retrieval of similar instances. Learning a good\nsimilarity function from human annotations improves the quality of retrievals.\nThis work uses deep metric learning to learn these user-defined similarity\nfunctions from few annotations for a large football trajectory dataset. We\nadapt an entropy-based active learning method with recent work from triplet\nmining to collect easy-to-answer but still informative annotations from human\nparticipants and use them to train a deep convolutional network that\ngeneralizes to unseen samples. Our user study shows that our approach improves\nthe quality of the information retrieval compared to a previous deep metric\nlearning approach that relies on a Siamese network. Specifically, we shed light\non the strengths and weaknesses of passive sampling heuristics and active\nlearners alike by analyzing the participants' response efficacy. To this end,\nwe collect accuracy, algorithmic time complexity, the participants' fatigue and\ntime-to-response, qualitative self-assessment and statements, as well as the\neffects of mixed-expertise annotators and their consistency on model\nperformance and transfer-learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.11382,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Density-Aware Personalized Training for Risk Prediction in Imbalanced\n  Medical Data\n\n  Medical events of interest, such as mortality, often happen at a low rate in\nelectronic medical records, as most admitted patients survive. Training models\nwith this imbalance rate (class density discrepancy) may lead to suboptimal\nprediction. Traditionally this problem is addressed through ad-hoc methods such\nas resampling or reweighting but performance in many cases is still limited. We\npropose a framework for training models for this imbalance issue: 1) we first\ndecouple the feature extraction and classification process, adjusting training\nbatches separately for each component to mitigate bias caused by class density\ndiscrepancy; 2) we train the network with both a density-aware loss and a\nlearnable cost matrix for misclassifications. We demonstrate our model's\nimproved performance in real-world medical datasets (TOPCAT and MIMIC-III) to\nshow improved AUC-ROC, AUC-PRC, Brier Skill Score compared with the baselines\nin the domain.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.06835,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000096692,
      "text":"Instance Selection Mechanisms for Human-in-the-Loop Systems in Few-Shot\n  Learning\n\n  Business analytics and machine learning have become essential success factors\nfor various industries - with the downside of cost-intensive gathering and\nlabeling of data. Few-shot learning addresses this challenge and reduces data\ngathering and labeling costs by learning novel classes with very few labeled\ndata. In this paper, we design a human-in-the-loop (HITL) system for few-shot\nlearning and analyze an extensive range of mechanisms that can be used to\nacquire human expert knowledge for instances that have an uncertain prediction\noutcome. We show that the acquisition of human expert knowledge significantly\naccelerates the few-shot model performance given a negligible labeling effort.\nWe validate our findings in various experiments on a benchmark dataset in\ncomputer vision and real-world datasets. We further demonstrate the\ncost-effectiveness of HITL systems for few-shot learning. Overall, our work\naims at supporting researchers and practitioners in effectively adapting\nmachine learning models to novel classes at reduced costs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.03678,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Stability of Aggregation Graph Neural Networks\n\n  In this paper we study the stability properties of aggregation graph neural\nnetworks (Agg-GNNs) considering perturbations of the underlying graph. An\nAgg-GNN is a hybrid architecture where information is defined on the nodes of a\ngraph, but it is processed block-wise by Euclidean CNNs on the nodes after\nseveral diffusions on the graph shift operator. We derive stability bounds for\nthe mapping operator associated to a generic Agg-GNN, and we specify conditions\nunder which such operators can be stable to deformations. We prove that the\nstability bounds are defined by the properties of the filters in the first\nlayer of the CNN that acts on each node. Additionally, we show that there is a\nclose relationship between the number of aggregations, the filter's\nselectivity, and the size of the stability constants. We also conclude that in\nAgg-GNNs the selectivity of the mapping operators is tied to the properties of\nthe filters only in the first layer of the CNN stage. This shows a substantial\ndifference with respect to the stability properties of selection GNNs, where\nthe selectivity of the filters in all layers is constrained by their stability.\nWe provide numerical evidence corroborating the results derived, testing the\nbehavior of Agg-GNNs in real life application scenarios considering\nperturbations of different magnitude.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.0864,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Lightweight Automated Feature Monitoring for Data Streams\n\n  Monitoring the behavior of automated real-time stream processing systems has\nbecome one of the most relevant problems in real world applications. Such\nsystems have grown in complexity relying heavily on high dimensional input\ndata, and data hungry Machine Learning (ML) algorithms. We propose a flexible\nsystem, Feature Monitoring (FM), that detects data drifts in such data sets,\nwith a small and constant memory footprint and a small computational cost in\nstreaming applications. The method is based on a multi-variate statistical test\nand is data driven by design (full reference distributions are estimated from\nthe data). It monitors all features that are used by the system, while\nproviding an interpretable features ranking whenever an alarm occurs (to aid in\nroot cause analysis). The computational and memory lightness of the system\nresults from the use of Exponential Moving Histograms. In our experimental\nstudy, we analyze the system's behavior with its parameters and, more\nimportantly, show examples where it detects problems that are not directly\nrelated to a single feature. This illustrates how FM eliminates the need to add\ncustom signals to detect specific types of problems and that monitoring the\navailable space of features is often enough.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.06531,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000133448,
      "text":"Reachability Analysis of a General Class of Neural Ordinary Differential\n  Equations\n\n  Continuous deep learning models, referred to as Neural Ordinary Differential\nEquations (Neural ODEs), have received considerable attention over the last\nseveral years. Despite their burgeoning impact, there is a lack of formal\nanalysis techniques for these systems. In this paper, we consider a general\nclass of neural ODEs with varying architectures and layers, and introduce a\nnovel reachability framework that allows for the formal analysis of their\nbehavior. The methods developed for the reachability analysis of neural ODEs\nare implemented in a new tool called NNVODE. Specifically, our work extends an\nexisting neural network verification tool to support neural ODEs. We\ndemonstrate the capabilities and efficacy of our methods through the analysis\nof a set of benchmarks that include neural ODEs used for classification, and in\ncontrol and dynamical systems, including an evaluation of the efficacy and\ncapabilities of our approach with respect to existing software tools within the\ncontinuous-time systems reachability literature, when it is possible to do so.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.11048,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Quantized Sparse Weight Decomposition for Neural Network Compression\n\n  In this paper, we introduce a novel method of neural network weight\ncompression. In our method, we store weight tensors as sparse, quantized matrix\nfactors, whose product is computed on the fly during inference to generate the\ntarget model's weights. We use projected gradient descent methods to find\nquantized and sparse factorization of the weight tensors. We show that this\napproach can be seen as a unification of weight SVD, vector quantization, and\nsparse PCA. Combined with end-to-end fine-tuning our method exceeds or is on\npar with previous state-of-the-art methods in terms of the trade-off between\naccuracy and model size. Our method is applicable to both moderate compression\nregimes, unlike vector quantization, and extreme compression regimes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.11842,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"$\\textit{FastSVD-ML-ROM}$: A Reduced-Order Modeling Framework based on\n  Machine Learning for Real-Time Applications\n\n  Digital twins have emerged as a key technology for optimizing the performance\nof engineering products and systems. High-fidelity numerical simulations\nconstitute the backbone of engineering design, providing an accurate insight\ninto the performance of complex systems. However, large-scale, dynamic,\nnon-linear models require significant computational resources and are\nprohibitive for real-time digital twin applications. To this end, reduced order\nmodels (ROMs) are employed, to approximate the high-fidelity solutions while\naccurately capturing the dominant aspects of the physical behavior. The present\nwork proposes a new machine learning (ML) platform for the development of ROMs,\nto handle large-scale numerical problems dealing with transient nonlinear\npartial differential equations. Our framework, mentioned as\n$\\textit{FastSVD-ML-ROM}$, utilizes $\\textit{(i)}$ a singular value\ndecomposition (SVD) update methodology, to compute a linear subspace of the\nmulti-fidelity solutions during the simulation process, $\\textit{(ii)}$\nconvolutional autoencoders for nonlinear dimensionality reduction,\n$\\textit{(iii)}$ feed-forward neural networks to map the input parameters to\nthe latent spaces, and $\\textit{(iv)}$ long short-term memory networks to\npredict and forecast the dynamics of parametric solutions. The efficiency of\nthe $\\textit{FastSVD-ML-ROM}$ framework is demonstrated for a 2D linear\nconvection-diffusion equation, the problem of fluid around a cylinder, and the\n3D blood flow inside an arterial segment. The accuracy of the reconstructed\nresults demonstrates the robustness and assesses the efficiency of the proposed\napproach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.08258,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000023875,
      "text":"Minimum Description Length Control\n\n  We propose a novel framework for multitask reinforcement learning based on\nthe minimum description length (MDL) principle. In this approach, which we term\nMDL-control (MDL-C), the agent learns the common structure among the tasks with\nwhich it is faced and then distills it into a simpler representation which\nfacilitates faster convergence and generalization to new tasks. In doing so,\nMDL-C naturally balances adaptation to each task with epistemic uncertainty\nabout the task distribution. We motivate MDL-C via formal connections between\nthe MDL principle and Bayesian inference, derive theoretical performance\nguarantees, and demonstrate MDL-C's empirical effectiveness on both discrete\nand high-dimensional continuous control tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.13329,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000145038,
      "text":"Gaia: Graph Neural Network with Temporal Shift aware Attention for Gross\n  Merchandise Value Forecast in E-commerce\n\n  E-commerce has gone a long way in empowering merchants through the internet.\nIn order to store the goods efficiently and arrange the marketing resource\nproperly, it is important for them to make the accurate gross merchandise value\n(GMV) prediction. However, it's nontrivial to make accurate prediction with the\ndeficiency of digitized data. In this article, we present a solution to better\nforecast GMV inside Alipay app. Thanks to graph neural networks (GNN) which has\ngreat ability to correlate different entities to enrich information, we propose\nGaia, a graph neural network (GNN) model with temporal shift aware attention.\nGaia leverages the relevant e-seller' sales information and learn neighbor\ncorrelation based on temporal dependencies. By testing on Alipay's real dataset\nand comparing with other baselines, Gaia has shown the best performance. And\nGaia is deployed in the simulated online environment, which also achieves great\nimprovement compared with baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.04821,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000003775,
      "text":"Long-term Reproducibility for Neural Architecture Search\n\n  It is a sad reflection of modern academia that code is often ignored after\npublication -- there is no academic 'kudos' for bug fixes \/ maintenance. Code\nis often unavailable or, if available, contains bugs, is incomplete, or relies\non out-of-date \/ unavailable libraries. This has a significant impact on\nreproducibility and general scientific progress. Neural Architecture Search\n(NAS) is no exception to this, with some prior work in reproducibility.\nHowever, we argue that these do not consider long-term reproducibility issues.\nWe therefore propose a checklist for long-term NAS reproducibility. We evaluate\nour checklist against common NAS approaches along with proposing how we can\nretrospectively make these approaches more long-term reproducible.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.10305,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000050333,
      "text":"Detecting Small Query Graphs in A Large Graph via Neural Subgraph Search\n\n  Recent advances have shown the success of using reinforcement learning and\nsearch to solve NP-hard graph-related tasks, such as Traveling Salesman\nOptimization, Graph Edit Distance computation, etc. However, it remains unclear\nhow one can efficiently and accurately detect the occurrences of a small query\ngraph in a large target graph, which is a core operation in graph database\nsearch, biomedical analysis, social group finding, etc. This task is called\nSubgraph Matching which essentially performs subgraph isomorphism check between\na query graph and a large target graph. One promising approach to this\nclassical problem is the \"learning-to-search\" paradigm, where a reinforcement\nlearning (RL) agent is designed with a learned policy to guide a search\nalgorithm to quickly find the solution without any solved instances for\nsupervision. However, for the specific task of Subgraph Matching, though the\nquery graph is usually small given by the user as input, the target graph is\noften orders-of-magnitude larger. It poses challenges to the neural network\ndesign and can lead to solution and reward sparsity. In this paper, we propose\nNSUBS with two innovations to tackle the challenges: (1) A novel\nencoder-decoder neural network architecture to dynamically compute the matching\ninformation between the query and the target graphs at each search state; (2) A\nnovel look-ahead loss function for training the policy network. Experiments on\nsix large real-world target graphs show that NSUBS can significantly improve\nthe subgraph matching performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.06883,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000042386,
      "text":"Confidence-Guided Learning Process for Continuous Classification of Time\n  Series\n\n  In the real world, the class of a time series is usually labeled at the final\ntime, but many applications require to classify time series at every time\npoint. e.g. the outcome of a critical patient is only determined at the end,\nbut he should be diagnosed at all times for timely treatment. Thus, we propose\na new concept: Continuous Classification of Time Series (CCTS). It requires the\nmodel to learn data in different time stages. But the time series evolves\ndynamically, leading to different data distributions. When a model learns\nmulti-distribution, it always forgets or overfits. We suggest that meaningful\nlearning scheduling is potential due to an interesting observation: Measured by\nconfidence, the process of model learning multiple distributions is similar to\nthe process of human learning multiple knowledge. Thus, we propose a novel\nConfidence-guided method for CCTS (C3TS). It can imitate the alternating human\nconfidence described by the Dunning-Kruger Effect. We define the objective-\nconfidence to arrange data, and the self-confidence to control the learning\nduration. Experiments on four real-world datasets show that C3TS is more\naccurate than all baselines for CCTS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.03075,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000113249,
      "text":"PGX: A Multi-level GNN Explanation Framework Based on Separate Knowledge\n  Distillation Processes\n\n  Graph Neural Networks (GNNs) are widely adopted in advanced AI systems due to\ntheir capability of representation learning on graph data. Even though GNN\nexplanation is crucial to increase user trust in the systems, it is challenging\ndue to the complexity of GNN execution. Lately, many works have been proposed\nto address some of the issues in GNN explanation. However, they lack\ngeneralization capability or suffer from computational burden when the size of\ngraphs is enormous. To address these challenges, we propose a multi-level GNN\nexplanation framework based on an observation that GNN is a multimodal learning\nprocess of multiple components in graph data. The complexity of the original\nproblem is relaxed by breaking into multiple sub-parts represented as a\nhierarchical structure. The top-level explanation aims at specifying the\ncontribution of each component to the model execution and predictions, while\nfine-grained levels focus on feature attribution and graph structure\nattribution analysis based on knowledge distillation. Student models are\ntrained in standalone modes and are responsible for capturing different teacher\nbehaviors, later used for particular component interpretation. Besides, we also\naim for personalized explanations as the framework can generate different\nresults based on user preferences. Finally, extensive experiments demonstrate\nthe effectiveness and fidelity of our proposed approach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.12433,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Towards Automated Imbalanced Learning with Deep Hierarchical\n  Reinforcement Learning\n\n  Imbalanced learning is a fundamental challenge in data mining, where there is\na disproportionate ratio of training samples in each class. Over-sampling is an\neffective technique to tackle imbalanced learning through generating synthetic\nsamples for the minority class. While numerous over-sampling algorithms have\nbeen proposed, they heavily rely on heuristics, which could be sub-optimal\nsince we may need different sampling strategies for different datasets and base\nclassifiers, and they cannot directly optimize the performance metric.\nMotivated by this, we investigate developing a learning-based over-sampling\nalgorithm to optimize the classification performance, which is a challenging\ntask because of the huge and hierarchical decision space. At the high level, we\nneed to decide how many synthetic samples to generate. At the low level, we\nneed to determine where the synthetic samples should be located, which depends\non the high-level decision since the optimal locations of the samples may\ndiffer for different numbers of samples. To address the challenges, we propose\nAutoSMOTE, an automated over-sampling algorithm that can jointly optimize\ndifferent levels of decisions. Motivated by the success of\nSMOTE~\\cite{chawla2002smote} and its extensions, we formulate the generation\nprocess as a Markov decision process (MDP) consisting of three levels of\npolicies to generate synthetic samples within the SMOTE search space. Then we\nleverage deep hierarchical reinforcement learning to optimize the performance\nmetric on the validation data. Extensive experiments on six real-world datasets\ndemonstrate that AutoSMOTE significantly outperforms the state-of-the-art\nresampling algorithms. The code is at https:\/\/github.com\/daochenzha\/autosmote\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.09309,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000028478,
      "text":"Graph Convolutional Networks from the Perspective of Sheaves and the\n  Neural Tangent Kernel\n\n  Graph convolutional networks are a popular class of deep neural network\nalgorithms which have shown success in a number of relational learning tasks.\nDespite their success, graph convolutional networks exhibit a number of\npeculiar features, including a bias towards learning oversmoothed and\nhomophilic functions, which are not easily diagnosed due to the complex nature\nof these algorithms. We propose to bridge this gap in understanding by studying\nthe neural tangent kernel of sheaf convolutional networks--a topological\ngeneralization of graph convolutional networks. To this end, we derive a\nparameterization of the neural tangent kernel for sheaf convolutional networks\nwhich separates the function into two parts: one driven by a forward diffusion\nprocess determined by the graph, and the other determined by the composite\neffect of nodes' activations on the output layer. This geometrically-focused\nderivation produces a number of immediate insights which we discuss in detail.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.02389,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.00001106,
      "text":"Risk-Aware Linear Bandits: Theory and Applications in Smart Order\n  Routing\n\n  Motivated by practical considerations in machine learning for financial\ndecision-making, such as risk aversion and large action space, we consider\nrisk-aware bandits optimization with applications in smart order routing (SOR).\nSpecifically, based on preliminary observations of linear price impacts made\nfrom the NASDAQ ITCH dataset, we initiate the study of risk-aware linear\nbandits. In this setting, we aim at minimizing regret, which measures our\nperformance deficit compared to the optimum's, under the mean-variance metric\nwhen facing a set of actions whose rewards are linear functions of (initially)\nunknown parameters. Driven by the variance-minimizing globally-optimal\n(G-optimal) design, we propose the novel instance-independent Risk-Aware\nExplore-then-Commit (RISE) algorithm and the instance-dependent Risk-Aware\nSuccessive Elimination (RISE++) algorithm. Then, we rigorously analyze their\nnear-optimal regret upper bounds to show that, by leveraging the linear\nstructure, our algorithms can dramatically reduce the regret when compared to\nexisting methods. Finally, we demonstrate the performance of the algorithms by\nconducting extensive numerical experiments in the SOR setup using both\nsynthetic datasets and the NASDAQ ITCH dataset. Our results reveal that 1) The\nlinear structure assumption can indeed be well supported by the Nasdaq dataset;\nand more importantly 2) Both RISE and RISE++ can significantly outperform the\ncompeting methods, in terms of regret, especially in complex decision-making\nscenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.03523,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Generalizing Downsampling from Regular Data to Graphs\n\n  Downsampling produces coarsened, multi-resolution representations of data and\nit is used, for example, to produce lossy compression and visualization of\nlarge images, reduce computational costs, and boost deep neural representation\nlearning. Unfortunately, due to their lack of a regular structure, there is\nstill no consensus on how downsampling should apply to graphs and linked data.\nIndeed reductions in graph data are still needed for the goals described above,\nbut reduction mechanisms do not have the same focus on preserving topological\nstructures and properties, while allowing for resolution-tuning, as is the case\nin regular data downsampling.\n  In this paper, we take a step in this direction, introducing a unifying\ninterpretation of downsampling in regular and graph data. In particular, we\ndefine a graph coarsening mechanism which is a graph-structured counterpart of\ncontrollable equispaced coarsening mechanisms in regular data. We prove\ntheoretical guarantees for distortion bounds on path lengths, as well as the\nability to preserve key topological properties in the coarsened graphs. We\nleverage these concepts to define a graph pooling mechanism that we empirically\nassess in graph classification tasks, providing a greedy algorithm that allows\nefficient parallel implementation on GPUs, and showing that it compares\nfavorably against pooling methods in literature.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.10462,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000029471,
      "text":"Shapelet-Based Counterfactual Explanations for Multivariate Time Series\n\n  As machine learning and deep learning models have become highly prevalent in\na multitude of domains, the main reservation in their adoption for\ndecision-making processes is their black-box nature. The Explainable Artificial\nIntelligence (XAI) paradigm has gained a lot of momentum lately due to its\nability to reduce models opacity. XAI methods have not only increased\nstakeholders' trust in the decision process but also helped developers ensure\nits fairness. Recent efforts have been invested in creating transparent models\nand post-hoc explanations. However, fewer methods have been developed for time\nseries data, and even less when it comes to multivariate datasets. In this\nwork, we take advantage of the inherent interpretability of shapelets to\ndevelop a model agnostic multivariate time series (MTS) counterfactual\nexplanation algorithm. Counterfactuals can have a tremendous impact on making\nblack-box models explainable by indicating what changes have to be performed on\nthe input to change the final decision. We test our approach on a real-life\nsolar flare prediction dataset and prove that our approach produces\nhigh-quality counterfactuals. Moreover, a comparison to the only MTS\ncounterfactual generation algorithm shows that, in addition to being visually\ninterpretable, our explanations are superior in terms of proximity, sparsity,\nand plausibility.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.06573,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"GEDI: A Graph-based End-to-end Data Imputation Framework\n\n  Data imputation is an effective way to handle missing data, which is common\nin practical applications. In this study, we propose and test a novel data\nimputation process that achieve two important goals: (1) preserve the row-wise\nsimilarities among observations and column-wise contextual relationships among\nfeatures in the feature matrix, and (2) tailor the imputation process to\nspecific downstream label prediction task. The proposed imputation process uses\nTransformer network and graph structure learning to iteratively refine the\ncontextual relationships among features and similarities among observations.\nMoreover, it uses a meta-learning framework to select features that are\ninfluential to the downstream prediction task of interest. We conduct\nexperiments on real-world large data sets, and show that the proposed\nimputation process consistently improves imputation and label prediction\nperformance over a variety of benchmark methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.06125,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000237756,
      "text":"A Practical Second-order Latent Factor Model via Distributed Particle\n  Swarm Optimization\n\n  Latent Factor (LF) models are effective in representing high-dimension and\nsparse (HiDS) data via low-rank matrices approximation. Hessian-free (HF)\noptimization is an efficient method to utilizing second-order information of an\nLF model's objective function and it has been utilized to optimize second-order\nLF (SLF) model. However, the low-rank representation ability of a SLF model\nheavily relies on its multiple hyperparameters. Determining these\nhyperparameters is time-consuming and it largely reduces the practicability of\nan SLF model. To address this issue, a practical SLF (PSLF) model is proposed\nin this work. It realizes hyperparameter self-adaptation with a distributed\nparticle swarm optimizer (DPSO), which is gradient-free and parallelized.\nExperiments on real HiDS data sets indicate that PSLF model has a competitive\nadvantage over state-of-the-art models in data representation ability.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.13909,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"\"Prompt-Gamma Neutron Activation Analysis (PGNAA)\" Metal Spectral\n  Classification using Deep Learning Method\n\n  There is a pressing market demand to minimize the test time of Prompt Gamma\nNeutron Activation Analysis (PGNAA) spectra measurement machine, so that it\ncould function as an instant material analyzer, e.g. to classify waste samples\ninstantaneously and determine the best recycling method based on the detected\ncompositions of the testing sample.\n  This article introduces a new development of the deep learning classification\nand contrive to reduce the test time for PGNAA machine. We propose both Random\nSampling Methods and Class Activation Map (CAM) to generate \"downsized\" samples\nand train the CNN model continuously. Random Sampling Methods (RSM) aims to\nreduce the measuring time within a sample, and Class Activation Map (CAM) is\nfor filtering out the less important energy range of the downsized samples.\n  We shorten the overall PGNAA measuring time down to 2.5 seconds while\nensuring the accuracy is around 96.88 % for our dataset with 12 different\nspecies of substances. Compared with classifying different species of\nmaterials, it requires more test time (sample count rate) for substances having\nthe same elements to archive good accuracy. For example, the classification of\ncopper alloys requires nearly 24 seconds test time to reach 98 % accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.00953,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.9487304688,
      "text":"Visual Interpretable and Explainable Deep Learning Models for Brain\n  Tumor MRI and COVID-19 Chest X-ray Images\n\n  Deep learning shows promise for medical image analysis but lacks\ninterpretability, hindering adoption in healthcare. Attribution techniques that\nexplain model reasoning may increase trust in deep learning among clinical\nstakeholders. This paper aimed to evaluate attribution methods for illuminating\nhow deep neural networks analyze medical images. Using adaptive path-based\ngradient integration, we attributed predictions from brain tumor MRI and\nCOVID-19 chest X-ray datasets made by recent deep convolutional neural network\nmodels. The technique highlighted possible biomarkers, exposed model biases,\nand offered insights into the links between input and prediction. Our analysis\ndemonstrates the method's ability to elucidate model reasoning on these\ndatasets. The resulting attributions show promise for improving deep learning\ntransparency for domain experts by revealing the rationale behind predictions.\nThis study advances model interpretability to increase trust in deep learning\namong healthcare stakeholders.\n",
      "prediction":"Highly Likely AI",
      "llm_prediction":{
        "GPT35":0.0000298023,
        "GPT4":0.0001266003,
        "CLAUDE":0.994140625,
        "GOOGLE":0.0003154278,
        "OPENAI_O_SERIES":0.0000626445,
        "DEEPSEEK":0.0001517534,
        "GROK":0.0000001192,
        "NOVA":0.0000005364,
        "OTHER":0.0000216365,
        "HUMAN":0.0051460266
      }
    }
  },
  {
    "arxiv_id":2208.02249,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000035432,
      "text":"Reinforcement Learning for Joint V2I Network Selection and Autonomous\n  Driving Policies\n\n  Vehicle-to-Infrastructure (V2I) communication is becoming critical for the\nenhanced reliability of autonomous vehicles (AVs). However, the uncertainties\nin the road-traffic and AVs' wireless connections can severely impair timely\ndecision-making. It is thus critical to simultaneously optimize the AVs'\nnetwork selection and driving policies in order to minimize road collisions\nwhile maximizing the communication data rates. In this paper, we develop a\nreinforcement learning (RL) framework to characterize efficient network\nselection and autonomous driving policies in a multi-band vehicular network\n(VNet) operating on conventional sub-6GHz spectrum and Terahertz (THz)\nfrequencies. The proposed framework is designed to (i) maximize the traffic\nflow and minimize collisions by controlling the vehicle's motion dynamics\n(i.e., speed and acceleration) from autonomous driving perspective, and (ii)\nmaximize the data rates and minimize handoffs by jointly controlling the\nvehicle's motion dynamics and network selection from telecommunication\nperspective. We cast this problem as a Markov Decision Process (MDP) and\ndevelop a deep Q-learning based solution to optimize the actions such as\nacceleration, deceleration, lane-changes, and AV-base station assignments for a\ngiven AV's state. The AV's state is defined based on the velocities and\ncommunication channel states of AVs. Numerical results demonstrate interesting\ninsights related to the inter-dependency of vehicle's motion dynamics,\nhandoffs, and the communication data rate. The proposed policies enable AVs to\nadopt safe driving behaviors with improved connectivity.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.02777,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000033114,
      "text":"QC-ODKLA: Quantized and Communication-Censored Online Decentralized\n  Kernel Learning via Linearized ADMM\n\n  This paper focuses on online kernel learning over a decentralized network.\nEach agent in the network receives continuous streaming data locally and works\ncollaboratively to learn a nonlinear prediction function that is globally\noptimal in the reproducing kernel Hilbert space with respect to the total\ninstantaneous costs of all agents. In order to circumvent the curse of\ndimensionality issue in traditional online kernel learning, we utilize random\nfeature (RF) mapping to convert the non-parametric kernel learning problem into\na fixed-length parametric one in the RF space. We then propose a novel learning\nframework named Online Decentralized Kernel learning via Linearized ADMM\n(ODKLA) to efficiently solve the online decentralized kernel learning problem.\nTo further improve the communication efficiency, we add the quantization and\ncensoring strategies in the communication stage and develop the Quantized and\nCommunication-censored ODKLA (QC-ODKLA) algorithm. We theoretically prove that\nboth ODKLA and QC-ODKLA can achieve the optimal sublinear regret\n$\\mathcal{O}(\\sqrt{T})$ over $T$ time slots. Through numerical experiments, we\nevaluate the learning effectiveness, communication, and computation\nefficiencies of the proposed methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.12595,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000020199,
      "text":"PDD-SHAP: Fast Approximations for Shapley Values using Functional\n  Decomposition\n\n  Because of their strong theoretical properties, Shapley values have become\nvery popular as a way to explain predictions made by black box models.\nUnfortuately, most existing techniques to compute Shapley values are\ncomputationally very expensive. We propose PDD-SHAP, an algorithm that uses an\nANOVA-based functional decomposition model to approximate the black-box model\nbeing explained. This allows us to calculate Shapley values orders of magnitude\nfaster than existing methods for large datasets, significantly reducing the\namortized cost of computing Shapley values when many predictions need to be\nexplained.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.01841,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Robust Learning of Deep Time Series Anomaly Detection Models with\n  Contaminated Training Data\n\n  Time series anomaly detection (TSAD) is an important data mining task with\nnumerous applications in the IoT era. In recent years, a large number of deep\nneural network-based methods have been proposed, demonstrating significantly\nbetter performance than conventional methods on addressing challenging TSAD\nproblems in a variety of areas. Nevertheless, these deep TSAD methods typically\nrely on a clean training dataset that is not polluted by anomalies to learn the\n\"normal profile\" of the underlying dynamics. This requirement is nontrivial\nsince a clean dataset can hardly be provided in practice. Moreover, without the\nawareness of their robustness, blindly applying deep TSAD methods with\npotentially contaminated training data can possibly incur significant\nperformance degradation in the detection phase. In this work, to tackle this\nimportant challenge, we firstly investigate the robustness of commonly used\ndeep TSAD methods with contaminated training data which provides a guideline\nfor applying these methods when the provided training data are not guaranteed\nto be anomaly-free. Furthermore, we propose a model-agnostic method which can\neffectively improve the robustness of learning mainstream deep TSAD models with\npotentially contaminated data. Experiment results show that our method can\nconsistently prevent or mitigate performance degradation of mainstream deep\nTSAD models on widely used benchmark datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.04055,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Neural Set Function Extensions: Learning with Discrete Functions in High\n  Dimensions\n\n  Integrating functions on discrete domains into neural networks is key to\ndeveloping their capability to reason about discrete objects. But, discrete\ndomains are (1) not naturally amenable to gradient-based optimization, and (2)\nincompatible with deep learning architectures that rely on representations in\nhigh-dimensional vector spaces. In this work, we address both difficulties for\nset functions, which capture many important discrete problems. First, we\ndevelop a framework for extending set functions onto low-dimensional continuous\ndomains, where many extensions are naturally defined. Our framework subsumes\nmany well-known extensions as special cases. Second, to avoid undesirable\nlow-dimensional neural network bottlenecks, we convert low-dimensional\nextensions into representations in high-dimensional spaces, taking inspiration\nfrom the success of semidefinite programs for combinatorial optimization.\nEmpirically, we observe benefits of our extensions for unsupervised neural\ncombinatorial optimization, in particular with high-dimensional\nrepresentations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.11212,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"DeepPicarMicro: Applying TinyML to Autonomous Cyber Physical Systems\n\n  Running deep neural networks (DNNs) on tiny Micro-controller Units (MCUs) is\nchallenging due to their limitations in computing, memory, and storage\ncapacity. Fortunately, recent advances in both MCU hardware and machine\nlearning software frameworks make it possible to run fairly complex neural\nnetworks on modern MCUs, resulting in a new field of study widely known as\nTinyML. However, there have been few studies to show the potential for TinyML\napplications in cyber physical systems (CPS). In this paper, we present\nDeepPicarMicro, a small self-driving RC car testbed, which runs a convolutional\nneural network (CNN) on a Raspberry Pi Pico MCU. We apply a state-of-the-art\nDNN optimization to successfully fit the well-known PilotNet CNN architecture,\nwhich was used to drive NVIDIA's real self-driving car, on the MCU. We apply a\nstate-of-art network architecture search (NAS) approach to find further\noptimized networks that can effectively control the car in real-time in an\nend-to-end manner. From an extensive systematic experimental evaluation study,\nwe observe an interesting relationship between the accuracy, latency, and\ncontrol performance of a system. From this, we propose a joint optimization\nstrategy that takes both accuracy and latency of a model in the network\narchitecture search process for AI enabled CPS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.0104,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000077817,
      "text":"CircuitNet: An Open-Source Dataset for Machine Learning Applications in\n  Electronic Design Automation (EDA)\n\n  The electronic design automation (EDA) community has been actively exploring\nmachine learning (ML) for very large-scale integrated computer-aided design\n(VLSI CAD). Many studies explored learning-based techniques for cross-stage\nprediction tasks in the design flow to achieve faster design convergence.\nAlthough building ML models usually requires a large amount of data, most\nstudies can only generate small internal datasets for validation because of the\nlack of large public datasets. In this essay, we present the first open-source\ndataset called CircuitNet for ML tasks in VLSI CAD.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.12458,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"Another Use of SMOTE for Interpretable Data Collaboration Analysis\n\n  Recently, data collaboration (DC) analysis has been developed for\nprivacy-preserving integrated analysis across multiple institutions. DC\nanalysis centralizes individually constructed dimensionality-reduced\nintermediate representations and realizes integrated analysis via collaboration\nrepresentations without sharing the original data. To construct the\ncollaboration representations, each institution generates and shares a\nshareable anchor dataset and centralizes its intermediate representation.\nAlthough, random anchor dataset functions well for DC analysis in general,\nusing an anchor dataset whose distribution is close to that of the raw dataset\nis expected to improve the recognition performance, particularly for the\ninterpretable DC analysis. Based on an extension of the synthetic minority\nover-sampling technique (SMOTE), this study proposes an anchor data\nconstruction technique to improve the recognition performance without\nincreasing the risk of data leakage. Numerical results demonstrate the\nefficiency of the proposed SMOTE-based method over the existing anchor data\nconstructions for artificial and real-world datasets. Specifically, the\nproposed method achieves 9 percentage point and 38 percentage point performance\nimprovements regarding accuracy and essential feature selection, respectively,\nover existing methods for an income dataset. The proposed method provides\nanother use of SMOTE not for imbalanced data classifications but for a key\ntechnology of privacy-preserving integrated analysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.06956,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000266234,
      "text":"ARIEL: Adversarial Graph Contrastive Learning\n\n  Contrastive learning is an effective unsupervised method in graph\nrepresentation learning, and the key component of contrastive learning lies in\nthe construction of positive and negative samples. Previous methods usually\nutilize the proximity of nodes in the graph as the principle. Recently, the\ndata-augmentation-based contrastive learning method has advanced to show great\npower in the visual domain, and some works extended this method from images to\ngraphs. However, unlike the data augmentation on images, the data augmentation\non graphs is far less intuitive and much harder to provide high-quality\ncontrastive samples, which leaves much space for improvement. In this work, by\nintroducing an adversarial graph view for data augmentation, we propose a\nsimple but effective method, Adversarial Graph Contrastive Learning (ARIEL), to\nextract informative contrastive samples within reasonable constraints. We\ndevelop a new technique called information regularization for stable training\nand use subgraph sampling for scalability. We generalize our method from\nnode-level contrastive learning to the graph level by treating each graph\ninstance as a super-node. ARIEL consistently outperforms the current graph\ncontrastive learning methods for both node-level and graph-level classification\ntasks on real-world datasets. We further demonstrate that ARIEL is more robust\nin the face of adversarial attacks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.11464,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"Smart Active Sampling to enhance Quality Assurance Efficiency\n\n  We propose a new sampling strategy, called smart active sapling, for quality\ninspections outside the production line. Based on the principles of active\nlearning a machine learning model decides which samples are sent to quality\ninspection. On the one hand, this minimizes the production of scrap parts due\nto earlier detection of quality violations. On the other hand, quality\ninspection costs are reduced for smooth operation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.14996,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000080135,
      "text":"Multiple Modes for Continual Learning\n\n  Adapting model parameters to incoming streams of data is a crucial factor to\ndeep learning scalability. Interestingly, prior continual learning strategies\nin online settings inadvertently anchor their updated parameters to a local\nparameter subspace to remember old tasks, else drift away from the subspace and\nforget. From this observation, we formulate a trade-off between constructing\nmultiple parameter modes and allocating tasks per mode. Mode-Optimized Task\nAllocation (MOTA), our contributed adaptation strategy, trains multiple modes\nin parallel, then optimizes task allocation per mode. We empirically\ndemonstrate improvements over baseline continual learning strategies and across\nvarying distribution shifts, namely sub-population, domain, and task shift.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.0889,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"An information-theoretic perspective on intrinsic motivation in\n  reinforcement learning: a survey\n\n  The reinforcement learning (RL) research area is very active, with an\nimportant number of new contributions; especially considering the emergent\nfield of deep RL (DRL). However a number of scientific and technical challenges\nstill need to be resolved, amongst which we can mention the ability to abstract\nactions or the difficulty to explore the environment in sparse-reward settings\nwhich can be addressed by intrinsic motivation (IM). We propose to survey these\nresearch works through a new taxonomy based on information theory: we\ncomputationally revisit the notions of surprise, novelty and skill learning.\nThis allows us to identify advantages and disadvantages of methods and exhibit\ncurrent outlooks of research. Our analysis suggests that novelty and surprise\ncan assist the building of a hierarchy of transferable skills that further\nabstracts the environment and makes the exploration process more robust.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.10585,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000046028,
      "text":"Grape Cold Hardiness Prediction via Multi-Task Learning\n\n  Cold temperatures during fall and spring have the potential to cause frost\ndamage to grapevines and other fruit plants, which can significantly decrease\nharvest yields. To help prevent these losses, farmers deploy expensive frost\nmitigation measures such as sprinklers, heaters, and wind machines when they\njudge that damage may occur. This judgment, however, is challenging because the\ncold hardiness of plants changes throughout the dormancy period and it is\ndifficult to directly measure. This has led scientists to develop cold\nhardiness prediction models that can be tuned to different grape cultivars\nbased on laborious field measurement data. In this paper, we study whether deep\nlearning models can improve cold hardiness prediction for grapes based on data\nthat has been collected over a 30-year time period. A key challenge is that the\namount of data per cultivar is highly variable, with some cultivars having only\na small amount. For this purpose, we investigate the use of multi-task learning\nto leverage data across cultivars in order to improve prediction performance\nfor individual cultivars. We evaluate a number of multi-task learning\napproaches and show that the highest performing approach is able to\nsignificantly improve over learning for single cultivars and outperforms the\ncurrent state-of-the-art scientific model for most cultivars.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.03933,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"NeuralFMU: Presenting a workflow for integrating hybrid NeuralODEs into\n  real world applications\n\n  The term NeuralODE describes the structural combination of an Artifical\nNeural Network (ANN) and a numerical solver for Ordinary Differential Equations\n(ODEs), the former acts as the right-hand side of the ODE to be solved. This\nconcept was further extended by a black-box model in the form of a Functional\nMock-up Unit (FMU) to obtain a subclass of NeuralODEs, named NeuralFMUs. The\nresulting structure features the advantages of first-principle and data-driven\nmodeling approaches in one single simulation model: A higher prediction\naccuracy compared to conventional First Principle Models (FPMs), while also a\nlower training effort compared to purely data-driven models. We present an\nintuitive workflow to setup and use NeuralFMUs, enabling the encapsulation and\nreuse of existing conventional models exported from common modeling tools.\nMoreover, we exemplify this concept by deploying a NeuralFMU for a consumption\nsimulation based on a Vehicle Longitudinal Dynamics Model (VLDM), which is a\ntypical use case in automotive industry. Related challenges that are often\nneglected in scientific use cases, like real measurements (e.g. noise), an\nunknown system state or high-frequent discontinuities, are handled in this\ncontribution. For the aim to build a hybrid model with a higher prediction\nquality than the original FPM, we briefly highlight two open-source libraries:\nFMI.jl for integrating FMUs into the Julia programming environment, as well as\nan extension to this library called FMIFlux.jl, that allows for the integration\nof FMUs into a neural network topology to finally obtain a NeuralFMU.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.08928,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000044041,
      "text":"UMIX: Improving Importance Weighting for Subpopulation Shift via\n  Uncertainty-Aware Mixup\n\n  Subpopulation shift widely exists in many real-world machine learning\napplications, referring to the training and test distributions containing the\nsame subpopulation groups but varying in subpopulation frequencies. Importance\nreweighting is a normal way to handle the subpopulation shift issue by imposing\nconstant or adaptive sampling weights on each sample in the training dataset.\nHowever, some recent studies have recognized that most of these approaches fail\nto improve the performance over empirical risk minimization especially when\napplied to over-parameterized neural networks. In this work, we propose a\nsimple yet practical framework, called uncertainty-aware mixup (UMIX), to\nmitigate the overfitting issue in over-parameterized models by reweighting the\n''mixed'' samples according to the sample uncertainty. The\ntraining-trajectories-based uncertainty estimation is equipped in the proposed\nUMIX for each sample to flexibly characterize the subpopulation distribution.\nWe also provide insightful theoretical analysis to verify that UMIX achieves\nbetter generalization bounds over prior works. Further, we conduct extensive\nempirical studies across a wide range of tasks to validate the effectiveness of\nour method both qualitatively and quantitatively. Code is available at\nhttps:\/\/github.com\/TencentAILabHealthcare\/UMIX.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.07475,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"Neural Networks Reduction via Lumping\n\nThe increasing size of recently proposed Neural Networks makes it hard to implement them on embedded devices, where memory, battery and computational power are a non-trivial bottleneck. For this reason during the last years network compression literature has been thriving and a large number of solutions has been been published to reduce both the number of operations and the parameters involved with the models. Unfortunately, most of these reducing techniques are actually heuristic methods and usually require at least one re-training step to recover the accuracy. The need of procedures for model reduction is well-known also in the fields of Verification and Performances Evaluation, where large efforts have been devoted to the definition of quotients that preserve the observable underlying behaviour. In this paper we try to bridge the gap between the most popular and very effective network reduction strategies and formal notions, such as lumpability, introduced for verification and evaluation of Markov Chains. Elaborating on lumpability we propose a pruning approach that reduces the number of neurons in a network without using any data or fine-tuning, while completely preserving the exact behaviour. Relaxing the constraints on the exact definition of the quotienting method we can give a formal explanation of some of the most common reduction techniques.",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.09481,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000031458,
      "text":"Feature embedding in click-through rate prediction\n\n  We tackle the challenge of feature embedding for the purposes of improving\nthe click-through rate prediction process. We select three models: logistic\nregression, factorization machines and deep factorization machines, as our\nbaselines and propose five different feature embedding modules: embedding\nscaling, FM embedding, embedding encoding, NN embedding and the embedding\nreweighting module. The embedding modules act as a way to improve baseline\nmodel feature embeddings and are trained alongside the rest of the model\nparameters in an end-to-end manner. Each module is individually added to a\nbaseline model to obtain a new augmented model. We test the predictive\nperformance of our augmented models on a publicly accessible dataset used for\nbenchmarking click-through rate prediction models. Our results show that\nseveral proposed embedding modules provide an important increase in predictive\nperformance without a drastic increase in training time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.00945,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000060598,
      "text":"IMG2IMU: Translating Knowledge from Large-Scale Images to IMU Sensing\n  Applications\n\n  Pre-training representations acquired via self-supervised learning could\nachieve high accuracy on even tasks with small training data. Unlike in vision\nand natural language processing domains, pre-training for IMU-based\napplications is challenging, as there are few public datasets with sufficient\nsize and diversity to learn generalizable representations. To overcome this\nproblem, we propose IMG2IMU that adapts pre-trained representation from\nlarge-scale images to diverse IMU sensing tasks. We convert the sensor data\ninto visually interpretable spectrograms for the model to utilize the knowledge\ngained from vision. We further present a sensor-aware pre-training method for\nimages that enables models to acquire particularly impactful knowledge for IMU\nsensing applications. This involves using contrastive learning on our\naugmentation set customized for the properties of sensor data. Our evaluation\nwith four different IMU sensing tasks shows that IMG2IMU outperforms the\nbaselines pre-trained on sensor data by an average of 9.6%p F1-score,\nillustrating that vision knowledge can be usefully incorporated into IMU\nsensing applications where only limited training data is available.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.08335,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"Efficient Deep Clustering of Human Activities and How to Improve\n  Evaluation\n\n  There has been much recent research on human activity re\\-cog\\-ni\\-tion\n(HAR), due to the proliferation of wearable sensors in watches and phones, and\nthe advances of deep learning methods, which avoid the need to manually extract\nfeatures from raw sensor signals. A significant disadvantage of deep learning\napplied to HAR is the need for manually labelled training data, which is\nespecially difficult to obtain for HAR datasets. Progress is starting to be\nmade in the unsupervised setting, in the form of deep HAR clustering models,\nwhich can assign labels to data without having been given any labels to train\non, but there are problems with evaluating deep HAR clustering models, which\nmakes assessing the field and devising new methods difficult. In this paper, we\nhighlight several distinct problems with how deep HAR clustering models are\nevaluated, describing these problems in detail and conducting careful\nexperiments to explicate the effect that they can have on results. We then\ndiscuss solutions to these problems, and suggest standard evaluation settings\nfor future deep HAR clustering models. Additionally, we present a new deep\nclustering model for HAR. When tested under our proposed settings, our model\nperforms better than (or on par with) existing models, while also being more\nefficient and better able to scale to more complex datasets by avoiding the\nneed for an autoencoder.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.07702,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Federated Coordinate Descent for Privacy-Preserving Multiparty Linear\n  Regression\n\n  Distributed privacy-preserving regression schemes have been developed and\nextended in various fields, where multiparty collaboratively and privately run\noptimization algorithms, e.g., Gradient Descent, to learn a set of optimal\nparameters. However, traditional Gradient-Descent based methods fail to solve\nproblems which contains objective functions with L1 regularization, such as\nLasso regression. In this paper, we present Federated Coordinate Descent, a new\ndistributed scheme called FCD, to address this issue securely under multiparty\nscenarios. Specifically, through secure aggregation and added perturbations,\nour scheme guarantees that: (1) no local information is leaked to other\nparties, and (2) global model parameters are not exposed to cloud servers. The\nadded perturbations can eventually be eliminated by each party to derive a\nglobal model with high performance. We show that the FCD scheme fills the gap\nof multiparty secure Coordinate Descent methods and is applicable for general\nlinear regressions, including linear, ridge and lasso regressions. Theoretical\nsecurity analysis and experimental results demonstrate that FCD can be\nperformed effectively and efficiently, and provide as low MAE measure as\ncentralized methods under tasks of three types of linear regressions on\nreal-world UCI datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.10178,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Deep Learning based pipeline for anomaly detection and quality\n  enhancement in industrial binder jetting processes\n\n  Anomaly detection describes methods of finding abnormal states, instances or\ndata points that differ from a normal value space. Industrial processes are a\ndomain where predicitve models are needed for finding anomalous data instances\nfor quality enhancement. A main challenge, however, is absence of labels in\nthis environment. This paper contributes to a data-centric way of approaching\nartificial intelligence in industrial production. With a use case from additive\nmanufacturing for automotive components we present a deep-learning-based image\nprocessing pipeline. Additionally, we integrate the concept of domain\nrandomisation and synthetic data in the loop that shows promising results for\nbridging advances in deep learning and its application to real-world,\nindustrial production processes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.12309,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Feature Encodings for Gradient Boosting with Automunge\n\n  Automunge is a tabular preprocessing library that encodes dataframes for\nsupervised learning. When selecting a default feature encoding strategy for\ngradient boosted learning, one may consider metrics of training duration and\nachieved predictive performance associated with the feature representations.\nAutomunge offers a default of binarization for categoric features and z-score\nnormalization for numeric. The presented study sought to validate those\ndefaults by way of benchmarking on a series of diverse data sets by encoding\nvariations with tuned gradient boosted learning. We found that on average our\nchosen defaults were top performers both from a tuning duration and a model\nperformance standpoint. Another key finding was that one hot encoding did not\nperform in a manner consistent with suitability to serve as a categoric default\nin comparison to categoric binarization. We present here these and further\nbenchmarks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.07926,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000031458,
      "text":"Explainability in subgraphs-enhanced Graph Neural Networks\n\n  Recently, subgraphs-enhanced Graph Neural Networks (SGNNs) have been\nintroduced to enhance the expressive power of Graph Neural Networks (GNNs),\nwhich was proved to be not higher than the 1-dimensional Weisfeiler-Leman\nisomorphism test. The new paradigm suggests using subgraphs extracted from the\ninput graph to improve the model's expressiveness, but the additional\ncomplexity exacerbates an already challenging problem in GNNs: explaining their\npredictions. In this work, we adapt PGExplainer, one of the most recent\nexplainers for GNNs, to SGNNs. The proposed explainer accounts for the\ncontribution of all the different subgraphs and can produce a meaningful\nexplanation that humans can interpret. The experiments that we performed both\non real and synthetic datasets show that our framework is successful in\nexplaining the decision process of an SGNN on graph classification tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.03798,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"ReX: A Framework for Incorporating Temporal Information in Model-Agnostic Local Explanation Techniques\n\nExisting local model-agnostic explanation techniques are ineffective for machine learning models that consider inputs of variable lengths, as they do not consider temporal information embedded in these models. To address this limitation, we propose \\textsc{ReX}, a general framework for incorporating temporal information in these techniques. Our key insight is that these techniques typically learn a model surrogate by sampling model inputs and outputs, and we can incorporate temporal information in a uniform way by only changing the sampling process and the surrogate features. We instantiate our approach on three popular explanation techniques: Anchors, LIME, and Kernel SHAP. To evaluate the effectiveness of \\textsc{ReX}, we apply our approach to six models in three different tasks. Our evaluation results demonstrate that our approach 1) significantly improves the fidelity of explanations, making model-agnostic techniques outperform a state-of-the-art model-specific technique on its target model, and 2) helps end users better understand the models' behaviors.",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.01397,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000079804,
      "text":"Disconnected Emerging Knowledge Graph Oriented Inductive Link Prediction\n\n  Inductive link prediction (ILP) is to predict links for unseen entities in\nemerging knowledge graphs (KGs), considering the evolving nature of KGs. A more\nchallenging scenario is that emerging KGs consist of only unseen entities,\ncalled as disconnected emerging KGs (DEKGs). Existing studies for DEKGs only\nfocus on predicting enclosing links, i.e., predicting links inside the emerging\nKG. The bridging links, which carry the evolutionary information from the\noriginal KG to DEKG, have not been investigated by previous work so far. To\nfill in the gap, we propose a novel model entitled DEKG-ILP (Disconnected\nEmerging Knowledge Graph Oriented Inductive Link Prediction) that consists of\nthe following two components. (1) The module CLRM (Contrastive Learning-based\nRelation-specific Feature Modeling) is developed to extract global\nrelation-based semantic features that are shared between original KGs and DEKGs\nwith a novel sampling strategy. (2) The module GSM (GNN-based Subgraph\nModeling) is proposed to extract the local subgraph topological information\naround each link in KGs. The extensive experiments conducted on several\nbenchmark datasets demonstrate that DEKG-ILP has obvious performance\nimprovements compared with state-of-the-art methods for both enclosing and\nbridging link prediction. The source code is available online.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.12647,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000095036,
      "text":"PL-kNN: A Parameterless Nearest Neighbors Classifier\n\n  Demands for minimum parameter setup in machine learning models are desirable\nto avoid time-consuming optimization processes. The $k$-Nearest Neighbors is\none of the most effective and straightforward models employed in numerous\nproblems. Despite its well-known performance, it requires the value of $k$ for\nspecific data distribution, thus demanding expensive computational efforts.\nThis paper proposes a $k$-Nearest Neighbors classifier that bypasses the need\nto define the value of $k$. The model computes the $k$ value adaptively\nconsidering the data distribution of the training set. We compared the proposed\nmodel against the standard $k$-Nearest Neighbors classifier and two\nparameterless versions from the literature. Experiments over 11 public datasets\nconfirm the robustness of the proposed approach, for the obtained results were\nsimilar or even better than its counterpart versions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.11785,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000057618,
      "text":"Tiered Pruning for Efficient Differentialble Inference-Aware Neural\n  Architecture Search\n\n  We propose three novel pruning techniques to improve the cost and results of\ninference-aware Differentiable Neural Architecture Search (DNAS). First, we\nintroduce Prunode, a stochastic bi-path building block for DNAS, which can\nsearch over inner hidden dimensions with O(1) memory and compute complexity.\nSecond, we present an algorithm for pruning blocks within a stochastic layer of\nthe SuperNet during the search. Third, we describe a novel technique for\npruning unnecessary stochastic layers during the search. The optimized models\nresulting from the search are called PruNet and establishes a new\nstate-of-the-art Pareto frontier for NVIDIA V100 in terms of inference latency\nfor ImageNet Top-1 image classification accuracy. PruNet as a backbone also\noutperforms GPUNet and EfficientNet on the COCO object detection task on\ninference latency relative to mean Average Precision (mAP).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.06575,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000003212,
      "text":"Efficient multi-relational network representation using primes\n\n  In this work, we propose a novel representation of complex multi-relational\nnetworks, which is compact and allows very efficient network analysis.\nMulti-relational networks capture complex data relationships and have a variety\nof applications, ranging from biomedical to financial, social, etc. As they get\nto be used with ever larger quantities of data, it is crucial to find efficient\nways to represent and analyse such networks. This paper introduces the concept\nof Prime Adjacency Matrices (PAMs), which utilize prime numbers, to represent\nthe relations of the network. Due to the fundamental theorem of arithmetic,\nthis allows for a lossless, compact representation of a complete\nmulti-relational graph, using a single adjacency matrix. Moreover, this\nrepresentation enables the fast computation of multi-hop adjacency matrices,\nwhich can be useful for a variety of downstream tasks. We illustrate the\nbenefits of using the proposed approach through various simple and complex\nnetwork analysis tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.14764,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Model Zoos: A Dataset of Diverse Populations of Neural Network Models\n\n  In the last years, neural networks (NN) have evolved from laboratory\nenvironments to the state-of-the-art for many real-world problems. It was shown\nthat NN models (i.e., their weights and biases) evolve on unique trajectories\nin weight space during training. Following, a population of such neural network\nmodels (referred to as model zoo) would form structures in weight space. We\nthink that the geometry, curvature and smoothness of these structures contain\ninformation about the state of training and can reveal latent properties of\nindividual models. With such model zoos, one could investigate novel approaches\nfor (i) model analysis, (ii) discover unknown learning dynamics, (iii) learn\nrich representations of such populations, or (iv) exploit the model zoos for\ngenerative modelling of NN weights and biases. Unfortunately, the lack of\nstandardized model zoos and available benchmarks significantly increases the\nfriction for further research about populations of NNs. With this work, we\npublish a novel dataset of model zoos containing systematically generated and\ndiverse populations of NN models for further research. In total the proposed\nmodel zoo dataset is based on eight image datasets, consists of 27 model zoos\ntrained with varying hyperparameter combinations and includes 50'360 unique NN\nmodels as well as their sparsified twins, resulting in over 3'844'360 collected\nmodel states. Additionally, to the model zoo data we provide an in-depth\nanalysis of the zoos and provide benchmarks for multiple downstream tasks. The\ndataset can be found at www.modelzoos.cc.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.14784,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"Categorical SDEs with Simplex Diffusion\n\n  Diffusion models typically operate in the standard framework of generative\nmodelling by producing continuously-valued datapoints. To this end, they rely\non a progressive Gaussian smoothing of the original data distribution, which\nadmits an SDE interpretation involving increments of a standard Brownian\nmotion. However, some applications such as text generation or reinforcement\nlearning might naturally be better served by diffusing categorical-valued data,\ni.e., lifting the diffusion to a space of probability distributions. To this\nend, this short theoretical note proposes Simplex Diffusion, a means to\ndirectly diffuse datapoints located on an n-dimensional probability simplex. We\nshow how this relates to the Dirichlet distribution on the simplex and how the\nanalogous SDE is realized thanks to a multi-dimensional Cox-Ingersoll-Ross\nprocess (abbreviated as CIR), previously used in economics and mathematical\nfinance. Finally, we make remarks as to the numerical implementation of\ntrajectories of the CIR process, and discuss some limitations of our approach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.16524,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000136097,
      "text":"Federated clustering with GAN-based data synthesis\n\n  Federated clustering (FC) is an extension of centralized clustering in\nfederated settings. The key here is how to construct a global similarity\nmeasure without sharing private data, since the local similarity may be\ninsufficient to group local data correctly and the similarity of samples across\nclients cannot be directly measured due to privacy constraints. Obviously, the\nmost straightforward way to analyze FC is to employ the methods extended from\ncentralized ones, such as K-means (KM) and fuzzy c-means (FCM). However, they\nare vulnerable to non independent-and-identically-distributed (non-IID) data\namong clients. To handle this, we propose a new federated clustering framework,\nnamed synthetic data aided federated clustering (SDA-FC). It trains generative\nadversarial network locally in each client and uploads the generated synthetic\ndata to the server, where KM or FCM is performed on the synthetic data. The\nsynthetic data can make the model immune to the non-IID problem and enable us\nto capture the global similarity characteristics more effectively without\nsharing private data. Comprehensive experiments reveals the advantages of\nSDA-FC, including superior performance in addressing the non-IID problem and\nthe device failures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.06964,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000071857,
      "text":"Causality-driven Hierarchical Structure Discovery for Reinforcement\n  Learning\n\n  Hierarchical reinforcement learning (HRL) effectively improves agents'\nexploration efficiency on tasks with sparse reward, with the guide of\nhigh-quality hierarchical structures (e.g., subgoals or options). However, how\nto automatically discover high-quality hierarchical structures is still a great\nchallenge. Previous HRL methods can hardly discover the hierarchical structures\nin complex environments due to the low exploration efficiency by exploiting the\nrandomness-driven exploration paradigm. To address this issue, we propose\nCDHRL, a causality-driven hierarchical reinforcement learning framework,\nleveraging a causality-driven discovery instead of a randomness-driven\nexploration to effectively build high-quality hierarchical structures in\ncomplicated environments. The key insight is that the causalities among\nenvironment variables are naturally fit for modeling reachable subgoals and\ntheir dependencies and can perfectly guide to build high-quality hierarchical\nstructures. The results in two complex environments, 2D-Minecraft and Eden,\nshow that CDHRL significantly boosts exploration efficiency with the\ncausality-driven paradigm.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.10019,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000028809,
      "text":"Towards Understanding GD with Hard and Conjugate Pseudo-labels for\n  Test-Time Adaptation\n\n  We consider a setting that a model needs to adapt to a new domain under\ndistribution shifts, given that only unlabeled test samples from the new domain\nare accessible at test time. A common idea in most of the related works is\nconstructing pseudo-labels for the unlabeled test samples and applying gradient\ndescent (GD) to a loss function with the pseudo-labels. Recently, \\cite{GSRK22}\npropose conjugate labels, which is a new kind of pseudo-labels for\nself-training at test time. They empirically show that the conjugate label\noutperforms other ways of pseudo-labeling on many domain adaptation benchmarks.\nHowever, provably showing that GD with conjugate labels learns a good\nclassifier for test-time adaptation remains open. In this work, we aim at\ntheoretically understanding GD with hard and conjugate labels for a binary\nclassification problem. We show that for square loss, GD with conjugate labels\nconverges to an $\\epsilon$-optimal predictor under a Gaussian model for any\narbitrarily small $\\epsilon$, while GD with hard pseudo-labels fails in this\ntask. We also analyze them under different loss functions for the update. Our\nresults shed lights on understanding when and why GD with hard labels or\nconjugate labels works in test-time adaptation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.00368,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"Parameter-varying neural ordinary differential equations with\n  partition-of-unity networks\n\n  In this study, we propose parameter-varying neural ordinary differential\nequations (NODEs) where the evolution of model parameters is represented by\npartition-of-unity networks (POUNets), a mixture of experts architecture. The\nproposed variant of NODEs, synthesized with POUNets, learn a meshfree partition\nof space and represent the evolution of ODE parameters using sets of\npolynomials associated to each partition. We demonstrate the effectiveness of\nthe proposed method for three important tasks: data-driven dynamics modeling of\n(1) hybrid systems, (2) switching linear dynamical systems, and (3) latent\ndynamics for dynamical systems with varying external forcing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.12402,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000080135,
      "text":"DIGMN: Dynamic Intent Guided Meta Network for Differentiated User\n  Engagement Forecasting in Online Professional Social Platforms\n\n  User engagement prediction plays a critical role for designing interaction\nstrategies to grow user engagement and increase revenue in online social\nplatforms. Through the in-depth analysis of the real-world data from the\nworld's largest professional social platforms, i.e., LinkedIn, we find that\nusers expose diverse engagement patterns, and a major reason for the\ndifferences in user engagement patterns is that users have different intents.\nThat is, people have different intents when using LinkedIn, e.g., applying for\njobs, building connections, or checking notifications, which shows quite\ndifferent engagement patterns. Meanwhile, user intents and the corresponding\nengagement patterns may change over time. Although such pattern differences and\ndynamics are essential for user engagement prediction, differentiating user\nengagement patterns based on user dynamic intents for better user engagement\nforecasting has not received enough attention in previous works. In this paper,\nwe proposed a Dynamic Intent Guided Meta Network (DIGMN), which can explicitly\nmodel user intent varying with time and perform differentiated user engagement\nforecasting. Specifically, we derive some interpretable basic user intents as\nprior knowledge from data mining and introduce prior intents in explicitly\nmodeling dynamic user intent. Furthermore, based on the dynamic user intent\nrepresentations, we propose a meta predictor to perform differentiated user\nengagement forecasting. Through a comprehensive evaluation on LinkedIn\nanonymous user data, our method outperforms state-of-the-art baselines\nsignificantly, i.e., 2.96% and 3.48% absolute error reduction, on\ncoarse-grained and fine-grained user engagement prediction tasks, respectively,\ndemonstrating the effectiveness of our method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.14012,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Gradient-based Weight Density Balancing for Robust Dynamic Sparse\n  Training\n\n  Training a sparse neural network from scratch requires optimizing connections\nat the same time as the weights themselves. Typically, the weights are\nredistributed after a predefined number of weight updates, removing a fraction\nof the parameters of each layer and inserting them at different locations in\nthe same layers. The density of each layer is determined using heuristics,\noften purely based on the size of the parameter tensor. While the connections\nper layer are optimized multiple times during training, the density of each\nlayer remains constant. This leaves great unrealized potential, especially in\nscenarios with a high sparsity of 90% and more. We propose Global\nGradient-based Redistribution, a technique which distributes weights across all\nlayers - adding more weights to the layers that need them most. Our evaluation\nshows that our approach is less prone to unbalanced weight distribution at\ninitialization than previous work and that it is able to find better performing\nsparse subnetworks at very high sparsity levels.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.00226,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Towards Understanding and Mitigating Dimensional Collapse in\n  Heterogeneous Federated Learning\n\n  Federated learning aims to train models collaboratively across different\nclients without the sharing of data for privacy considerations. However, one\nmajor challenge for this learning paradigm is the {\\em data heterogeneity}\nproblem, which refers to the discrepancies between the local data distributions\namong various clients. To tackle this problem, we first study how data\nheterogeneity affects the representations of the globally aggregated models.\nInterestingly, we find that heterogeneous data results in the global model\nsuffering from severe {\\em dimensional collapse}, in which representations tend\nto reside in a lower-dimensional space instead of the ambient space. Moreover,\nwe observe a similar phenomenon on models locally trained on each client and\ndeduce that the dimensional collapse on the global model is inherited from\nlocal models. In addition, we theoretically analyze the gradient flow dynamics\nto shed light on how data heterogeneity result in dimensional collapse for\nlocal models. To remedy this problem caused by the data heterogeneity, we\npropose {\\sc FedDecorr}, a novel method that can effectively mitigate\ndimensional collapse in federated learning. Specifically, {\\sc FedDecorr}\napplies a regularization term during local training that encourages different\ndimensions of representations to be uncorrelated. {\\sc FedDecorr}, which is\nimplementation-friendly and computationally-efficient, yields consistent\nimprovements over baselines on standard benchmark datasets. Code:\nhttps:\/\/github.com\/bytedance\/FedDecorr.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.03821,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Large Language Models can Implement Policy Iteration\n\n  This work presents In-Context Policy Iteration, an algorithm for performing\nReinforcement Learning (RL), in-context, using foundation models. While the\napplication of foundation models to RL has received considerable attention,\nmost approaches rely on either (1) the curation of expert demonstrations\n(either through manual design or task-specific pretraining) or (2) adaptation\nto the task of interest using gradient methods (either fine-tuning or training\nof adapter layers). Both of these techniques have drawbacks. Collecting\ndemonstrations is labor-intensive, and algorithms that rely on them do not\noutperform the experts from which the demonstrations were derived. All gradient\ntechniques are inherently slow, sacrificing the \"few-shot\" quality that made\nin-context learning attractive to begin with. In this work, we present an\nalgorithm, ICPI, that learns to perform RL tasks without expert demonstrations\nor gradients. Instead we present a policy-iteration method in which the prompt\ncontent is the entire locus of learning. ICPI iteratively updates the contents\nof the prompt from which it derives its policy through trial-and-error\ninteraction with an RL environment. In order to eliminate the role of\nin-weights learning (on which approaches like Decision Transformer rely\nheavily), we demonstrate our algorithm using Codex, a language model with no\nprior knowledge of the domains on which we evaluate it.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.06873,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Data augmentation on-the-fly and active learning in data stream classification\n\nThere is an emerging need for predictive models to be trained on-the-fly, since in numerous machine learning applications data are arriving in an online fashion. A critical challenge encountered is that of limited availability of ground truth information (e.g., labels in classification tasks) as new data are observed one-by-one online, while another significant challenge is that of class imbalance. This work introduces the novel Augmented Queues method, which addresses the dual-problem by combining in a synergistic manner online active learning, data augmentation, and a multi-queue memory to maintain separate and balanced queues for each class. We perform an extensive experimental study using image and time-series augmentations, in which we examine the roles of the active learning budget, memory size, imbalance level, and neural network type. We demonstrate two major advantages of Augmented Queues. First, it does not reserve additional memory space as the generation of synthetic data occurs only at training times. Second, learning models have access to more labelled data without the need to increase the active learning budget and \/ or the original memory size. Learning on-the-fly poses major challenges which, typically, hinder the deployment of learning models. Augmented Queues significantly improves the performance in terms of learning quality and speed. Our code is made publicly available.",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.02573,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"Efficient Learning of Mesh-Based Physical Simulation with BSMS-GNN\n\n  Learning the physical simulation on large-scale meshes with flat Graph Neural\nNetworks (GNNs) and stacking Message Passings (MPs) is challenging due to the\nscaling complexity w.r.t. the number of nodes and over-smoothing. There has\nbeen growing interest in the community to introduce \\textit{multi-scale}\nstructures to GNNs for physical simulation. However, current state-of-the-art\nmethods are limited by their reliance on the labor-intensive drawing of coarser\nmeshes or building coarser levels based on spatial proximity, which can\nintroduce wrong edges across geometry boundaries. Inspired by the bipartite\ngraph determination, we propose a novel pooling strategy, \\textit{bi-stride} to\ntackle the aforementioned limitations. Bi-stride pools nodes on every other\nfrontier of the breadth-first search (BFS), without the need for the manual\ndrawing of coarser meshes and avoiding the wrong edges by spatial proximity.\nAdditionally, it enables a one-MP scheme per level and non-parametrized pooling\nand unpooling by interpolations, resembling U-Nets, which significantly reduces\ncomputational costs. Experiments show that the proposed framework,\n\\textit{BSMS-GNN}, significantly outperforms existing methods in terms of both\naccuracy and computational efficiency in representative physical simulations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.02099,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000031789,
      "text":"Automated Graph Self-supervised Learning via Multi-teacher Knowledge\n  Distillation\n\n  Self-supervised learning on graphs has recently achieved remarkable success\nin graph representation learning. With hundreds of self-supervised pretext\ntasks proposed over the past few years, the research community has greatly\ndeveloped, and the key is no longer to design more powerful but complex pretext\ntasks, but to make more effective use of those already on hand. This paper\nstudies the problem of how to automatically, adaptively, and dynamically learn\ninstance-level self-supervised learning strategies for each node from a given\npool of pretext tasks. In this paper, we propose a novel multi-teacher\nknowledge distillation framework for Automated Graph Self-Supervised Learning\n(AGSSL), which consists of two main branches: (i) Knowledge Extraction:\ntraining multiple teachers with different pretext tasks, so as to extract\ndifferent levels of knowledge with different inductive biases; (ii) Knowledge\nIntegration: integrating different levels of knowledge and distilling them into\nthe student model. Without simply treating different teachers as equally\nimportant, we provide a provable theoretical guideline for how to integrate the\nknowledge of different teachers, i.e., the integrated teacher probability\nshould be close to the true Bayesian class-probability. To approach the\ntheoretical optimum in practice, two adaptive knowledge integration strategies\nare proposed to construct a relatively \"good\" integrated teacher. Extensive\nexperiments on eight datasets show that AGSSL can benefit from multiple pretext\ntasks, outperforming the corresponding individual tasks; by combining a few\nsimple but classical pretext tasks, the resulting performance is comparable to\nother leading counterparts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.12506,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"Self-supervised Graph-based Point-of-interest Recommendation\n\n  The exponential growth of Location-based Social Networks (LBSNs) has greatly\nstimulated the demand for precise location-based recommendation services. Next\nPoint-of-Interest (POI) recommendation, which aims to provide personalised POI\nsuggestions for users based on their visiting histories, has become a prominent\ncomponent in location-based e-commerce. Recent POI recommenders mainly employ\nself-attention mechanism or graph neural networks to model complex high-order\nPOI-wise interactions. However, most of them are merely trained on the\nhistorical check-in data in a standard supervised learning manner, which fail\nto fully explore each user's multi-faceted preferences, and suffer from data\nscarcity and long-tailed POI distribution, resulting in sub-optimal\nperformance. To this end, we propose a Self-s}upervised Graph-enhanced POI\nRecommender (S2GRec) for next POI recommendation. In particular, we devise a\nnovel Graph-enhanced Self-attentive layer to incorporate the collaborative\nsignals from both global transition graph and local trajectory graphs to\nuncover the transitional dependencies among POIs and capture a user's temporal\ninterests. In order to counteract the scarcity and incompleteness of POI\ncheck-ins, we propose a novel self-supervised learning paradigm in \\ssgrec,\nwhere the trajectory representations are contrastively learned from two\naugmented views on geolocations and temporal transitions. Extensive experiments\nare conducted on three real-world LBSN datasets, demonstrating the\neffectiveness of our model against state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.11033,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"Neural Estimation of Submodular Functions with Applications to\n  Differentiable Subset Selection\n\n  Submodular functions and variants, through their ability to characterize\ndiversity and coverage, have emerged as a key tool for data selection and\nsummarization. Many recent approaches to learn submodular functions suffer from\nlimited expressiveness. In this work, we propose FLEXSUBNET, a family of\nflexible neural models for both monotone and non-monotone submodular functions.\nTo fit a latent submodular function from (set, value) observations, FLEXSUBNET\napplies a concave function on modular functions in a recursive manner. We do\nnot draw the concave function from a restricted family, but rather learn from\ndata using a highly expressive neural network that implements a differentiable\nquadrature procedure. Such an expressive neural model for concave functions may\nbe of independent interest. Next, we extend this setup to provide a novel\ncharacterization of monotone \\alpha-submodular functions, a recently introduced\nnotion of approximate submodular functions. We then use this characterization\nto design a novel neural model for such functions. Finally, we consider\nlearning submodular set functions under distant supervision in the form of\n(perimeter-set, high-value-subset) pairs. This yields a novel subset selection\nmethod based on an order-invariant, yet greedy sampler built around the above\nneural set functions. Our experiments on synthetic and real data show that\nFLEXSUBNET outperforms several baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.06089,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"When are Local Queries Useful for Robust Learning?\n\n  Distributional assumptions have been shown to be necessary for the robust\nlearnability of concept classes when considering the exact-in-the-ball robust\nrisk and access to random examples by Gourdeau et al. (2019). In this paper, we\nstudy learning models where the learner is given more power through the use of\nlocal queries, and give the first distribution-free algorithms that perform\nrobust empirical risk minimization (ERM) for this notion of robustness. The\nfirst learning model we consider uses local membership queries (LMQ), where the\nlearner can query the label of points near the training sample. We show that,\nunder the uniform distribution, LMQs do not increase the robustness threshold\nof conjunctions and any superclass, e.g., decision lists and halfspaces. Faced\nwith this negative result, we introduce the local equivalence query\n($\\mathsf{LEQ}$) oracle, which returns whether the hypothesis and target\nconcept agree in the perturbation region around a point in the training sample,\nas well as a counterexample if it exists. We show a separation result: on the\none hand, if the query radius $\\lambda$ is strictly smaller than the\nadversary's perturbation budget $\\rho$, then distribution-free robust learning\nis impossible for a wide variety of concept classes; on the other hand, the\nsetting $\\lambda=\\rho$ allows us to develop robust ERM algorithms. We then\nbound the query complexity of these algorithms based on online learning\nguarantees and further improve these bounds for the special case of\nconjunctions. We finish by giving robust learning algorithms for halfspaces on\n$\\{0,1\\}^n$ and then obtaining robustness guarantees for halfspaces in\n$\\mathbb{R}^n$ against precision-bounded adversaries.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.04427,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Asymmetric Temperature Scaling Makes Larger Networks Teach Well Again\n\n  Knowledge Distillation (KD) aims at transferring the knowledge of a\nwell-performed neural network (the {\\it teacher}) to a weaker one (the {\\it\nstudent}). A peculiar phenomenon is that a more accurate model doesn't\nnecessarily teach better, and temperature adjustment can neither alleviate the\nmismatched capacity. To explain this, we decompose the efficacy of KD into\nthree parts: {\\it correct guidance}, {\\it smooth regularization}, and {\\it\nclass discriminability}. The last term describes the distinctness of {\\it wrong\nclass probabilities} that the teacher provides in KD. Complex teachers tend to\nbe over-confident and traditional temperature scaling limits the efficacy of\n{\\it class discriminability}, resulting in less discriminative wrong class\nprobabilities. Therefore, we propose {\\it Asymmetric Temperature Scaling\n(ATS)}, which separately applies a higher\/lower temperature to the\ncorrect\/wrong class. ATS enlarges the variance of wrong class probabilities in\nthe teacher's label and makes the students grasp the absolute affinities of\nwrong classes to the target class as discriminative as possible. Both\ntheoretical analysis and extensive experimental results demonstrate the\neffectiveness of ATS. The demo developed in Mindspore is available at\nhttps:\/\/gitee.com\/lxcnju\/ats-mindspore and will be available at\nhttps:\/\/gitee.com\/mindspore\/models\/tree\/master\/research\/cv\/ats.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.04002,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000015563,
      "text":"Dynamically meeting performance objectives for multiple services on a\n  service mesh\n\n  We present a framework that lets a service provider achieve end-to-end\nmanagement objectives under varying load. Dynamic control actions are performed\nby a reinforcement learning (RL) agent. Our work includes experimentation and\nevaluation on a laboratory testbed where we have implemented basic information\nservices on a service mesh supported by the Istio and Kubernetes platforms. We\ninvestigate different management objectives that include end-to-end delay\nbounds on service requests, throughput objectives, and service differentiation.\nThese objectives are mapped onto reward functions that an RL agent learns to\noptimize, by executing control actions, namely, request routing and request\nblocking. We compute the control policies not on the testbed, but in a\nsimulator, which speeds up the learning process by orders of magnitude. In our\napproach, the system model is learned on the testbed; it is then used to\ninstantiate the simulator, which produces near-optimal control policies for\nvarious management objectives. The learned policies are then evaluated on the\ntestbed using unseen load patterns.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.12239,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Auto-Encoder Neural Network Incorporating X-Ray Fluorescence Fundamental\n  Parameters with Machine Learning\n\n  We consider energy-dispersive X-ray Fluorescence (EDXRF) applications where\nthe fundamental parameters method is impractical such as when instrument\nparameters are unavailable. For example, on a mining shovel or conveyor belt,\nrocks are constantly moving (leading to varying angles of incidence and\ndistances) and there may be other factors not accounted for (like dust). Neural\nnetworks do not require instrument and fundamental parameters but training\nneural networks requires XRF spectra labelled with elemental composition, which\nis often limited because of its expense. We develop a neural network model that\nlearns from limited labelled data and also benefits from domain knowledge by\nlearning to invert a forward model. The forward model uses transition energies\nand probabilities of all elements and parameterized distributions to\napproximate other fundamental and instrument parameters. We evaluate the model\nand baseline models on a rock dataset from a lithium mineral exploration\nproject. Our model works particularly well for some low-Z elements (Li, Mg, Al,\nand K) as well as some high-Z elements (Sn and Pb) despite these elements being\noutside the suitable range for common spectrometers to directly measure, likely\nowing to the ability of neural networks to learn correlations and non-linear\nrelationships.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.14362,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000103977,
      "text":"Federated Learning Using Variance Reduced Stochastic Gradient for\n  Probabilistically Activated Agents\n\n  This paper proposes an algorithm for Federated Learning (FL) with a two-layer\nstructure that achieves both variance reduction and a faster convergence rate\nto an optimal solution in the setting where each agent has an arbitrary\nprobability of selection in each iteration. In distributed machine learning,\nwhen privacy matters, FL is a functional tool. Placing FL in an environment\nwhere it has some irregular connections of agents (devices), reaching a trained\nmodel in both an economical and quick way can be a demanding job. The first\nlayer of our algorithm corresponds to the model parameter propagation across\nagents done by the server. In the second layer, each agent does its local\nupdate with a stochastic and variance-reduced technique called Stochastic\nVariance Reduced Gradient (SVRG). We leverage the concept of variance reduction\nfrom stochastic optimization when the agents want to do their local update step\nto reduce the variance caused by stochastic gradient descent (SGD). We provide\na convergence bound for our algorithm which improves the rate from\n$O(\\frac{1}{\\sqrt{K}})$ to $O(\\frac{1}{K})$ by using a constant step-size. We\ndemonstrate the performance of our algorithm using numerical examples.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.01625,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Energy Consumption of Neural Networks on NVIDIA Edge Boards: an\n  Empirical Model\n\n  Recently, there has been a trend of shifting the execution of deep learning\ninference tasks toward the edge of the network, closer to the user, to reduce\nlatency and preserve data privacy. At the same time, growing interest is being\ndevoted to the energetic sustainability of machine learning. At the\nintersection of these trends, we hence find the energetic characterization of\nmachine learning at the edge, which is attracting increasing attention.\nUnfortunately, calculating the energy consumption of a given neural network\nduring inference is complicated by the heterogeneity of the possible underlying\nhardware implementation. In this work, we hence aim at profiling the energetic\nconsumption of inference tasks for some modern edge nodes and deriving simple\nbut realistic models. To this end, we performed a large number of experiments\nto collect the energy consumption of convolutional and fully connected layers\non two well-known edge boards by NVIDIA, namely Jetson TX2 and Xavier. From the\nmeasurements, we have then distilled a simple, practical model that can provide\nan estimate of the energy consumption of a certain inference task on the\nconsidered boards. We believe that this model can be used in many contexts as,\nfor instance, to guide the search for efficient architectures in Neural\nArchitecture Search, as a heuristic in Neural Network pruning, or to find\nenergy-efficient offloading strategies in a Split computing context, or simply\nto evaluate the energetic performance of Deep Neural Network architectures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.04539,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"Physics-informed inference of aerial animal movements from weather radar\n  data\n\n  Studying animal movements is essential for effective wildlife conservation\nand conflict mitigation. For aerial movements, operational weather radars have\nbecome an indispensable data source in this respect. However, partial\nmeasurements, incomplete spatial coverage, and poor understanding of animal\nbehaviours make it difficult to reconstruct complete spatio-temporal movement\npatterns from available radar data. We tackle this inverse problem by learning\na mapping from high-dimensional radar measurements to low-dimensional latent\nrepresentations using a convolutional encoder. Under the assumption that the\nlatent system dynamics are well approximated by a locally linear Gaussian\ntransition model, we perform efficient posterior estimation using the classical\nKalman smoother. A convolutional decoder maps the inferred latent system states\nback to the physical space in which the known radar observation model can be\napplied, enabling fully unsupervised training. To encourage physical\nconsistency, we additionally introduce a physics-informed loss term that\nleverages known mass conservation constraints. Our experiments on synthetic\nradar data show promising results in terms of reconstruction quality and\ndata-efficiency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.14932,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"Eluder-based Regret for Stochastic Contextual MDPs\n\n  We present the E-UC$^3$RL algorithm for regret minimization in Stochastic\nContextual Markov Decision Processes (CMDPs). The algorithm operates under the\nminimal assumptions of realizable function class and access to \\emph{offline}\nleast squares and log loss regression oracles. Our algorithm is efficient\n(assuming efficient offline regression oracles) and enjoys a regret guarantee\nof $ \\widetilde{O}(H^3 \\sqrt{T |S| |A|d_{\\mathrm{E}}(\\mathcal{P}) \\log\n(|\\mathcal{F}| |\\mathcal{P}|\/ \\delta) )}) , $ with $T$ being the number of\nepisodes, $S$ the state space, $A$ the action space, $H$ the horizon,\n$\\mathcal{P}$ and $\\mathcal{F}$ are finite function classes used to approximate\nthe context-dependent dynamics and rewards, respectively, and\n$d_{\\mathrm{E}}(\\mathcal{P})$ is the Eluder dimension of $\\mathcal{P}$ w.r.t\nthe Hellinger distance. To the best of our knowledge, our algorithm is the\nfirst efficient and rate-optimal regret minimization algorithm for CMDPs that\noperates under the general offline function approximation setting. In addition,\nwe extend the Eluder dimension to general bounded metrics which may be of\nseparate interest.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.02001,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language\n  Model\n\n  Progress in machine learning (ML) comes with a cost to the environment, given\nthat training ML models requires significant computational resources, energy\nand materials. In the present article, we aim to quantify the carbon footprint\nof BLOOM, a 176-billion parameter language model, across its life cycle. We\nestimate that BLOOM's final training emitted approximately 24.7 tonnes\nof~\\carboneq~if we consider only the dynamic power consumption, and 50.5 tonnes\nif we account for all processes ranging from equipment manufacturing to\nenergy-based operational consumption. We also study the energy requirements and\ncarbon emissions of its deployment for inference via an API endpoint receiving\nuser queries in real-time. We conclude with a discussion regarding the\ndifficulty of precisely estimating the carbon footprint of ML models and future\nresearch directions that can contribute towards improving carbon emissions\nreporting.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.00124,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"One Risk to Rule Them All: A Risk-Sensitive Perspective on Model-Based\n  Offline Reinforcement Learning\n\n  Offline reinforcement learning (RL) is suitable for safety-critical domains\nwhere online exploration is too costly or dangerous. In such safety-critical\nsettings, decision-making should take into consideration the risk of\ncatastrophic outcomes. In other words, decision-making should be\nrisk-sensitive. Previous works on risk in offline RL combine together offline\nRL techniques, to avoid distributional shift, with risk-sensitive RL\nalgorithms, to achieve risk-sensitivity. In this work, we propose\nrisk-sensitivity as a mechanism to jointly address both of these issues. Our\nmodel-based approach is risk-averse to both epistemic and aleatoric\nuncertainty. Risk-aversion to epistemic uncertainty prevents distributional\nshift, as areas not covered by the dataset have high epistemic uncertainty.\nRisk-aversion to aleatoric uncertainty discourages actions that may result in\npoor outcomes due to environment stochasticity. Our experiments show that our\nalgorithm achieves competitive performance on deterministic benchmarks, and\noutperforms existing approaches for risk-sensitive objectives in stochastic\ndomains.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.03032,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Decentralized Policy Optimization\n\n  The study of decentralized learning or independent learning in cooperative\nmulti-agent reinforcement learning has a history of decades. Recently empirical\nstudies show that independent PPO (IPPO) can obtain good performance, close to\nor even better than the methods of centralized training with decentralized\nexecution, in several benchmarks. However, decentralized actor-critic with\nconvergence guarantee is still open. In this paper, we propose\n\\textit{decentralized policy optimization} (DPO), a decentralized actor-critic\nalgorithm with monotonic improvement and convergence guarantee. We derive a\nnovel decentralized surrogate for policy optimization such that the monotonic\nimprovement of joint policy can be guaranteed by each agent\n\\textit{independently} optimizing the surrogate. In practice, this\ndecentralized surrogate can be realized by two adaptive coefficients for policy\noptimization at each agent. Empirically, we compare DPO with IPPO in a variety\nof cooperative multi-agent tasks, covering discrete and continuous action\nspaces, and fully and partially observable environments. The results show DPO\noutperforms IPPO in most tasks, which can be the evidence for our theoretical\nresults.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.084,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000076824,
      "text":"Air Pollution Hotspot Detection and Source Feature Analysis using\n  Cross-domain Urban Data\n\n  Air pollution is a major global environmental health threat, in particular\nfor people who live or work near pollution sources. Areas adjacent to pollution\nsources often have high ambient pollution concentrations, and those areas are\ncommonly referred to as air pollution hotspots. Detecting and characterizing\npollution hotspots are of great importance for air quality management, but are\nchallenging due to the high spatial and temporal variability of air pollutants.\nIn this work, we explore the use of mobile sensing data (i.e., air quality\nsensors installed on vehicles) to detect pollution hotspots. One major\nchallenge with mobile sensing data is uneven sampling, i.e., data collection\ncan vary by both space and time. To address this challenge, we propose a\ntwo-step approach to detect hotspots from mobile sensing data, which includes\nlocal spike detection and sample-weighted clustering. Essentially, this\napproach tackles the uneven sampling issue by weighting samples based on their\nspatial frequency and temporal hit rate, so as to identify robust and\npersistent hotspots. To contextualize the hotspots and discover potential\npollution source characteristics, we explore a variety of cross-domain urban\ndata and extract features from them. As a soft-validation of the extracted\nfeatures, we build hotspot inference models for cities with and without mobile\nsensing data. Evaluation results using real-world mobile sensing air quality\ndata as well as cross-domain urban data demonstrate the effectiveness of our\napproach in detecting and inferring pollution hotspots. Furthermore, the\nempirical analysis of hotspots and source features yields useful insights\nregarding neighborhood pollution sources.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.1477,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000027153,
      "text":"ReGrAt: Regularization in Graphs using Attention to handle class\n  imbalance\n\n  Node classification is an important task to solve in graph-based learning.\nEven though a lot of work has been done in this field, imbalance is neglected.\nReal-world data is not perfect, and is imbalanced in representations most of\nthe times. Apart from text and images, data can be represented using graphs,\nand thus addressing the imbalance in graphs has become of paramount importance.\nIn the context of node classification, one class has less examples than others.\nChanging data composition is a popular way to address the imbalance in node\nclassification. This is done by resampling the data to balance the dataset.\nHowever, that can sometimes lead to loss of information or add noise to the\ndataset. Therefore, in this work, we implicitly solve the problem by changing\nthe model loss. Specifically, we study how attention networks can help tackle\nimbalance. Moreover, we observe that using a regularizer to assign larger\nweights to minority nodes helps to mitigate this imbalance. We achieve State of\nthe Art results than the existing methods on several standard citation\nbenchmark datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.06605,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000260605,
      "text":"Comprehensive Analysis of Over-smoothing in Graph Neural Networks from\n  Markov Chains Perspective\n\n  The over-smoothing problem is an obstacle of developing deep graph neural\nnetwork (GNN). Although many approaches to improve the over-smoothing problem\nhave been proposed, there is still a lack of comprehensive understanding and\nconclusion of this problem. In this work, we analyze the over-smoothing problem\nfrom the Markov chain perspective. We focus on message passing of GNN and first\nestablish a connection between GNNs and Markov chains on the graph. GNNs are\ndivided into two classes of operator-consistent and operator-inconsistent based\non whether the corresponding Markov chains are time-homogeneous. Next we\nattribute the over-smoothing problem to the convergence of an arbitrary initial\ndistribution to a stationary distribution. Based on this, we prove that\nalthough the previously proposed methods can alleviate over-smoothing, but\nthese methods cannot avoid the over-smoothing problem. In addition, we give the\nconclusion of the over-smoothing problem in two types of GNNs in the Markovian\nsense. On the one hand, operator-consistent GNN cannot avoid over-smoothing at\nan exponential rate. On the other hand, operator-inconsistent GNN is not always\nover-smoothing. Further, we investigate the existence of the limiting\ndistribution of the time-inhomogeneous Markov chain, from which we derive a\nsufficient condition for operator-inconsistent GNN to avoid over-smoothing.\nFinally, we design experiments to verify our findings. Results show that our\nproposed sufficient condition can effectively improve over-smoothing problem in\noperator-inconsistent GNN and enhance the performance of the model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.11656,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"SIFU: Sequential Informed Federated Unlearning for Efficient and\n  Provable Client Unlearning in Federated Optimization\n\n  Machine Unlearning (MU) is an increasingly important topic in machine\nlearning safety, aiming at removing the contribution of a given data point from\na training procedure. Federated Unlearning (FU) consists in extending MU to\nunlearn a given client's contribution from a federated training routine. While\nseveral FU methods have been proposed, we currently lack a general approach\nproviding formal unlearning guarantees to the FedAvg routine, while ensuring\nscalability and generalization beyond the convex assumption on the clients'\nloss functions. We aim at filling this gap by proposing SIFU (Sequential\nInformed Federated Unlearning), a new FU method applying to both convex and\nnon-convex optimization regimes. SIFU naturally applies to FedAvg without\nadditional computational cost for the clients and provides formal guarantees on\nthe quality of the unlearning task. We provide a theoretical analysis of the\nunlearning properties of SIFU, and practically demonstrate its effectiveness as\ncompared to a panel of unlearning methods from the state-of-the-art.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.04284,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000112255,
      "text":"Efficient Compressed Ratio Estimation Using Online Sequential Learning\n  for Edge Computing\n\n  Owing to the widespread adoption of the Internet of Things, a vast amount of\nsensor information is being acquired in real time. Accordingly, the\ncommunication cost of data from edge devices is increasing. Compressed sensing\n(CS), a data compression method that can be used on edge devices, has been\nattracting attention as a method to reduce communication costs. In CS,\nestimating the appropriate compression ratio is important. There is a method to\nadaptively estimate the compression ratio for the acquired data using\nreinforcement learning (RL). However, the computational costs associated with\nexisting RL methods that can be utilized on edges are often high. In this\nstudy, we developed an efficient RL method for edge devices, referred to as the\nactor--critic online sequential extreme learning machine (AC-OSELM), and a\nsystem to compress data by estimating an appropriate compression ratio on the\nedge using AC-OSELM. The performance of the proposed method in estimating the\ncompression ratio is evaluated by comparing it with other RL methods for edge\ndevices. The experimental results indicate that AC-OSELM demonstrated the same\nor better compression performance and faster compression ratio estimation than\nthe existing methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.01455,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"PI is back! Switching Acquisition Functions in Bayesian Optimization\n\n  Bayesian Optimization (BO) is a powerful, sample-efficient technique to\noptimize expensive-to-evaluate functions. Each of the BO components, such as\nthe surrogate model, the acquisition function (AF), or the initial design, is\nsubject to a wide range of design choices. Selecting the right components for a\ngiven optimization task is a challenging task, which can have significant\nimpact on the quality of the obtained results. In this work, we initiate the\nanalysis of which AF to favor for which optimization scenarios. To this end, we\nbenchmark SMAC3 using Expected Improvement (EI) and Probability of Improvement\n(PI) as acquisition functions on the 24 BBOB functions of the COCO environment.\nWe compare their results with those of schedules switching between AFs. One\nschedule aims to use EI's explorative behavior in the early optimization steps,\nand then switches to PI for a better exploitation in the final steps. We also\ncompare this to a random schedule and round-robin selection of EI and PI. We\nobserve that dynamic schedules oftentimes outperform any single static one. Our\nresults suggest that a schedule that allocates the first 25 % of the\noptimization budget to EI and the last 75 % to PI is a reliable default.\nHowever, we also observe considerable performance differences for the 24\nfunctions, suggesting that a per-instance allocation, possibly learned on the\nfly, could offer significant improvement over the state-of-the-art BO designs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.09001,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Multi-Timescale Modeling of Human Behavior\n\n  In recent years, the role of artificially intelligent (AI) agents has evolved\nfrom being basic tools to socially intelligent agents working alongside humans\ntowards common goals. In such scenarios, the ability to predict future behavior\nby observing past actions of their human teammates is highly desirable in an AI\nagent. Goal-oriented human behavior is complex, hierarchical, and unfolds\nacross multiple timescales. Despite this observation, relatively little\nattention has been paid towards using multi-timescale features to model such\nbehavior. In this paper, we propose an LSTM network architecture that processes\nbehavioral information at multiple timescales to predict future behavior. We\ndemonstrate that our approach for modeling behavior in multiple timescales\nsubstantially improves prediction of future behavior compared to methods that\ndo not model behavior at multiple timescales. We evaluate our architecture on\ndata collected in an urban search and rescue scenario simulated in a virtual\nMinecraft-based testbed, and compare its performance to that of a number of\nvalid baselines as well as other methods that do not process inputs at multiple\ntimescales.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.07089,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000045697,
      "text":"PMR: Prototypical Modal Rebalance for Multimodal Learning\n\n  Multimodal learning (MML) aims to jointly exploit the common priors of\ndifferent modalities to compensate for their inherent limitations. However,\nexisting MML methods often optimize a uniform objective for different\nmodalities, leading to the notorious \"modality imbalance\" problem and\ncounterproductive MML performance. To address the problem, some existing\nmethods modulate the learning pace based on the fused modality, which is\ndominated by the better modality and eventually results in a limited\nimprovement on the worse modal. To better exploit the features of multimodal,\nwe propose Prototypical Modality Rebalance (PMR) to perform stimulation on the\nparticular slow-learning modality without interference from other modalities.\nSpecifically, we introduce the prototypes that represent general features for\neach class, to build the non-parametric classifiers for uni-modal performance\nevaluation. Then, we try to accelerate the slow-learning modality by enhancing\nits clustering toward prototypes. Furthermore, to alleviate the suppression\nfrom the dominant modality, we introduce a prototype-based entropy\nregularization term during the early training stage to prevent premature\nconvergence. Besides, our method only relies on the representations of each\nmodality and without restrictions from model structures and fusion methods,\nmaking it with great application potential for various scenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.17264,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000042054,
      "text":"Interpretability with full complexity by constraining feature\n  information\n\n  Interpretability is a pressing issue for machine learning. Common approaches\nto interpretable machine learning constrain interactions between features of\nthe input, rendering the effects of those features on a model's output\ncomprehensible but at the expense of model complexity. We approach\ninterpretability from a new angle: constrain the information about the features\nwithout restricting the complexity of the model. Borrowing from information\ntheory, we use the Distributed Information Bottleneck to find optimal\ncompressions of each feature that maximally preserve information about the\noutput. The learned information allocation, by feature and by feature value,\nprovides rich opportunities for interpretation, particularly in problems with\nmany features and complex feature interactions. The central object of analysis\nis not a single trained model, but rather a spectrum of models serving as\napproximations that leverage variable amounts of information about the inputs.\nInformation is allocated to features by their relevance to the output, thereby\nsolving the problem of feature selection by constructing a learned continuum of\nfeature inclusion-to-exclusion. The optimal compression of each feature -- at\nevery stage of approximation -- allows fine-grained inspection of the\ndistinctions among feature values that are most impactful for prediction. We\ndevelop a framework for extracting insight from the spectrum of approximate\nmodels and demonstrate its utility on a range of tabular datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.08177,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Premonition Net, A Multi-Timeline Transformer Network Architecture\n  Towards Strawberry Tabletop Yield Forecasting\n\n  Yield forecasting is a critical first step necessary for yield optimisation,\nwith important consequences for the broader food supply chain, procurement,\nprice-negotiation, logistics, and supply. However yield forecasting is\nnotoriously difficult, and oft-inaccurate. Premonition Net is a multi-timeline,\ntime sequence ingesting approach towards processing the past, the present, and\npremonitions of the future. We show how this structure combined with\ntransformers attains critical yield forecasting proficiency towards improving\nfood security, lowering prices, and reducing waste. We find data availability\nto be a continued difficulty however using our premonition network and our own\ncollected data we attain yield forecasts 3 weeks ahead with a a testing set\nRMSE loss of ~0.08 across our latest season.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.06525,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Actionable Recourse via GANs for Mobile Health\n\n  Mobile health apps provide a unique means of collecting data that can be used\nto deliver adaptive interventions.The predicted outcomes considerably influence\nthe selection of such interventions. Recourse via counterfactuals provides\ntangible mechanisms to modify user predictions. By identifying plausible\nactions that increase the likelihood of a desired prediction, stakeholders are\nafforded agency over their predictions. Furthermore, recourse mechanisms enable\ncounterfactual reasoning that can help provide insights into candidates for\ncausal interventional features. We demonstrate the feasibility of GAN-generated\nrecourse for mobile health applications on ensemble-survival-analysis-based\nprediction of medium-term engagement in the Safe Delivery App, a digital\ntraining tool for skilled birth attendants.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.00572,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000216232,
      "text":"Position-Aware Subgraph Neural Networks with Data-Efficient Learning\n\n  Data-efficient learning on graphs (GEL) is essential in real-world\napplications. Existing GEL methods focus on learning useful representations for\nnodes, edges, or entire graphs with ``small'' labeled data. But the problem of\ndata-efficient learning for subgraph prediction has not been explored. The\nchallenges of this problem lie in the following aspects: 1) It is crucial for\nsubgraphs to learn positional features to acquire structural information in the\nbase graph in which they exist. Although the existing subgraph neural network\nmethod is capable of learning disentangled position encodings, the overall\ncomputational complexity is very high. 2) Prevailing graph augmentation methods\nfor GEL, including rule-based, sample-based, adaptive, and automated methods,\nare not suitable for augmenting subgraphs because a subgraph contains fewer\nnodes but richer information such as position, neighbor, and structure.\nSubgraph augmentation is more susceptible to undesirable perturbations. 3) Only\na small number of nodes in the base graph are contained in subgraphs, which\nleads to a potential ``bias'' problem that the subgraph representation learning\nis dominated by these ``hot'' nodes. By contrast, the remaining nodes fail to\nbe fully learned, which reduces the generalization ability of subgraph\nrepresentation learning. In this paper, we aim to address the challenges above\nand propose a Position-Aware Data-Efficient Learning framework for subgraph\nneural networks called PADEL. Specifically, we propose a novel node position\nencoding method that is anchor-free, and design a new generative subgraph\naugmentation method based on a diffused variational subgraph autoencoder, and\nwe propose exploratory and exploitable views for subgraph contrastive learning.\nExtensive experiment results on three real-world datasets show the superiority\nof our proposed method over state-of-the-art baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.05456,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000822544,
      "text":"Review of Methods for Handling Class-Imbalanced in Classification\n  Problems\n\n  Learning classifiers using skewed or imbalanced datasets can occasionally\nlead to classification issues; this is a serious issue. In some cases, one\nclass contains the majority of examples while the other, which is frequently\nthe more important class, is nevertheless represented by a smaller proportion\nof examples. Using this kind of data could make many carefully designed\nmachine-learning systems ineffective. High training fidelity was a term used to\ndescribe biases vs. all other instances of the class. The best approach to all\npossible remedies to this issue is typically to gain from the minority class.\nThe article examines the most widely used methods for addressing the problem of\nlearning with a class imbalance, including data-level, algorithm-level, hybrid,\ncost-sensitive learning, and deep learning, etc. including their advantages and\nlimitations. The efficiency and performance of the classifier are assessed\nusing a myriad of evaluation metrics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.04411,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000053975,
      "text":"Motif-guided Time Series Counterfactual Explanations\n\n  With the rising need of interpretable machine learning methods, there is a\nnecessity for a rise in human effort to provide diverse explanations of the\ninfluencing factors of the model decisions. To improve the trust and\ntransparency of AI-based systems, the EXplainable Artificial Intelligence (XAI)\nfield has emerged. The XAI paradigm is bifurcated into two main categories:\nfeature attribution and counterfactual explanation methods. While feature\nattribution methods are based on explaining the reason behind a model decision,\ncounterfactual explanation methods discover the smallest input changes that\nwill result in a different decision. In this paper, we aim at building trust\nand transparency in time series models by using motifs to generate\ncounterfactual explanations. We propose Motif-Guided Counterfactual Explanation\n(MG-CF), a novel model that generates intuitive post-hoc counterfactual\nexplanations that make full use of important motifs to provide interpretive\ninformation in decision-making processes. To the best of our knowledge, this is\nthe first effort that leverages motifs to guide the counterfactual explanation\ngeneration. We validated our model using five real-world time-series datasets\nfrom the UCR repository. Our experimental results show the superiority of MG-CF\nin balancing all the desirable counterfactual explanations properties in\ncomparison with other competing state-of-the-art baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.07814,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"Extending the Neural Additive Model for Survival Analysis with EHR Data\n\n  With increasing interest in applying machine learning to develop healthcare\nsolutions, there is a desire to create interpretable deep learning models for\nsurvival analysis. In this paper, we extend the Neural Additive Model (NAM) by\nincorporating pairwise feature interaction networks and equip these models with\nloss functions that fit both proportional and non-proportional extensions of\nthe Cox model. We show that within this extended framework, we can construct\nnon-proportional hazard models, which we call TimeNAM, that significantly\nimprove performance over the standard NAM model architecture on benchmark\nsurvival datasets. We apply these model architectures to data from the\nElectronic Health Record (EHR) database of Seoul National University Hospital\nGangnam Center (SNUHGC) to build an interpretable neural network survival model\nfor gastric cancer prediction. We demonstrate that on both benchmark survival\nanalysis datasets, as well as on our gastric cancer dataset, our model\narchitectures yield performance that matches, or surpasses, the current\nstate-of-the-art black-box methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.04548,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"STLGRU: Spatio-Temporal Lightweight Graph GRU for Traffic Flow\n  Prediction\n\n  Reliable forecasting of traffic flow requires efficient modeling of traffic\ndata. Indeed, different correlations and influences arise in a dynamic traffic\nnetwork, making modeling a complicated task. Existing literature has proposed\nmany different methods to capture traffic networks' complex underlying\nspatial-temporal relations. However, given the heterogeneity of traffic data,\nconsistently capturing both spatial and temporal dependencies presents a\nsignificant challenge. Also, as more and more sophisticated methods are being\nproposed, models are increasingly becoming memory-heavy and, thus, unsuitable\nfor low-powered devices. To this end, we propose Spatio-Temporal Lightweight\nGraph GRU, namely STLGRU, a novel traffic forecasting model for predicting\ntraffic flow accurately. Specifically, our proposed STLGRU can effectively\ncapture dynamic local and global spatial-temporal relations of traffic networks\nusing memory-augmented attention and gating mechanisms in a continuously\nsynchronized manner. Moreover, instead of employing separate temporal and\nspatial components, we show that our memory module and gated unit can\nsuccessfully learn the spatial-temporal dependencies with reduced memory usage\nand fewer parameters. Extensive experimental results on three real-world public\ntraffic datasets demonstrate that our method can not only achieve\nstate-of-the-art performance but also exhibit competitive computational\nefficiency. Our code is available at https:\/\/github.com\/Kishor-Bhaumik\/STLGRU\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.01174,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000032451,
      "text":"Utilizing Prior Solutions for Reward Shaping and Composition in\n  Entropy-Regularized Reinforcement Learning\n\n  In reinforcement learning (RL), the ability to utilize prior knowledge from\npreviously solved tasks can allow agents to quickly solve new problems. In some\ncases, these new problems may be approximately solved by composing the\nsolutions of previously solved primitive tasks (task composition). Otherwise,\nprior knowledge can be used to adjust the reward function for a new problem, in\na way that leaves the optimal policy unchanged but enables quicker learning\n(reward shaping). In this work, we develop a general framework for reward\nshaping and task composition in entropy-regularized RL. To do so, we derive an\nexact relation connecting the optimal soft value functions for two\nentropy-regularized RL problems with different reward functions and dynamics.\nWe show how the derived relation leads to a general result for reward shaping\nin entropy-regularized RL. We then generalize this approach to derive an exact\nrelation connecting optimal value functions for the composition of multiple\ntasks in entropy-regularized RL. We validate these theoretical contributions\nwith experiments showing that reward shaping and task composition lead to\nfaster learning in various settings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.05908,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Instance-Conditional Timescales of Decay for Non-Stationary Learning\n\n  Slow concept drift is a ubiquitous, yet under-studied problem in practical\nmachine learning systems. In such settings, although recent data is more\nindicative of future data, naively prioritizing recent instances runs the risk\nof losing valuable information from the past. We propose an optimization-driven\napproach towards balancing instance importance over large training windows.\nFirst, we model instance relevance using a mixture of multiple timescales of\ndecay, allowing us to capture rich temporal trends. Second, we learn an\nauxiliary scorer model that recovers the appropriate mixture of timescales as a\nfunction of the instance itself. Finally, we propose a nested optimization\nobjective for learning the scorer, by which it maximizes forward transfer for\nthe learned model. Experiments on a large real-world dataset of 39M photos over\na 9 year period show upto 15% relative gains in accuracy compared to other\nrobust learning baselines. We replicate our gains on two collections of\nreal-world datasets for non-stationary learning, and extend our work to\ncontinual learning settings where, too, we beat SOTA methods by large margins.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.05707,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Human Mobility Modeling During the COVID-19 Pandemic via Deep Graph\n  Diffusion Infomax\n\n  Non-Pharmaceutical Interventions (NPIs), such as social gathering\nrestrictions, have shown effectiveness to slow the transmission of COVID-19 by\nreducing the contact of people. To support policy-makers, multiple studies have\nfirst modeled human mobility via macro indicators (e.g., average daily travel\ndistance) and then studied the effectiveness of NPIs. In this work, we focus on\nmobility modeling and, from a micro perspective, aim to predict locations that\nwill be visited by COVID-19 cases. Since NPIs generally cause economic and\nsocietal loss, such a micro perspective prediction benefits governments when\nthey design and evaluate them. However, in real-world situations, strict\nprivacy data protection regulations result in severe data sparsity problems\n(i.e., limited case and location information). To address these challenges, we\nformulate the micro perspective mobility modeling into computing the relevance\nscore between a diffusion and a location, conditional on a geometric graph. we\npropose a model named Deep Graph Diffusion Infomax (DGDI), which jointly models\nvariables including a geometric graph, a set of diffusions and a set of\nlocations.To facilitate the research of COVID-19 prediction, we present two\nbenchmarks that contain geometric graphs and location histories of COVID-19\ncases. Extensive experiments on the two benchmarks show that DGDI significantly\noutperforms other competing methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.03853,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000073844,
      "text":"Clustering with Neural Network and Index\n\n  A new model called Clustering with Neural Network and Index (CNNI) is\nintroduced. CNNI uses a Neural Network to cluster data points. Training of the\nNeural Network mimics supervised learning, with an internal clustering\nevaluation index acting as the loss function. An experiment is conducted to\ntest the feasibility of the new model, and compared with results of other\nclustering models like K-means and Gaussian Mixture Model (GMM). The result\nshows CNNI can work properly for clustering data; CNNI equipped with MMJ-SC,\nachieves the first parametric (inductive) clustering model that can deal with\nnon-convex shaped (non-flat geometry) data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.01348,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Predict-and-Critic: Accelerated End-to-End Predictive Control for Cloud\n  Computing through Reinforcement Learning\n\n  Cloud computing holds the promise of reduced costs through economies of\nscale. To realize this promise, cloud computing vendors typically solve\nsequential resource allocation problems, where customer workloads are packed on\nshared hardware. Virtual machines (VM) form the foundation of modern cloud\ncomputing as they help logically abstract user compute from shared physical\ninfrastructure. Traditionally, VM packing problems are solved by predicting\ndemand, followed by a Model Predictive Control (MPC) optimization over a future\nhorizon. We introduce an approximate formulation of an industrial VM packing\nproblem as an MILP with soft-constraints parameterized by the predictions.\nRecently, predict-and-optimize (PnO) was proposed for end-to-end training of\nprediction models by back-propagating the cost of decisions through the\noptimization problem. But, PnO is unable to scale to the large prediction\nhorizons prevalent in cloud computing. To tackle this issue, we propose the\nPredict-and-Critic (PnC) framework that outperforms PnO with just a two-step\nhorizon by leveraging reinforcement learning. PnC jointly trains a prediction\nmodel and a terminal Q function that approximates cost-to-go over a long\nhorizon, by back-propagating the cost of decisions through the optimization\nproblem \\emph{and from the future}. The terminal Q function allows us to solve\na much smaller two-step horizon optimization problem than the multi-step\nhorizon necessary in PnO. We evaluate PnO and the PnC framework on two\ndatasets, three workloads, and with disturbances not modeled in the\noptimization problem. We find that PnC significantly improves decision quality\nover PnO, even when the optimization problem is not a perfect representation of\nreality. We also find that hardening the soft constraints of the MILP and\nback-propagating through the constraints improves decision quality for both PnO\nand PnC.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.04966,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000110931,
      "text":"Towards High-Order Complementary Recommendation via Logical Reasoning\n  Network\n\n  Complementary recommendation gains increasing attention in e-commerce since\nit expedites the process of finding frequently-bought-with products for users\nin their shopping journey. Therefore, learning the product representation that\ncan reflect this complementary relationship plays a central role in modern\nrecommender systems. In this work, we propose a logical reasoning network,\nLOGIREC, to effectively learn embeddings of products as well as various\ntransformations (projection, intersection, negation) between them. LOGIREC is\ncapable of capturing the asymmetric complementary relationship between products\nand seamlessly extending to high-order recommendations where more comprehensive\nand meaningful complementary relationship is learned for a query set of\nproducts. Finally, we further propose a hybrid network that is jointly\noptimized for learning a more generic product representation. We demonstrate\nthe effectiveness of our LOGIREC on multiple public real-world datasets in\nterms of various ranking-based metrics under both low-order and high-order\nrecommendation scenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.12121,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"Federated PCA on Grassmann Manifold for Anomaly Detection in IoT\n  Networks\n\n  In the era of Internet of Things (IoT), network-wide anomaly detection is a\ncrucial part of monitoring IoT networks due to the inherent security\nvulnerabilities of most IoT devices. Principal Components Analysis (PCA) has\nbeen proposed to separate network traffics into two disjoint subspaces\ncorresponding to normal and malicious behaviors for anomaly detection. However,\nthe privacy concerns and limitations of devices' computing resources compromise\nthe practical effectiveness of PCA. We propose a federated PCA-based\nGrassmannian optimization framework that coordinates IoT devices to aggregate a\njoint profile of normal network behaviors for anomaly detection. First, we\nintroduce a privacy-preserving federated PCA framework to simultaneously\ncapture the profile of various IoT devices' traffic. Then, we investigate the\nalternating direction method of multipliers gradient-based learning on the\nGrassmann manifold to guarantee fast training and the absence of detecting\nlatency using limited computational resources. Empirical results on the NSL-KDD\ndataset demonstrate that our method outperforms baseline approaches. Finally,\nwe show that the Grassmann manifold algorithm is highly adapted for IoT anomaly\ndetection, which permits drastically reducing the analysis time of the system.\nTo the best of our knowledge, this is the first federated PCA algorithm for\nanomaly detection meeting the requirements of IoT networks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.04805,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Understanding electricity prices beyond the merit order principle using\n  explainable AI\n\n  Electricity prices in liberalized markets are determined by the supply and\ndemand for electric power, which are in turn driven by various external\ninfluences that vary strongly in time. In perfect competition, the merit order\nprinciple describes that dispatchable power plants enter the market in the\norder of their marginal costs to meet the residual load, i.e. the difference of\nload and renewable generation. Many market models implement this principle to\npredict electricity prices but typically require certain assumptions and\nsimplifications. In this article, we present an explainable machine learning\nmodel for the prices on the German day-ahead market, which substantially\noutperforms a benchmark model based on the merit order principle. Our model is\ndesigned for the ex-post analysis of prices and thus builds on various external\nfeatures. Using Shapley Additive exPlanation (SHAP) values, we can disentangle\nthe role of the different features and quantify their importance from empiric\ndata. Load, wind and solar generation are most important, as expected, but wind\npower appears to affect prices stronger than solar power does. Fuel prices also\nrank highly and show nontrivial dependencies, including strong interactions\nwith other features revealed by a SHAP interaction analysis. Large generation\nramps are correlated with high prices, again with strong feature interactions,\ndue to the limited flexibility of nuclear and lignite plants. Our results\nfurther contribute to model development by providing quantitative insights\ndirectly from data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.08199,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"Asymptotic Analysis of Deep Residual Networks\n\n  We investigate the asymptotic properties of deep Residual networks (ResNets)\nas the number of layers increases. We first show the existence of scaling\nregimes for trained weights markedly different from those implicitly assumed in\nthe neural ODE literature. We study the convergence of the hidden state\ndynamics in these scaling regimes, showing that one may obtain an ODE, a\nstochastic differential equation (SDE) or neither of these. In particular, our\nfindings point to the existence of a diffusive regime in which the deep network\nlimit is described by a class of stochastic differential equations (SDEs).\nFinally, we derive the corresponding scaling limits for the backpropagation\ndynamics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.12989,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000046028,
      "text":"Improved Kernel Alignment Regret Bound for Online Kernel Learning\n\n  In this paper, we improve the kernel alignment regret bound for online kernel\nlearning in the regime of the Hinge loss function. Previous algorithm achieves\na regret of $O((\\mathcal{A}_TT\\ln{T})^{\\frac{1}{4}})$ at a computational\ncomplexity (space and per-round time) of $O(\\sqrt{\\mathcal{A}_TT\\ln{T}})$,\nwhere $\\mathcal{A}_T$ is called \\textit{kernel alignment}. We propose an\nalgorithm whose regret bound and computational complexity are better than\nprevious results. Our results depend on the decay rate of eigenvalues of the\nkernel matrix. If the eigenvalues of the kernel matrix decay exponentially,\nthen our algorithm enjoys a regret of $O(\\sqrt{\\mathcal{A}_T})$ at a\ncomputational complexity of $O(\\ln^2{T})$. Otherwise, our algorithm enjoys a\nregret of $O((\\mathcal{A}_TT)^{\\frac{1}{4}})$ at a computational complexity of\n$O(\\sqrt{\\mathcal{A}_TT})$. We extend our algorithm to batch learning and\nobtain a $O(\\frac{1}{T}\\sqrt{\\mathbb{E}[\\mathcal{A}_T]})$ excess risk bound\nwhich improves the previous $O(1\/\\sqrt{T})$ bound.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.1136,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000037418,
      "text":"Feature Acquisition using Monte Carlo Tree Search\n\n  Feature acquisition algorithms address the problem of acquiring informative\nfeatures while balancing the costs of acquisition to improve the learning\nperformances of ML models. Previous approaches have focused on calculating the\nexpected utility values of features to determine the acquisition sequences.\nOther approaches formulated the problem as a Markov Decision Process (MDP) and\napplied reinforcement learning based algorithms. In comparison to previous\napproaches, we focus on 1) formulating the feature acquisition problem as a MDP\nand applying Monte Carlo Tree Search, 2) calculating the intermediary rewards\nfor each acquisition step based on model improvements and acquisition costs and\n3) simultaneously optimizing model improvement and acquisition costs with\nmulti-objective Monte Carlo Tree Search. With Proximal Policy Optimization and\nDeep Q-Network algorithms as benchmark, we show the effectiveness of our\nproposed approach with experimental study.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.0144,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"SMARTQUERY: An Active Learning Framework for Graph Neural Networks\n  through Hybrid Uncertainty Reduction\n\n  Graph neural networks have achieved significant success in representation\nlearning. However, the performance gains come at a cost; acquiring\ncomprehensive labeled data for training can be prohibitively expensive. Active\nlearning mitigates this issue by searching the unexplored data space and\nprioritizing the selection of data to maximize model's performance gain. In\nthis paper, we propose a novel method SMARTQUERY, a framework to learn a graph\nneural network with very few labeled nodes using a hybrid uncertainty reduction\nfunction. This is achieved using two key steps: (a) design a multi-stage active\ngraph learning framework by exploiting diverse explicit graph information and\n(b) introduce label propagation to efficiently exploit known labels to assess\nthe implicit embedding information. Using a comprehensive set of experiments on\nthree network datasets, we demonstrate the competitive performance of our\nmethod against state-of-the-arts on very few labeled data (up to 5 labeled\nnodes per class).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.0997,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000023511,
      "text":"Data Augmentation on Graphs: A Technical Survey\n\n  In recent years, graph representation learning has achieved remarkable\nsuccess while suffering from low-quality data problems. As a mature technology\nto improve data quality in computer vision, data augmentation has also\nattracted increasing attention in graph domain. To advance research in this\nemerging direction, this survey provides a comprehensive review and summary of\nexisting graph data augmentation (GDAug) techniques. Specifically, this survey\nfirst provides an overview of various feasible taxonomies and categorizes\nexisting GDAug studies based on multi-scale graph elements. Subsequently, for\neach type of GDAug technique, this survey formalizes standardized technical\ndefinition, discuss the technical details, and provide schematic illustration.\nThe survey also reviews domain-specific graph data augmentation techniques,\nincluding those for heterogeneous graphs, temporal graphs, spatio-temporal\ngraphs, and hypergraphs. In addition, this survey provides a summary of\navailable evaluation metrics and design guidelines for graph data augmentation.\nLastly, it outlines the applications of GDAug at both the data and model\nlevels, discusses open issues in the field, and looks forward to future\ndirections. The latest advances in GDAug are summarized in GitHub.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.05653,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000675519,
      "text":"Spatial-temporal traffic modeling with a fusion graph reconstructed by\n  tensor decomposition\n\n  Accurate spatial-temporal traffic flow forecasting is essential for helping\ntraffic managers to take control measures and drivers to choose the optimal\ntravel routes. Recently, graph convolutional networks (GCNs) have been widely\nused in traffic flow prediction owing to their powerful ability to capture\nspatial-temporal dependencies. The design of the spatial-temporal graph\nadjacency matrix is a key to the success of GCNs, and it is still an open\nquestion. This paper proposes reconstructing the binary adjacency matrix via\ntensor decomposition, and a traffic flow forecasting method is proposed. First,\nwe reformulate the spatial-temporal fusion graph adjacency matrix into a\nthree-way adjacency tensor. Then, we reconstructed the adjacency tensor via\nTucker decomposition, wherein more informative and global spatial-temporal\ndependencies are encoded. Finally, a Spatial-temporal Synchronous Graph\nConvolutional module for localized spatial-temporal correlations learning and a\nDilated Convolution module for global correlations learning are assembled to\naggregate and learn the comprehensive spatial-temporal dependencies of the road\nnetwork. Experimental results on four open-access datasets demonstrate that the\nproposed model outperforms state-of-the-art approaches in terms of the\nprediction performance and computational cost.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.13634,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000002318,
      "text":"On the Equivalence of the Weighted Tsetlin Machine and the Perceptron\n\n  Tsetlin Machine (TM) has been gaining popularity as an inherently\ninterpretable machine leaning method that is able to achieve promising\nperformance with low computational complexity on a variety of applications. The\ninterpretability and the low computational complexity of the TM are inherited\nfrom the Boolean expressions for representing various sub-patterns. Although\npossessing favorable properties, TM has not been the go-to method for AI\napplications, mainly due to its conceptual and theoretical differences compared\nwith perceptrons and neural networks, which are more widely known and well\nunderstood. In this paper, we provide detailed insights for the operational\nconcept of the TM, and try to bridge the gap in the theoretical understanding\nbetween the perceptron and the TM. More specifically, we study the operational\nconcept of the TM following the analytical structure of perceptrons, showing\nthe resemblance between the perceptrons and the TM. Through the analysis, we\nindicated that the TM's weight update can be considered as a special case of\nthe gradient weight update. We also perform an empirical analysis of TM by\nshowing the flexibility in determining the clause length, visualization of\ndecision boundaries and obtaining interpretable boolean expressions from TM. In\naddition, we also discuss the advantages of TM in terms of its structure and\nits ability to solve more complex problems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.08174,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000035101,
      "text":"Non-IID Transfer Learning on Graphs\n\n  Transfer learning refers to the transfer of knowledge or information from a\nrelevant source domain to a target domain. However, most existing transfer\nlearning theories and algorithms focus on IID tasks, where the source\/target\nsamples are assumed to be independent and identically distributed. Very little\neffort is devoted to theoretically studying the knowledge transferability on\nnon-IID tasks, e.g., cross-network mining. To bridge the gap, in this paper, we\npropose rigorous generalization bounds and algorithms for cross-network\ntransfer learning from a source graph to a target graph. The crucial idea is to\ncharacterize the cross-network knowledge transferability from the perspective\nof the Weisfeiler-Lehman graph isomorphism test. To this end, we propose a\nnovel Graph Subtree Discrepancy to measure the graph distribution shift between\nsource and target graphs. Then the generalization error bounds on cross-network\ntransfer learning, including both cross-network node classification and link\nprediction tasks, can be derived in terms of the source knowledge and the Graph\nSubtree Discrepancy across domains. This thereby motivates us to propose a\ngeneric graph adaptive network (GRADE) to minimize the distribution shift\nbetween source and target graphs for cross-network transfer learning.\nExperimental results verify the effectiveness and efficiency of our GRADE\nframework on both cross-network node classification and cross-domain\nrecommendation tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.00852,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Symphony in the Latent Space: Provably Integrating High-dimensional\n  Techniques with Non-linear Machine Learning Models\n\n  This paper revisits building machine learning algorithms that involve\ninteractions between entities, such as those between financial assets in an\nactively managed portfolio, or interactions between users in a social network.\nOur goal is to forecast the future evolution of ensembles of multivariate time\nseries in such applications (e.g., the future return of a financial asset or\nthe future popularity of a Twitter account). Designing ML algorithms for such\nsystems requires addressing the challenges of high-dimensional interactions and\nnon-linearity. Existing approaches usually adopt an ad-hoc approach to\nintegrating high-dimensional techniques into non-linear models and recent\nstudies have shown these approaches have questionable efficacy in time-evolving\ninteracting systems.\n  To this end, we propose a novel framework, which we dub as the additive\ninfluence model. Under our modeling assumption, we show that it is possible to\ndecouple the learning of high-dimensional interactions from the learning of\nnon-linear feature interactions. To learn the high-dimensional interactions, we\nleverage kernel-based techniques, with provable guarantees, to embed the\nentities in a low-dimensional latent space. To learn the non-linear\nfeature-response interactions, we generalize prominent machine learning\ntechniques, including designing a new statistically sound non-parametric method\nand an ensemble learning algorithm optimized for vector regressions. Extensive\nexperiments on two common applications demonstrate that our new algorithms\ndeliver significantly stronger forecasting power compared to standard and\nrecently proposed methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.06965,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000041723,
      "text":"Error-Aware B-PINNs: Improving Uncertainty Quantification in Bayesian\n  Physics-Informed Neural Networks\n\n  Physics-Informed Neural Networks (PINNs) are gaining popularity as a method\nfor solving differential equations. While being more feasible in some contexts\nthan the classical numerical techniques, PINNs still lack credibility. A remedy\nfor that can be found in Uncertainty Quantification (UQ) which is just\nbeginning to emerge in the context of PINNs. Assessing how well the trained\nPINN complies with imposed differential equation is the key to tackling\nuncertainty, yet there is lack of comprehensive methodology for this task. We\npropose a framework for UQ in Bayesian PINNs (B-PINNs) that incorporates the\ndiscrepancy between the B-PINN solution and the unknown true solution. We\nexploit recent results on error bounds for PINNs on linear dynamical systems\nand demonstrate the predictive uncertainty on a class of linear ODEs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.05211,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000047684,
      "text":"OpenD: A Benchmark for Language-Driven Door and Drawer Opening\n\n  We introduce OPEND, a benchmark for learning how to use a hand to open\ncabinet doors or drawers in a photo-realistic and physics-reliable simulation\nenvironment driven by language instruction. To solve the task, we propose a\nmulti-step planner composed of a deep neural network and rule-base controllers.\nThe network is utilized to capture spatial relationships from images and\nunderstand semantic meaning from language instructions. Controllers efficiently\nexecute the plan based on the spatial and semantic understanding. We evaluate\nour system by measuring its zero-shot performance in test data set.\nExperimental results demonstrate the effectiveness of decision planning by our\nmulti-step planner for different hands, while suggesting that there is\nsignificant room for developing better models to address the challenge brought\nby language understanding, spatial reasoning, and long-term manipulation. We\nwill release OPEND and host challenges to promote future research in this area.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  }
]