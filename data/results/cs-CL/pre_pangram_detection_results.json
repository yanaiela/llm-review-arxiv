[
  {
    "arxiv_id":2001.11453,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Parameter Space Factorization for Zero-Shot Learning across Tasks and\n  Languages\n\n  Most combinations of NLP tasks and language varieties lack in-domain examples\nfor supervised training because of the paucity of annotated data. How can\nneural models make sample-efficient generalizations from task-language\ncombinations with available data to low-resource ones? In this work, we propose\na Bayesian generative model for the space of neural parameters. We assume that\nthis space can be factorized into latent variables for each language and each\ntask. We infer the posteriors over such latent variables based on data from\nseen task-language combinations through variational inference. This enables\nzero-shot classification on unseen combinations at prediction time. For\ninstance, given training data for named entity recognition (NER) in Vietnamese\nand for part-of-speech (POS) tagging in Wolof, our model can perform accurate\npredictions for NER in Wolof. In particular, we experiment with a typologically\ndiverse sample of 33 languages from 4 continents and 11 families, and show that\nour model yields comparable or better results than state-of-the-art, zero-shot\ncross-lingual transfer methods. Moreover, we demonstrate that approximate\nBayesian model averaging results in smoother predictive distributions, whose\nentropy inversely correlates with accuracy. Hence, the proposed framework also\noffers robust estimates of prediction uncertainty. Our code is located at\ngithub.com\/cambridgeltl\/parameter-factorization\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.00735,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000189741,
      "text":"Self-attention-based BiGRU and capsule network for named entity\n  recognition\n\n  Named entity recognition(NER) is one of the tasks of natural language\nprocessing(NLP). In view of the problem that the traditional character\nrepresentation ability is weak and the neural network method is unable to\ncapture the important sequence information. An self-attention-based\nbidirectional gated recurrent unit(BiGRU) and capsule network(CapsNet) for NER\nis proposed. This model generates character vectors through bidirectional\nencoder representation of transformers(BERT) pre-trained model. BiGRU is used\nto capture sequence context features, and self-attention mechanism is proposed\nto give different focus on the information captured by hidden layer of BiGRU.\nFinally, we propose to use CapsNet for entity recognition. We evaluated the\nrecognition performance of the model on two datasets. Experimental results show\nthat the model has better performance without relying on external dictionary\ninformation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.05286,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Exploring and Improving Robustness of Multi Task Deep Neural Networks\n  via Domain Agnostic Defenses\n\n  In this paper, we explore the robustness of the Multi-Task Deep Neural\nNetworks (MT-DNN) against non-targeted adversarial attacks across Natural\nLanguage Understanding (NLU) tasks as well as some possible ways to defend\nagainst them. Liu et al., have shown that the Multi-Task Deep Neural Network,\ndue to the regularization effect produced when training as a result of its\ncross task data, is more robust than a vanilla BERT model trained only on one\ntask (1.1%-1.5% absolute difference). We further show that although the MT-DNN\nhas generalized better, making it easily transferable across domains and tasks,\nit can still be compromised as after only 2 attacks (1-character and\n2-character) the accuracy drops by 42.05% and 32.24% for the SNLI and SciTail\ntasks. Finally, we propose a domain agnostic defense which restores the model's\naccuracy (36.75% and 25.94% respectively) as opposed to a general-purpose\ndefense or an off-the-shelf spell checker.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.08728,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000064572,
      "text":"Coordinated Reasoning for Cross-Lingual Knowledge Graph Alignment\n\n  Existing entity alignment methods mainly vary on the choices of encoding the\nknowledge graph, but they typically use the same decoding method, which\nindependently chooses the local optimal match for each source entity. This\ndecoding method may not only cause the \"many-to-one\" problem but also neglect\nthe coordinated nature of this task, that is, each alignment decision may\nhighly correlate to the other decisions. In this paper, we introduce two\ncoordinated reasoning methods, i.e., the Easy-to-Hard decoding strategy and\njoint entity alignment algorithm. Specifically, the Easy-to-Hard strategy first\nretrieves the model-confident alignments from the predicted results and then\nincorporates them as additional knowledge to resolve the remaining\nmodel-uncertain alignments. To achieve this, we further propose an enhanced\nalignment model that is built on the current state-of-the-art baseline. In\naddition, to address the many-to-one problem, we propose to jointly predict\nentity alignments so that the one-to-one constraint can be naturally\nincorporated into the alignment prediction. Experimental results show that our\nmodel achieves the state-of-the-art performance and our reasoning methods can\nalso significantly improve existing baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.01167,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Computationally Efficient NER Taggers with Combined Embeddings and\n  Constrained Decoding\n\n  Current State-of-the-Art models in Named Entity Recognition (NER) are neural\nmodels with a Conditional Random Field (CRF) as the final network layer, and\npre-trained \"contextual embeddings\". The CRF layer is used to facilitate global\ncoherence between labels, and the contextual embeddings provide a better\nrepresentation of words in context. However, both of these improvements come at\na high computational cost. In this work, we explore two simple techniques that\nsubstantially improve NER performance over a strong baseline with negligible\ncost. First, we use multiple pre-trained embeddings as word representations via\nconcatenation. Second, we constrain the tagger, trained using a cross-entropy\nloss, during decoding to eliminate illegal transitions. While training a tagger\non CoNLL 2003 we find a $786$\\% speed-up over a contextual embeddings-based\ntagger without sacrificing strong performance. We also show that the\nconcatenation technique works across multiple tasks and datasets. We analyze\naspects of similarity and coverage between pre-trained embeddings and the\ndynamics of tag co-occurrence to explain why these techniques work. We provide\nan open source implementation of our tagger using these techniques in three\npopular deep learning frameworks --- TensorFlow, Pytorch, and DyNet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.06206,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000048346,
      "text":"Multi-step Joint-Modality Attention Network for Scene-Aware Dialogue\n  System\n\n  Understanding dynamic scenes and dialogue contexts in order to converse with\nusers has been challenging for multimodal dialogue systems. The 8-th Dialog\nSystem Technology Challenge (DSTC8) proposed an Audio Visual Scene-Aware Dialog\n(AVSD) task, which contains multiple modalities including audio, vision, and\nlanguage, to evaluate how dialogue systems understand different modalities and\nresponse to users. In this paper, we proposed a multi-step joint-modality\nattention network (JMAN) based on recurrent neural network (RNN) to reason on\nvideos. Our model performs a multi-step attention mechanism and jointly\nconsiders both visual and textual representations in each reasoning process to\nbetter integrate information from the two different modalities. Compared to the\nbaseline released by AVSD organizers, our model achieves a relative 12.1% and\n22.4% improvement over the baseline on ROUGE-L score and CIDEr score.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.11701,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Teaching Machines to Converse\n\n  The ability of a machine to communicate with humans has long been associated\nwith the general success of AI. This dates back to Alan Turing's epoch-making\nwork in the early 1950s, which proposes that a machine's intelligence can be\ntested by how well it, the machine, can fool a human into believing that the\nmachine is a human through dialogue conversations. Many systems learn\ngeneration rules from a minimal set of authored rules or labels on top of\nhand-coded rules or templates, and thus are both expensive and difficult to\nextend to open-domain scenarios. Recently, the emergence of neural network\nmodels the potential to solve many of the problems in dialogue learning that\nearlier systems cannot tackle: the end-to-end neural frameworks offer the\npromise of scalability and language-independence, together with the ability to\ntrack the dialogue state and then mapping between states and dialogue actions\nin a way not possible with conventional systems. On the other hand, neural\nsystems bring about new challenges: they tend to output dull and generic\nresponses; they lack a consistent or a coherent persona; they are usually\noptimized through single-turn conversations and are incapable of handling the\nlong-term success of a conversation; and they are not able to take the\nadvantage of the interactions with humans. This dissertation attempts to tackle\nthese challenges: Contributions are two-fold: (1) we address new challenges\npresented by neural network models in open-domain dialogue generation systems;\n(2) we develop interactive question-answering dialogue systems by (a) giving\nthe agent the ability to ask questions and (b) training a conversation agent\nthrough interactions with humans in an online fashion, where a bot improves\nthrough communicating with humans and learning from the mistakes that it makes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.11383,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"Fact-aware Sentence Split and Rephrase with Permutation Invariant\n  Training\n\n  Sentence Split and Rephrase aims to break down a complex sentence into\nseveral simple sentences with its meaning preserved. Previous studies tend to\naddress the issue by seq2seq learning from parallel sentence pairs, which takes\na complex sentence as input and sequentially generates a series of simple\nsentences. However, the conventional seq2seq learning has two limitations for\nthis task: (1) it does not take into account the facts stated in the long\nsentence; As a result, the generated simple sentences may miss or inaccurately\nstate the facts in the original sentence. (2) The order variance of the simple\nsentences to be generated may confuse the seq2seq model during training because\nthe simple sentences derived from the long source sentence could be in any\norder.\n  To overcome the challenges, we first propose the Fact-aware Sentence\nEncoding, which enables the model to learn facts from the long sentence and\nthus improves the precision of sentence split; then we introduce Permutation\nInvariant Training to alleviate the effects of order variance in seq2seq\nlearning for this task. Experiments on the WebSplit-v1.0 benchmark dataset show\nthat our approaches can largely improve the performance over the previous\nseq2seq learning approaches. Moreover, an extrinsic evaluation on oie-benchmark\nverifies the effectiveness of our approaches by an observation that splitting\nlong sentences with our state-of-the-art model as preprocessing is helpful for\nimproving OpenIE performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.05139,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000123514,
      "text":"A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation\n\n  Story generation, namely generating a reasonable story from a leading\ncontext, is an important but challenging task. In spite of the success in\nmodeling fluency and local coherence, existing neural language generation\nmodels (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of\nlong-range coherence in generated stories. We conjecture that this is because\nof the difficulty of associating relevant commonsense knowledge, understanding\nthe causal relationships, and planning entities and events with proper temporal\norder. In this paper, we devise a knowledge-enhanced pretraining model for\ncommonsense story generation. We propose to utilize commonsense knowledge from\nexternal knowledge bases to generate reasonable stories. To further capture the\ncausal and temporal dependencies between the sentences in a reasonable story,\nwe employ multi-task learning which combines a discriminative objective to\ndistinguish true and fake stories during fine-tuning. Automatic and manual\nevaluation shows that our model can generate more reasonable stories than\nstate-of-the-art baselines, particularly in terms of logic and global\ncoherence.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.01582,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000206629,
      "text":"A Survey on Machine Reading Comprehension Systems\n\n  Machine reading comprehension is a challenging task and hot topic in natural\nlanguage processing. Its goal is to develop systems to answer the questions\nregarding a given context. In this paper, we present a comprehensive survey on\ndifferent aspects of machine reading comprehension systems, including their\napproaches, structures, input\/outputs, and research novelties. We illustrate\nthe recent trends in this field based on 241 reviewed papers from 2016 to 2020.\nOur investigations demonstrate that the focus of research has changed in recent\nyears from answer extraction to answer generation, from single to\nmulti-document reading comprehension, and from learning from scratch to using\npre-trained embeddings. We also discuss the popular datasets and the evaluation\nmetrics in this field. The paper ends with investigating the most cited papers\nand their contributions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.00738,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000038743,
      "text":"An Efficient Architecture for Predicting the Case of Characters using\n  Sequence Models\n\n  The dearth of clean textual data often acts as a bottleneck in several\nnatural language processing applications. The data available often lacks proper\ncase (uppercase or lowercase) information. This often comes up when text is\nobtained from social media, messaging applications and other online platforms.\nThis paper attempts to solve this problem by restoring the correct case of\ncharacters, commonly known as Truecasing. Doing so improves the accuracy of\nseveral processing tasks further down in the NLP pipeline. Our proposed\narchitecture uses a combination of convolutional neural networks (CNN),\nbi-directional long short-term memory networks (LSTM) and conditional random\nfields (CRF), which work at a character level without any explicit feature\nengineering. In this study we compare our approach to previous statistical and\ndeep learning based approaches. Our method shows an increment of 0.83 in F1\nscore over the current state of the art. Since truecasing acts as a\npreprocessing step in several applications, every increment in the F1 score\nleads to a significant improvement in the language processing tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.05297,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000048677,
      "text":"Dialectal Layers in West Iranian: a Hierarchical Dirichlet Process\n  Approach to Linguistic Relationships\n\n  This paper addresses a series of complex and unresolved issues in the\nhistorical phonology of West Iranian languages. The West Iranian languages\n(Persian, Kurdish, Balochi, and other languages) display a high degree of\nnon-Lautgesetzlich behavior. Most of this irregularity is undoubtedly due to\nlanguage contact; we argue, however, that an oversimplified view of the\nprocesses at work has prevailed in the literature on West Iranian dialectology,\nwith specialists assuming that deviations from an expected outcome in a given\nnon-Persian language are due to lexical borrowing from some chronological stage\nof Persian. It is demonstrated that this qualitative approach yields at times\nproblematic conclusions stemming from the lack of explicit probabilistic\ninferences regarding the distribution of the data: Persian may not be the sole\ndonor language; additionally, borrowing at the lexical level is not always the\nmechanism that introduces irregularity. In many cases, the possibility that\nWest Iranian languages show different reflexes in different conditioning\nenvironments remains under-explored. We employ a novel Bayesian approach\ndesigned to overcome these problems and tease apart the different determinants\nof irregularity in patterns of West Iranian sound change. Our methodology\nallows us to provisionally resolve a number of outstanding questions in the\nliterature on West Iranian dialectology concerning the dialectal affiliation of\ncertain sound changes. We outline future directions for work of this sort.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.02885,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000091394,
      "text":"Resolving the Scope of Speculation and Negation using Transformer-Based\n  Architectures\n\n  Speculation is a naturally occurring phenomena in textual data, forming an\nintegral component of many systems, especially in the biomedical information\nretrieval domain. Previous work addressing cue detection and scope resolution\n(the two subtasks of speculation detection) have ranged from rule-based systems\nto deep learning-based approaches. In this paper, we apply three popular\ntransformer-based architectures, BERT, XLNet and RoBERTa to this task, on two\npublicly available datasets, BioScope Corpus and SFU Review Corpus, reporting\nsubstantial improvements over previously reported results (by at least 0.29 F1\npoints on cue detection and 4.27 F1 points on scope resolution). We also\nexperiment with joint training of the model on multiple datasets, which\noutperforms the single dataset training approach by a good margin. We observe\nthat XLNet consistently outperforms BERT and RoBERTa, contrary to results on\nother benchmark datasets. To confirm this observation, we apply XLNet and\nRoBERTa to negation detection and scope resolution, reporting state-of-the-art\nresults on negation scope resolution for the BioScope Corpus (increase of 3.16\nF1 points on the BioScope Full Papers, 0.06 F1 points on the BioScope\nAbstracts) and the SFU Review Corpus (increase of 0.3 F1 points).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.03131,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000004073,
      "text":"Offensive Language Detection: A Comparative Analysis\n\n  Offensive behaviour has become pervasive in the Internet community.\nIndividuals take the advantage of anonymity in the cyber world and indulge in\noffensive communications which they may not consider in the real life.\nGovernments, online communities, companies etc are investing into prevention of\noffensive behaviour content in social media. One of the most effective solution\nfor tacking this enigmatic problem is the use of computational techniques to\nidentify offensive content and take action. The current work focuses on\ndetecting offensive language in English tweets. The dataset used for the\nexperiment is obtained from SemEval-2019 Task 6 on Identifying and Categorizing\nOffensive Language in Social Media (OffensEval). The dataset contains 14,460\nannotated English tweets. The present paper provides a comparative analysis and\nRandom kitchen sink (RKS) based approach for offensive language detection. We\nexplore the effectiveness of Google sentence encoder, Fasttext, Dynamic mode\ndecomposition (DMD) based features and Random kitchen sink (RKS) method for\noffensive language detection. From the experiments and evaluation we observed\nthat RKS with fastetxt achieved competing results. The evaluation measures used\nare accuracy, precision, recall, f1-score.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.07418,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"A Physical Embedding Model for Knowledge Graphs\n\n  Knowledge graph embedding methods learn continuous vector representations for\nentities in knowledge graphs and have been used successfully in a large number\nof applications. We present a novel and scalable paradigm for the computation\nof knowledge graph embeddings, which we dub PYKE . Our approach combines a\nphysical model based on Hooke's law and its inverse with ideas from simulated\nannealing to compute embeddings for knowledge graphs efficiently. We prove that\nPYKE achieves a linear space complexity. While the time complexity for the\ninitialization of our approach is quadratic, the time complexity of each of its\niterations is linear in the size of the input knowledge graph. Hence, PYKE's\noverall runtime is close to linear. Consequently, our approach easily scales up\nto knowledge graphs containing millions of triples. We evaluate our approach\nagainst six state-of-the-art embedding approaches on the DrugBank and DBpedia\ndatasets in two series of experiments. The first series shows that the cluster\npurity achieved by PYKE is up to 26% (absolute) better than that of the state\nof art. In addition, PYKE is more than 22 times faster than existing embedding\nsolutions in the best case. The results of our second series of experiments\nshow that PYKE is up to 23% (absolute) better than the state of art on the task\nof type prediction while maintaining its superior scalability. Our\nimplementation and results are open-source and are available at\nhttp:\/\/github.com\/dice-group\/PYKE.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.02091,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000015133,
      "text":"Knowledge-aware Attention Network for Protein-Protein Interaction\n  Extraction\n\n  Protein-protein interaction (PPI) extraction from published scientific\nliterature provides additional support for precision medicine efforts. However,\nmany of the current PPI extraction methods need extensive feature engineering\nand cannot make full use of the prior knowledge in knowledge bases (KB). KBs\ncontain huge amounts of structured information about entities and\nrelationships, therefore plays a pivotal role in PPI extraction. This paper\nproposes a knowledge-aware attention network (KAN) to fuse prior knowledge\nabout protein-protein pairs and context information for PPI extraction. The\nproposed model first adopts a diagonal-disabled multi-head attention mechanism\nto encode context sequence along with knowledge representations learned from\nKB. Then a novel multi-dimensional attention mechanism is used to select the\nfeatures that can best describe the encoded context. Experiment results on the\nBioCreative VI PPI dataset show that the proposed approach could acquire\nknowledge-aware dependencies between different words in a sequence and lead to\na new state-of-the-art performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.03216,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"Simulating Lexical Semantic Change from Sense-Annotated Data\n\n  We present a novel procedure to simulate lexical semantic change from\nsynchronic sense-annotated data, and demonstrate its usefulness for assessing\nlexical semantic change detection models. The induced dataset represents a\nstronger correspondence to empirically observed lexical semantic change than\nprevious synthetic datasets, because it exploits the intimate relationship\nbetween synchronic polysemy and diachronic change. We publish the data and\nprovide the first large-scale evaluation gold standard for LSC detection\nmodels.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.02943,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000290407,
      "text":"Binary and Multitask Classification Model for Dutch Anaphora Resolution:\n  Die\/Dat Prediction\n\n  The correct use of Dutch pronouns 'die' and 'dat' is a stumbling block for\nboth native and non-native speakers of Dutch due to the multiplicity of\nsyntactic functions and the dependency on the antecedent's gender and number.\nDrawing on previous research conducted on neural context-dependent dt-mistake\ncorrection models (Heyman et al. 2018), this study constructs the first neural\nnetwork model for Dutch demonstrative and relative pronoun resolution that\nspecifically focuses on the correction and part-of-speech prediction of these\ntwo pronouns. Two separate datasets are built with sentences obtained from,\nrespectively, the Dutch Europarl corpus (Koehn 2015) - which contains the\nproceedings of the European Parliament from 1996 to the present - and the SoNaR\ncorpus (Oostdijk et al. 2013) - which contains Dutch texts from a variety of\ndomains such as newspapers, blogs and legal texts. Firstly, a binary\nclassification model solely predicts the correct 'die' or 'dat'. The classifier\nwith a bidirectional long short-term memory architecture achieves 84.56%\naccuracy. Secondly, a multitask classification model simultaneously predicts\nthe correct 'die' or 'dat' and its part-of-speech tag. The model containing a\ncombination of a sentence and context encoder with both a bidirectional long\nshort-term memory architecture results in 88.63% accuracy for die\/dat\nprediction and 87.73% accuracy for part-of-speech prediction. More\nevenly-balanced data, larger word embeddings, an extra bidirectional long\nshort-term memory layer and integrated part-of-speech knowledge positively\naffects die\/dat prediction performance, while a context encoder architecture\nraises part-of-speech prediction performance. This study shows promising\nresults and can serve as a starting point for future research on machine\nlearning models for Dutch anaphora resolution.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.03361,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Co-evolution of language and agents in referential games\n\n  Referential games offer a grounded learning environment for neural agents\nwhich accounts for the fact that language is functionally used to communicate.\nHowever, they do not take into account a second constraint considered to be\nfundamental for the shape of human language: that it must be learnable by new\nlanguage learners.\n  Cogswell et al. (2019) introduced cultural transmission within referential\ngames through a changing population of agents to constrain the emerging\nlanguage to be learnable. However, the resulting languages remain inherently\nbiased by the agents' underlying capabilities.\n  In this work, we introduce Language Transmission Engine to model both\ncultural and architectural evolution in a population of agents. As our core\ncontribution, we empirically show that the optimal situation is to take into\naccount also the learning biases of the language learners and thus let language\nand agents co-evolve. When we allow the agent population to evolve through\narchitectural evolution, we achieve across the board improvements on all\nconsidered metrics and surpass the gains made with cultural transmission. These\nresults stress the importance of studying the underlying agent architecture and\npave the way to investigate the co-evolution of language and agent in language\nemergence studies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.0821,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Multilingual Denoising Pre-training for Neural Machine Translation\n\n  This paper demonstrates that multilingual denoising pre-training produces\nsignificant performance gains across a wide variety of machine translation (MT)\ntasks. We present mBART -- a sequence-to-sequence denoising auto-encoder\npre-trained on large-scale monolingual corpora in many languages using the BART\nobjective. mBART is one of the first methods for pre-training a complete\nsequence-to-sequence model by denoising full texts in multiple languages, while\nprevious approaches have focused only on the encoder, decoder, or\nreconstructing parts of the text. Pre-training a complete model allows it to be\ndirectly fine tuned for supervised (both sentence-level and document-level) and\nunsupervised machine translation, with no task-specific modifications. We\ndemonstrate that adding mBART initialization produces performance gains in all\nbut the highest-resource settings, including up to 12 BLEU points for low\nresource MT and over 5 BLEU points for many document-level and unsupervised\nmodels. We also show it also enables new types of transfer to language pairs\nwith no bi-text or that were not in the pre-training corpus, and present\nextensive analysis of which factors contribute the most to effective\npre-training.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.0314,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000079804,
      "text":"HHH: An Online Medical Chatbot System based on Knowledge Graph and\n  Hierarchical Bi-Directional Attention\n\n  This paper proposes a chatbot framework that adopts a hybrid model which\nconsists of a knowledge graph and a text similarity model. Based on this\nchatbot framework, we build HHH, an online question-and-answer (QA) Healthcare\nHelper system for answering complex medical questions. HHH maintains a\nknowledge graph constructed from medical data collected from the Internet. HHH\nalso implements a novel text representation and similarity deep learning model,\nHierarchical BiLSTM Attention Model (HBAM), to find the most similar question\nfrom a large QA dataset. We compare HBAM with other state-of-the-art language\nmodels such as bidirectional encoder representation from transformers (BERT)\nand Manhattan LSTM Model (MaLSTM). We train and test the models with a subset\nof the Quora duplicate questions dataset in the medical area. The experimental\nresults show that our model is able to achieve a superior performance than\nthese existing methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.10348,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000143713,
      "text":"Low-Resource Knowledge-Grounded Dialogue Generation\n\n  Responding with knowledge has been recognized as an important capability for\nan intelligent conversational agent. Yet knowledge-grounded dialogues, as\ntraining data for learning such a response generation model, are difficult to\nobtain. Motivated by the challenge in practice, we consider knowledge-grounded\ndialogue generation under a natural assumption that only limited training\nexamples are available. In such a low-resource setting, we devise a\ndisentangled response decoder in order to isolate parameters that depend on\nknowledge-grounded dialogues from the entire generation model. By this means,\nthe major part of the model can be learned from a large number of ungrounded\ndialogues and unstructured documents, while the remaining small parameters can\nbe well fitted using the limited training examples. Evaluation results on two\nbenchmarks indicate that with only 1\/8 training data, our model can achieve the\nstate-of-the-art performance and generalize well on out-of-domain knowledge.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.00293,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Beat the AI: Investigating Adversarial Human Annotation for Reading\n  Comprehension\n\n  Innovations in annotation methodology have been a catalyst for Reading\nComprehension (RC) datasets and models. One recent trend to challenge current\nRC models is to involve a model in the annotation process: humans create\nquestions adversarially, such that the model fails to answer them correctly. In\nthis work we investigate this annotation methodology and apply it in three\ndifferent settings, collecting a total of 36,000 samples with progressively\nstronger models in the annotation loop. This allows us to explore questions\nsuch as the reproducibility of the adversarial effect, transfer from data\ncollected with varying model-in-the-loop strengths, and generalisation to data\ncollected without a model. We find that training on adversarially collected\nsamples leads to strong generalisation to non-adversarially collected datasets,\nyet with progressive performance deterioration with increasingly stronger\nmodels-in-the-loop. Furthermore, we find that stronger models can still learn\nfrom datasets collected with substantially weaker models-in-the-loop. When\ntrained on data collected with a BiDAF model in the loop, RoBERTa achieves\n39.9F1 on questions that it cannot answer when trained on SQuAD - only\nmarginally lower than when trained on data collected using RoBERTa itself\n(41.0F1).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.00175,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000025498,
      "text":"UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image\n  Captioning\n\n  Image Captioning, the task of automatic generation of image captions, has\nattracted attentions from researchers in many fields of computer science, being\ncomputer vision, natural language processing and machine learning in recent\nyears. This paper contributes to research on Image Captioning task in terms of\nextending dataset to a different language - Vietnamese. So far, there is no\nexisted Image Captioning dataset for Vietnamese language, so this is the\nforemost fundamental step for developing Vietnamese Image Captioning. In this\nscope, we first build a dataset which contains manually written captions for\nimages from Microsoft COCO dataset relating to sports played with balls, we\ncalled this dataset UIT-ViIC. UIT-ViIC consists of 19,250 Vietnamese captions\nfor 3,850 images. Following that, we evaluate our dataset on deep neural\nnetwork models and do comparisons with English dataset and two Vietnamese\ndatasets built by different methods. UIT-ViIC is published on our lab website\nfor research purposes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.1021,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000044703,
      "text":"Learning to Select Bi-Aspect Information for Document-Scale Text Content\n  Manipulation\n\n  In this paper, we focus on a new practical task, document-scale text content\nmanipulation, which is the opposite of text style transfer and aims to preserve\ntext styles while altering the content. In detail, the input is a set of\nstructured records and a reference text for describing another recordset. The\noutput is a summary that accurately describes the partial content in the source\nrecordset with the same writing style of the reference. The task is\nunsupervised due to lack of parallel data, and is challenging to select\nsuitable records and style words from bi-aspect inputs respectively and\ngenerate a high-fidelity long document. To tackle those problems, we first\nbuild a dataset based on a basketball game report corpus as our testbed, and\npresent an unsupervised neural model with interactive attention mechanism,\nwhich is used for learning the semantic relationship between records and\nreference texts to achieve better content transfer and better style\npreservation. In addition, we also explore the effectiveness of the\nback-translation in our task for constructing some pseudo-training pairs.\nEmpirical results show superiority of our approaches over competitive methods,\nand the models also yield a new state-of-the-art result on a sentence-level\ndataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.09543,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Modelling Latent Skills for Multitask Language Generation\n\n  We present a generative model for multitask conditional language generation.\nOur guiding hypothesis is that a shared set of latent skills underlies many\ndisparate language generation tasks, and that explicitly modelling these skills\nin a task embedding space can help with both positive transfer across tasks and\nwith efficient adaptation to new tasks. We instantiate this task embedding\nspace as a latent variable in a latent variable sequence-to-sequence model. We\nevaluate this hypothesis by curating a series of monolingual text-to-text\nlanguage generation datasets - covering a broad range of tasks and domains -\nand comparing the performance of models both in the multitask and few-shot\nregimes. We show that our latent task variable model outperforms other\nsequence-to-sequence baselines on average across tasks in the multitask\nsetting. In the few-shot learning setting on an unseen test dataset (i.e., a\nnew task), we demonstrate that model adaptation based on inference in the\nlatent task space is more robust than standard fine-tuning based parameter\nadaptation and performs comparably in terms of overall performance. Finally, we\nexamine the latent task representations learnt by our model and show that they\ncluster tasks in a natural way.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.07458,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"A New Clustering neural network for Chinese word segmentation\n\n  In this article I proposed a new model to achieve Chinese word\nsegmentation(CWS),which may have the potentiality to apply in other domains in\nthe future.It is a new thinking in CWS compared to previous works,to consider\nit as a clustering problem instead of a labeling problem.In this model,LSTM and\nself attention structures are used to collect context also sentence level\nfeatures in every layer,and after several layers,a clustering model is applied\nto split characters into groups,which are the final segmentation results.I call\nthis model CLNN.This algorithm can reach 98 percent of F score (without OOV\nwords) and 85 percent to 95 percent F score (with OOV words) in training data\nsets.Error analyses shows that OOV words will greatly reduce performances,which\nneeds a deeper research in the future.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.00166,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Depth-Adaptive Graph Recurrent Network for Text Classification\n\n  The Sentence-State LSTM (S-LSTM) is a powerful and high efficient graph\nrecurrent network, which views words as nodes and performs layer-wise recurrent\nsteps between them simultaneously. Despite its successes on text\nrepresentations, the S-LSTM still suffers from two drawbacks. Firstly, given a\nsentence, certain words are usually more ambiguous than others, and thus more\ncomputation steps need to be taken for these difficult words and vice versa.\nHowever, the S-LSTM takes fixed computation steps for all words, irrespective\nof their hardness. The secondary one comes from the lack of sequential\ninformation (e.g., word order) that is inherently important for natural\nlanguage. In this paper, we try to address these issues and propose a\ndepth-adaptive mechanism for the S-LSTM, which allows the model to learn how\nmany computational steps to conduct for different words as required. In\naddition, we integrate an extra RNN layer to inject sequential information,\nwhich also serves as an input feature for the decision of adaptive depths.\nResults on the classic text classification task (24 datasets in various sizes\nand domains) show that our model brings significant improvements against the\nconventional S-LSTM and other high-performance models (e.g., the Transformer),\nmeanwhile achieving a good accuracy-speed trade off.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.0335,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000008146,
      "text":"Short Text Classification via Knowledge powered Attention with\n  Similarity Matrix based CNN\n\n  Short text is becoming more and more popular on the web, such as Chat\nMessage, SMS and Product Reviews. Accurately classifying short text is an\nimportant and challenging task. A number of studies have difficulties in\naddressing this problem because of the word ambiguity and data sparsity. To\naddress this issue, we propose a knowledge powered attention with similarity\nmatrix based convolutional neural network (KASM) model, which can compute\ncomprehensive information by utilizing the knowledge and deep neural network.\nWe use knowledge graph (KG) to enrich the semantic representation of short\ntext, specially, the information of parent-entity is introduced in our model.\nMeanwhile, we consider the word interaction in the literal-level between short\ntext and the representation of label, and utilize similarity matrix based\nconvolutional neural network (CNN) to extract it. For the purpose of measuring\nthe importance of knowledge, we introduce the attention mechanisms to choose\nthe important information. Experimental results on five standard datasets show\nthat our model significantly outperforms state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.12097,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"Improving cross-lingual model transfer by chunking\n\n  We present a shallow parser guided cross-lingual model transfer approach in\norder to address the syntactic differences between source and target languages\nmore effectively. In this work, we assume the chunks or phrases in a sentence\nas transfer units in order to address the syntactic differences between the\nsource and target languages arising due to the differences in ordering of words\nin the phrases and the ordering of phrases in a sentence separately.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.09637,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"Markov Chain Monte-Carlo Phylogenetic Inference Construction in\n  Computational Historical Linguistics\n\n  More and more languages in the world are under study nowadays, as a result,\nthe traditional way of historical linguistics study is facing some challenges.\nFor example, the linguistic comparative research among languages needs manual\nannotation, which becomes more and more impossible with the increasing amount\nof language data coming out all around the world. Although it could hardly\nreplace linguists work, the automatic computational methods have been taken\ninto consideration and it can help people reduce their workload. One of the\nmost important work in historical linguistics is word comparison from different\nlanguages and find the cognate words for them, which means people try to figure\nout if the two languages are related to each other or not. In this paper, I am\ngoing to use computational method to cluster the languages and use Markov Chain\nMonte Carlo (MCMC) method to build the language typology relationship tree\nbased on the clusters.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.02154,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000039074,
      "text":"Related Tasks can Share! A Multi-task Framework for Affective language\n\n  Expressing the polarity of sentiment as 'positive' and 'negative' usually\nhave limited scope compared with the intensity\/degree of polarity. These two\ntasks (i.e. sentiment classification and sentiment intensity prediction) are\nclosely related and may offer assistance to each other during the learning\nprocess. In this paper, we propose to leverage the relatedness of multiple\ntasks in a multi-task learning framework. Our multi-task model is based on\nconvolutional-Gated Recurrent Unit (GRU) framework, which is further assisted\nby a diverse hand-crafted feature set. Evaluation and analysis suggest that\njoint-learning of the related tasks in a multi-task framework can outperform\neach of the individual tasks in the single-task frameworks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.12591,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000095036,
      "text":"DC-BERT: Decoupling Question and Document for Efficient Contextual\n  Encoding\n\n  Recent studies on open-domain question answering have achieved prominent\nperformance improvement using pre-trained language models such as BERT.\nState-of-the-art approaches typically follow the \"retrieve and read\" pipeline\nand employ BERT-based reranker to filter retrieved documents before feeding\nthem into the reader module. The BERT retriever takes as input the\nconcatenation of question and each retrieved document. Despite the success of\nthese approaches in terms of QA accuracy, due to the concatenation, they can\nbarely handle high-throughput of incoming questions each with a large\ncollection of retrieved documents. To address the efficiency problem, we\npropose DC-BERT, a decoupled contextual encoding framework that has dual BERT\nmodels: an online BERT which encodes the question only once, and an offline\nBERT which pre-encodes all the documents and caches their encodings. On SQuAD\nOpen and Natural Questions Open datasets, DC-BERT achieves 10x speedup on\ndocument retrieval, while retaining most (about 98%) of the QA performance\ncompared to state-of-the-art approaches for open-domain question answering.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.04165,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"Training with Streaming Annotation\n\n  In this paper, we address a practical scenario where training data is\nreleased in a sequence of small-scale batches and annotation in earlier phases\nhas lower quality than the later counterparts. To tackle the situation, we\nutilize a pre-trained transformer network to preserve and integrate the most\nsalient document information from the earlier batches while focusing on the\nannotation (presumably with higher quality) from the current batch. Using event\nextraction as a case study, we demonstrate in the experiments that our proposed\nframework can perform better than conventional approaches (the improvement\nranges from 3.6 to 14.9% absolute F-score gain), especially when there is more\nnoise in the early annotation; and our approach spares 19.1% time with regard\nto the best conventional method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.09419,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000034438,
      "text":"Guider l'attention dans les modeles de sequence a sequence pour la\n  prediction des actes de dialogue\n\n  The task of predicting dialog acts (DA) based on conversational dialog is a\nkey component in the development of conversational agents. Accurately\npredicting DAs requires a precise modeling of both the conversation and the\nglobal tag dependencies. We leverage seq2seq approaches widely adopted in\nNeural Machine Translation (NMT) to improve the modelling of tag sequentiality.\nSeq2seq models are known to learn complex global dependencies while currently\nproposed approaches using linear conditional random fields (CRF) only model\nlocal tag dependencies. In this work, we introduce a seq2seq model tailored for\nDA classification using: a hierarchical encoder, a novel guided attention\nmechanism and beam search applied to both training and inference. Compared to\nthe state of the art our model does not require handcrafted features and is\ntrained end-to-end. Furthermore, the proposed approach achieves an unmatched\naccuracy score of 85% on SwDA, and state-of-the-art accuracy score of 91.6% on\nMRDA.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.11023,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Semantic Relatedness for Keyword Disambiguation: Exploiting Different\n  Embeddings\n\n  Understanding the meaning of words is crucial for many tasks that involve\nhuman-machine interaction. This has been tackled by research in Word Sense\nDisambiguation (WSD) in the Natural Language Processing (NLP) field. Recently,\nWSD and many other NLP tasks have taken advantage of embeddings-based\nrepresentation of words, sentences, and documents. However, when it comes to\nWSD, most embeddings models suffer from ambiguity as they do not capture the\ndifferent possible meanings of the words. Even when they do, the list of\npossible meanings for a word (sense inventory) has to be known in advance at\ntraining time to be included in the embeddings space. Unfortunately, there are\nsituations in which such a sense inventory is not known in advance (e.g., an\nontology selected at run-time), or it evolves with time and its status diverges\nfrom the one at training time. This hampers the use of embeddings models for\nWSD. Furthermore, traditional WSD techniques do not perform well in situations\nin which the available linguistic information is very scarce, such as the case\nof keyword-based queries. In this paper, we propose an approach to keyword\ndisambiguation which grounds on a semantic relatedness between words and senses\nprovided by an external inventory (ontology) that is not known at training\ntime. Building on previous works, we present a semantic relatedness measure\nthat uses word embeddings, and explore different disambiguation algorithms to\nalso exploit both word and sentence representations. Experimental results show\nthat this approach achieves results comparable with the state of the art when\napplied for WSD, without training for a particular domain.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.04985,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Adv-BERT: BERT is not robust on misspellings! Generating nature\n  adversarial samples on BERT\n\n  There is an increasing amount of literature that claims the brittleness of\ndeep neural networks in dealing with adversarial examples that are created\nmaliciously. It is unclear, however, how the models will perform in realistic\nscenarios where \\textit{natural rather than malicious} adversarial instances\noften exist. This work systematically explores the robustness of BERT, the\nstate-of-the-art Transformer-style model in NLP, in dealing with noisy data,\nparticularly mistakes in typing the keyboard, that occur inadvertently.\nIntensive experiments on sentiment analysis and question answering benchmarks\nindicate that: (i) Typos in various words of a sentence do not influence\nequally. The typos in informative words make severer damages; (ii) Mistype is\nthe most damaging factor, compared with inserting, deleting, etc.; (iii) Humans\nand machines have different focuses on recognizing adversarial attacks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.00317,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Explaining Relationships Between Scientific Documents\n\n  We address the task of explaining relationships between two scientific\ndocuments using natural language text. This task requires modeling the complex\ncontent of long technical documents, deducing a relationship between these\ndocuments, and expressing the details of that relationship in text. In addition\nto the theoretical interest of this task, successful solutions can help improve\nresearcher efficiency in search and review. In this paper we establish a\ndataset of 622K examples from 154K documents. We pretrain a large language\nmodel to serve as the foundation for autoregressive approaches to the task. We\nexplore the impact of taking different views on the two documents, including\nthe use of dense representations extracted with scientific IE systems. We\nprovide extensive automatic and human evaluations which show the promise of\nsuch models, but make clear challenges for future work.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.08899,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Compositional Neural Machine Translation by Removing the Lexicon from\n  Syntax\n\n  The meaning of a natural language utterance is largely determined from its\nsyntax and words. Additionally, there is evidence that humans process an\nutterance by separating knowledge about the lexicon from syntax knowledge.\nTheories from semantics and neuroscience claim that complete word meanings are\nnot encoded in the representation of syntax. In this paper, we propose neural\nunits that can enforce this constraint over an LSTM encoder and decoder. We\ndemonstrate that our model achieves competitive performance across a variety of\ndomains including semantic parsing, syntactic parsing, and English to Mandarin\nChinese translation. In these cases, our model outperforms the standard LSTM\nencoder and decoder architecture on many or all of our metrics. To demonstrate\nthat our model achieves the desired separation between the lexicon and syntax,\nwe analyze its weights and explore its behavior when different neural modules\nare damaged. When damaged, we find that the model displays the knowledge\ndistortions that aphasics are evidenced to have.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.07767,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000047353,
      "text":"Learning by Semantic Similarity Makes Abstractive Summarization Better\n\n  By harnessing pre-trained language models, summarization models had rapid\nprogress recently. However, the models are mainly assessed by automatic\nevaluation metrics such as ROUGE. Although ROUGE is known for having a positive\ncorrelation with human evaluation scores, it has been criticized for its\nvulnerability and the gap between actual qualities. In this paper, we compare\nthe generated summaries from recent LM, BART, and the reference summaries from\na benchmark dataset, CNN\/DM, using a crowd-sourced human evaluation metric.\nInterestingly, model-generated summaries receive higher scores relative to\nreference summaries. Stemming from our experimental results, we first argue the\nintrinsic characteristics of the CNN\/DM dataset, the progress of pre-trained\nlanguage models, and their ability to generalize on the training data. Finally,\nwe share our insights into the model-generated summaries and presents our\nthought on learning methods for abstractive summarization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.13785,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Span-based discontinuous constituency parsing: a family of exact\n  chart-based algorithms with time complexities from O(n^6) down to O(n^3)\n\n  We introduce a novel chart-based algorithm for span-based parsing of\ndiscontinuous constituency trees of block degree two, including ill-nested\nstructures. In particular, we show that we can build variants of our parser\nwith smaller search spaces and time complexities ranging from $\\mathcal O(n^6)$\ndown to $\\mathcal O(n^3)$. The cubic time variant covers 98\\% of constituents\nobserved in linguistic treebanks while having the same complexity as continuous\nconstituency parsers. We evaluate our approach on German and English treebanks\n(Negra, Tiger and Discontinuous PTB) and report state-of-the-art results in the\nfully supervised setting. We also experiment with pre-trained word embeddings\nand \\bert{}-based neural networks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.09244,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000348025,
      "text":"Language Technology Programme for Icelandic 2019-2023\n\n  In this paper, we describe a new national language technology programme for\nIcelandic. The programme, which spans a period of five years, aims at making\nIcelandic usable in communication and interactions in the digital world, by\ndeveloping accessible, open-source language resources and software. The\nresearch and development work within the programme is carried out by a\nconsortium of universities, institutions, and private companies, with a strong\nemphasis on cooperation between academia and industries. Five core projects\nwill be the main content of the programme: language resources, speech\nrecognition, speech synthesis, machine translation, and spell and grammar\nchecking. We also describe other national language technology programmes and\ngive an overview over the history of language technology in Iceland.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.03131,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000033114,
      "text":"Morfessor EM+Prune: Improved Subword Segmentation with Expectation\n  Maximization and Pruning\n\n  Data-driven segmentation of words into subword units has been used in various\nnatural language processing applications such as automatic speech recognition\nand statistical machine translation for almost 20 years. Recently it has became\nmore widely adopted, as models based on deep neural networks often benefit from\nsubword units even for morphologically simpler languages. In this paper, we\ndiscuss and compare training algorithms for a unigram subword model, based on\nthe Expectation Maximization algorithm and lexicon pruning. Using English,\nFinnish, North Sami, and Turkish data sets, we show that this approach is able\nto find better solutions to the optimization problem defined by the Morfessor\nBaseline model than its original recursive training algorithm. The improved\noptimization also leads to higher morphological segmentation accuracy when\ncompared to a linguistic gold standard. We publish implementations of the new\nalgorithms in the widely-used Morfessor software package.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.13342,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"A Corpus of Controlled Opinionated and Knowledgeable Movie Discussions\n  for Training Neural Conversation Models\n\n  Fully data driven Chatbots for non-goal oriented dialogues are known to\nsuffer from inconsistent behaviour across their turns, stemming from a general\ndifficulty in controlling parameters like their assumed background personality\nand knowledge of facts. One reason for this is the relative lack of labeled\ndata from which personality consistency and fact usage could be learned\ntogether with dialogue behaviour. To address this, we introduce a new labeled\ndialogue dataset in the domain of movie discussions, where every dialogue is\nbased on pre-specified facts and opinions. We thoroughly validate the collected\ndialogue for adherence of the participants to their given fact and opinion\nprofile, and find that the general quality in this respect is high. This\nprocess also gives us an additional layer of annotation that is potentially\nuseful for training models. We introduce as a baseline an end-to-end trained\nself-attention decoder model trained on this data and show that it is able to\ngenerate opinionated responses that are judged to be natural and knowledgeable\nand show attentiveness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.0168,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"Hybrid Generative-Retrieval Transformers for Dialogue Domain Adaptation\n\n  Domain adaptation has recently become a key problem in dialogue systems\nresearch. Deep learning, while being the preferred technique for modeling such\nsystems, works best given massive training data. However, in the real-world\nscenario, such resources aren't available for every new domain, so the ability\nto train with a few dialogue examples can be considered essential. Pre-training\non large data sources and adapting to the target data has become the standard\nmethod for few-shot problems within the deep learning framework. In this paper,\nwe present the winning entry at the fast domain adaptation task of DSTC8, a\nhybrid generative-retrieval model based on GPT-2 fine-tuned to the multi-domain\nMetaLWOz dataset. Robust and diverse in response generation, our model uses\nretrieval logic as a fallback, being SoTA on MetaLWOz in human evaluation (>4%\nimprovement over the 2nd place system) and attaining competitive generalization\nperformance in adaptation to the unseen MultiWOZ dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.10224,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Unsupervised Word Polysemy Quantification with Multiresolution Grids of\n  Contextual Embeddings\n\n  The number of senses of a given word, or polysemy, is a very subjective\nnotion, which varies widely across annotators and resources. We propose a novel\nmethod to estimate polysemy, based on simple geometry in the contextual\nembedding space. Our approach is fully unsupervised and purely data-driven. We\nshow through rigorous experiments that our rankings are well correlated (with\nstrong statistical significance) with 6 different rankings derived from famous\nhuman-constructed resources such as WordNet, OntoNotes, Oxford, Wikipedia etc.,\nfor 6 different standard metrics. We also visualize and analyze the correlation\nbetween the human rankings. A valuable by-product of our method is the ability\nto sample, at no extra cost, sentences containing different senses of a given\nword. Finally, the fully unsupervised nature of our method makes it applicable\nto any language.\n  Code and data are publicly available at\nhttps:\/\/github.com\/ksipos\/polysemy-assessment .\n  The paper was accepted as a long paper at EACL 2021.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.08529,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000260605,
      "text":"Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics\n  for Text Collections\n\n  Summarizing data samples by quantitative measures has a long history, with\ndescriptive statistics being a case in point. However, as natural language\nprocessing methods flourish, there are still insufficient characteristic\nmetrics to describe a collection of texts in terms of the words, sentences, or\nparagraphs they comprise. In this work, we propose metrics of diversity,\ndensity, and homogeneity that quantitatively measure the dispersion, sparsity,\nand uniformity of a text collection. We conduct a series of simulations to\nverify that each metric holds desired properties and resonates with human\nintuitions. Experiments on real-world datasets demonstrate that the proposed\ncharacteristic metrics are highly correlated with text classification\nperformance of a renowned model, BERT, which could inspire future applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.0619,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000032451,
      "text":"WAC: A Corpus of Wikipedia Conversations for Online Abuse Detection\n\n  With the spread of online social networks, it is more and more difficult to\nmonitor all the user-generated content. Automating the moderation process of\nthe inappropriate exchange content on Internet has thus become a priority task.\nMethods have been proposed for this purpose, but it can be challenging to find\na suitable dataset to train and develop them. This issue is especially true for\napproaches based on information derived from the structure and the dynamic of\nthe conversation. In this work, we propose an original framework, based on the\nWikipedia Comment corpus, with comment-level abuse annotations of different\ntypes. The major contribution concerns the reconstruction of conversations, by\ncomparison to existing corpora, which focus only on isolated messages (i.e.\ntaken out of their conversational context). This large corpus of more than 380k\nannotated messages opens perspectives for online abuse detection and especially\nfor context-based approaches. We also propose, in addition to this corpus, a\ncomplete benchmarking platform to stimulate and fairly compare scientific works\naround the problem of content abuse detection, trying to avoid the recurring\nproblem of result replication. Finally, we apply two classification methods to\nour dataset to demonstrate its potential.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.04073,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000117885,
      "text":"A Multi-Source Entity-Level Sentiment Corpus for the Financial Domain:\n  The FinLin Corpus\n\n  We introduce FinLin, a novel corpus containing investor reports, company\nreports, news articles, and microblogs from StockTwits, targeting multiple\nentities stemming from the automobile industry and covering a 3-month period.\nFinLin was annotated with a sentiment score and a relevance score in the range\n[-1.0, 1.0] and [0.0, 1.0], respectively. The annotations also include the text\nspans selected for the sentiment, thus, providing additional insight into the\nannotators' reasoning. Overall, FinLin aims to complement the current knowledge\nby providing a novel and publicly available financial sentiment corpus and to\nfoster research on the topic of financial sentiment analysis and potential\napplications in behavioural science.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.11523,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000125832,
      "text":"Tigrinya Neural Machine Translation with Transfer Learning for\n  Humanitarian Response\n\n  We report our experiments in building a domain-specific Tigrinya-to-English\nneural machine translation system. We use transfer learning from other Ge'ez\nscript languages and report an improvement of 1.3 BLEU points over a classic\nneural baseline. We publish our development pipeline as an open-source library\nand also provide a demonstration application.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.05443,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"A Precisely Xtreme-Multi Channel Hybrid Approach For Roman Urdu\n  Sentiment Analysis\n\n  In order to accelerate the performance of various Natural Language Processing\ntasks for Roman Urdu, this paper for the very first time provides 3 neural word\nembeddings prepared using most widely used approaches namely Word2vec,\nFastText, and Glove. The integrity of generated neural word embeddings is\nevaluated using intrinsic and extrinsic evaluation approaches. Considering the\nlack of publicly available benchmark datasets, it provides a first-ever Roman\nUrdu dataset which consists of 3241 sentiments annotated against positive,\nnegative and neutral classes. To provide benchmark baseline performance over\nthe presented dataset, we adapt diverse machine learning (Support Vector\nMachine Logistic Regression, Naive Bayes), deep learning (convolutional neural\nnetwork, recurrent neural network), and hybrid approaches. Effectiveness of\ngenerated neural word embeddings is evaluated by comparing the performance of\nmachine and deep learning based methodologies using 7, and 5 distinct feature\nrepresentation approaches respectively. Finally, it proposes a novel precisely\nextreme multi-channel hybrid methodology which outperforms state-of-the-art\nadapted machine and deep learning approaches by the figure of 9%, and 4% in\nterms of F1-score. Roman Urdu Sentiment Analysis, Pretrain word embeddings for\nRoman Urdu, Word2Vec, Glove, Fast-Text\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.03014,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"A Framework for the Computational Linguistic Analysis of Dehumanization\n\n  Dehumanization is a pernicious psychological process that often leads to\nextreme intergroup bias, hate speech, and violence aimed at targeted social\ngroups. Despite these serious consequences and the wealth of available data,\ndehumanization has not yet been computationally studied on a large scale.\nDrawing upon social psychology research, we create a computational linguistic\nframework for analyzing dehumanizing language by identifying linguistic\ncorrelates of salient components of dehumanization. We then apply this\nframework to analyze discussions of LGBTQ people in the New York Times from\n1986 to 2015. Overall, we find increasingly humanizing descriptions of LGBTQ\npeople over time. However, we find that the label homosexual has emerged to be\nmuch more strongly associated with dehumanizing attitudes than other labels,\nsuch as gay. Our proposed techniques highlight processes of linguistic\nvariation and change in discourses surrounding marginalized groups.\nFurthermore, the ability to analyze dehumanizing language at a large scale has\nimplications for automatically detecting and understanding media bias as well\nas abusive language online.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.07456,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"HELFI: a Hebrew-Greek-Finnish Parallel Bible Corpus with Cross-Lingual\n  Morpheme Alignment\n\n  Twenty-five years ago, morphologically aligned Hebrew-Finnish and\nGreek-Finnish bitexts (texts accompanied by a translation) were constructed\nmanually in order to create an analytical concordance (Luoto et al., 1997) for\na Finnish Bible translation. The creators of the bitexts recently secured the\npublisher's permission to release its fine-grained alignment, but the alignment\nwas still dependent on proprietary, third-party resources such as a copyrighted\ntext edition and proprietary morphological analyses of the source texts. In\nthis paper, we describe a nontrivial editorial process starting from the\ncreation of the original one-purpose database and ending with its\nreconstruction using only freely available text editions and annotations. This\nprocess produced an openly available dataset that contains (i) the source texts\nand their translations, (ii) the morphological analyses, (iii) the\ncross-lingual morpheme alignments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.03716,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Investigating the Decoders of Maximum Likelihood Sequence Models: A\n  Look-ahead Approach\n\n  We demonstrate how we can practically incorporate multi-step future\ninformation into a decoder of maximum likelihood sequence models. We propose a\n\"k-step look-ahead\" module to consider the likelihood information of a rollout\nup to k steps. Unlike other approaches that need to train another value network\nto evaluate the rollouts, we can directly apply this look-ahead module to\nimprove the decoding of any sequence model trained in a maximum likelihood\nframework. We evaluate our look-ahead module on three datasets of varying\ndifficulties: IM2LATEX-100k OCR image to LaTeX, WMT16 multimodal machine\ntranslation, and WMT14 machine translation. Our look-ahead module improves the\nperformance of the simpler datasets such as IM2LATEX-100k and WMT16 multimodal\nmachine translation. However, the improvement of the more difficult dataset\n(e.g., containing longer sequences), WMT14 machine translation, becomes\nmarginal. Our further investigation using the k-step look-ahead suggests that\nthe more difficult tasks suffer from the overestimated EOS (end-of-sentence)\nprobability. We argue that the overestimated EOS probability also causes the\ndecreased performance of beam search when increasing its beam width. We tackle\nthe EOS problem by integrating an auxiliary EOS loss into the training to\nestimate if the model should emit EOS or other words. Our experiments show that\nimproving EOS estimation not only increases the performance of our proposed\nlook-ahead module but also the robustness of the beam search.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.09288,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000097354,
      "text":"FedNER: Privacy-preserving Medical Named Entity Recognition with\n  Federated Learning\n\n  Medical named entity recognition (NER) has wide applications in intelligent\nhealthcare. Sufficient labeled data is critical for training accurate medical\nNER model. However, the labeled data in a single medical platform is usually\nlimited. Although labeled datasets may exist in many different medical\nplatforms, they cannot be directly shared since medical data is highly\nprivacy-sensitive. In this paper, we propose a privacy-preserving medical NER\nmethod based on federated learning, which can leverage the labeled data in\ndifferent platforms to boost the training of medical NER model and remove the\nneed of exchanging raw data among different platforms. Since the labeled data\nin different platforms usually has some differences in entity type and\nannotation criteria, instead of constraining different platforms to share the\nsame model, we decompose the medical NER model in each platform into a shared\nmodule and a private module. The private module is used to capture the\ncharacteristics of the local data in each platform, and is updated using local\nlabeled data. The shared module is learned across different medical platform to\ncapture the shared NER knowledge. Its local gradients from different platforms\nare aggregated to update the global shared module, which is further delivered\nto each platform to update their local shared modules. Experiments on three\npublicly available datasets validate the effectiveness of our method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.13205,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000340078,
      "text":"Learning Contextualized Sentence Representations for Document-Level\n  Neural Machine Translation\n\n  Document-level machine translation incorporates inter-sentential dependencies\ninto the translation of a source sentence. In this paper, we propose a new\nframework to model cross-sentence dependencies by training neural machine\ntranslation (NMT) to predict both the target translation and surrounding\nsentences of a source sentence. By enforcing the NMT model to predict source\ncontext, we want the model to learn \"contextualized\" source sentence\nrepresentations that capture document-level dependencies on the source side. We\nfurther propose two different methods to learn and integrate such\ncontextualized sentence embeddings into NMT: a joint training method that\njointly trains an NMT model with the source context prediction model and a\npre-training & fine-tuning method that pretrains the source context prediction\nmodel on a large-scale monolingual document corpus and then fine-tunes it with\nthe NMT model. Experiments on Chinese-English and English-German translation\nshow that both methods can substantially improve the translation quality over a\nstrong document-level Transformer baseline.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.02197,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000034107,
      "text":"Evaluating Low-Resource Machine Translation between Chinese and\n  Vietnamese with Back-Translation\n\n  Back translation (BT) has been widely used and become one of standard\ntechniques for data augmentation in Neural Machine Translation (NMT), BT has\nproven to be helpful for improving the performance of translation effectively,\nespecially for low-resource scenarios. While most works related to BT mainly\nfocus on European languages, few of them study languages in other areas around\nthe world. In this paper, we investigate the impacts of BT on Asia language\ntranslations between the extremely low-resource Chinese and Vietnamese language\npair. We evaluate and compare the effects of different sizes of synthetic data\non both NMT and Statistical Machine Translation (SMT) models for Chinese to\nVietnamese and Vietnamese to Chinese, with character-based and word-based\nsettings. Some conclusions from previous works are partially confirmed and we\nalso draw some other interesting findings and conclusions, which are beneficial\nto understand BT further.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.08925,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"Utilizing Language Relatedness to improve Machine Translation: A Case\n  Study on Languages of the Indian Subcontinent\n\n  In this work, we present an extensive study of statistical machine\ntranslation involving languages of the Indian subcontinent. These languages are\nrelated by genetic and contact relationships. We describe the similarities\nbetween Indic languages arising from these relationships. We explore how\nlexical and orthographic similarity among these languages can be utilized to\nimprove translation quality between Indic languages when limited parallel\ncorpora is available. We also explore how the structural correspondence between\nIndic languages can be utilized to re-use linguistic resources for English to\nIndic language translation. Our observations span 90 language pairs from 9\nIndic languages and English. To the best of our knowledge, this is the first\nlarge-scale study specifically devoted to utilizing language relatedness to\nimprove translation between related languages.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.02955,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000051326,
      "text":"Automatic Compilation of Resources for Academic Writing and Evaluating\n  with Informal Word Identification and Paraphrasing System\n\n  We present the first approach to automatically building resources for\nacademic writing. The aim is to build a writing aid system that automatically\nedits a text so that it better adheres to the academic style of writing. On top\nof existing academic resources, such as the Corpus of Contemporary American\nEnglish (COCA) academic Word List, the New Academic Word List, and the Academic\nCollocation List, we also explore how to dynamically build such resources that\nwould be used to automatically identify informal or non-academic words or\nphrases. The resources are compiled using different generic approaches that can\nbe extended for different domains and languages. We describe the evaluation of\nresources with a system implementation. The system consists of an informal word\nidentification (IWI), academic candidate paraphrase generation, and paraphrase\nranking components. To generate candidates and rank them in context, we have\nused the PPDB and WordNet paraphrase resources. We use the Concepts in Context\n(CoInCO) \"All-Words\" lexical substitution dataset both for the informal word\nidentification and paraphrase generation experiments. Our informal word\nidentification component achieves an F-1 score of 82%, significantly\noutperforming a stratified classifier baseline. The main contribution of this\nwork is a domain-independent methodology to build targeted resources for\nwriting aids.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.02877,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Distill, Adapt, Distill: Training Small, In-Domain Models for Neural\n  Machine Translation\n\n  We explore best practices for training small, memory efficient machine\ntranslation models with sequence-level knowledge distillation in the domain\nadaptation setting. While both domain adaptation and knowledge distillation are\nwidely-used, their interaction remains little understood. Our large-scale\nempirical results in machine translation (on three language pairs with three\ndomains each) suggest distilling twice for best performance: once using\ngeneral-domain data and again using in-domain data with an adapted teacher.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.11867,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000025498,
      "text":"Improving Massively Multilingual Neural Machine Translation and\n  Zero-Shot Translation\n\n  Massively multilingual models for neural machine translation (NMT) are\ntheoretically attractive, but often underperform bilingual models and deliver\npoor zero-shot translations. In this paper, we explore ways to improve them. We\nargue that multilingual NMT requires stronger modeling capacity to support\nlanguage pairs with varying typological characteristics, and overcome this\nbottleneck via language-specific components and deepening NMT architectures. We\nidentify the off-target translation issue (i.e. translating into a wrong target\nlanguage) as the major source of the inferior zero-shot performance, and\npropose random online backtranslation to enforce the translation of unseen\ntraining language pairs. Experiments on OPUS-100 (a novel multilingual dataset\nwith 100 languages) show that our approach substantially narrows the\nperformance gap with bilingual models in both one-to-many and many-to-many\nsettings, and improves zero-shot performance by ~10 BLEU, approaching\nconventional pivot-based methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.05483,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Unsupervised Commonsense Question Answering with Self-Talk\n\n  Natural language understanding involves reading between the lines with\nimplicit background knowledge. Current systems either rely on pre-trained\nlanguage models as the sole implicit source of world knowledge, or resort to\nexternal knowledge bases (KBs) to incorporate additional relevant knowledge. We\npropose an unsupervised framework based on self-talk as a novel alternative to\nmultiple-choice commonsense tasks. Inspired by inquiry-based discovery learning\n(Bruner, 1961), our approach inquires language models with a number of\ninformation seeking questions such as \"$\\textit{what is the definition of\n...}$\" to discover additional background knowledge. Empirical results\ndemonstrate that the self-talk procedure substantially improves the performance\nof zero-shot language model baselines on four out of six commonsense\nbenchmarks, and competes with models that obtain knowledge from external KBs.\nWhile our approach improves performance on several benchmarks, the self-talk\ninduced knowledge even when leading to correct answers is not always seen as\nuseful by human judges, raising interesting questions about the inner-workings\nof pre-trained language models for commonsense reasoning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.06871,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000371204,
      "text":"TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented\n  Dialogue\n\n  The underlying difference of linguistic patterns between general text and\ntask-oriented dialogue makes existing pre-trained language models less useful\nin practice. In this work, we unify nine human-human and multi-turn\ntask-oriented dialogue datasets for language modeling. To better model dialogue\nbehavior during pre-training, we incorporate user and system tokens into the\nmasked language modeling. We propose a contrastive objective function to\nsimulate the response selection task. Our pre-trained task-oriented dialogue\nBERT (TOD-BERT) outperforms strong baselines like BERT on four downstream\ntask-oriented dialogue applications, including intention recognition, dialogue\nstate tracking, dialogue act prediction, and response selection. We also show\nthat TOD-BERT has a stronger few-shot ability that can mitigate the data\nscarcity problem for task-oriented dialogue.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.02346,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Domain-based Latent Personal Analysis and its use for impersonation\n  detection in social media\n\n  Zipf's law defines an inverse proportion between a word's ranking in a given\ncorpus and its frequency in it, roughly dividing the vocabulary into frequent\nwords and infrequent ones. Here, we stipulate that within a domain an author's\nsignature can be derived from, in loose terms, the author's missing popular\nwords and frequently used infrequent-words. We devise a method, termed Latent\nPersonal Analysis (LPA), for finding domain-based attributes for entities in a\ndomain: their distance from the domain and their signature, which determines\nhow they most differ from a domain. We identify the most suitable distance\nmetric for the method among several and construct the distances and personal\nsignatures for authors, the domain's entities. The signature consists of both\nover-used terms (compared to the average), and missing popular terms. We\nvalidate the correctness and power of the signatures in identifying users and\nset existence conditions. We then show uses for the method in explainable\nauthorship attribution: we define algorithms that utilize LPA to identify two\ntypes of impersonation in social media: (1) authors with sockpuppets (multiple)\naccounts; (2) front users accounts, operated by several authors. We validate\nthe algorithms and employ them over a large scale dataset obtained from a\nsocial media site with over 4000 users. We corroborate these results using\ntemporal rate analysis. LPA can further be used to devise personal attributes\nin a wide range of scientific domains in which the constituents have a\nlong-tail distribution of elements.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.03176,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Machine Translation with Unsupervised Length-Constraints\n\n  We have seen significant improvements in machine translation due to the usage\nof deep learning. While the improvements in translation quality are impressive,\nthe encoder-decoder architecture enables many more possibilities. In this\npaper, we explore one of these, the generation of constraint translation. We\nfocus on length constraints, which are essential if the translation should be\ndisplayed in a given format. In this work, we propose an end-to-end approach\nfor this task. Compared to a traditional method that first translates and then\nperforms sentence compression, the text compression is learned completely\nunsupervised. By combining the idea with zero-shot multilingual machine\ntranslation, we are also able to perform unsupervised monolingual sentence\ncompression. In order to fulfill the length constraints, we investigated\nseveral methods to integrate the constraints into the model. Using the\npresented technique, we are able to significantly improve the translation\nquality under constraints. Furthermore, we are able to perform unsupervised\nmonolingual sentence compression.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.14532,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000049008,
      "text":"Hierarchical Encoders for Modeling and Interpreting Screenplays\n\n  While natural language understanding of long-form documents is still an open\nchallenge, such documents often contain structural information that can inform\nthe design of models for encoding them. Movie scripts are an example of such\nrichly structured text - scripts are segmented into scenes, which are further\ndecomposed into dialogue and descriptive components. In this work, we propose a\nneural architecture for encoding this structure, which performs robustly on a\npair of multi-label tag classification datasets, without the need for\nhandcrafted features. We add a layer of insight by augmenting an unsupervised\n\"interpretability\" module to the encoder, allowing for the extraction and\nvisualization of narrative trajectories. Though this work specifically tackles\nscreenplays, we discuss how the underlying approach can be generalized to a\nrange of structured documents.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.13952,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000126494,
      "text":"Data Augmentation for Spoken Language Understanding via Pretrained\n  Language Models\n\n  The training of spoken language understanding (SLU) models often faces the\nproblem of data scarcity. In this paper, we put forward a data augmentation\nmethod using pretrained language models to boost the variability and accuracy\nof generated utterances. Furthermore, we investigate and propose solutions to\ntwo previously overlooked semi-supervised learning scenarios of data scarcity\nin SLU: i) Rich-in-Ontology: ontology information with numerous valid dialogue\nacts is given; ii) Rich-in-Utterance: a large number of unlabelled utterances\nare available. Empirical results show that our method can produce synthetic\ntraining data that boosts the performance of language understanding models in\nvarious scenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.03066,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000167886,
      "text":"Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature\n  and PRESupposition\n\n  Natural language inference (NLI) is an increasingly important task for\nnatural language understanding, which requires one to infer whether a sentence\nentails another. However, the ability of NLI models to make pragmatic\ninferences remains understudied. We create an IMPlicature and PRESupposition\ndiagnostic dataset (IMPPRES), consisting of >25k semiautomatically generated\nsentence pairs illustrating well-studied pragmatic inference types. We use\nIMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on\nMultiNLI (Williams et al., 2018) learn to make pragmatic inferences. Although\nMultiNLI appears to contain very few pairs illustrating these inference types,\nwe find that BERT learns to draw pragmatic inferences. It reliably treats\nscalar implicatures triggered by \"some\" as entailments. For some presupposition\ntriggers like \"only\", BERT reliably recognizes the presupposition as an\nentailment, even when the trigger is embedded under an entailment canceling\noperator like negation. BOW and InferSent show weaker evidence of pragmatic\nreasoning. We conclude that NLI training encourages models to learn some, but\nnot all, pragmatic inferences.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.05744,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000040068,
      "text":"Integrated Eojeol Embedding for Erroneous Sentence Classification in\n  Korean Chatbots\n\n  This paper attempts to analyze the Korean sentence classification system for\na chatbot. Sentence classification is the task of classifying an input sentence\nbased on predefined categories. However, spelling or space error contained in\nthe input sentence causes problems in morphological analysis and tokenization.\nThis paper proposes a novel approach of Integrated Eojeol (Korean syntactic\nword separated by space) Embedding to reduce the effect that poorly analyzed\nmorphemes may make on sentence classification. It also proposes two noise\ninsertion methods that further improve classification performance. Our\nevaluation results indicate that the proposed system classifies erroneous\nsentences more accurately than the baseline system by 17%p.0\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.0299,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000029802,
      "text":"Evaluating the Evaluation of Diversity in Natural Language Generation\n\n  Despite growing interest in natural language generation (NLG) models that\nproduce diverse outputs, there is currently no principled method for evaluating\nthe diversity of an NLG system. In this work, we propose a framework for\nevaluating diversity metrics. The framework measures the correlation between a\nproposed diversity metric and a diversity parameter, a single parameter that\ncontrols some aspect of diversity in generated text. For example, a diversity\nparameter might be a binary variable used to instruct crowdsourcing workers to\ngenerate text with either low or high content diversity. We demonstrate the\nutility of our framework by: (a) establishing best practices for eliciting\ndiversity judgments from humans, (b) showing that humans substantially\noutperform automatic metrics in estimating content diversity, and (c)\ndemonstrating that existing methods for controlling diversity by tuning a\n\"decoding parameter\" mostly affect form but not meaning. Our framework can\nadvance the understanding of different diversity metrics, an essential step on\nthe road towards better NLG systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.14626,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Rethinking Coherence Modeling: Synthetic vs. Downstream Tasks\n\n  Although coherence modeling has come a long way in developing novel models,\ntheir evaluation on downstream applications for which they are purportedly\ndeveloped has largely been neglected. With the advancements made by neural\napproaches in applications such as machine translation (MT), summarization and\ndialog systems, the need for coherence evaluation of these tasks is now more\ncrucial than ever. However, coherence models are typically evaluated only on\nsynthetic tasks, which may not be representative of their performance in\ndownstream applications. To investigate how representative the synthetic tasks\nare of downstream use cases, we conduct experiments on benchmarking well-known\ntraditional and neural coherence models on synthetic sentence ordering tasks,\nand contrast this with their performance on three downstream applications:\ncoherence evaluation for MT and summarization, and next utterance prediction in\nretrieval-based dialog. Our results demonstrate a weak correlation between the\nmodel performances in the synthetic tasks and the downstream applications,\n{motivating alternate training and evaluation methods for coherence models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.08076,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Neural Approaches for Data Driven Dependency Parsing in Sanskrit\n\n  Data-driven approaches for dependency parsing have been of great interest in\nNatural Language Processing for the past couple of decades. However, Sanskrit\nstill lacks a robust purely data-driven dependency parser, probably with an\nexception to Krishna (2019). This can primarily be attributed to the lack of\navailability of task-specific labelled data and the morphologically rich nature\nof the language. In this work, we evaluate four different data-driven machine\nlearning models, originally proposed for different languages, and compare their\nperformances on Sanskrit data. We experiment with 2 graph based and 2\ntransition based parsers. We compare the performance of each of the models in a\nlow-resource setting, with 1,500 sentences for training. Further, since our\nfocus is on the learning power of each of the models, we do not incorporate any\nSanskrit specific features explicitly into the models, and rather use the\ndefault settings in each of the paper for obtaining the feature functions. In\nthis work, we analyse the performance of the parsers using both an in-domain\nand an out-of-domain test dataset. We also investigate the impact of word\nordering in which the sentences are provided as input to these systems, by\nparsing verses and their corresponding prose order (anvaya) sentences.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.1408,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000588099,
      "text":"Modeling Long Context for Task-Oriented Dialogue State Generation\n\n  Based on the recently proposed transferable dialogue state generator (TRADE)\nthat predicts dialogue states from utterance-concatenated dialogue context, we\npropose a multi-task learning model with a simple yet effective utterance\ntagging technique and a bidirectional language model as an auxiliary task for\ntask-oriented dialogue state generation. By enabling the model to learn a\nbetter representation of the long dialogue context, our approaches attempt to\nsolve the problem that the performance of the baseline significantly drops when\nthe input dialogue context sequence is long. In our experiments, our proposed\nmodel achieves a 7.03% relative improvement over the baseline, establishing a\nnew state-of-the-art joint goal accuracy of 52.04% on the MultiWOZ 2.0 dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.06176,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"AREDSUM: Adaptive Redundancy-Aware Iterative Sentence Ranking for\n  Extractive Document Summarization\n\n  Redundancy-aware extractive summarization systems score the redundancy of the\nsentences to be included in a summary either jointly with their salience\ninformation or separately as an additional sentence scoring step. Previous work\nshows the efficacy of jointly scoring and selecting sentences with neural\nsequence generation models. It is, however, not well-understood if the gain is\ndue to better encoding techniques or better redundancy reduction approaches.\nSimilarly, the contribution of salience versus diversity components on the\ncreated summary is not studied well. Building on the state-of-the-art encoding\nmethods for summarization, we present two adaptive learning models: AREDSUM-SEQ\nthat jointly considers salience and novelty during sentence selection; and a\ntwo-step AREDSUM-CTX that scores salience first, then learns to balance\nsalience and redundancy, enabling the measurement of the impact of each aspect.\nEmpirical results on CNN\/DailyMail and NYT50 datasets show that by modeling\ndiversity explicitly in a separate step, AREDSUM-CTX achieves significantly\nbetter performance than AREDSUM-SEQ as well as state-of-the-art extractive\nsummarization baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.15016,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words\n  in Context\n\n  We present WiC-TSV, a new multi-domain evaluation benchmark for Word Sense\nDisambiguation. More specifically, we introduce a framework for Target Sense\nVerification of Words in Context which grounds its uniqueness in the\nformulation as a binary classification task thus being independent of external\nsense inventories, and the coverage of various domains. This makes the dataset\nhighly flexible for the evaluation of a diverse set of models and systems in\nand across domains. WiC-TSV provides three different evaluation settings,\ndepending on the input signals provided to the model. We set baseline\nperformance on the dataset using state-of-the-art language models. Experimental\nresults show that even though these models can perform decently on the task,\nthere remains a gap between machine and human performance, especially in\nout-of-domain settings. WiC-TSV data is available at\nhttps:\/\/competitions.codalab.org\/competitions\/23683\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.03875,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"Diverse, Controllable, and Keyphrase-Aware: A Corpus and Method for News\n  Multi-Headline Generation\n\n  News headline generation aims to produce a short sentence to attract readers\nto read the news. One news article often contains multiple keyphrases that are\nof interest to different users, which can naturally have multiple reasonable\nheadlines. However, most existing methods focus on the single headline\ngeneration. In this paper, we propose generating multiple headlines with\nkeyphrases of user interests, whose main idea is to generate multiple\nkeyphrases of interest to users for the news first, and then generate multiple\nkeyphrase-relevant headlines. We propose a multi-source Transformer decoder,\nwhich takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered\narticle, and (c) original article to generate keyphrase-relevant, high-quality,\nand diverse headlines. Furthermore, we propose a simple and effective method to\nmine the keyphrases of interest in the news article and build a first\nlarge-scale keyphrase-aware news headline corpus, which contains over 180K\naligned triples of $<$news article, headline, keyphrase$>$. Extensive\nexperimental comparisons on the real-world dataset show that the proposed\nmethod achieves state-of-the-art results in terms of quality and diversity\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.10171,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000109606,
      "text":"Knowledge Distillation for Multilingual Unsupervised Neural Machine\n  Translation\n\n  Unsupervised neural machine translation (UNMT) has recently achieved\nremarkable results for several language pairs. However, it can only translate\nbetween a single language pair and cannot produce translation results for\nmultiple language pairs at the same time. That is, research on multilingual\nUNMT has been limited. In this paper, we empirically introduce a simple method\nto translate between thirteen languages using a single encoder and a single\ndecoder, making use of multilingual data to improve UNMT for all language\npairs. On the basis of the empirical findings, we propose two knowledge\ndistillation methods to further enhance multilingual UNMT performance. Our\nexperiments on a dataset with English translated to and from twelve other\nlanguages (including three language families and six language branches) show\nremarkable results, surpassing strong unsupervised individual baselines while\nachieving promising performance between non-English language pairs in zero-shot\ntranslation scenarios and alleviating poor performance in low-resource language\npairs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.12651,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000258287,
      "text":"Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less\n  Forgetting\n\n  Deep pretrained language models have achieved great success in the way of\npretraining first and then fine-tuning. But such a sequential transfer learning\nparadigm often confronts the catastrophic forgetting problem and leads to\nsub-optimal performance. To fine-tune with less forgetting, we propose a recall\nand learn mechanism, which adopts the idea of multi-task learning and jointly\nlearns pretraining tasks and downstream tasks. Specifically, we propose a\nPretraining Simulation mechanism to recall the knowledge from pretraining tasks\nwithout data, and an Objective Shifting mechanism to focus the learning on\ndownstream tasks gradually. Experiments show that our method achieves\nstate-of-the-art performance on the GLUE benchmark. Our method also enables\nBERT-base to achieve better performance than directly fine-tuning of\nBERT-large. Further, we provide the open-source RecAdam optimizer, which\nintegrates the proposed mechanisms into Adam optimizer, to facility the NLP\ncommunity.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.04295,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"Severing the Edge Between Before and After: Neural Architectures for\n  Temporal Ordering of Events\n\n  In this paper, we propose a neural architecture and a set of training methods\nfor ordering events by predicting temporal relations. Our proposed models\nreceive a pair of events within a span of text as input and they identify\ntemporal relations (Before, After, Equal, Vague) between them. Given that a key\nchallenge with this task is the scarcity of annotated data, our models rely on\neither pretrained representations (i.e. RoBERTa, BERT or ELMo), transfer and\nmulti-task learning (by leveraging complementary datasets), and self-training\ntechniques. Experiments on the MATRES dataset of English documents establish a\nnew state-of-the-art on this task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.00036,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"Improving Factual Consistency Between a Response and Persona Facts\n\n  Neural models for response generation produce responses that are semantically\nplausible but not necessarily factually consistent with facts describing the\nspeaker's persona. These models are trained with fully supervised learning\nwhere the objective function barely captures factual consistency. We propose to\nfine-tune these models by reinforcement learning and an efficient reward\nfunction that explicitly captures the consistency between a response and\npersona facts as well as semantic plausibility. Our automatic and human\nevaluations on the PersonaChat corpus confirm that our approach increases the\nrate of responses that are factually consistent with persona facts over its\nsupervised counterpart while retaining the language quality of responses.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.06588,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000035101,
      "text":"PERLEX: A Bilingual Persian-English Gold Dataset for Relation Extraction\n\n  Relation extraction is the task of extracting semantic relations between\nentities in a sentence. It is an essential part of some natural language\nprocessing tasks such as information extraction, knowledge extraction, and\nknowledge base population. The main motivations of this research stem from a\nlack of a dataset for relation extraction in the Persian language as well as\nthe necessity of extracting knowledge from the growing big-data in the Persian\nlanguage for different applications. In this paper, we present \"PERLEX\" as the\nfirst Persian dataset for relation extraction, which is an expert-translated\nversion of the \"Semeval-2010-Task-8\" dataset. Moreover, this paper addresses\nPersian relation extraction utilizing state-of-the-art language-agnostic\nalgorithms. We employ six different models for relation extraction on the\nproposed bilingual dataset, including a non-neural model (as the baseline),\nthree neural models, and two deep learning models fed by multilingual-BERT\ncontextual word representations. The experiments result in the maximum f-score\n77.66% (provided by BERTEM-MTB method) as the state-of-the-art of relation\nextraction in the Persian language.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.00672,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000095699,
      "text":"DagoBERT: Generating Derivational Morphology with a Pretrained Language\n  Model\n\n  Can pretrained language models (PLMs) generate derivationally complex words?\nWe present the first study investigating this question, taking BERT as the\nexample PLM. We examine BERT's derivational capabilities in different settings,\nranging from using the unmodified pretrained model to full finetuning. Our best\nmodel, DagoBERT (Derivationally and generatively optimized BERT), clearly\noutperforms the previous state of the art in derivation generation (DG).\nFurthermore, our experiments show that the input segmentation crucially impacts\nBERT's derivational knowledge, suggesting that the performance of PLMs could be\nfurther improved if a morphologically informed vocabulary of units were used.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.08417,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"Syntax-guided Controlled Generation of Paraphrases\n\n  Given a sentence (e.g., \"I like mangoes\") and a constraint (e.g., sentiment\nflip), the goal of controlled text generation is to produce a sentence that\nadapts the input sentence to meet the requirements of the constraint (e.g., \"I\nhate mangoes\"). Going beyond such simple constraints, recent works have started\nexploring the incorporation of complex syntactic-guidance as constraints in the\ntask of controlled paraphrase generation. In these methods, syntactic-guidance\nis sourced from a separate exemplar sentence. However, these prior works have\nonly utilized limited syntactic information available in the parse tree of the\nexemplar sentence. We address this limitation in the paper and propose Syntax\nGuided Controlled Paraphraser (SGCP), an end-to-end framework for syntactic\nparaphrase generation. We find that SGCP can generate syntax conforming\nsentences while not compromising on relevance. We perform extensive automated\nand human evaluations over multiple real-world English language datasets to\ndemonstrate the efficacy of SGCP over state-of-the-art baselines. To drive\nfuture research, we have made SGCP's source code available\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.12889,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000141064,
      "text":"Refining Implicit Argument Annotation for UCCA\n\n  Predicate-argument structure analysis is a central component in meaning\nrepresentations of text. The fact that some arguments are not explicitly\nmentioned in a sentence gives rise to ambiguity in language understanding, and\nrenders it difficult for machines to interpret text correctly. However, only\nfew resources represent implicit roles for NLU, and existing studies in NLP\nonly make coarse distinctions between categories of arguments omitted from\nlinguistic form. This paper proposes a typology for fine-grained implicit\nargument annotation on top of Universal Conceptual Cognitive Annotation's\nfoundational layer. The proposed implicit argument categorisation is driven by\ntheories of implicit role interpretation and consists of six types: Deictic,\nGeneric, Genre-based, Type-identifiable, Non-specific, and Iterated-set. We\nexemplify our design by revisiting part of the UCCA EWT corpus, providing a new\ndataset annotated with the refinement layer, and making a comparative analysis\nwith other schemes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.06383,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000024504,
      "text":"Sanskrit Segmentation Revisited\n\n  Computationally analyzing Sanskrit texts requires proper segmentation in the\ninitial stages. There have been various tools developed for Sanskrit text\nsegmentation. Of these, G\\'erard Huet's Reader in the Sanskrit Heritage Engine\nanalyzes the input text and segments it based on the word parameters - phases\nlike iic, ifc, Pr, Subst, etc., and sandhi (or transition) that takes place at\nthe end of a word with the initial part of the next word. And it enlists all\nthe possible solutions differentiating them with the help of the phases. The\nphases and their analyses have their use in the domain of sentential parsers.\nIn segmentation, though, they are not used beyond deciding whether the words\nformed with the phases are morphologically valid. This paper tries to modify\nthe above segmenter by ignoring the phase details (except for a few cases), and\nalso proposes a probability function to prioritize the list of solutions to\nbring up the most valid solutions at the top.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.10232,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000060598,
      "text":"Sentence level estimation of psycholinguistic norms using joint\n  multidimensional annotations\n\n  Psycholinguistic normatives represent various affective and mental constructs\nusing numeric scores and are used in a variety of applications in natural\nlanguage processing. They are commonly used at the sentence level, the scores\nof which are estimated by extrapolating word level scores using simple\naggregation strategies, which may not always be optimal. In this work, we\npresent a novel approach to estimate the psycholinguistic norms at sentence\nlevel. We apply a multidimensional annotation fusion model on annotations at\nthe word level to estimate a parameter which captures relationships between\ndifferent norms. We then use this parameter at sentence level to estimate the\nnorms. We evaluate our approach by predicting sentence level scores for various\nnormative dimensions and compare with standard word aggregation schemes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.02324,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000223849,
      "text":"Neural CRF Model for Sentence Alignment in Text Simplification\n\n  The success of a text simplification system heavily depends on the quality\nand quantity of complex-simple sentence pairs in the training corpus, which are\nextracted by aligning sentences between parallel articles. To evaluate and\nimprove sentence alignment quality, we create two manually annotated\nsentence-aligned datasets from two commonly used text simplification corpora,\nNewsela and Wikipedia. We propose a novel neural CRF alignment model which not\nonly leverages the sequential nature of sentences in parallel documents but\nalso utilizes a neural sentence pair model to capture semantic similarity.\nExperiments demonstrate that our proposed approach outperforms all the previous\nwork on monolingual sentence alignment task by more than 5 points in F1. We\napply our CRF aligner to construct two new text simplification datasets,\nNewsela-Auto and Wiki-Auto, which are much larger and of better quality\ncompared to the existing datasets. A Transformer-based seq2seq model trained on\nour datasets establishes a new state-of-the-art for text simplification in both\nautomatic and human evaluation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.11838,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000047353,
      "text":"How Does That Sound? Multi-Language SpokenName2Vec Algorithm Using\n  Speech Generation and Deep Learning\n\n  Searching for information about a specific person is an online activity\nfrequently performed by many users. In most cases, users are aided by queries\ncontaining a name and sending back to the web search engines for finding their\nwill. Typically, Web search engines provide just a few accurate results\nassociated with a name-containing query. Currently, most solutions for\nsuggesting synonyms in online search are based on pattern matching and phonetic\nencoding, however very often, the performance of such solutions is less than\noptimal. In this paper, we propose SpokenName2Vec, a novel and generic approach\nwhich addresses the similar name suggestion problem by utilizing automated\nspeech generation, and deep learning to produce spoken name embeddings. This\nsophisticated and innovative embeddings captures the way people pronounce names\nin any language and accent. Utilizing the name pronunciation can be helpful for\nboth differentiating and detecting names that sound alike, but are written\ndifferently. The proposed approach was demonstrated on a large-scale dataset\nconsisting of 250,000 forenames and evaluated using a machine learning\nclassifier and 7,399 names with their verified synonyms. The performance of the\nproposed approach was found to be superior to 10 other algorithms evaluated in\nthis study, including well used phonetic and string similarity algorithms, and\ntwo recently proposed algorithms. The results obtained suggest that the\nproposed approach could serve as a useful and valuable tool for solving the\nsimilar name suggestion problem.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.00247,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000033445,
      "text":"AdapterFusion: Non-Destructive Task Composition for Transfer Learning\n\n  Sequential fine-tuning and multi-task learning are methods aiming to\nincorporate knowledge from multiple tasks; however, they suffer from\ncatastrophic forgetting and difficulties in dataset balancing. To address these\nshortcomings, we propose AdapterFusion, a new two stage learning algorithm that\nleverages knowledge from multiple tasks. First, in the knowledge extraction\nstage we learn task specific parameters called adapters, that encapsulate the\ntask-specific information. We then combine the adapters in a separate knowledge\ncomposition step. We show that by separating the two stages, i.e., knowledge\nextraction and knowledge composition, the classifier can effectively exploit\nthe representations learned from multiple tasks in a non-destructive manner. We\nempirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it\neffectively combines various types of knowledge at different layers of the\nmodel. We show that our approach outperforms traditional strategies such as\nfull fine-tuning as well as multi-task learning. Our code and adapters are\navailable at AdapterHub.ml.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.04044,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Literature Triage on Genomic Variation Publications by\n  Knowledge-enhanced Multi-channel CNN\n\n  Background: To investigate the correlation between genomic variation and\ncertain diseases or phenotypes, the fundamental task is to screen out the\nconcerning publications from massive literature, which is called literature\ntriage. Some knowledge bases, including UniProtKB\/Swiss-Prot and NHGRI-EBI GWAS\nCatalog are created for collecting concerning publications. These publications\nare manually curated by experts, which is time-consuming. Moreover, the manual\ncuration of information from literature is not scalable due to the rapidly\nincreasing amount of publications. In order to cut down the cost of literature\ntriage, machine-learning models were adopted to automatically identify\nbiomedical publications. Methods: Comparing to previous studies utilizing\nmachine-learning models for literature triage, we adopt a multi-channel\nconvolutional network to utilize rich textual information and meanwhile bridge\nthe semantic gaps from different corpora. In addition, knowledge embeddings\nlearned from UMLS is also used to provide extra medical knowledge beyond\ntextual features in the process of triage. Results: We demonstrate that our\nmodel outperforms the state-of-the-art models over 5 datasets with the help of\nknowledge embedding and multiple channels. Our model improves the accuracy of\nbiomedical literature triage results. Conclusions: Multiple channels and\nknowledge embeddings enhance the performance of the CNN model in the task of\nbiomedical literature triage. Keywords: Literature Triage; Knowledge Embedding;\nMulti-channel Convolutional Network\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.11768,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000268552,
      "text":"KaLM at SemEval-2020 Task 4: Knowledge-aware Language Models for\n  Comprehension And Generation\n\n  This paper presents our strategies in SemEval 2020 Task 4: Commonsense\nValidation and Explanation. We propose a novel way to search for evidence and\nchoose the different large-scale pre-trained models as the backbone for three\nsubtasks. The results show that our evidence-searching approach improves model\nperformance on commonsense explanation task. Our team ranks 2nd in subtask C\naccording to human evaluation score.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.13756,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000067552,
      "text":"The SIGMORPHON 2020 Shared Task on Unsupervised Morphological Paradigm\n  Completion\n\n  In this paper, we describe the findings of the SIGMORPHON 2020 shared task on\nunsupervised morphological paradigm completion (SIGMORPHON 2020 Task 2), a\nnovel task in the field of inflectional morphology. Participants were asked to\nsubmit systems which take raw text and a list of lemmas as input, and output\nall inflected forms, i.e., the entire morphological paradigm, of each lemma. In\norder to simulate a realistic use case, we first released data for 5\ndevelopment languages. However, systems were officially evaluated on 9 surprise\nlanguages, which were only revealed a few days before the submission deadline.\nWe provided a modular baseline system, which is a pipeline of 4 components. 3\nteams submitted a total of 7 systems, but, surprisingly, none of the submitted\nsystems was able to improve over the baseline on average over all 9 test\nlanguages. Only on 3 languages did a submitted system obtain the best results.\nThis shows that unsupervised morphological paradigm completion is still largely\nunsolved. We present an analysis here, so that this shared task will ground\nfurther research on the topic.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.00614,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000060598,
      "text":"Multi-Dimensional Gender Bias Classification\n\n  Machine learning models are trained to find patterns in data. NLP models can\ninadvertently learn socially undesirable patterns when training on gender\nbiased text. In this work, we propose a general framework that decomposes\ngender bias in text along several pragmatic and semantic dimensions: bias from\nthe gender of the person being spoken about, bias from the gender of the person\nbeing spoken to, and bias from the gender of the speaker. Using this\nfine-grained framework, we automatically annotate eight large scale datasets\nwith gender information. In addition, we collect a novel, crowdsourced\nevaluation benchmark of utterance-level gender rewrites. Distinguishing between\ngender bias along multiple dimensions is important, as it enables us to train\nfiner-grained gender bias classifiers. We show our classifiers prove valuable\nfor a variety of important applications, such as controlling for gender bias in\ngenerative models, detecting gender bias in arbitrary text, and shed light on\noffensive language in terms of genderedness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.0119,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Influence Paths for Characterizing Subject-Verb Number Agreement in LSTM\n  Language Models\n\n  LSTM-based recurrent neural networks are the state-of-the-art for many\nnatural language processing (NLP) tasks. Despite their performance, it is\nunclear whether, or how, LSTMs learn structural features of natural languages\nsuch as subject-verb number agreement in English. Lacking this understanding,\nthe generality of LSTM performance on this task and their suitability for\nrelated tasks remains uncertain. Further, errors cannot be properly attributed\nto a lack of structural capability, training data omissions, or other\nexceptional faults. We introduce *influence paths*, a causal account of\nstructural properties as carried by paths across gates and neurons of a\nrecurrent neural network. The approach refines the notion of influence (the\nsubject's grammatical number has influence on the grammatical number of the\nsubsequent verb) into a set of gate or neuron-level paths. The set localizes\nand segments the concept (e.g., subject-verb agreement), its constituent\nelements (e.g., the subject), and related or interfering elements (e.g.,\nattractors). We exemplify the methodology on a widely-studied multi-layer LSTM\nlanguage model, demonstrating its accounting for subject-verb number agreement.\nThe results offer both a finer and a more complete view of an LSTM's handling\nof this structural aspect of the English language than prior results based on\ndiagnostic classifiers and ablation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.01027,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000029802,
      "text":"A Position Aware Decay Weighted Network for Aspect based Sentiment\n  Analysis\n\n  Aspect Based Sentiment Analysis (ABSA) is the task of identifying sentiment\npolarity of a text given another text segment or aspect. In ABSA, a text can\nhave multiple sentiments depending upon each aspect. Aspect Term Sentiment\nAnalysis (ATSA) is a subtask of ABSA, in which aspect terms are contained\nwithin the given sentence. Most of the existing approaches proposed for ATSA,\nincorporate aspect information through a different subnetwork thereby\noverlooking the advantage of aspect terms' presence within the sentence. In\nthis paper, we propose a model that leverages the positional information of the\naspect. The proposed model introduces a decay mechanism based on position. This\ndecay function mandates the contribution of input words for ABSA. The\ncontribution of a word declines as farther it is positioned from the aspect\nterms in the sentence. The performance is measured on two standard datasets\nfrom SemEval 2014 Task 4. In comparison with recent architectures, the\neffectiveness of the proposed model is demonstrated.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.0077,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000383125,
      "text":"Exploring and Predicting Transferability across NLP Tasks\n\n  Recent advances in NLP demonstrate the effectiveness of training large-scale\nlanguage models and transferring them to downstream tasks. Can fine-tuning\nthese models on tasks other than language modeling further improve performance?\nIn this paper, we conduct an extensive study of the transferability between 33\nNLP tasks across three broad classes of problems (text classification, question\nanswering, and sequence labeling). Our results show that transfer learning is\nmore beneficial than previously thought, especially when target task data is\nscarce, and can improve performance even when the source task is small or\ndiffers substantially from the target task (e.g., part-of-speech tagging\ntransfers well to the DROP QA dataset). We also develop task embeddings that\ncan be used to predict the most transferable source tasks for a given target\ntask, and we validate their effectiveness in experiments controlled for source\nand target data size. Overall, our experiments reveal that factors such as\nsource data size, task and domain similarity, and task complexity all play a\nrole in determining transferability.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.06579,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000037087,
      "text":"Document-Level Event Role Filler Extraction using Multi-Granularity\n  Contextualized Encoding\n\n  Few works in the literature of event extraction have gone beyond individual\nsentences to make extraction decisions. This is problematic when the\ninformation needed to recognize an event argument is spread across multiple\nsentences. We argue that document-level event extraction is a difficult task\nsince it requires a view of a larger context to determine which spans of text\ncorrespond to event role fillers. We first investigate how end-to-end neural\nsequence models (with pre-trained language model representations) perform on\ndocument-level role filler extraction, as well as how the length of context\ncaptured affects the models' performance. To dynamically aggregate information\ncaptured by neural representations learned at different levels of granularity\n(e.g., the sentence- and paragraph-level), we propose a novel multi-granularity\nreader. We evaluate our models on the MUC-4 event extraction dataset, and show\nthat our best system performs substantially better than prior work. We also\nreport findings on the relationship between context length and neural model\nperformance on the task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.12898,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000196033,
      "text":"Analysis of the Penn Korean Universal Dependency Treebank (PKT-UD):\n  Manual Revision to Build Robust Parsing Model in Korean\n\n  In this paper, we first open on important issues regarding the Penn Korean\nUniversal Treebank (PKT-UD) and address these issues by revising the entire\ncorpus manually with the aim of producing cleaner UD annotations that are more\nfaithful to Korean grammar. For compatibility to the rest of UD corpora, we\nfollow the UDv2 guidelines, and extensively revise the part-of-speech tags and\nthe dependency relations to reflect morphological features and flexible\nword-order aspects in Korean. The original and the revised versions of PKT-UD\nare experimented with transformer-based parsing models using biaffine\nattention. The parsing model trained on the revised corpus shows a significant\nimprovement of 3.0% in labeled attachment score over the model trained on the\nprevious corpus. Our error analysis demonstrates that this revision allows the\nparsing model to learn relations more robustly, reducing several critical\nerrors that used to be made by the previous model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.10678,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000023842,
      "text":"Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in\n  Multitask End-to-End Speech Translation\n\n  Speech translation (ST) aims to learn transformations from speech in the\nsource language to the text in the target language. Previous works show that\nmultitask learning improves the ST performance, in which the recognition\ndecoder generates the text of the source language, and the translation decoder\nobtains the final translations based on the output of the recognition decoder.\nBecause whether the output of the recognition decoder has the correct semantics\nis more critical than its accuracy, we propose to improve the multitask ST\nmodel by utilizing word embedding as the intermediate.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.0121,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000540747,
      "text":"The 'Letter' Distribution in the Chinese Language\n\n  Corpus-based statistical analysis plays a significant role in linguistic\nresearch, and ample evidence has shown that different languages exhibit some\ncommon laws. Studies have found that letters in some alphabetic writing\nlanguages have strikingly similar statistical usage frequency distributions.\nDoes this hold for Chinese, which employs ideogram writing? We obtained letter\nfrequency data of some alphabetic writing languages and found the common law of\nthe letter distributions. In addition, we collected Chinese literature corpora\nfor different historical periods from the Tang Dynasty to the present, and we\ndismantled the Chinese written language into three kinds of basic particles:\ncharacters, strokes and constructive parts. The results of the statistical\nanalysis showed that, in different historical periods, the intensity of the use\nof basic particles in Chinese writing varied, but the form of the distribution\nwas consistent. In particular, the distributions of the Chinese constructive\nparts are certainly consistent with those alphabetic writing languages. This\nstudy provides new evidence of the consistency of human languages.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.01189,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000036756,
      "text":"An Effective Contextual Language Modeling Framework for Speech\n  Summarization with Augmented Features\n\n  Tremendous amounts of multimedia associated with speech information are\ndriving an urgent need to develop efficient and effective automatic\nsummarization methods. To this end, we have seen rapid progress in applying\nsupervised deep neural network-based methods to extractive speech\nsummarization. More recently, the Bidirectional Encoder Representations from\nTransformers (BERT) model was proposed and has achieved record-breaking success\non many natural language processing (NLP) tasks such as question answering and\nlanguage understanding. In view of this, we in this paper contextualize and\nenhance the state-of-the-art BERT-based model for speech summarization, while\nits contributions are at least three-fold. First, we explore the incorporation\nof confidence scores into sentence representations to see if such an attempt\ncould help alleviate the negative effects caused by imperfect automatic speech\nrecognition (ASR). Secondly, we also augment the sentence embeddings obtained\nfrom BERT with extra structural and linguistic features, such as sentence\nposition and inverse document frequency (IDF) statistics. Finally, we validate\nthe effectiveness of our proposed method on a benchmark dataset, in comparison\nto several classic and celebrated speech summarization methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.15454,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000079804,
      "text":"A Deep Reinforced Model for Zero-Shot Cross-Lingual Summarization with\n  Bilingual Semantic Similarity Rewards\n\n  Cross-lingual text summarization aims at generating a document summary in one\nlanguage given input in another language. It is a practically important but\nunder-explored task, primarily due to the dearth of available data. Existing\nmethods resort to machine translation to synthesize training data, but such\npipeline approaches suffer from error propagation. In this work, we propose an\nend-to-end cross-lingual text summarization model. The model uses reinforcement\nlearning to directly optimize a bilingual semantic similarity metric between\nthe summaries generated in a target language and gold summaries in a source\nlanguage. We also introduce techniques to pre-train the model leveraging\nmonolingual summarization and machine translation objectives. Experimental\nresults in both English--Chinese and English--German cross-lingual\nsummarization settings demonstrate the effectiveness of our methods. In\naddition, we find that reinforcement learning models with bilingual semantic\nsimilarity as rewards generate more fluent sentences than strong baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.14209,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Automatic Domain Adaptation Outperforms Manual Domain Adaptation for\n  Predicting Financial Outcomes\n\n  In this paper, we automatically create sentiment dictionaries for predicting\nfinancial outcomes. We compare three approaches: (I) manual adaptation of the\ndomain-general dictionary H4N, (ii) automatic adaptation of H4N and (iii) a\ncombination consisting of first manual, then automatic adaptation. In our\nexperiments, we demonstrate that the automatically adapted sentiment dictionary\noutperforms the previous state of the art in predicting the financial outcomes\nexcess return and volatility. In particular, automatic adaptation performs\nbetter than manual adaptation. In our analysis, we find that annotation based\non an expert's a priori belief about a word's meaning can be incorrect --\nannotation should be performed based on the word's contexts in the target\ndomain instead.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.00838,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000143713,
      "text":"Efficient EUD Parsing\n\n  We present the system submission from the FASTPARSE team for the EUD Shared\nTask at IWPT 2020. We engaged with the task by focusing on efficiency. For this\nwe considered training costs and inference efficiency. Our models are a\ncombination of distilled neural dependency parsers and a rule-based system that\nprojects UD trees into EUD graphs. We obtained an average ELAS of 74.04 for our\nofficial submission, ranking 4th overall.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.00995,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000071194,
      "text":"Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals\n\n  A growing body of work makes use of probing to investigate the working of\nneural models, often considered black boxes. Recently, an ongoing debate\nemerged surrounding the limitations of the probing paradigm. In this work, we\npoint out the inability to infer behavioral conclusions from probing results\nand offer an alternative method that focuses on how the information is being\nused, rather than on what information is encoded. Our method, Amnesic Probing,\nfollows the intuition that the utility of a property for a given task can be\nassessed by measuring the influence of a causal intervention that removes it\nfrom the representation. Equipped with this new analysis tool, we can ask\nquestions that were not possible before, e.g. is part-of-speech information\nimportant for word prediction? We perform a series of analyses on BERT to\nanswer these types of questions. Our findings demonstrate that conventional\nprobing performance is not correlated to task importance, and we call for\nincreased scrutiny of claims that draw behavioral or causal conclusions from\nprobing results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.05003,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Universal Vector Neural Machine Translation With Effective Attention\n\n  Neural Machine Translation (NMT) leverages one or more trained neural\nnetworks for the translation of phrases. Sutskever introduced a sequence to\nsequence based encoder-decoder model which became the standard for NMT based\nsystems. Attention mechanisms were later introduced to address the issues with\nthe translation of long sentences and improving overall accuracy. In this\npaper, we propose a singular model for Neural Machine Translation based on\nencoder-decoder models. Most translation models are trained as one model for\none translation. We introduce a neutral\/universal model representation that can\nbe used to predict more than one language depending on the source and a\nprovided target. Secondly, we introduce an attention model by adding an overall\nlearning vector to the multiplicative model. With these two changes, by using\nthe novel universal model the number of models needed for multiple language\ntranslation applications are reduced.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.03866,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"A Cross-Task Analysis of Text Span Representations\n\n  Many natural language processing (NLP) tasks involve reasoning with textual\nspans, including question answering, entity recognition, and coreference\nresolution. While extensive research has focused on functional architectures\nfor representing words and sentences, there is less work on representing\narbitrary spans of text within sentences. In this paper, we conduct a\ncomprehensive empirical evaluation of six span representation methods using\neight pretrained language representation models across six tasks, including two\ntasks that we introduce. We find that, although some simple span\nrepresentations are fairly reliable across tasks, in general the optimal span\nrepresentation varies by task, and can also vary within different facets of\nindividual tasks. We also find that the choice of span representation has a\nbigger impact with a fixed pretrained encoder than with a fine-tuned encoder.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.1373,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Attention-Based Neural Networks for Sentiment Attitude Extraction using\n  Distant Supervision\n\n  In the sentiment attitude extraction task, the aim is to identify\n<<attitudes>> -- sentiment relations between entities mentioned in text. In\nthis paper, we provide a study on attention-based context encoders in the\nsentiment attitude extraction task. For this task, we adapt attentive context\nencoders of two types: (1) feature-based; (2) self-based. In our study, we\nutilize the corpus of Russian analytical texts RuSentRel and automatically\nconstructed news collection RuAttitudes for enriching the training set. We\nconsider the problem of attitude extraction as two-class (positive, negative)\nand three-class (positive, negative, neutral) classification tasks for whole\ndocuments. Our experiments with the RuSentRel corpus show that the three-class\nclassification models, which employ the RuAttitudes corpus for training, result\nin 10% increase and extra 3% by F1, when model architectures include the\nattention mechanism. We also provide the analysis of attention weight\ndistributions in dependence on the term type.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.14198,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0,
      "text":"A Simple Approach to Case-Based Reasoning in Knowledge Bases\n\n  We present a surprisingly simple yet accurate approach to reasoning in\nknowledge graphs (KGs) that requires \\emph{no training}, and is reminiscent of\ncase-based reasoning in classical artificial intelligence (AI). Consider the\ntask of finding a target entity given a source entity and a binary relation.\nOur non-parametric approach derives crisp logical rules for each query by\nfinding multiple \\textit{graph path patterns} that connect similar source\nentities through the given relation. Using our method, we obtain new\nstate-of-the-art accuracy, outperforming all previous models, on NELL-995 and\nFB-122. We also demonstrate that our model is robust in low data settings,\noutperforming recently proposed meta-learning approaches\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.04721,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0001160304,
      "text":"Modeling Discourse Structure for Document-level Neural Machine\n  Translation\n\n  Recently, document-level neural machine translation (NMT) has become a hot\ntopic in the community of machine translation. Despite its success, most of\nexisting studies ignored the discourse structure information of the input\ndocument to be translated, which has shown effective in other tasks. In this\npaper, we propose to improve document-level NMT with the aid of discourse\nstructure information. Our encoder is based on a hierarchical attention network\n(HAN). Specifically, we first parse the input document to obtain its discourse\nstructure. Then, we introduce a Transformer-based path encoder to embed the\ndiscourse structure information of each word. Finally, we combine the discourse\nstructure information with the word embedding before it is fed into the\nencoder. Experimental results on the English-to-German dataset show that our\nmodel can significantly outperform both Transformer and Transformer+HAN.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.043,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000032451,
      "text":"Normalizador Neural de Datas e Endere\\c{c}os\n\n  Documents of any kind present a wide variety of date and address formats, in\nsome cases dates can be written entirely in full or even have different types\nof separators. The pattern disorder in addresses is even greater due to the\ngreater possibility of interchanging between streets, neighborhoods, cities and\nstates. In the context of natural language processing, problems of this nature\nare handled by rigid tools such as ReGex or DateParser, which are efficient as\nlong as the expected input is pre-configured. When these algorithms are given\nan unexpected format, errors and unwanted outputs happen. To circumvent this\nchallenge, we present a solution with deep neural networks state of art T5 that\ntreats non-preconfigured formats of dates and addresses with accuracy above 90%\nin some cases. With this model, our proposal brings generalization to the task\nof normalizing dates and addresses. We also deal with this problem with noisy\ndata that simulates possible errors in the text.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.14668,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"THEaiTRE: Artificial Intelligence to Write a Theatre Play\n\n  We present THEaiTRE, a starting project aimed at automatic generation of\ntheatre play scripts. This paper reviews related work and drafts an approach we\nintend to follow. We plan to adopt generative neural language models and\nhierarchical generation approaches, supported by summarization and machine\ntranslation methods, and complemented with a human-in-the-loop approach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.06202,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"A Monolingual Approach to Contextualized Word Embeddings for\n  Mid-Resource Languages\n\n  We use the multilingual OSCAR corpus, extracted from Common Crawl via\nlanguage classification, filtering and cleaning, to train monolingual\ncontextualized word embeddings (ELMo) for five mid-resource languages. We then\ncompare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for\nthese languages on the part-of-speech tagging and parsing tasks. We show that,\ndespite the noise in the Common-Crawl-based OSCAR data, embeddings trained on\nOSCAR perform much better than monolingual embeddings trained on Wikipedia.\nThey actually equal or improve the current state of the art in tagging and\nparsing for all five languages. In particular, they also improve over\nmultilingual Wikipedia-based contextual embeddings (multilingual BERT), which\nalmost always constitutes the previous state of the art, thereby showing that\nthe benefit of a larger, more diverse corpus surpasses the cross-lingual\nbenefit of multilingual embedding architectures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.10413,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000026822,
      "text":"Are Pretrained Language Models Symbolic Reasoners Over Knowledge?\n\n  How can pretrained language models (PLMs) learn factual knowledge from the\ntraining set? We investigate the two most important mechanisms: reasoning and\nmemorization. Prior work has attempted to quantify the number of facts PLMs\nlearn, but we present, using synthetic data, the first study that investigates\nthe causal relation between facts present in training and facts learned by the\nPLM. For reasoning, we show that PLMs seem to learn to apply some symbolic\nreasoning rules correctly but struggle with others, including two-hop\nreasoning. Further analysis suggests that even the application of learned\nreasoning rules is flawed. For memorization, we identify schema conformity\n(facts systematically supported by other facts) and frequency as key factors\nfor its success.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.01554,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000128481,
      "text":"A Contextual Hierarchical Attention Network with Adaptive Objective for\n  Dialogue State Tracking\n\n  Recent studies in dialogue state tracking (DST) leverage historical\ninformation to determine states which are generally represented as slot-value\npairs. However, most of them have limitations to efficiently exploit relevant\ncontext due to the lack of a powerful mechanism for modeling interactions\nbetween the slot and the dialogue history. Besides, existing methods usually\nignore the slot imbalance problem and treat all slots indiscriminately, which\nlimits the learning of hard slots and eventually hurts overall performance. In\nthis paper, we propose to enhance the DST through employing a contextual\nhierarchical attention network to not only discern relevant information at both\nword level and turn level but also learn contextual representations. We further\npropose an adaptive objective to alleviate the slot imbalance problem by\ndynamically adjust weights of different slots during training. Experimental\nresults show that our approach reaches 52.68% and 58.55% joint accuracy on\nMultiWOZ 2.0 and MultiWOZ 2.1 datasets respectively and achieves new\nstate-of-the-art performance with considerable improvements (+1.24% and\n+5.98%).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.01563,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Exploring Cross-sentence Contexts for Named Entity Recognition with BERT\n\n  Named entity recognition (NER) is frequently addressed as a sequence\nclassification task where each input consists of one sentence of text. It is\nnevertheless clear that useful information for the task can often be found\noutside of the scope of a single-sentence context. Recently proposed\nself-attention models such as BERT can both efficiently capture long-distance\nrelationships in input as well as represent inputs consisting of several\nsentences, creating new opportunitites for approaches that incorporate\ncross-sentence information in natural language processing tasks. In this paper,\nwe present a systematic study exploring the use of cross-sentence information\nfor NER using BERT models in five languages. We find that adding context in the\nform of additional sentences to BERT input systematically increases NER\nperformance on all of the tested languages and models. Including multiple\nsentences in each input also allows us to study the predictions of the same\nsentences in different contexts. We propose a straightforward method,\nContextual Majority Voting (CMV), to combine different predictions for\nsentences and demonstrate this to further increase NER performance with BERT.\nOur approach does not require any changes to the underlying BERT architecture,\nrather relying on restructuring examples for training and prediction.\nEvaluation on established datasets, including the CoNLL'02 and CoNLL'03 NER\nbenchmarks, demonstrates that our proposed approach can improve on the\nstate-of-the-art NER results on English, Dutch, and Finnish, achieves the best\nreported BERT-based results on German, and is on par with performance reported\nwith other BERT-based approaches in Spanish. We release all methods implemented\nin this work under open licenses.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.12106,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000101328,
      "text":"Exploiting Non-Taxonomic Relations for Measuring Semantic Similarity and\n  Relatedness in WordNet\n\n  Various applications in the areas of computational linguistics and artificial\nintelligence employ semantic similarity to solve challenging tasks, such as\nword sense disambiguation, text classification, information retrieval, machine\ntranslation, and document clustering. Previous work on semantic similarity\nfollowed a mono-relational approach using mostly the taxonomic relation \"ISA\".\nThis paper explores the benefits of using all types of non-taxonomic relations\nin large linked data, such as WordNet knowledge graph, to enhance existing\nsemantic similarity and relatedness measures. We propose a holistic\npoly-relational approach based on a new relation-based information content and\nnon-taxonomic-based weighted paths to devise a comprehensive semantic\nsimilarity and relatedness measure. To demonstrate the benefits of exploiting\nnon-taxonomic relations in a knowledge graph, we used three strategies to\ndeploy non-taxonomic relations at different granularity levels. We conducted\nexperiments on four well-known gold standard datasets, and the results\ndemonstrated the robustness and scalability of the proposed semantic similarity\nand relatedness measure, which significantly improves existing similarity\nmeasures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.07425,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Measuring Forecasting Skill from Text\n\n  People vary in their ability to make accurate predictions about the future.\nPrior studies have shown that some individuals can predict the outcome of\nfuture events with consistently better accuracy. This leads to a natural\nquestion: what makes some forecasters better than others? In this paper we\nexplore connections between the language people use to describe their\npredictions and their forecasting skill. Datasets from two different\nforecasting domains are explored: (1) geopolitical forecasts from Good Judgment\nOpen, an online prediction forum and (2) a corpus of company earnings forecasts\nmade by financial analysts. We present a number of linguistic metrics which are\ncomputed over text associated with people's predictions about the future\nincluding: uncertainty, readability, and emotion. By studying linguistic\nfactors associated with predictions, we are able to shed some light on the\napproach taken by skilled forecasters. Furthermore, we demonstrate that it is\npossible to accurately predict forecasting skill using a model that is based\nsolely on language. This could potentially be useful for identifying accurate\npredictions or potentially skilled forecasters earlier.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.12816,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000027816,
      "text":"Inductive Unsupervised Domain Adaptation for Few-Shot Classification via\n  Clustering\n\n  Few-shot classification tends to struggle when it needs to adapt to diverse\ndomains. Due to the non-overlapping label space between domains, the\nperformance of conventional domain adaptation is limited. Previous work tackles\nthe problem in a transductive manner, by assuming access to the full set of\ntest data, which is too restrictive for many real-world applications. In this\npaper, we set out to tackle this issue by introducing a inductive framework,\nDaFeC, to improve Domain adaptation performance for Few-shot classification via\nClustering. We first build a representation extractor to derive features for\nunlabeled data from the target domain (no test data is necessary) and then\ngroup them with a cluster miner. The generated pseudo-labeled data and the\nlabeled source-domain data are used as supervision to update the parameters of\nthe few-shot classifier. In order to derive high-quality pseudo labels, we\npropose a Clustering Promotion Mechanism, to learn better features for the\ntarget domain via Similarity Entropy Minimization and Adversarial Distribution\nAlignment, which are combined with a Cosine Annealing Strategy. Experiments are\nperformed on the FewRel 2.0 dataset. Our approach outperforms previous work\nwith absolute gains (in classification accuracy) of 4.95%, 9.55%, 3.99% and\n11.62%, respectively, under four few-shot settings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.00814,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000095367,
      "text":"Online Versus Offline NMT Quality: An In-depth Analysis on\n  English-German and German-English\n\n  We conduct in this work an evaluation study comparing offline and online\nneural machine translation architectures. Two sequence-to-sequence models:\nconvolutional Pervasive Attention (Elbayad et al. 2018) and attention-based\nTransformer (Vaswani et al. 2017) are considered. We investigate, for both\narchitectures, the impact of online decoding constraints on the translation\nquality through a carefully designed human evaluation on English-German and\nGerman-English language pairs, the latter being particularly sensitive to\nlatency constraints. The evaluation results allow us to identify the strengths\nand shortcomings of each model when we shift to the online setup.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.07834,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000108282,
      "text":"InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language\n  Model Pre-Training\n\n  In this work, we present an information-theoretic framework that formulates\ncross-lingual language model pre-training as maximizing mutual information\nbetween multilingual-multi-granularity texts. The unified view helps us to\nbetter understand the existing methods for learning cross-lingual\nrepresentations. More importantly, inspired by the framework, we propose a new\npre-training task based on contrastive learning. Specifically, we regard a\nbilingual sentence pair as two views of the same meaning and encourage their\nencoded representations to be more similar than the negative examples. By\nleveraging both monolingual and parallel corpora, we jointly train the pretext\ntasks to improve the cross-lingual transferability of pre-trained models.\nExperimental results on several benchmarks show that our approach achieves\nconsiderably better performance. The code and pre-trained models are available\nat https:\/\/aka.ms\/infoxlm.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.02259,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000047684,
      "text":"EmotionGIF-Yankee: A Sentiment Classifier with Robust Model Based\n  Ensemble Methods\n\n  This paper provides a method to classify sentiment with robust model based\nensemble methods. We preprocess tweet data to enhance coverage of tokenizer. To\nreduce domain bias, we first train tweet dataset for pre-trained language\nmodel. Besides, each classifier has its strengths and weakness, we leverage\ndifferent types of models with ensemble methods: average and power weighted\nsum. From the experiments, we show that our approach has achieved positive\neffect for sentiment classification. Our system reached third place among 26\nteams from the evaluation in SocialNLP 2020 EmotionGIF competition.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.00968,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000055631,
      "text":"Project PIAF: Building a Native French Question-Answering Dataset\n\n  Motivated by the lack of data for non-English languages, in particular for\nthe evaluation of downstream tasks such as Question Answering, we present a\nparticipatory effort to collect a native French Question Answering Dataset.\nFurthermore, we describe and publicly release the annotation tool developed for\nour collection effort, along with the data obtained and preliminary baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.04792,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Targeting the Benchmark: On Methodology in Current Natural Language\n  Processing Research\n\n  It has become a common pattern in our field: One group introduces a language\ntask, exemplified by a dataset, which they argue is challenging enough to serve\nas a benchmark. They also provide a baseline model for it, which then soon is\nimproved upon by other groups. Often, research efforts then move on, and the\npattern repeats itself. What is typically left implicit is the argumentation\nfor why this constitutes progress, and progress towards what. In this paper, we\ntry to step back for a moment from this pattern and work out possible\nargumentations and their parts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.0267,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000029802,
      "text":"A Broad-Coverage Deep Semantic Lexicon for Verbs\n\n  Progress on deep language understanding is inhibited by the lack of a broad\ncoverage lexicon that connects linguistic behavior to ontological concepts and\naxioms. We have developed COLLIE-V, a deep lexical resource for verbs, with the\ncoverage of WordNet and syntactic and semantic details that meet or exceed\nexisting resources. Bootstrapping from a hand-built lexicon and ontology, new\nontological concepts and lexical entries, together with semantic role\npreferences and entailment axioms, are automatically derived by combining\nmultiple constraints from parsing dictionary definitions and examples. We\nevaluated the accuracy of the technique along a number of different dimensions\nand were able to obtain high accuracy in deriving new concepts and lexical\nentries. COLLIE-V is publicly available.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.12432,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000158615,
      "text":"MULTISEM at SemEval-2020 Task 3: Fine-tuning BERT for Lexical Meaning\n\n  We present the MULTISEM systems submitted to SemEval 2020 Task 3: Graded Word\nSimilarity in Context (GWSC). We experiment with injecting semantic knowledge\ninto pre-trained BERT models through fine-tuning on lexical semantic tasks\nrelated to GWSC. We use existing semantically annotated datasets and propose to\napproximate similarity through automatically generated lexical substitutes in\ncontext. We participate in both GWSC subtasks and address two languages,\nEnglish and Finnish. Our best English models occupy the third and fourth\npositions in the ranking for the two subtasks. Performance is lower for the\nFinnish models which are mid-ranked in the respective subtasks, highlighting\nthe important role of data availability for fine-tuning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.14936,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"#Brexit: Leave or Remain? The Role of User's Community and Diachronic\n  Evolution on Stance Detection\n\n  Interest has grown around the classification of stance that users assume\nwithin online debates in recent years. Stance has been usually addressed by\nconsidering users posts in isolation, while social studies highlight that\nsocial communities may contribute to influence users' opinion. Furthermore,\nstance should be studied in a diachronic perspective, since it could help to\nshed light on users' opinion shift dynamics that can be recorded during the\ndebate. We analyzed the political discussion in UK about the BREXIT referendum\non Twitter, proposing a novel approach and annotation schema for stance\ndetection, with the main aim of investigating the role of features related to\nsocial network community and diachronic stance evolution. Classification\nexperiments show that such features provide very useful clues for detecting\nstance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.08416,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000089738,
      "text":"SLK-NER: Exploiting Second-order Lexicon Knowledge for Chinese NER\n\n  Although character-based models using lexicon have achieved promising results\nfor Chinese named entity recognition (NER) task, some lexical words would\nintroduce erroneous information due to wrongly matched words. Existing\nresearches proposed many strategies to integrate lexicon knowledge. However,\nthey performed with simple first-order lexicon knowledge, which provided\ninsufficient word information and still faced the challenge of matched word\nboundary conflicts; or explored the lexicon knowledge with graph where\nhigher-order information introducing negative words may disturb the\nidentification. To alleviate the above limitations, we present new insight into\nsecond-order lexicon knowledge (SLK) of each character in the sentence to\nprovide more lexical word information including semantic and word boundary\nfeatures. Based on these, we propose a SLK-based model with a novel strategy to\nintegrate the above lexicon knowledge. The proposed model can exploit more\ndiscernible lexical words information with the help of global context.\nExperimental results on three public datasets demonstrate the validity of SLK.\nThe proposed model achieves more excellent performance than the\nstate-of-the-art comparison methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.04303,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000040399,
      "text":"Tweets Sentiment Analysis via Word Embeddings and Machine Learning\n  Techniques\n\n  Sentiment analysis of social media data consists of attitudes, assessments,\nand emotions which can be considered a way human think. Understanding and\nclassifying the large collection of documents into positive and negative\naspects are a very difficult task. Social networks such as Twitter, Facebook,\nand Instagram provide a platform in order to gather information about peoples\nsentiments and opinions. Considering the fact that people spend hours daily on\nsocial media and share their opinion on various different topics helps us\nanalyze sentiments better. More and more companies are using social media tools\nto provide various services and interact with customers. Sentiment Analysis\n(SA) classifies the polarity of given tweets to positive and negative tweets in\norder to understand the sentiments of the public. This paper aims to perform\nsentiment analysis of real-time 2019 election twitter data using the feature\nselection model word2vec and the machine learning algorithm random forest for\nsentiment classification. Word2vec with Random Forest improves the accuracy of\nsentiment analysis significantly compared to traditional methods such as BOW\nand TF-IDF. Word2vec improves the quality of features by considering contextual\nsemantics of words in a text hence improving the accuracy of machine learning\nand sentiment analysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.04874,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000014239,
      "text":"CompRes: A Dataset for Narrative Structure in News\n\n  This paper addresses the task of automatically detecting narrative structures\nin raw texts. Previous works have utilized the oral narrative theory by Labov\nand Waletzky to identify various narrative elements in personal stories texts.\nInstead, we direct our focus to news articles, motivated by their growing\nsocial impact as well as their role in creating and shaping public opinion.\n  We introduce CompRes -- the first dataset for narrative structure in news\nmedia. We describe the process in which the dataset was constructed: first, we\ndesigned a new narrative annotation scheme, better suited for news media, by\nadapting elements from the narrative theory of Labov and Waletzky (Complication\nand Resolution) and adding a new narrative element of our own (Success); then,\nwe used that scheme to annotate a set of 29 English news articles (containing\n1,099 sentences) collected from news and partisan websites. We use the\nannotated dataset to train several supervised models to identify the different\nnarrative elements, achieving an $F_1$ score of up to 0.7. We conclude by\nsuggesting several promising directions for future work.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.01658,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Playing with Words at the National Library of Sweden -- Making a Swedish\n  BERT\n\n  This paper introduces the Swedish BERT (\"KB-BERT\") developed by the KBLab for\ndata-driven research at the National Library of Sweden (KB). Building on recent\nefforts to create transformer-based BERT models for languages other than\nEnglish, we explain how we used KB's collections to create and train a new\nlanguage-specific BERT model for Swedish. We also present the results of our\nmodel in comparison with existing models - chiefly that produced by the Swedish\nPublic Employment Service, Arbetsf\\\"ormedlingen, and Google's multilingual\nM-BERT - where we demonstrate that KB-BERT outperforms these in a range of NLP\ntasks from named entity recognition (NER) to part-of-speech tagging (POS). Our\ndiscussion highlights the difficulties that continue to exist given the lack of\ntraining data and testbeds for smaller languages like Swedish. We release our\nmodel for further exploration and research here:\nhttps:\/\/github.com\/Kungbib\/swedish-bert-models .\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.07691,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"A Multilingual Parallel Corpora Collection Effort for Indian Languages\n\n  We present sentence aligned parallel corpora across 10 Indian Languages -\nHindi, Telugu, Tamil, Malayalam, Gujarati, Urdu, Bengali, Oriya, Marathi,\nPunjabi, and English - many of which are categorized as low resource. The\ncorpora are compiled from online sources which have content shared across\nlanguages. The corpora presented significantly extends present resources that\nare either not large enough or are restricted to a specific domain (such as\nhealth). We also provide a separate test corpus compiled from an independent\nonline source that can be independently used for validating the performance in\n10 Indian languages. Alongside, we report on the methods of constructing such\ncorpora using tools enabled by recent advances in machine translation and\ncross-lingual retrieval using deep neural network based methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.11865,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"AI4D -- African Language Dataset Challenge\n\n  As language and speech technologies become more advanced, the lack of\nfundamental digital resources for African languages, such as data, spell\ncheckers and Part of Speech taggers, means that the digital divide between\nthese languages and others keeps growing. This work details the organisation of\nthe AI4D - African Language Dataset Challenge, an effort to incentivize the\ncreation, organization and discovery of African language datasets through a\ncompetitive challenge. We particularly encouraged the submission of annotated\ndatasets which can be used for training task-specific supervised machine\nlearning models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.12081,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000093712,
      "text":"NITS-Hinglish-SentiMix at SemEval-2020 Task 9: Sentiment Analysis For\n  Code-Mixed Social Media Text Using an Ensemble Model\n\n  Sentiment Analysis is the process of deciphering what a sentence emotes and\nclassifying them as either positive, negative, or neutral. In recent times,\nIndia has seen a huge influx in the number of active social media users and\nthis has led to a plethora of unstructured text data. Since the Indian\npopulation is generally fluent in both Hindi and English, they end up\ngenerating code-mixed Hinglish social media text i.e. the expressions of Hindi\nlanguage, written in the Roman script alongside other English words. The\nability to adequately comprehend the notions in these texts is truly necessary.\nOur team, rns2020 participated in Task 9 at SemEval2020 intending to design a\nsystem to carry out the sentiment analysis of code-mixed social media text.\nThis work proposes a system named NITS-Hinglish-SentiMix to viably complete the\nsentiment analysis of such code-mixed Hinglish text. The proposed framework has\nrecorded an F-Score of 0.617 on the test data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.11648,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Effects of Language Relatedness for Cross-lingual Transfer Learning in\n  Character-Based Language Models\n\n  Character-based Neural Network Language Models (NNLM) have the advantage of\nsmaller vocabulary and thus faster training times in comparison to NNLMs based\non multi-character units. However, in low-resource scenarios, both the\ncharacter and multi-character NNLMs suffer from data sparsity. In such\nscenarios, cross-lingual transfer has improved multi-character NNLM performance\nby allowing information transfer from a source to the target language. In the\nsame vein, we propose to use cross-lingual transfer for character NNLMs applied\nto low-resource Automatic Speech Recognition (ASR). However, applying\ncross-lingual transfer to character NNLMs is not as straightforward. We observe\nthat relatedness of the source language plays an important role in\ncross-lingual pretraining of character NNLMs. We evaluate this aspect on ASR\ntasks for two target languages: Finnish (with English and Estonian as source)\nand Swedish (with Danish, Norwegian, and English as source). Prior work has\nobserved no difference between using the related or unrelated language for\nmulti-character NNLMs. We, however, show that for character-based NNLMs, only\npretraining with a related language improves the ASR performance, and using an\nunrelated language may deteriorate it. We also observe that the benefits are\nlarger when there is much lesser target data than source data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.07389,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000158615,
      "text":"Emoji Prediction: Extensions and Benchmarking\n\n  Emojis are a succinct form of language which can express concrete meanings,\nemotions, and intentions. Emojis also carry signals that can be used to better\nunderstand communicative intent. They have become a ubiquitous part of our\ndaily lives, making them an important part of understanding user-generated\ncontent. The emoji prediction task aims at predicting the proper set of emojis\nassociated with a piece of text. Through emoji prediction, models can learn\nrich representations of the communicative intent of the written text. While\nexisting research on the emoji prediction task focus on a small subset of emoji\ntypes closely related to certain emotions, this setting oversimplifies the task\nand wastes the expressive power of emojis. In this paper, we extend the\nexisting setting of the emoji prediction task to include a richer set of emojis\nand to allow multi-label classification on the task. We propose novel models\nfor multi-class and multi-label emoji prediction based on Transformer networks.\nWe also construct multiple emoji prediction datasets from Twitter using\nheuristics. The BERT models achieve state-of-the-art performances on all our\ndatasets under all the settings, with relative improvements of 27.21% to\n236.36% in accuracy, 2.01% to 88.28% in top-5 accuracy and 65.19% to 346.79% in\nF-1 score, compared to the prior state-of-the-art. Our results demonstrate the\nefficacy of deep Transformer-based models on the emoji prediction task. We also\nrelease our datasets at\nhttps:\/\/github.com\/hikari-NYU\/Emoji_Prediction_Datasets_MMS for future\nresearchers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.09774,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000018875,
      "text":"An Overview of Natural Language State Representation for Reinforcement\n  Learning\n\n  A suitable state representation is a fundamental part of the learning process\nin Reinforcement Learning. In various tasks, the state can either be described\nby natural language or be natural language itself. This survey outlines the\nstrategies used in the literature to build natural language state\nrepresentations. We appeal for more linguistically interpretable and grounded\nrepresentations, careful justification of design decisions and evaluation of\nthe effectiveness of different approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.06761,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Can neural networks acquire a structural bias from raw linguistic data?\n\n  We evaluate whether BERT, a widely used neural network for sentence\nprocessing, acquires an inductive bias towards forming structural\ngeneralizations through pretraining on raw data. We conduct four experiments\ntesting its preference for structural vs. linear generalizations in different\nstructure-dependent phenomena. We find that BERT makes a structural\ngeneralization in 3 out of 4 empirical domains---subject-auxiliary inversion,\nreflexive binding, and verb tense detection in embedded clauses---but makes a\nlinear generalization when tested on NPI licensing. We argue that these results\nare the strongest evidence so far from artificial learners supporting the\nproposition that a structural bias can be acquired from raw data. If this\nconclusion is correct, it is tentative evidence that some linguistic universals\ncan be acquired by learners without innate biases. However, the precise\nimplications for human language acquisition are unclear, as humans learn\nlanguage from significantly less data than BERT.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.03541,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000097685,
      "text":"scb-mt-en-th-2020: A Large English-Thai Parallel Corpus\n\n  The primary objective of our work is to build a large-scale English-Thai\ndataset for machine translation. We construct an English-Thai machine\ntranslation dataset with over 1 million segment pairs, curated from various\nsources, namely news, Wikipedia articles, SMS messages, task-based dialogs,\nweb-crawled data and government documents. Methodology for gathering data,\nbuilding parallel texts and removing noisy sentence pairs are presented in a\nreproducible manner. We train machine translation models based on this dataset.\nOur models' performance are comparable to that of Google Translation API (as of\nMay 2020) for Thai-English and outperform Google when the Open Parallel Corpus\n(OPUS) is included in the training data for both Thai-English and English-Thai\ntranslation. The dataset, pre-trained models, and source code to reproduce our\nwork are available for public use.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.157,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"The Unreasonable Effectiveness of Machine Learning in Moldavian versus\n  Romanian Dialect Identification\n\n  Motivated by the seemingly high accuracy levels of machine learning models in\nMoldavian versus Romanian dialect identification and the increasing research\ninterest on this topic, we provide a follow-up on the Moldavian versus Romanian\nCross-Dialect Topic Identification (MRC) shared task of the VarDial 2019\nEvaluation Campaign. The shared task included two sub-task types: one that\nconsisted in discriminating between the Moldavian and Romanian dialects and one\nthat consisted in classifying documents by topic across the two dialects of\nRomanian. Participants achieved impressive scores, e.g. the top model for\nMoldavian versus Romanian dialect identification obtained a macro F1 score of\n0.895. We conduct a subjective evaluation by human annotators, showing that\nhumans attain much lower accuracy rates compared to machine learning (ML)\nmodels. Hence, it remains unclear why the methods proposed by participants\nattain such high accuracy rates. Our goal is to understand (i) why the proposed\nmethods work so well (by visualizing the discriminative features) and (ii) to\nwhat extent these methods can keep their high accuracy levels, e.g. when we\nshorten the text samples to single sentences or when we use tweets at inference\ntime. A secondary goal of our work is to propose an improved ML model using\nensemble learning. Our experiments show that ML models can accurately identify\nthe dialects, even at the sentence level and across different domains (news\narticles versus tweets). We also analyze the most discriminative features of\nthe best performing models, providing some explanations behind the decisions\ntaken by these models. Interestingly, we learn new dialectal patterns\npreviously unknown to us or to our human annotators. Furthermore, we conduct\nexperiments showing that the machine learning performance on the MRC shared\ntask can be improved through an ensemble based on stacking.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.02879,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Efficient Neural Query Auto Completion\n\n  Query Auto Completion (QAC), as the starting point of information retrieval\ntasks, is critical to user experience. Generally it has two steps: generating\ncompleted query candidates according to query prefixes, and ranking them based\non extracted features. Three major challenges are observed for a query auto\ncompletion system: (1) QAC has a strict online latency requirement. For each\nkeystroke, results must be returned within tens of milliseconds, which poses a\nsignificant challenge in designing sophisticated language models for it. (2)\nFor unseen queries, generated candidates are of poor quality as contextual\ninformation is not fully utilized. (3) Traditional QAC systems heavily rely on\nhandcrafted features such as the query candidate frequency in search logs,\nlacking sufficient semantic understanding of the candidate.\n  In this paper, we propose an efficient neural QAC system with effective\ncontext modeling to overcome these challenges. On the candidate generation\nside, this system uses as much information as possible in unseen prefixes to\ngenerate relevant candidates, increasing the recall by a large margin. On the\ncandidate ranking side, an unnormalized language model is proposed, which\neffectively captures deep semantics of queries. This approach presents better\nranking performance over state-of-the-art neural ranking methods and reduces\n$\\sim$95\\% latency compared to neural language modeling methods. The empirical\nresults on public datasets show that our model achieves a good balance between\naccuracy and efficiency. This system is served in LinkedIn job search with\nsignificant product impact observed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.02385,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Efficient MDI Adaptation for n-gram Language Models\n\n  This paper presents an efficient algorithm for n-gram language model\nadaptation under the minimum discrimination information (MDI) principle, where\nan out-of-domain language model is adapted to satisfy the constraints of\nmarginal probabilities of the in-domain data. The challenge for MDI language\nmodel adaptation is its computational complexity. By taking advantage of the\nbackoff structure of n-gram model and the idea of hierarchical training method,\noriginally proposed for maximum entropy (ME) language models, we show that MDI\nadaptation can be computed in linear-time complexity to the inputs in each\niteration. The complexity remains the same as ME models, although MDI is more\ngeneral than ME. This makes MDI adaptation practical for large corpus and\nvocabulary. Experimental results confirm the scalability of our algorithm on\nvery large datasets, while MDI adaptation gets slightly worse perplexity but\nbetter word error rate results compared to simple linear interpolation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.07347,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"HunFlair: An Easy-to-Use Tool for State-of-the-Art Biomedical Named\n  Entity Recognition\n\n  Summary: Named Entity Recognition (NER) is an important step in biomedical\ninformation extraction pipelines. Tools for NER should be easy to use, cover\nmultiple entity types, highly accurate, and robust towards variations in text\ngenre and style. To this end, we propose HunFlair, an NER tagger covering\nmultiple entity types integrated into the widely used NLP framework Flair.\nHunFlair outperforms other state-of-the-art standalone NER tools with an\naverage gain of 7.26 pp over the next best tool, can be installed with a single\ncommand and is applied with only four lines of code. Availability: HunFlair is\nfreely available through the Flair framework under an MIT license:\nhttps:\/\/github.com\/flairNLP\/flair and is compatible with all major operating\nsystems. Contact:{weberple,saengema,alan.akbik}@informatik.hu-berlin.de\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.11841,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"On the Optimality of Vagueness: \"Around\", \"Between\", and the Gricean\n  Maxims\n\n  Why is ordinary language vague? We argue that in contexts in which a\ncooperative speaker is not perfectly informed about the world, the use of vague\nexpressions can offer an optimal tradeoff between truthfulness (Gricean\nQuality) and informativeness (Gricean Quantity). Focusing on expressions of\napproximation such as \"around\", which are semantically vague, we show that they\nallow the speaker to convey indirect probabilistic information, in a way that\ncan give the listener a more accurate representation of the information\navailable to the speaker than any more precise expression would (intervals of\nthe form \"between\"). That is, vague sentences can be more informative than\ntheir precise counterparts. We give a probabilistic treatment of the\ninterpretation of \"around\", and offer a model for the interpretation and use of\n\"around\"-statements within the Rational Speech Act (RSA) framework. In our\naccount the shape of the speaker's distribution matters in ways not predicted\nby the Lexical Uncertainty model standardly used in the RSA framework for vague\npredicates. We use our approach to draw further lessons concerning the semantic\nflexibility of vague expressions and their irreducibility to more precise\nmeanings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.04277,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000200669,
      "text":"SemEval-2020 Task 9: Overview of Sentiment Analysis of Code-Mixed Tweets\n\n  In this paper, we present the results of the SemEval-2020 Task 9 on Sentiment\nAnalysis of Code-Mixed Tweets (SentiMix 2020). We also release and describe our\nHinglish (Hindi-English) and Spanglish (Spanish-English) corpora annotated with\nword-level language identification and sentence-level sentiment labels. These\ncorpora are comprised of 20K and 19K examples, respectively. The sentiment\nlabels are - Positive, Negative, and Neutral. SentiMix attracted 89 submissions\nin total including 61 teams that participated in the Hinglish contest and 28\nsubmitted systems to the Spanglish competition. The best performance achieved\nwas 75.0% F1 score for Hinglish and 80.6% F1 for Spanglish. We observe that\nBERT-like models and ensemble methods are the most common and successful\napproaches among the participants.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.00563,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000228816,
      "text":"SemEval-2020 Task 5: Counterfactual Recognition\n\n  We present a counterfactual recognition (CR) task, the shared Task 5 of\nSemEval-2020. Counterfactuals describe potential outcomes (consequents)\nproduced by actions or circumstances that did not happen or cannot happen and\nare counter to the facts (antecedent). Counterfactual thinking is an important\ncharacteristic of the human cognitive system; it connects antecedents and\nconsequents with causal relations. Our task provides a benchmark for\ncounterfactual recognition in natural language with two subtasks. Subtask-1\naims to determine whether a given sentence is a counterfactual statement or\nnot. Subtask-2 requires the participating systems to extract the antecedent and\nconsequent in a given counterfactual statement. During the SemEval-2020\nofficial evaluation period, we received 27 submissions to Subtask-1 and 11 to\nSubtask-2. The data, baseline code, and leaderboard can be found at\nhttps:\/\/competitions.codalab.org\/competitions\/21691. The data and baseline code\nare also available at https:\/\/zenodo.org\/record\/3932442.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.05666,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000002914,
      "text":"Dialogue State Induction Using Neural Latent Variable Models\n\n  Dialogue state modules are a useful component in a task-oriented dialogue\nsystem. Traditional methods find dialogue states by manually labeling training\ncorpora, upon which neural models are trained. However, the labeling process\ncan be costly, slow, error-prone, and more importantly, cannot cover the vast\nrange of domains in real-world dialogues for customer service. We propose the\ntask of dialogue state induction, building two neural latent variable models\nthat mine dialogue states automatically from unlabeled customer service\ndialogue records. Results show that the models can effectively find meaningful\nslots. In addition, equipped with induced dialogue states, a state-of-the-art\ndialogue system gives better performance compared with not using a dialogue\nstate module.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.04885,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0002570947,
      "text":"The Sockeye 2 Neural Machine Translation Toolkit at AMTA 2020\n\n  We present Sockeye 2, a modernized and streamlined version of the Sockeye\nneural machine translation (NMT) toolkit. New features include a simplified\ncode base through the use of MXNet's Gluon API, a focus on state of the art\nmodel architectures, distributed mixed precision training, and efficient CPU\ndecoding with 8-bit quantization. These improvements result in faster training\nand inference, higher automatic metric scores, and a shorter path from research\nto production.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.03979,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000039074,
      "text":"KR-BERT: A Small-Scale Korean-Specific Language Model\n\n  Since the appearance of BERT, recent works including XLNet and RoBERTa\nutilize sentence embedding models pre-trained by large corpora and a large\nnumber of parameters. Because such models have large hardware and a huge amount\nof data, they take a long time to pre-train. Therefore it is important to\nattempt to make smaller models that perform comparatively. In this paper, we\ntrained a Korean-specific model KR-BERT, utilizing a smaller vocabulary and\ndataset. Since Korean is one of the morphologically rich languages with poor\nresources using non-Latin alphabets, it is also important to capture\nlanguage-specific linguistic phenomena that the Multilingual BERT model missed.\nWe tested several tokenizers including our BidirectionalWordPiece Tokenizer and\nadjusted the minimal span of tokens for tokenization ranging from sub-character\nlevel to character-level to construct a better vocabulary for our model. With\nthose adjustments, our KR-BERT model performed comparably and even better than\nother existing pre-trained models using a corpus about 1\/10 of the size.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.0302,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000112587,
      "text":"A Context-based Disambiguation Model for Sentiment Concepts Using a\n  Bag-of-concepts Approach\n\n  With the widespread dissemination of user-generated content on different\nsocial networks, and online consumer systems such as Amazon, the quantity of\nopinionated information available on the Internet has been increased. One of\nthe main tasks of the sentiment analysis is to detect polarity within a text.\nThe existing polarity detection methods mainly focus on keywords and their\nnaive frequency counts; however, they less regard the meanings and implicit\ndimensions of the natural concepts. Although background knowledge plays a\ncritical role in determining the polarity of concepts, it has been disregarded\nin polarity detection methods. This study presents a context-based model to\nsolve ambiguous polarity concepts using commonsense knowledge. First, a model\nis presented to generate a source of ambiguous sentiment concepts based on\nSenticNet by computing the probability distribution. Then the model uses a\nbag-of-concepts approach to remove ambiguities and semantic augmentation with\nthe ConceptNet handling to overcome lost knowledge. ConceptNet is a large-scale\nsemantic network with a large number of commonsense concepts. In this paper,\nthe point mutual information (PMI) measure is used to select the contextual\nconcepts having strong relationships with ambiguous concepts. The polarity of\nthe ambiguous concepts is precisely detected using positive\/negative contextual\nconcepts and the relationship of the concepts in the semantic knowledge base.\nThe text representation scheme is semantically enriched using Numberbatch,\nwhich is a word embedding model based on the concepts from the ConceptNet\nsemantic network. The proposed model is evaluated by applying a corpus of\nproduct reviews, called Semeval. The experimental results revealed an accuracy\nrate of 82.07%, representing the effectiveness of the proposed model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.0334,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000035432,
      "text":"Retrofitting Vector Representations of Adverse Event Reporting Data to\n  Structured Knowledge to Improve Pharmacovigilance Signal Detection\n\n  Adverse drug events (ADE) are prevalent and costly. Clinical trials are\nconstrained in their ability to identify potential ADEs, motivating the\ndevelopment of spontaneous reporting systems for post-market surveillance.\nStatistical methods provide a convenient way to detect signals from these\nreports but have limitations in leveraging relationships between drugs and ADEs\ngiven their discrete count-based nature. A previously proposed method, aer2vec,\ngenerates distributed vector representations of ADE report entities that\ncapture patterns of similarity but cannot utilize lexical knowledge. We address\nthis limitation by retrofitting aer2vec drug embeddings to knowledge from\nRxNorm and developing a novel retrofitting variant using vector rescaling to\npreserve magnitude. When evaluated in the context of a pharmacovigilance signal\ndetection task, aer2vec with retrofitting consistently outperforms\ndisproportionality metrics when trained on minimally preprocessed data.\nRetrofitting with rescaling results in further improvements in the larger and\nmore challenging of two pharmacovigilance reference sets used for evaluation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.06865,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Discovering Lexical Similarity Through Articulatory Feature-based\n  Phonetic Edit Distance\n\n  Lexical Similarity (LS) between two languages uncovers many interesting\nlinguistic insights such as genetic relationship, mutual intelligibility, and\nthe usage of one's vocabulary into other. There are various methods through\nwhich LS is evaluated. In the same regard, this paper presents a method of\nPhonetic Edit Distance (PED) that uses a soft comparison of letters using the\narticulatory features associated with them. The system converts the words into\nthe corresponding International Phonetic Alphabet (IPA), followed by the\nconversion of IPA into its set of articulatory features. Later, the lists of\nthe set of articulatory features are compared using the proposed method. As an\nexample, PED gives edit distance of German word vater and Persian word pidar as\n0.82; and similarly, Hebrew word shalom and Arabic word salaam as 0.93, whereas\nfor a juxtapose comparison, their IPA based edit distances are 4 and 2\nrespectively. Experiments are performed with six languages (Arabic, Hindi,\nMarathi, Persian, Sanskrit, and Urdu). In this regard, we extracted part of\nspeech wise word-lists from the Universal Dependency corpora and evaluated the\nLS for every pair of language. Thus, with the proposed approach, we find the\ngenetic affinity, similarity, and borrowing\/loan-words despite having script\ndifferences and sound variation phenomena among these languages.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.09703,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0001475546,
      "text":"Team DoNotDistribute at SemEval-2020 Task 11: Features, Finetuning, and\n  Data Augmentation in Neural Models for Propaganda Detection in News Articles\n\n  This paper presents our systems for SemEval 2020 Shared Task 11: Detection of\nPropaganda Techniques in News Articles. We participate in both the span\nidentification and technique classification subtasks and report on experiments\nusing different BERT-based models along with handcrafted features. Our models\nperform well above the baselines for both tasks, and we contribute ablation\nstudies and discussion of our results to dissect the effectiveness of different\nfeatures and techniques with the goal of aiding future studies in propaganda\ndetection.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.1236,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000003212,
      "text":"Language Models as Emotional Classifiers for Textual Conversations\n\n  Emotions play a critical role in our everyday lives by altering how we\nperceive, process and respond to our environment. Affective computing aims to\ninstill in computers the ability to detect and act on the emotions of human\nactors. A core aspect of any affective computing system is the classification\nof a user's emotion. In this study we present a novel methodology for\nclassifying emotion in a conversation. At the backbone of our proposed\nmethodology is a pre-trained Language Model (LM), which is supplemented by a\nGraph Convolutional Network (GCN) that propagates information over the\npredicate-argument structure identified in an utterance. We apply our proposed\nmethodology on the IEMOCAP and Friends data sets, achieving state-of-the-art\nperformance on the former and a higher accuracy on certain emotional labels on\nthe latter. Furthermore, we examine the role context plays in our methodology\nby altering how much of the preceding conversation the model has access to when\nmaking a classification.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.03946,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000221199,
      "text":"A Large-Scale Chinese Short-Text Conversation Dataset\n\n  The advancements of neural dialogue generation models show promising results\non modeling short-text conversations. However, training such models usually\nneeds a large-scale high-quality dialogue corpus, which is hard to access. In\nthis paper, we present a large-scale cleaned Chinese conversation dataset,\nLCCC, which contains a base version (6.8million dialogues) and a large version\n(12.0 million dialogues). The quality of our dataset is ensured by a rigorous\ndata cleaning pipeline, which is built based on a set of rules and a classifier\nthat is trained on manually annotated 110K dialogue pairs. We also release\npre-training dialogue models which are trained on LCCC-base and LCCC-large\nrespectively. The cleaned dataset and the pre-training models will facilitate\nthe research of short-text conversation modeling. All the models and datasets\nare available at https:\/\/github.com\/thu-coai\/CDial-GPT.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.13609,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Detecting Generic Music Features with Single Layer Feedforward Network\n  using Unsupervised Hebbian Computation\n\n  With the ever-increasing number of digital music and vast music track\nfeatures through popular online music streaming software and apps, feature\nrecognition using the neural network is being used for experimentation to\nproduce a wide range of results across a variety of experiments recently.\nThrough this work, the authors extract information on such features from a\npopular open-source music corpus and explored new recognition techniques, by\napplying unsupervised Hebbian learning techniques on their single-layer neural\nnetwork using the same dataset. The authors show the detailed empirical\nfindings to simulate how such an algorithm can help a single layer feedforward\nnetwork in training for music feature learning as patterns. The unsupervised\ntraining algorithm enhances their proposed neural network to achieve an\naccuracy of 90.36% for successful music feature detection. For comparative\nanalysis against similar tasks, authors put their results with the likes of\nseveral previous benchmark works. They further discuss the limitations and\nthorough error analysis of their work. The authors hope to discover and gather\nnew information about this particular classification technique and its\nperformance, and further understand future potential directions and prospects\nthat could improve the art of computational music feature recognition.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.04935,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Paraphrase Generation as Zero-Shot Multilingual Translation:\n  Disentangling Semantic Similarity from Lexical and Syntactic Diversity\n\n  Recent work has shown that a multilingual neural machine translation (NMT)\nmodel can be used to judge how well a sentence paraphrases another sentence in\nthe same language (Thompson and Post, 2020); however, attempting to generate\nparaphrases from such a model using standard beam search produces trivial\ncopies or near copies. We introduce a simple paraphrase generation algorithm\nwhich discourages the production of n-grams that are present in the input. Our\napproach enables paraphrase generation in many languages from a single\nmultilingual NMT model. Furthermore, the amount of lexical diversity between\nthe input and output can be controlled at generation time. We conduct a human\nevaluation to compare our method to a paraphraser trained on the large English\nsynthetic paraphrase database ParaBank 2 (Hu et al., 2019c) and find that our\nmethod produces paraphrases that better preserve meaning and are more\ngramatical, for the same level of lexical diversity. Additional smaller human\nassessments demonstrate our approach also works in two non-English languages.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.11257,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"The Impact of Indirect Machine Translation on Sentiment Classification\n\n  Sentiment classification has been crucial for many natural language\nprocessing (NLP) applications, such as the analysis of movie reviews, tweets,\nor customer feedback. A sufficiently large amount of data is required to build\na robust sentiment classification system. However, such resources are not\nalways available for all domains or for all languages.\n  In this work, we propose employing a machine translation (MT) system to\ntranslate customer feedback into another language to investigate in which cases\ntranslated sentences can have a positive or negative impact on an automatic\nsentiment classifier. Furthermore, as performing a direct translation is not\nalways possible, we explore the performance of automatic classifiers on\nsentences that have been translated using a pivot MT system.\n  We conduct several experiments using the above approaches to analyse the\nperformance of our proposed sentiment classification system and discuss the\nadvantages and drawbacks of classifying translated sentences.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.03101,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Privacy Guarantees for De-identifying Text Transformations\n\n  Machine Learning approaches to Natural Language Processing tasks benefit from\na comprehensive collection of real-life user data. At the same time, there is a\nclear need for protecting the privacy of the users whose data is collected and\nprocessed. For text collections, such as, e.g., transcripts of voice\ninteractions or patient records, replacing sensitive parts with benign\nalternatives can provide de-identification. However, how much privacy is\nactually guaranteed by such text transformations, and are the resulting texts\nstill useful for machine learning? In this paper, we derive formal privacy\nguarantees for general text transformation-based de-identification methods on\nthe basis of Differential Privacy. We also measure the effect that different\nways of masking private information in dialog transcripts have on a subsequent\nmachine learning task. To this end, we formulate different masking strategies\nand compare their privacy-utility trade-offs. In particular, we compare a\nsimple redact approach with more sophisticated word-by-word replacement using\ndeep learning models on multiple natural language understanding tasks like\nnamed entity recognition, intent detection, and dialog act classification. We\nfind that only word-by-word replacement is robust against performance drops in\nvarious tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.07138,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Logical Semantics, Dialogical Argumentation, and Textual Entailment\n\n  In this chapter, we introduce a new dialogical system for first order\nclassical logic which is close to natural language argumentation, and we prove\nits completeness with respect to usual classical validity. We combine our\ndialogical system with the Grail syntactic and semantic parser developed by the\nsecond author in order to address automated textual entailment, that is, we use\nit for deciding whether or not a sentence is a consequence of a short text.\nThis work-which connects natural language semantics and argumentation with\ndialogical logic-can be viewed as a step towards an inferentialist view of\nnatural language semantics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.12719,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000094374,
      "text":"Stylized Dialogue Response Generation Using Stylized Unpaired Texts\n\n  Generating stylized responses is essential to build intelligent and engaging\ndialogue systems. However, this task is far from well-explored due to the\ndifficulties of rendering a particular style in coherent responses, especially\nwhen the target style is embedded only in unpaired texts that cannot be\ndirectly used to train the dialogue model. This paper proposes a stylized\ndialogue generation method that can capture stylistic features embedded in\nunpaired texts. Specifically, our method can produce dialogue responses that\nare both coherent to the given context and conform to the target style. In this\nstudy, an inverse dialogue model is first introduced to predict possible posts\nfor the input responses, and then this inverse model is used to generate\nstylized pseudo dialogue pairs based on these stylized unpaired texts. Further,\nthese pseudo pairs are employed to train the stylized dialogue model with a\njoint training process, and a style routing approach is proposed to intensify\nstylistic features in the decoder. Automatic and manual evaluations on two\ndatasets demonstrate that our method outperforms competitive baselines in\nproducing coherent and style-intensive dialogue responses.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.09427,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000078811,
      "text":"Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired\n  Data\n\n  Recent advances in open-domain dialogue systems rely on the success of neural\nmodels that are trained on large-scale data. However, collecting large-scale\ndialogue data is usually time-consuming and labor-intensive. To address this\ndata dilemma, we propose a novel data augmentation method for training\nopen-domain dialogue models by utilizing unpaired data. Specifically, a\ndata-level distillation process is first proposed to construct augmented\ndialogues where both post and response are retrieved from the unpaired data. A\nranking module is employed to filter out low-quality dialogues. Further, a\nmodel-level distillation process is employed to distill a teacher model trained\non high-quality paired data to augmented dialogue pairs, thereby preventing\ndialogue models from being affected by the noise in the augmented data.\nAutomatic and manual evaluation indicates that our method can produce\nhigh-quality dialogue pairs with diverse contents, and the proposed data-level\nand model-level dialogue distillation can improve the performance of\ncompetitive baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.11382,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000032783,
      "text":"Multi-Pass Transformer for Machine Translation\n\n  In contrast with previous approaches where information flows only towards\ndeeper layers of a stack, we consider a multi-pass transformer (MPT)\narchitecture in which earlier layers are allowed to process information in\nlight of the output of later layers. To maintain a directed acyclic graph\nstructure, the encoder stack of a transformer is repeated along a new\nmulti-pass dimension, keeping the parameters tied, and information is allowed\nto proceed unidirectionally both towards deeper layers within an encoder stack\nand towards any layer of subsequent stacks. We consider both soft (i.e.,\ncontinuous) and hard (i.e., discrete) connections between parallel encoder\nstacks, relying on a neural architecture search to find the best connection\npattern in the hard case. We perform an extensive ablation study of the\nproposed MPT architecture and compare it with other state-of-the-art\ntransformer architectures. Surprisingly, Base Transformer equipped with MPT can\nsurpass the performance of Large Transformer on the challenging machine\ntranslation En-De and En-Fr datasets. In the hard connection case, the optimal\nconnection pattern found for En-De also leads to improved performance for\nEn-Fr.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.05639,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Solving Arithmetic Word Problems by Scoring Equations with Recursive\n  Neural Networks\n\n  Solving arithmetic word problems is a cornerstone task in assessing language\nunderstanding and reasoning capabilities in NLP systems. Recent works use\nautomatic extraction and ranking of candidate solution equations providing the\nanswer to arithmetic word problems. In this work, we explore novel approaches\nto score such candidate solution equations using tree-structured recursive\nneural network (Tree-RNN) configurations. The advantage of this Tree-RNN\napproach over using more established sequential representations, is that it can\nnaturally capture the structure of the equations. Our proposed method consists\nof transforming the mathematical expression of the equation into an expression\ntree. Further, we encode this tree into a Tree-RNN by using different Tree-LSTM\narchitectures. Experimental results show that our proposed method (i) improves\noverall performance with more than 3% accuracy points compared to previous\nstate-of-the-art, and with over 15% points on a subset of problems that require\nmore complex reasoning, and (ii) outperforms sequential LSTMs by 4% accuracy\npoints on such more complex problems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.09162,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000155303,
      "text":"Extracting Summary Knowledge Graphs from Long Documents\n\n  Knowledge graphs capture entities and relations from long documents and can\nfacilitate reasoning in many downstream applications. Extracting compact\nknowledge graphs containing only salient entities and relations is important\nbut challenging for understanding and summarizing long documents. We introduce\na new text-to-graph task of predicting summarized knowledge graphs from long\ndocuments. We develop a dataset of 200k document\/graph pairs using automatic\nand human annotations. We also develop strong baselines for this task based on\ngraph learning and text summarization, and provide quantitative and qualitative\nstudies of their effect.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.0207,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"AutoTrans: Automating Transformer Design via Reinforced Architecture\n  Search\n\n  Though the transformer architectures have shown dominance in many natural\nlanguage understanding tasks, there are still unsolved issues for the training\nof transformer models, especially the need for a principled way of warm-up\nwhich has shown importance for stable training of a transformer, as well as\nwhether the task at hand prefer to scale the attention product or not. In this\npaper, we empirically explore automating the design choices in the transformer\nmodel, i.e., how to set layer-norm, whether to scale, number of layers, number\nof heads, activation function, etc, so that one can obtain a transformer\narchitecture that better suits the tasks at hand. RL is employed to navigate\nalong search space, and special parameter sharing strategies are designed to\naccelerate the search. It is shown that sampling a proportion of training data\nper epoch during search help to improve the search quality. Experiments on the\nCoNLL03, Multi-30k, IWSLT14 and WMT-14 shows that the searched transformer\nmodel can outperform the standard transformers. In particular, we show that our\nlearned model can be trained more robustly with large learning rates without\nwarm-up.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.07502,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000160601,
      "text":"Contextualized Perturbation for Textual Adversarial Attack\n\n  Adversarial examples expose the vulnerabilities of natural language\nprocessing (NLP) models, and can be used to evaluate and improve their\nrobustness. Existing techniques of generating such examples are typically\ndriven by local heuristic rules that are agnostic to the context, often\nresulting in unnatural and ungrammatical outputs. This paper presents CLARE, a\nContextuaLized AdversaRial Example generation model that produces fluent and\ngrammatical outputs through a mask-then-infill procedure. CLARE builds on a\npre-trained masked language model and modifies the inputs in a context-aware\nmanner. We propose three contextualized perturbations, Replace, Insert and\nMerge, allowing for generating outputs of varied lengths. With a richer range\nof available strategies, CLARE is able to attack a victim model more\nefficiently with fewer edits. Extensive experiments and human evaluation\ndemonstrate that CLARE outperforms the baselines in terms of attack success\nrate, textual similarity, fluency and grammaticality.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.0807,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000031458,
      "text":"On the Transferability of Minimal Prediction Preserving Inputs in\n  Question Answering\n\n  Recent work (Feng et al., 2018) establishes the presence of short,\nuninterpretable input fragments that yield high confidence and accuracy in\nneural models. We refer to these as Minimal Prediction Preserving Inputs\n(MPPIs). In the context of question answering, we investigate competing\nhypotheses for the existence of MPPIs, including poor posterior calibration of\nneural models, lack of pretraining, and \"dataset bias\" (where a model learns to\nattend to spurious, non-generalizable cues in the training data). We discover a\nperplexing invariance of MPPIs to random training seed, model architecture,\npretraining, and training domain. MPPIs demonstrate remarkable transferability\nacross domains achieving significantly higher performance than comparably short\nqueries. Additionally, penalizing over-confidence on MPPIs fails to improve\neither generalization or adversarial robustness. These results suggest the\ninterpretability of MPPIs is insufficient to characterize generalization\ncapacity of these models. We hope this focused investigation encourages more\nsystematic analysis of model behavior outside of the human interpretable\ndistribution of examples.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.04798,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000036756,
      "text":"The Grievance Dictionary: Understanding Threatening Language Use\n\n  This paper introduces the Grievance Dictionary, a psycholinguistic dictionary\nwhich can be used to automatically understand language use in the context of\ngrievance-fuelled violence threat assessment. We describe the development the\ndictionary, which was informed by suggestions from experienced threat\nassessment practitioners. These suggestions and subsequent human and\ncomputational word list generation resulted in a dictionary of 20,502 words\nannotated by 2,318 participants. The dictionary was validated by applying it to\ntexts written by violent and non-violent individuals, showing strong evidence\nfor a difference between populations in several dictionary categories. Further\nclassification tasks showed promising performance, but future improvements are\nstill needed. Finally, we provide instructions and suggestions for the use of\nthe Grievance Dictionary by security professionals and (violence) researchers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.0604,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Span-based Semantic Parsing for Compositional Generalization\n\n  Despite the success of sequence-to-sequence (seq2seq) models in semantic\nparsing, recent work has shown that they fail in compositional generalization,\ni.e., the ability to generalize to new structures built of components observed\nduring training. In this work, we posit that a span-based parser should lead to\nbetter compositional generalization. we propose SpanBasedSP, a parser that\npredicts a span tree over an input utterance, explicitly encoding how partial\nprograms compose over spans in the input. SpanBasedSP extends Pasupat et al.\n(2019) to be comparable to seq2seq models by (i) training from programs,\nwithout access to gold trees, treating trees as latent variables, (ii) parsing\na class of non-projective trees through an extension to standard CKY. On\nGeoQuery, SCAN and CLOSURE datasets, SpanBasedSP performs similarly to strong\nseq2seq baselines on random splits, but dramatically improves performance\ncompared to baselines on splits that require compositional generalization: from\n$61.0 \\rightarrow 88.9$ average accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.01826,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"A Python Library for Exploratory Data Analysis on Twitter Data based on\n  Tokens and Aggregated Origin-Destination Information\n\n  Twitter is perhaps the social media more amenable for research. It requires\nonly a few steps to obtain information, and there are plenty of libraries that\ncan help in this regard. Nonetheless, knowing whether a particular event is\nexpressed on Twitter is a challenging task that requires a considerable\ncollection of tweets. This proposal aims to facilitate, to a researcher\ninterested, the process of mining events on Twitter by opening a collection of\nprocessed information taken from Twitter since December 2015. The events could\nbe related to natural disasters, health issues, and people's mobility, among\nother studies that can be pursued with the library proposed. Different\napplications are presented in this contribution to illustrate the library's\ncapabilities: an exploratory analysis of the topics discovered in tweets, a\nstudy on similarity among dialects of the Spanish language, and a mobility\nreport on different countries. In summary, the Python library presented is\napplied to different domains and retrieves a plethora of information in terms\nof frequencies by day of words and bi-grams of words for Arabic, English,\nSpanish, and Russian languages. As well as mobility information related to the\nnumber of travels among locations for more than 200 countries or territories.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.13972,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000435776,
      "text":"Neural Topic Modeling by Incorporating Document Relationship Graph\n\n  Graph Neural Networks (GNNs) that capture the relationships between graph\nnodes via message passing have been a hot research direction in the natural\nlanguage processing community. In this paper, we propose Graph Topic Model\n(GTM), a GNN based neural topic model that represents a corpus as a document\nrelationship graph. Documents and words in the corpus become nodes in the graph\nand are connected based on document-word co-occurrences. By introducing the\ngraph structure, the relationships between documents are established through\ntheir shared words and thus the topical representation of a document is\nenriched by aggregating information from its neighboring nodes using graph\nconvolution. Extensive experiments on three datasets were conducted and the\nresults demonstrate the effectiveness of the proposed approach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.14463,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000061591,
      "text":"Neural RST-based Evaluation of Discourse Coherence\n\n  This paper evaluates the utility of Rhetorical Structure Theory (RST) trees\nand relations in discourse coherence evaluation. We show that incorporating\nsilver-standard RST features can increase accuracy when classifying coherence.\nWe demonstrate this through our tree-recursive neural model, namely\nRST-Recursive, which takes advantage of the text's RST features produced by a\nstate of the art RST parser. We evaluate our approach on the Grammarly Corpus\nfor Discourse Coherence (GCDC) and show that when ensembled with the current\nstate of the art, we can achieve the new state of the art accuracy on this\nbenchmark. Furthermore, when deployed alone, RST-Recursive achieves competitive\naccuracy while having 62% fewer parameters.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.14375,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000084771,
      "text":"Generation of lyrics lines conditioned on music audio clips\n\n  We present a system for generating novel lyrics lines conditioned on music\naudio. A bimodal neural network model learns to generate lines conditioned on\nany given short audio clip. The model consists of a spectrogram variational\nautoencoder (VAE) and a text VAE. Both automatic and human evaluations\ndemonstrate effectiveness of our model in generating lines that have an\nemotional impact matching a given audio clip. The system is intended to serve\nas a creativity tool for songwriters.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.01134,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000019206,
      "text":"An exploratory study of L1-specific non-words\n\n  In this paper, we explore L1-specific non-words, i.e. non-words in a target\nlanguage (in this case Swedish) that are re-ranked by a different-language\nlanguage model. We surmise that speakers of a certain L1 will react different\nto L1-specific non-words than to general non-words. We present the results from\ntwo small case studies exploring whether re-ranking non-words with different\nlanguage models leads to a perceived difference in `Swedishness' (pilot study\n1) and whether German and English native speakers have longer reaction times in\na lexical decision task when presented with their respective L1-specific\nnon-words (pilot study 2). Tentative results seem to indicate that L1-specific\nnon-words are processed second-slowest, after purely Swedish-looking non-words.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.12643,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.00000553,
      "text":"Recurrent Inference in Text Editing\n\n  In neural text editing, prevalent sequence-to-sequence based approaches\ndirectly map the unedited text either to the edited text or the editing\noperations, in which the performance is degraded by the limited source text\nencoding and long, varying decoding steps. To address this problem, we propose\na new inference method, Recurrence, that iteratively performs editing actions,\nsignificantly narrowing the problem space. In each iteration, encoding the\npartially edited text, Recurrence decodes the latent representation, generates\nan action of short, fixed-length, and applies the action to complete a single\nedit. For a comprehensive comparison, we introduce three types of text editing\ntasks: Arithmetic Operators Restoration (AOR), Arithmetic Equation\nSimplification (AES), Arithmetic Equation Correction (AEC). Extensive\nexperiments on these tasks with varying difficulties demonstrate that\nRecurrence achieves improvements over conventional inference methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.1134,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000065896,
      "text":"The importance of fillers for text representations of speech transcripts\n\n  While being an essential component of spoken language, fillers (e.g.\"um\" or\n\"uh\") often remain overlooked in Spoken Language Understanding (SLU) tasks. We\nexplore the possibility of representing them with deep contextualised\nembeddings, showing improvements on modelling spoken language and two\ndownstream tasks - predicting a speaker's stance and expressed confidence.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.08153,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000030001,
      "text":"End-to-End Neural Event Coreference Resolution\n\n  Traditional event coreference systems usually rely on pipeline framework and\nhand-crafted features, which often face error propagation problem and have poor\ngeneralization ability. In this paper, we propose an End-to-End Event\nCoreference approach -- E3C neural network, which can jointly model event\ndetection and event coreference resolution tasks, and learn to extract features\nfrom raw text automatically. Furthermore, because event mentions are highly\ndiversified and event coreference is intricately governed by long-distance,\nsemantic-dependent decisions, a type-guided event coreference mechanism is\nfurther proposed in our E3C neural network. Experiments show that our method\nachieves new state-of-the-art performance on two standard datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.07481,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"Unsupervised Summarization by Jointly Extracting Sentences and Keywords\n\n  We present RepRank, an unsupervised graph-based ranking model for extractive\nmulti-document summarization in which the similarity between words, sentences,\nand word-to-sentence can be estimated by the distances between their vector\nrepresentations in a unified vector space. In order to obtain desirable\nrepresentations, we propose a self-attention based learning method that\nrepresent a sentence by the weighted sum of its word embeddings, and the\nweights are concentrated to those words hopefully better reflecting the content\nof a document. We show that salient sentences and keywords can be extracted in\na joint and mutual reinforcement process using our learned representations, and\nprove that this process always converges to a unique solution leading to\nimprovement in performance. A variant of absorbing random walk and the\ncorresponding sampling-based algorithm are also described to avoid redundancy\nand increase diversity in the summaries. Experiment results with multiple\nbenchmark datasets show that RepRank achieved the best or comparable\nperformance in ROUGE.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.02779,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000188417,
      "text":"UPB at SemEval-2020 Task 8: Joint Textual and Visual Modeling in a\n  Multi-Task Learning Architecture for Memotion Analysis\n\n  Users from the online environment can create different ways of expressing\ntheir thoughts, opinions, or conception of amusement. Internet memes were\ncreated specifically for these situations. Their main purpose is to transmit\nideas by using combinations of images and texts such that they will create a\ncertain state for the receptor, depending on the message the meme has to send.\nThese posts can be related to various situations or events, thus adding a funny\nside to any circumstance our world is situated in. In this paper, we describe\nthe system developed by our team for SemEval-2020 Task 8: Memotion Analysis.\nMore specifically, we introduce a novel system to analyze these posts, a\nmultimodal multi-task learning architecture that combines ALBERT for text\nencoding with VGG-16 for image representation. In this manner, we show that the\ninformation behind them can be properly revealed. Our approach achieves good\nperformance on each of the three subtasks of the current competition, ranking\n11th for Subtask A (0.3453 macro F1-score), 1st for Subtask B (0.5183 macro\nF1-score), and 3rd for Subtask C (0.3171 macro F1-score) while exceeding the\nofficial baseline results by high margins.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.03548,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Probabilistic Case-based Reasoning for Open-World Knowledge Graph\n  Completion\n\n  A case-based reasoning (CBR) system solves a new problem by retrieving\n`cases' that are similar to the given problem. If such a system can achieve\nhigh accuracy, it is appealing owing to its simplicity, interpretability, and\nscalability. In this paper, we demonstrate that such a system is achievable for\nreasoning in knowledge-bases (KBs). Our approach predicts attributes for an\nentity by gathering reasoning paths from similar entities in the KB. Our\nprobabilistic model estimates the likelihood that a path is effective at\nanswering a query about the given entity. The parameters of our model can be\nefficiently computed using simple path statistics and require no iterative\noptimization. Our model is non-parametric, growing dynamically as new entities\nand relations are added to the KB. On several benchmark datasets our approach\nsignificantly outperforms other rule learning approaches and performs\ncomparably to state-of-the-art embedding-based approaches. Furthermore, we\ndemonstrate the effectiveness of our model in an \"open-world\" setting where new\nentities arrive in an online fashion, significantly outperforming\nstate-of-the-art approaches and nearly matching the best offline method. Code\navailable at https:\/\/github.com\/ameyagodbole\/Prob-CBR\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.10669,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000041061,
      "text":"Transition-based Parsing with Stack-Transformers\n\n  Modeling the parser state is key to good performance in transition-based\nparsing. Recurrent Neural Networks considerably improved the performance of\ntransition-based systems by modelling the global state, e.g. stack-LSTM\nparsers, or local state modeling of contextualized features, e.g. Bi-LSTM\nparsers. Given the success of Transformer architectures in recent parsing\nsystems, this work explores modifications of the sequence-to-sequence\nTransformer architecture to model either global or local parser states in\ntransition-based parsing. We show that modifications of the cross attention\nmechanism of the Transformer considerably strengthen performance both on\ndependency and Abstract Meaning Representation (AMR) parsing tasks,\nparticularly for smaller models or limited training data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.1149,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"On the Effects of Using word2vec Representations in Neural Networks for\n  Dialogue Act Recognition\n\n  Dialogue act recognition is an important component of a large number of\nnatural language processing pipelines. Many research works have been carried\nout in this area, but relatively few investigate deep neural networks and word\nembeddings. This is surprising, given that both of these techniques have proven\nexceptionally good in most other language-related domains. We propose in this\nwork a new deep neural network that explores recurrent models to capture word\nsequences within sentences, and further study the impact of pretrained word\nembeddings. We validate this model on three languages: English, French and\nCzech. The performance of the proposed approach is consistent across these\nlanguages and it is comparable to the state-of-the-art results in English. More\nimportantly, we confirm that deep neural networks indeed outperform a Maximum\nEntropy classifier, which was expected. However , and this is more surprising,\nwe also found that standard word2vec em-beddings do not seem to bring valuable\ninformation for this task and the proposed model, whatever the size of the\ntraining corpus is. We thus further analyse the resulting embeddings and\nconclude that a possible explanation may be related to the mismatch between the\ntype of lexical-semantic information captured by the word2vec embeddings, and\nthe kind of relations between words that is the most useful for the dialogue\nact recognition task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.02301,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000018345,
      "text":"PAIR: Planning and Iterative Refinement in Pre-trained Transformers for\n  Long Text Generation\n\n  Pre-trained Transformers have enabled impressive breakthroughs in generating\nlong and fluent text, yet their outputs are often \"rambling\" without coherently\narranged content. In this work, we present a novel content-controlled text\ngeneration framework, PAIR, with planning and iterative refinement, which is\nbuilt upon a large model, BART. We first adapt the BERT model to automatically\nconstruct the content plans, consisting of keyphrase assignments and their\ncorresponding sentence-level positions. The BART model is employed for\ngeneration without modifying its structure. We then propose a refinement\nalgorithm to gradually enhance the generation quality within the\nsequence-to-sequence framework. Evaluation with automatic metrics shows that\nadding planning consistently improves the generation quality on three distinct\ndomains, with an average of 20 BLEU points and 12 METEOR points improvements.\nIn addition, human judges rate our system outputs to be more relevant and\ncoherent than comparisons without planning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.13057,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000091725,
      "text":"Contextualized Word Embeddings Encode Aspects of Human-Like Word Sense\n  Knowledge\n\n  Understanding context-dependent variation in word meanings is a key aspect of\nhuman language comprehension supported by the lexicon. Lexicographic resources\n(e.g., WordNet) capture only some of this context-dependent variation; for\nexample, they often do not encode how closely senses, or discretized word\nmeanings, are related to one another. Our work investigates whether recent\nadvances in NLP, specifically contextualized word embeddings, capture\nhuman-like distinctions between English word senses, such as polysemy and\nhomonymy. We collect data from a behavioral, web-based experiment, in which\nparticipants provide judgments of the relatedness of multiple WordNet senses of\na word in a two-dimensional spatial arrangement task. We find that\nparticipants' judgments of the relatedness between senses are correlated with\ndistances between senses in the BERT embedding space. Homonymous senses (e.g.,\nbat as mammal vs. bat as sports equipment) are reliably more distant from one\nanother in the embedding space than polysemous ones (e.g., chicken as animal\nvs. chicken as meat). Our findings point towards the potential utility of\ncontinuous-space representations of sense meanings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.08242,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000446041,
      "text":"Unsupervised Extractive Summarization by Pre-training Hierarchical\n  Transformers\n\n  Unsupervised extractive document summarization aims to select important\nsentences from a document without using labeled summaries during training.\nExisting methods are mostly graph-based with sentences as nodes and edge\nweights measured by sentence similarities. In this work, we find that\ntransformer attentions can be used to rank sentences for unsupervised\nextractive summarization. Specifically, we first pre-train a hierarchical\ntransformer model using unlabeled documents only. Then we propose a method to\nrank sentences using sentence-level self-attentions and pre-training\nobjectives. Experiments on CNN\/DailyMail and New York Times datasets show our\nmodel achieves state-of-the-art performance on unsupervised summarization. We\nalso find in experiments that our model is less dependent on sentence\npositions. When using a linear combination of our model and a recent\nunsupervised model explicitly modeling sentence positions, we obtain even\nbetter results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.01556,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000133117,
      "text":"Reverse Operation based Data Augmentation for Solving Math Word Problems\n\n  Automatically solving math word problems is a critical task in the field of\nnatural language processing. Recent models have reached their performance\nbottleneck and require more high-quality data for training. We propose a novel\ndata augmentation method that reverses the mathematical logic of math word\nproblems to produce new high-quality math problems and introduce new knowledge\npoints that can benefit learning the mathematical reasoning logic. We apply the\naugmented data on two SOTA math word problem solving models and compare our\nresults with a strong data augmentation baseline. Experimental results show the\neffectiveness of our approach. We release our code and data at\nhttps:\/\/github.com\/yiyunya\/RODA.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.12882,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000030796,
      "text":"FedE: Embedding Knowledge Graphs in Federated Setting\n\n  Knowledge graphs (KGs) consisting of triples are always incomplete, so it's\nimportant to do Knowledge Graph Completion (KGC) by predicting missing triples.\nMulti-Source KG is a common situation in real KG applications which can be\nviewed as a set of related individual KGs where different KGs contains\nrelations of different aspects of entities. It's intuitive that, for each\nindividual KG, its completion could be greatly contributed by the triples\ndefined and labeled in other ones. However, because of the data privacy and\nsensitivity, a set of relevant knowledge graphs cannot complement each other's\nKGC by just collecting data from different knowledge graphs together.\nTherefore, in this paper, we introduce federated setting to keep their privacy\nwithout triple transferring between KGs and apply it in embedding knowledge\ngraph, a typical method which have proven effective for KGC in the past decade.\nWe propose a Federated Knowledge Graph Embedding framework FedE, focusing on\nlearning knowledge graph embeddings by aggregating locally-computed updates.\nFinally, we conduct extensive experiments on datasets derived from KGE\nbenchmark datasets and results show the effectiveness of our proposed FedE.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.08197,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000061923,
      "text":"Lexicon-constrained Copying Network for Chinese Abstractive\n  Summarization\n\n  Copy mechanism allows sequence-to-sequence models to choose words from the\ninput and put them directly into the output, which is finding increasing use in\nabstractive summarization. However, since there is no explicit delimiter in\nChinese sentences, most existing models for Chinese abstractive summarization\ncan only perform character copy, resulting in inefficient. To solve this\nproblem, we propose a lexicon-constrained copying network that models\nmulti-granularity in both encoder and decoder. On the source side, words and\ncharacters are aggregated into the same input memory using a Transformerbased\nencoder. On the target side, the decoder can copy either a character or a\nmulti-character word at each time step, and the decoding process is guided by a\nword-enhanced search algorithm that facilitates the parallel computation and\nencourages the model to copy more words. Moreover, we adopt a word selector to\nintegrate keyword information. Experiments results on a Chinese social media\ndataset show that our model can work standalone or with the word selector. Both\nforms can outperform previous character-based models and achieve competitive\nperformances.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.01461,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000843075,
      "text":"Sentence Constituent-Aware Aspect-Category Sentiment Analysis with Graph\n  Attention Networks\n\n  Aspect category sentiment analysis (ACSA) aims to predict the sentiment\npolarities of the aspect categories discussed in sentences. Since a sentence\nusually discusses one or more aspect categories and expresses different\nsentiments toward them, various attention-based methods have been developed to\nallocate the appropriate sentiment words for the given aspect category and\nobtain promising results. However, most of these methods directly use the given\naspect category to find the aspect category-related sentiment words, which may\ncause mismatching between the sentiment words and the aspect categories when an\nunrelated sentiment word is semantically meaningful for the given aspect\ncategory. To mitigate this problem, we propose a Sentence Constituent-Aware\nNetwork (SCAN) for aspect-category sentiment analysis. SCAN contains two graph\nattention modules and an interactive loss function. The graph attention modules\ngenerate representations of the nodes in sentence constituency parse trees for\nthe aspect category detection (ACD) task and the ACSA task, respectively. ACD\naims to detect aspect categories discussed in sentences and is a auxiliary\ntask. For a given aspect category, the interactive loss function helps the ACD\ntask to find the nodes which can predict the aspect category but can't predict\nother aspect categories. The sentiment words in the nodes then are used to\npredict the sentiment polarity of the aspect category by the ACSA task. The\nexperimental results on five public datasets demonstrate the effectiveness of\nSCAN.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.11747,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0003809399,
      "text":"CUNI Systems for the Unsupervised and Very Low Resource Translation Task\n  in WMT20\n\n  This paper presents a description of CUNI systems submitted to the WMT20 task\non unsupervised and very low-resource supervised machine translation between\nGerman and Upper Sorbian. We experimented with training on synthetic data and\npre-training on a related language pair. In the fully unsupervised scenario, we\nachieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian,\nrespectively. Our low-resource systems relied on transfer learning from\nGerman-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an\nimprovement of 10 BLEU points over the baseline trained only on the available\nsmall German-Upper Sorbian parallel corpus.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.11032,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Classifying Syntactic Errors in Learner Language\n\n  We present a method for classifying syntactic errors in learner language,\nnamely errors whose correction alters the morphosyntactic structure of a\nsentence.\n  The methodology builds on the established Universal Dependencies syntactic\nrepresentation scheme, and provides complementary information to other\nerror-classification systems.\n  Unlike existing error classification methods, our method is applicable across\nlanguages, which we showcase by producing a detailed picture of syntactic\nerrors in learner English and learner Russian. We further demonstrate the\nutility of the methodology for analyzing the outputs of leading Grammatical\nError Correction (GEC) systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.11548,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Method of noun phrase detection in Ukrainian texts\n\n  Introduction. The area of natural language processing considers AI-complete\ntasks that cannot be solved using traditional algorithmic actions. Such tasks\nare commonly implemented with the usage of machine learning methodology and\nmeans of computer linguistics. One of the preprocessing tasks of a text is the\nsearch of noun phrases. The accuracy of this task has implications for the\neffectiveness of many other tasks in the area of natural language processing.\nIn spite of the active development of research in the area of natural language\nprocessing, the investigation of the search for noun phrases within Ukrainian\ntexts are still at an early stage. Results. The different methods of noun\nphrases detection have been analyzed. The expediency of the representation of\nsentences as a tree structure has been justified. The key disadvantage of many\nmethods of noun phrase detection is the severe dependence of the effectiveness\nof their detection from the features of a certain language. Taking into account\nthe unified format of sentence processing and the availability of the trained\nmodel for the building of sentence trees for Ukrainian texts, the Universal\nDependency model has been chosen. The complex method of noun phrases detection\nin Ukrainian texts utilizing Universal Dependencies means and named-entity\nrecognition model has been suggested. Experimental verification of the\neffectiveness of the suggested method on the corpus of Ukrainian news has been\nperformed. Different metrics of method accuracy have been calculated.\nConclusions. The results obtained can indicate that the suggested method can be\nused to find noun phrases in Ukrainian texts. An accuracy increase of the\nmethod can be made with the usage of appropriate named-entity recognition\nmodels according to a subject area.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.0084,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"MEGATRON-CNTRL: Controllable Story Generation with External Knowledge\n  Using Large-Scale Language Models\n\n  Existing pre-trained large language models have shown unparalleled generative\ncapabilities. However, they are not controllable. In this paper, we propose\nMEGATRON-CNTRL, a novel framework that uses large-scale language models and\nadds control to text generation by incorporating an external knowledge base.\nOur framework consists of a keyword predictor, a knowledge retriever, a\ncontextual knowledge ranker, and a conditional text generator. As we do not\nhave access to ground-truth supervision for the knowledge ranker, we make use\nof weak supervision from sentence embedding. The empirical results show that\nour model generates more fluent, consistent, and coherent stories with less\nrepetition and higher diversity compared to prior work on the ROC story\ndataset. We showcase the controllability of our model by replacing the keywords\nused to generate stories and re-running the generation process. Human\nevaluation results show that 77.5% of these stories are successfully controlled\nby the new keywords. Furthermore, by scaling our model from 124 million to 8.3\nbillion parameters we demonstrate that larger models improve both the quality\nof generation (from 74.5% to 93.0% for consistency) and controllability (from\n77.5% to 91.5%).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.09517,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Heads-up! Unsupervised Constituency Parsing via Self-Attention Heads\n\n  Transformer-based pre-trained language models (PLMs) have dramatically\nimproved the state of the art in NLP across many tasks. This has led to\nsubstantial interest in analyzing the syntactic knowledge PLMs learn. Previous\napproaches to this question have been limited, mostly using test suites or\nprobes. Here, we propose a novel fully unsupervised parsing approach that\nextracts constituency trees from PLM attention heads. We rank transformer\nattention heads based on their inherent properties, and create an ensemble of\nhigh-ranking heads to produce the final tree. Our method is adaptable to\nlow-resource languages, as it does not rely on development sets, which can be\nexpensive to annotate. Our experiments show that the proposed method often\noutperform existing approaches if there is no development set present. Our\nunsupervised parser can also be used as a tool to analyze the grammars PLMs\nlearn implicitly. For this, we use the parse trees induced by our method to\ntrain a neural PCFG and compare it to a grammar derived from a human-annotated\ntreebank.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.13062,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000172522,
      "text":"Transgender Community Sentiment Analysis from Social Media Data: A\n  Natural Language Processing Approach\n\n  Transgender community is experiencing a huge disparity in mental health\nconditions compared with the general population. Interpreting the social medial\ndata posted by transgender people may help us understand the sentiments of\nthese sexual minority groups better and apply early interventions. In this\nstudy, we manually categorize 300 social media comments posted by transgender\npeople to the sentiment of negative, positive, and neutral. 5 machine learning\nalgorithms and 2 deep neural networks are adopted to build sentiment analysis\nclassifiers based on the annotated data. Results show that our annotations are\nreliable with a high Cohen's Kappa score over 0.8 across all three classes.\nLSTM model yields an optimal performance of accuracy over 0.85 and AUC of\n0.876. Our next step will focus on using advanced natural language processing\nalgorithms on a larger annotated dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.10811,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000026789,
      "text":"STN4DST: A Scalable Dialogue State Tracking based on Slot Tagging\n  Navigation\n\n  Scalability for handling unknown slot values is a important problem in\ndialogue state tracking (DST). As far as we know, previous scalable DST\napproaches generally rely on either the candidate generation from slot tagging\noutput or the span extraction in dialogue context. However, the candidate\ngeneration based DST often suffers from error propagation due to its pipelined\ntwo-stage process; meanwhile span extraction based DST has the risk of\ngenerating invalid spans in the lack of semantic constraints between start and\nend position pointers. To tackle the above drawbacks, in this paper, we propose\na novel scalable dialogue state tracking method based on slot tagging\nnavigation, which implements an end-to-end single-step pointer to locate and\nextract slot value quickly and accurately by the joint learning of slot tagging\nand slot value position prediction in the dialogue context, especially for\nunknown slot values. Extensive experiments over several benchmark datasets show\nthat the proposed model performs better than state-of-the-art baselines\ngreatly.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.12428,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"Evaluating Language Tools for Fifteen EU-official Under-resourced\n  Languages\n\n  This article presents the results of the evaluation campaign of language\ntools available for fifteen EU-official under-resourced languages. The\nevaluation was conducted within the MSC ITN CLEOPATRA action that aims at\nbuilding the cross-lingual event-centric knowledge processing on top of the\napplication of linguistic processing chains (LPCs) for at least 24 EU-official\nlanguages. In this campaign, we concentrated on three existing NLP platforms\n(Stanford CoreNLP, NLP Cube, UDPipe) that all provide models for\nunder-resourced languages and in this first run we covered 15 under-resourced\nlanguages for which the models were available. We present the design of the\nevaluation campaign and present the results as well as discuss them. We\nconsidered the difference between reported and our tested results within a\nsingle percentage point as being within the limits of acceptable tolerance and\nthus consider this result as reproducible. However, for a number of languages,\nthe results are below what was reported in the literature, and in some cases,\nour testing results are even better than the ones reported previously.\nParticularly problematic was the evaluation of NERC systems. One of the reasons\nis the absence of universally or cross-lingually applicable named entities\nclassification scheme that would serve the NERC task in different languages\nanalogous to the Universal Dependency scheme in parsing task. To build such a\nscheme has become one of our the future research directions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.11383,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000155634,
      "text":"Exploit Multiple Reference Graphs for Semi-supervised Relation\n  Extraction\n\n  Manual annotation of the labeled data for relation extraction is\ntime-consuming and labor-intensive. Semi-supervised methods can offer helping\nhands for this problem and have aroused great research interests. Existing work\nfocuses on mapping the unlabeled samples to the classes to augment the labeled\ndataset. However, it is hard to find an overall good mapping function,\nespecially for the samples with complicated syntactic components in one\nsentence.\n  To tackle this limitation, we propose to build the connection between the\nunlabeled data and the labeled ones rather than directly mapping the unlabeled\nsamples to the classes. Specifically, we first use three kinds of information\nto construct reference graphs, including entity reference, verb reference, and\nsemantics reference. The goal is to semantically or lexically connect the\nunlabeled sample(s) to the labeled one(s). Then, we develop a Multiple\nReference Graph (MRefG) model to exploit the reference information for better\nrecognizing high-quality unlabeled samples. The effectiveness of our method is\ndemonstrated by extensive comparison experiments with the state-of-the-art\nbaselines on two public datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.01791,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000943078,
      "text":"Pruning Redundant Mappings in Transformer Models via Spectral-Normalized\n  Identity Prior\n\n  Traditional (unstructured) pruning methods for a Transformer model focus on\nregularizing the individual weights by penalizing them toward zero. In this\nwork, we explore spectral-normalized identity priors (SNIP), a structured\npruning approach that penalizes an entire residual module in a Transformer\nmodel toward an identity mapping. Our method identifies and discards\nunimportant non-linear mappings in the residual connections by applying a\nthresholding operator on the function norm. It is applicable to any structured\nmodule, including a single attention head, an entire attention block, or a\nfeed-forward subnetwork. Furthermore, we introduce spectral normalization to\nstabilize the distribution of the post-activation values of the Transformer\nlayers, further improving the pruning effectiveness of the proposed\nmethodology. We conduct experiments with BERT on 5 GLUE benchmark tasks to\ndemonstrate that SNIP achieves effective pruning results while maintaining\ncomparable performance. Specifically, we improve the performance over the\nstate-of-the-art by 0.5 to 1.0% on average at 50% compression ratio.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.06467,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Exploiting Cross-Dialectal Gold Syntax for Low-Resource Historical\n  Languages: Towards a Generic Parser for Pre-Modern Slavic\n\n  This paper explores the possibility of improving the performance of\nspecialized parsers for pre-modern Slavic by training them on data from\ndifferent related varieties. Because of their linguistic heterogeneity,\npre-modern Slavic varieties are treated as low-resource historical languages,\nwhereby cross-dialectal treebank data may be exploited to overcome data\nscarcity and attempt the training of a variety-agnostic parser. Previous\nexperiments on early Slavic dependency parsing are discussed, particularly with\nregard to their ability to tackle different orthographic, regional and\nstylistic features. A generic pre-modern Slavic parser and two specialized\nparsers -- one for East Slavic and one for South Slavic -- are trained using\njPTDP (Nguyen & Verspoor 2018), a neural network model for joint part-of-speech\n(POS) tagging and dependency parsing which had shown promising results on a\nnumber of Universal Dependency (UD) treebanks, including Old Church Slavonic\n(OCS). With these experiments, a new state of the art is obtained for both OCS\n(83.79\\% unlabelled attachment score (UAS) and 78.43\\% labelled attachement\nscore (LAS)) and Old East Slavic (OES) (85.7\\% UAS and 80.16\\% LAS).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.00682,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora\n\n  Reflexive anaphora present a challenge for semantic interpretation: their\nmeaning varies depending on context in a way that appears to require abstract\nvariables. Past work has raised doubts about the ability of recurrent networks\nto meet this challenge. In this paper, we explore this question in the context\nof a fragment of English that incorporates the relevant sort of contextual\nvariability. We consider sequence-to-sequence architectures with recurrent\nunits and show that such networks are capable of learning semantic\ninterpretations for reflexive anaphora which generalize to novel antecedents.\nWe explore the effect of attention mechanisms and different recurrent unit\ntypes on the type of training data that is needed for success as measured in\ntwo ways: how much lexical support is needed to induce an abstract reflexive\nmeaning (i.e., how many distinct reflexive antecedents must occur during\ntraining) and what contexts must a noun phrase occur in to support\ngeneralization of reflexive interpretation to this noun phrase?\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.07521,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"Procode: the Swiss Multilingual Solution for Automatic Coding and\n  Recoding of Occupations and Economic Activities\n\n  Objective. Epidemiological studies require data that are in alignment with\nthe classifications established for occupations or economic activities. The\nclassifications usually include hundreds of codes and titles. Manual coding of\nraw data may result in misclassification and be time consuming. The goal was to\ndevelop and test a web-tool, named Procode, for coding of free-texts against\nclassifications and recoding between different classifications. Methods. Three\ntext classifiers, i.e. Complement Naive Bayes (CNB), Support Vector Machine\n(SVM) and Random Forest Classifier (RFC), were investigated using a k-fold\ncross-validation. 30 000 free-texts with manually assigned classification codes\nof French classification of occupations (PCS) and French classification of\nactivities (NAF) were available. For recoding, Procode integrated a workflow\nthat converts codes of one classification to another according to existing\ncrosswalks. Since this is a straightforward operation, only the recoding time\nwas measured. Results. Among the three investigated text classifiers, CNB\nresulted in the best performance, where the classifier predicted accurately\n57-81% and 63-83% classification codes for PCS and NAF, respectively. SVM lead\nto somewhat lower results (by 1-2%), while RFC coded accurately up to 30% of\nthe data. The coding operation required one minute per 10 000 records, while\nthe recoding was faster, i.e. 5-10 seconds. Conclusion. The algorithm\nintegrated in Procode showed satisfactory performance, since the tool had to\nassign the right code by choosing between 500-700 different choices. Based on\nthe results, the authors decided to implement CNB in Procode. In future, if\nanother classifier shows a superior performance, an update will include the\nrequired modifications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.04946,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"When Do You Need Billions of Words of Pretraining Data?\n\n  NLP is currently dominated by general-purpose pretrained language models like\nRoBERTa, which achieve strong performance on NLU tasks through pretraining on\nbillions of words. But what exact knowledge or skills do Transformer LMs learn\nfrom large-scale pretraining that they cannot learn from less data? We adopt\nfour probing methods---classifier probing, information-theoretic probing,\nunsupervised relative acceptability judgment, and fine-tuning on NLU\ntasks---and draw learning curves that track the growth of these different\nmeasures of linguistic ability with respect to pretraining data volume using\nthe MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B\nwords. We find that LMs require only about 10M or 100M words to learn\nrepresentations that reliably encode most syntactic and semantic features we\ntest. A much larger quantity of data is needed in order to acquire enough\ncommonsense knowledge and other skills required to master typical downstream\nNLU tasks. The results suggest that, while the ability to encode linguistic\nfeatures is almost certainly necessary for language understanding, it is likely\nthat other forms of knowledge are the major drivers of recent improvements in\nlanguage understanding among large pretrained models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.04163,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000086096,
      "text":"Chapter Captor: Text Segmentation in Novels\n\n  Books are typically segmented into chapters and sections, representing\ncoherent subnarratives and topics. We investigate the task of predicting\nchapter boundaries, as a proxy for the general task of segmenting long texts.\nWe build a Project Gutenberg chapter segmentation data set of 9,126 English\nnovels, using a hybrid approach combining neural inference and rule matching to\nrecognize chapter title headers in books, achieving an F1-score of 0.77 on this\ntask. Using this annotated data as ground truth after removing structural cues,\nwe present cut-based and neural methods for chapter segmentation, achieving an\nF1-score of 0.453 on the challenging task of exact break prediction over\nbook-length documents. Finally, we reveal interesting historical trends in the\nchapter structure of novels.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.08835,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"Toward Understanding Clinical Context of Medication Change Events in\n  Clinical Narratives\n\n  Understanding medication events in clinical narratives is essential to\nachieving a complete picture of a patient's medication history. While prior\nresearch has explored classification of medication changes from clinical notes,\nstudies to date have not considered the necessary clinical context needed for\ntheir use in real-world applications, such as medication timeline generation\nand medication reconciliation. In this paper, we present the Contextualized\nMedication Event Dataset (CMED), a dataset for capturing relevant context of\nmedication changes documented in clinical notes, which was developed using a\nnovel conceptual framework that organizes context for clinical events into\nvarious orthogonal dimensions. In this process, we define specific contextual\naspects pertinent to medication change events, characterize the dataset, and\nreport the results of preliminary experiments. CMED consists of 9,013\nmedication mentions annotated over 500 clinical notes, and will be released to\nthe community as a shared task in 2021.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.01846,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000064903,
      "text":"Detecting Word Sense Disambiguation Biases in Machine Translation for\n  Model-Agnostic Adversarial Attacks\n\n  Word sense disambiguation is a well-known source of translation errors in\nNMT. We posit that some of the incorrect disambiguation choices are due to\nmodels' over-reliance on dataset artifacts found in training data, specifically\nsuperficial word co-occurrences, rather than a deeper understanding of the\nsource text. We introduce a method for the prediction of disambiguation errors\nbased on statistical data properties, demonstrating its effectiveness across\nseveral domains and model types. Moreover, we develop a simple adversarial\nattack strategy that minimally perturbs sentences in order to elicit\ndisambiguation errors to further probe the robustness of translation models.\nOur findings indicate that disambiguation robustness varies substantially\nbetween domains and that different models trained on the same data are\nvulnerable to different attacks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.05706,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000048677,
      "text":"Multilingual Irony Detection with Dependency Syntax and Neural Models\n\nThis paper presents an in-depth investigation of the effectiveness of dependency-based syntactic features on the irony detection task in a multilingual perspective (English, Spanish, French and Italian). It focuses on the contribution from syntactic knowledge, exploiting linguistic resources where syntax is annotated according to the Universal Dependencies scheme. Three distinct experimental settings are provided. In the first, a variety of syntactic dependency-based features combined with classical machine learning classifiers are explored. In the second scenario, two well-known types of word embeddings are trained on parsed data and tested against gold standard datasets. In the third setting, dependency-based syntactic features are combined into the Multilingual BERT architecture. The results suggest that fine-grained dependency-based syntactic information is informative for the detection of irony.",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.11673,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Does BERT Understand Sentiment? Leveraging Comparisons Between\n  Contextual and Non-Contextual Embeddings to Improve Aspect-Based Sentiment\n  Models\n\n  When performing Polarity Detection for different words in a sentence, we need\nto look at the words around to understand the sentiment. Massively pretrained\nlanguage models like BERT can encode not only just the words in a document but\nalso the context around the words along with them. This begs the questions,\n\"Does a pretrain language model also automatically encode sentiment information\nabout each word?\" and \"Can it be used to infer polarity towards different\naspects?\". In this work we try to answer this question by showing that training\na comparison of a contextual embedding from BERT and a generic word embedding\ncan be used to infer sentiment. We also show that if we finetune a subset of\nweights the model built on comparison of BERT and generic word embedding, it\ncan get state of the art results for Polarity Detection in Aspect Based\nSentiment Classification datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.13115,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000050002,
      "text":"Learning Causal Bayesian Networks from Text\n\n  Causal relationships form the basis for reasoning and decision-making in\nArtificial Intelligence systems. To exploit the large volume of textual data\navailable today, the automatic discovery of causal relationships from text has\nemerged as a significant challenge in recent years. Existing approaches in this\nrealm are limited to the extraction of low-level relations among individual\nevents. To overcome the limitations of the existing approaches, in this paper,\nwe propose a method for automatic inference of causal relationships from human\nwritten language at conceptual level. To this end, we leverage the\ncharacteristics of hierarchy of concepts and linguistic variables created from\ntext, and represent the extracted causal relationships in the form of a Causal\nBayesian Network. Our experiments demonstrate superiority of our approach over\nthe existing approaches in inferring complex causal reasoning from the text.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.03259,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000094043,
      "text":"Alquist 2.0: Alexa Prize Socialbot Based on Sub-Dialogue Models\n\n  This paper presents the second version of the dialogue system named Alquist\ncompeting in Amazon Alexa Prize 2018. We introduce a system leveraging\nontology-based topic structure called topic nodes. Each of the nodes consists\nof several sub-dialogues, and each sub-dialogue has its own LSTM-based model\nfor dialogue management. The sub-dialogues can be triggered according to the\ntopic hierarchy or a user intent which allows the bot to create a unique\nexperience during each session.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.06174,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000008444,
      "text":"Theoretical Rule-based Knowledge Graph Reasoning by Connectivity\n  Dependency Discovery\n\n  Discovering precise and interpretable rules from knowledge graphs is regarded\nas an essential challenge, which can improve the performances of many\ndownstream tasks and even provide new ways to approach some Natural Language\nProcessing research topics. In this paper, we present a fundamental theory for\nrule-based knowledge graph reasoning, based on which the connectivity\ndependencies in the graph are captured via multiple rule types. It is the first\ntime for some of these rule types in a knowledge graph to be considered. Based\non these rule types, our theory can provide precise interpretations to unknown\ntriples. Then, we implement our theory by what we call the RuleDict model.\nResults show that our RuleDict model not only provides precise rules to\ninterpret new triples, but also achieves state-of-the-art performances on one\nbenchmark knowledge graph completion task, and is competitive on other tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.00387,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000024835,
      "text":"Be More with Less: Hypergraph Attention Networks for Inductive Text\n  Classification\n\n  Text classification is a critical research topic with broad applications in\nnatural language processing. Recently, graph neural networks (GNNs) have\nreceived increasing attention in the research community and demonstrated their\npromising results on this canonical task. Despite the success, their\nperformance could be largely jeopardized in practice since they are: (1) unable\nto capture high-order interaction between words; (2) inefficient to handle\nlarge datasets and new documents. To address those issues, in this paper, we\npropose a principled model -- hypergraph attention networks (HyperGAT), which\ncan obtain more expressive power with less computational consumption for text\nrepresentation learning. Extensive experiments on various benchmark datasets\ndemonstrate the efficacy of the proposed approach on the text classification\ntask.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.09825,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Do We Need Online NLU Tools?\n\n  The intent recognition is an essential algorithm of any conversational AI\napplication. It is responsible for the classification of an input message into\nmeaningful classes. In many bot development platforms, we can configure the NLU\npipeline. Several intent recognition services are currently available as an\nAPI, or we choose from many open-source alternatives. However, there is no\ncomparison of intent recognition services and open-source algorithms. Many\nfactors make the selection of the right approach to the intent recognition\nchallenging in practice. In this paper, we suggest criteria to choose the best\nintent recognition algorithm for an application. We present a dataset for\nevaluation. Finally, we compare selected public NLU services with selected\nopen-source algorithms for intent recognition.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.08298,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000045035,
      "text":"Facebook AI's WMT20 News Translation Task Submission\n\n  This paper describes Facebook AI's submission to WMT20 shared news\ntranslation task. We focus on the low resource setting and participate in two\nlanguage pairs, Tamil <-> English and Inuktitut <-> English, where there are\nlimited out-of-domain bitext and monolingual data. We approach the low resource\nproblem using two main strategies, leveraging all available data and adapting\nthe system to the target news domain. We explore techniques that leverage\nbitext and monolingual data from all languages, such as self-supervised model\npretraining, multilingual models, data augmentation, and reranking. To better\nadapt the translation system to the test domain, we explore dataset tagging and\nfine-tuning on in-domain data. We observe that different techniques provide\nvaried improvements based on the available data of the language pair. Based on\nthe finding, we integrate these techniques into one training pipeline. For\nEn->Ta, we explore an unconstrained setup with additional Tamil bitext and\nmonolingual data and show that further improvement can be obtained. On the test\nset, our best submitted systems achieve 21.5 and 13.7 BLEU for Ta->En and\nEn->Ta respectively, and 27.9 and 13.0 for Iu->En and En->Iu respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.05257,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Medical Knowledge-enriched Textual Entailment Framework\n\n  One of the cardinal tasks in achieving robust medical question answering\nsystems is textual entailment. The existing approaches make use of an ensemble\nof pre-trained language models or data augmentation, often to clock higher\nnumbers on the validation metrics. However, two major shortcomings impede\nhigher success in identifying entailment: (1) understanding the focus\/intent of\nthe question and (2) ability to utilize the real-world background knowledge to\ncapture the context beyond the sentence. In this paper, we present a novel\nMedical Knowledge-Enriched Textual Entailment framework that allows the model\nto acquire a semantic and global representation of the input medical text with\nthe help of a relevant domain-specific knowledge graph. We evaluate our\nframework on the benchmark MEDIQA-RQE dataset and manifest that the use of\nknowledge enriched dual-encoding mechanism help in achieving an absolute\nimprovement of 8.27% over SOTA language models. We have made the source code\navailable here.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.0914,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000214577,
      "text":"Diverse and Non-redundant Answer Set Extraction on Community QA based on\n  DPPs\n\n  In community-based question answering (CQA) platforms, it takes time for a\nuser to get useful information from among many answers. Although one solution\nis an answer ranking method, the user still needs to read through the\ntop-ranked answers carefully. This paper proposes a new task of selecting a\ndiverse and non-redundant answer set rather than ranking the answers. Our\nmethod is based on determinantal point processes (DPPs), and it calculates the\nanswer importance and similarity between answers by using BERT. We built a\ndataset focusing on a Japanese CQA site, and the experiments on this dataset\ndemonstrated that the proposed method outperformed several baseline methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.14344,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Generative Pre-training for Paraphrase Generation by Representing and\n  Predicting Spans in Exemplars\n\n  Paraphrase generation is a long-standing problem and serves an essential role\nin many natural language processing problems. Despite some encouraging results,\nrecent methods either confront the problem of favoring generic utterance or\nneed to retrain the model from scratch for each new dataset. This paper\npresents a novel approach to paraphrasing sentences, extended from the GPT-2\nmodel. We develop a template masking technique, named first-order masking, to\nmasked out irrelevant words in exemplars utilizing POS taggers. So that, the\nparaphrasing task is changed to predicting spans in masked templates. Our\nproposed approach outperforms competitive baselines, especially in the semantic\npreservation aspect. To prevent the model from being biased towards a given\ntemplate, we introduce a technique, referred to as second-order masking, which\nutilizes Bernoulli distribution to control the visibility of the\nfirst-order-masked template's tokens. Moreover, this technique allows the model\nto provide various paraphrased sentences in testing by adjusting the\nsecond-order-masking level. For scale-up objectives, we compare the performance\nof two alternatives template-selection methods, which shows that they were\nequivalent in preserving semantic information.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.01856,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000037418,
      "text":"Finding Friends and Flipping Frenemies: Automatic Paraphrase Dataset\n  Augmentation Using Graph Theory\n\n  Most NLP datasets are manually labeled, so suffer from inconsistent labeling\nor limited size. We propose methods for automatically improving datasets by\nviewing them as graphs with expected semantic properties. We construct a\nparaphrase graph from the provided sentence pair labels, and create an\naugmented dataset by directly inferring labels from the original sentence pairs\nusing a transitivity property. We use structural balance theory to identify\nlikely mislabelings in the graph, and flip their labels. We evaluate our\nmethods on paraphrase models trained using these datasets starting from a\npretrained BERT model, and find that the automatically-enhanced training sets\nresult in more accurate models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.07009,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Matching Theory and Data with Personal-ITY: What a Corpus of Italian\n  YouTube Comments Reveals About Personality\n\n  As a contribution to personality detection in languages other than English,\nwe rely on distant supervision to create Personal-ITY, a novel corpus of\nYouTube comments in Italian, where authors are labelled with personality\ntraits. The traits are derived from one of the mainstream personality theories\nin psychology research, named MBTI. Using personality prediction experiments,\nwe (i) study the task of personality prediction in itself on our corpus as well\nas on TwiSty, a Twitter dataset also annotated with MBTI labels; (ii) carry out\nan extensive, in-depth analysis of the features used by the classifier, and\nview them specifically under the light of the original theory that we used to\ncreate the corpus in the first place. We observe that no single model is best\nat personality detection, and that while some traits are easier than others to\ndetect, and also to match back to theory, for other, less frequent traits the\npicture is much more blurred.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.03502,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000003775,
      "text":"Dialogue Discourse-Aware Graph Model and Data Augmentation for Meeting\n  Summarization\n\n  Meeting summarization is a challenging task due to its dynamic interaction\nnature among multiple speakers and lack of sufficient training data. Existing\nmethods view the meeting as a linear sequence of utterances while ignoring the\ndiverse relations between each utterance. Besides, the limited labeled data\nfurther hinders the ability of data-hungry neural models. In this paper, we try\nto mitigate the above challenges by introducing dialogue-discourse relations.\nFirst, we present a Dialogue Discourse-Dware Meeting Summarizer (DDAMS) to\nexplicitly model the interaction between utterances in a meeting by modeling\ndifferent discourse relations. The core module is a relational graph encoder,\nwhere the utterances and discourse relations are modeled in a graph interaction\nmanner. Moreover, we devise a Dialogue Discourse-Aware Data Augmentation\n(DDADA) strategy to construct a pseudo-summarization corpus from existing input\nmeetings, which is 20 times larger than the original dataset and can be used to\npretrain DDAMS. Experimental results on AMI and ICSI meeting datasets show that\nour full system can achieve SOTA performance. Our codes will be available at:\nhttps:\/\/github.com\/xcfcode\/DDAMS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.04443,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Extractive Opinion Summarization in Quantized Transformer Spaces\n\n  We present the Quantized Transformer (QT), an unsupervised system for\nextractive opinion summarization. QT is inspired by Vector-Quantized\nVariational Autoencoders, which we repurpose for popularity-driven\nsummarization. It uses a clustering interpretation of the quantized space and a\nnovel extraction algorithm to discover popular opinions among hundreds of\nreviews, a significant step towards opinion summarization of practical scope.\nIn addition, QT enables controllable summarization without further training, by\nutilizing properties of the quantized space to extract aspect-specific\nsummaries. We also make publicly available SPACE, a large-scale evaluation\nbenchmark for opinion summarizers, comprising general and aspect-specific\nsummaries for 50 hotels. Experiments demonstrate the promise of our approach,\nwhich is validated by human studies where judges showed clear preference for\nour method over competitive baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.14642,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000198351,
      "text":"Multiple Structural Priors Guided Self Attention Network for Language\n  Understanding\n\n  Self attention networks (SANs) have been widely utilized in recent NLP\nstudies. Unlike CNNs or RNNs, standard SANs are usually position-independent,\nand thus are incapable of capturing the structural priors between sequences of\nwords. Existing studies commonly apply one single mask strategy on SANs for\nincorporating structural priors while failing at modeling more abundant\nstructural information of texts. In this paper, we aim at introducing multiple\ntypes of structural priors into SAN models, proposing the Multiple Structural\nPriors Guided Self Attention Network (MS-SAN) that transforms different\nstructural priors into different attention heads by using a novel multi-mask\nbased multi-head attention mechanism. In particular, we integrate two\ncategories of structural priors, including the sequential order and the\nrelative position of words. For the purpose of capturing the latent\nhierarchical structure of the texts, we extract these information not only from\nthe word contexts but also from the dependency syntax trees. Experimental\nresults on two tasks show that MS-SAN achieves significant improvements against\nother strong baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.14781,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000059936,
      "text":"A Hierarchical Transformer with Speaker Modeling for Emotion Recognition\n  in Conversation\n\n  Emotion Recognition in Conversation (ERC) is a more challenging task than\nconventional text emotion recognition. It can be regarded as a personalized and\ninteractive emotion recognition task, which is supposed to consider not only\nthe semantic information of text but also the influences from speakers. The\ncurrent method models speakers' interactions by building a relation between\nevery two speakers. However, this fine-grained but complicated modeling is\ncomputationally expensive, hard to extend, and can only consider local context.\nTo address this problem, we simplify the complicated modeling to a binary\nversion: Intra-Speaker and Inter-Speaker dependencies, without identifying\nevery unique speaker for the targeted speaker. To better achieve the simplified\ninteraction modeling of speakers in Transformer, which shows excellent ability\nto settle long-distance dependency, we design three types of masks and\nrespectively utilize them in three independent Transformer blocks. The designed\nmasks respectively model the conventional context modeling, Intra-Speaker\ndependency, and Inter-Speaker dependency. Furthermore, different speaker-aware\ninformation extracted by Transformer blocks diversely contributes to the\nprediction, and therefore we utilize the attention mechanism to automatically\nweight them. Experiments on two ERC datasets indicate that our model is\nefficacious to achieve better performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.15837,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Using Natural Language Relations between Answer Choices for Machine\n  Comprehension\n\n  When evaluating an answer choice for Reading Comprehension task, other answer\nchoices available for the question and the answers of related questions about\nthe same paragraph often provide valuable information. In this paper, we\npropose a method to leverage the natural language relations between the answer\nchoices, such as entailment and contradiction, to improve the performance of\nmachine comprehension. We use a stand-alone question answering (QA) system to\nperform QA task and a Natural Language Inference (NLI) system to identify the\nrelations between the choice pairs. Then we perform inference using an Integer\nLinear Programming (ILP)-based relational framework to re-evaluate the\ndecisions made by the standalone QA system in light of the relations identified\nby the NLI system. We also propose a multitask learning model that learns both\nthe tasks jointly.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.10275,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000911951,
      "text":"An Empirical Study of Using Pre-trained BERT Models for Vietnamese\n  Relation Extraction Task at VLSP 2020\n\n  In this paper, we present an empirical study of using pre-trained BERT models\nfor the relation extraction task at the VLSP 2020 Evaluation Campaign. We\napplied two state-of-the-art BERT-based models: R-BERT and BERT model with\nentity starts. For each model, we compared two pre-trained BERT models:\nFPTAI\/vibert and NlpHUST\/vibert4news. We found that NlpHUST\/vibert4news model\nsignificantly outperforms FPTAI\/vibert for the Vietnamese relation extraction\ntask. Finally, we proposed an ensemble model that combines R-BERT and BERT with\nentity starts. Our proposed ensemble model slightly improved against two single\nmodels on the development data and the test data provided by the task\norganizers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.03477,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000221199,
      "text":"Document Graph for Neural Machine Translation\n\n  Previous works have shown that contextual information can improve the\nperformance of neural machine translation (NMT). However, most existing\ndocument-level NMT methods only consider a few number of previous sentences.\nHow to make use of the whole document as global contexts is still a challenge.\nTo address this issue, we hypothesize that a document can be represented as a\ngraph that connects relevant contexts regardless of their distances. We employ\nseveral types of relations, including adjacency, syntactic dependency, lexical\nconsistency, and coreference, to construct the document graph. Then, we\nincorporate both source and target graphs into the conventional Transformer\narchitecture with graph convolutional networks. Experiments on various NMT\nbenchmarks, including IWSLT English--French, Chinese-English, WMT\nEnglish--German and Opensubtitle English--Russian, demonstrate that using\ndocument graphs can significantly improve the translation quality. Extensive\nanalysis verifies that the document graph is beneficial for capturing discourse\nphenomena.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.02578,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"Ve'rdd. Narrowing the Gap between Paper Dictionaries, Low-Resource NLP\n  and Community Involvement\n\n  We present an open-source online dictionary editing system, Ve'rdd, that\noffers a chance to re-evaluate and edit grassroots dictionaries that have been\nexposed to multiple amateur editors. The idea is to incorporate community\nactivities into a state-of-the-art finite-state language description of a\nseriously endangered minority language, Skolt Sami. Problems involve getting\nthe community to take part in things above the pencil-and-paper level. At\ntimes, it seems that the native speakers and the dictionary oriented are\nlacking technical understanding to utilize the infrastructures which might make\ntheir work more meaningful in the future, i.e. multiple reuse of all of their\ninput. Therefore, our system integrates with the existing tools and\ninfrastructures for Uralic language masking the technical complexities behind a\nuser-friendly UI.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.11384,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000109606,
      "text":"Document-Level Relation Extraction with Reconstruction\n\n  In document-level relation extraction (DocRE), graph structure is generally\nused to encode relation information in the input document to classify the\nrelation category between each entity pair, and has greatly advanced the DocRE\ntask over the past several years. However, the learned graph representation\nuniversally models relation information between all entity pairs regardless of\nwhether there are relationships between these entity pairs. Thus, those entity\npairs without relationships disperse the attention of the encoder-classifier\nDocRE for ones with relationships, which may further hind the improvement of\nDocRE. To alleviate this issue, we propose a novel\nencoder-classifier-reconstructor model for DocRE. The reconstructor manages to\nreconstruct the ground-truth path dependencies from the graph representation,\nto ensure that the proposed DocRE model pays more attention to encode entity\npairs with relationships in the training. Furthermore, the reconstructor is\nregarded as a relationship indicator to assist relation classification in the\ninference, which can further improve the performance of DocRE model.\nExperimental results on a large-scale DocRE dataset show that the proposed\nmodel can significantly improve the accuracy of relation extraction on a strong\nheterogeneous graph-based baseline.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.15075,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Human Evaluation of Spoken vs. Visual Explanations for Open-Domain QA\n\n  While research on explaining predictions of open-domain QA systems (ODQA) to\nusers is gaining momentum, most works have failed to evaluate the extent to\nwhich explanations improve user trust. While few works evaluate explanations\nusing user studies, they employ settings that may deviate from the end-user's\nusage in-the-wild: ODQA is most ubiquitous in voice-assistants, yet current\nresearch only evaluates explanations using a visual display, and may\nerroneously extrapolate conclusions about the most performant explanations to\nother modalities. To alleviate these issues, we conduct user studies that\nmeasure whether explanations help users correctly decide when to accept or\nreject an ODQA system's answer. Unlike prior work, we control for explanation\nmodality, e.g., whether they are communicated to users through a spoken or\nvisual interface, and contrast effectiveness across modalities. Our results\nshow that explanations derived from retrieved evidence passages can outperform\nstrong baselines (calibrated confidence) across modalities but the best\nexplanation strategy in fact changes with the modality. We show common failure\ncases of current explanations, emphasize end-to-end evaluation of explanations,\nand caution against evaluating them in proxy modalities that are different from\ndeployment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.08789,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Learning from Mistakes: Using Mis-predictions as Harm Alerts in Language\n  Pre-Training\n\n  Fitting complex patterns in the training data, such as reasoning and\ncommonsense, is a key challenge for language pre-training. According to recent\nstudies and our empirical observations, one possible reason is that some\neasy-to-fit patterns in the training data, such as frequently co-occurring word\ncombinations, dominate and harm pre-training, making it hard for the model to\nfit more complex information. We argue that mis-predictions can help locate\nsuch dominating patterns that harm language understanding. When a\nmis-prediction occurs, there should be frequently co-occurring patterns with\nthe mis-predicted word fitted by the model that lead to the mis-prediction. If\nwe can add regularization to train the model to rely less on such dominating\npatterns when a mis-prediction occurs and focus more on the rest more subtle\npatterns, more information can be efficiently fitted at pre-training. Following\nthis motivation, we propose a new language pre-training method, Mis-Predictions\nas Harm Alerts (MPA). In MPA, when a mis-prediction occurs during pre-training,\nwe use its co-occurrence information to guide several heads of the\nself-attention modules. Some self-attention heads in the Transformer modules\nare optimized to assign lower attention weights to the words in the input\nsentence that frequently co-occur with the mis-prediction while assigning\nhigher weights to the other words. By doing so, the Transformer model is\ntrained to rely less on the dominating frequently co-occurring patterns with\nmis-predictions while focus more on the rest more complex information when\nmis-predictions occur. Our experiments show that MPA expedites the pre-training\nof BERT and ELECTRA and improves their performances on downstream tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.15409,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000131792,
      "text":"UNIMO: Towards Unified-Modal Understanding and Generation via\n  Cross-Modal Contrastive Learning\n\n  Existed pre-training methods either focus on single-modal tasks or\nmulti-modal tasks, and cannot effectively adapt to each other. They can only\nutilize single-modal data (i.e. text or image) or limited multi-modal data\n(i.e. image-text pairs). In this work, we propose a unified-modal pre-training\narchitecture, namely UNIMO, which can effectively adapt to both single-modal\nand multi-modal understanding and generation tasks. Large scale of free text\ncorpus and image collections can be utilized to improve the capability of\nvisual and textual understanding, and cross-modal contrastive learning (CMCL)\nis leveraged to align the textual and visual information into a unified\nsemantic space over a corpus of image-text pairs. As the non-paired\nsingle-modal data is very rich, our model can utilize much larger scale of data\nto learn more generalizable representations. Moreover, the textual knowledge\nand visual knowledge can enhance each other in the unified semantic space. The\nexperimental results show that UNIMO significantly improves the performance of\nseveral single-modal and multi-modal downstream tasks. Our code and pre-trained\nmodels are public at the UNIMO project page https:\/\/unimo-ptm.github.io\/\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.02015,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000089407,
      "text":"Context in Informational Bias Detection\n\n  Informational bias is bias conveyed through sentences or clauses that provide\ntangential, speculative or background information that can sway readers'\nopinions towards entities. By nature, informational bias is context-dependent,\nbut previous work on informational bias detection has not explored the role of\ncontext beyond the sentence. In this paper, we explore four kinds of context\nfor informational bias in English news articles: neighboring sentences, the\nfull article, articles on the same event from other news publishers, and\narticles from the same domain (but potentially different events). We find that\nintegrating event context improves classification performance over a very\nstrong baseline. In addition, we perform the first error analysis of models on\nthis task. We find that the best-performing context-inclusive model outperforms\nthe baseline on longer sentences, and sentences from politically centrist\narticles.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.15455,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Fully Synthetic Data Improves Neural Machine Translation with Knowledge\n  Distillation\n\n  This paper explores augmenting monolingual data for knowledge distillation in\nneural machine translation. Source language monolingual text can be\nincorporated as a forward translation. Interestingly, we find the best way to\nincorporate target language monolingual text is to translate it to the source\nlanguage and round-trip translate it back to the target language, resulting in\na fully synthetic corpus. We find that combining monolingual data from both\nsource and target languages yields better performance than a corpus twice as\nlarge only in one language. Moreover, experiments reveal that the improvement\ndepends upon the provenance of the test set. If the test set was originally in\nthe source language (with the target side written by translators), then forward\ntranslating source monolingual data matters. If the test set was originally in\nthe target language (with the source written by translators), then\nincorporating target monolingual data matters.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.10235,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000188086,
      "text":"AdvExpander: Generating Natural Language Adversarial Examples by\n  Expanding Text\n\n  Adversarial examples are vital to expose the vulnerability of machine\nlearning models. Despite the success of the most popular substitution-based\nmethods which substitutes some characters or words in the original examples,\nonly substitution is insufficient to uncover all robustness issues of models.\nIn this paper, we present AdvExpander, a method that crafts new adversarial\nexamples by expanding text, which is complementary to previous\nsubstitution-based methods. We first utilize linguistic rules to determine\nwhich constituents to expand and what types of modifiers to expand with. We\nthen expand each constituent by inserting an adversarial modifier searched from\na CVAE-based generative model which is pre-trained on a large scale corpus. To\nsearch adversarial modifiers, we directly search adversarial latent codes in\nthe latent space without tuning the pre-trained parameters. To ensure that our\nadversarial examples are label-preserving for text matching, we also constrain\nthe modifications with a heuristic rule. Experiments on three classification\ntasks verify the effectiveness of AdvExpander and the validity of our\nadversarial examples. AdvExpander crafts a new type of adversarial examples by\ntext expansion, thereby promising to reveal new robustness issues.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.04538,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0011210971,
      "text":"Big Green at WNUT 2020 Shared Task-1: Relation Extraction as\n  Contextualized Sequence Classification\n\n  Relation and event extraction is an important task in natural language\nprocessing. We introduce a system which uses contextualized knowledge graph\ncompletion to classify relations and events between known entities in a noisy\ntext environment. We report results which show that our system is able to\neffectively extract relations and events from a dataset of wet lab protocols.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.15534,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000014868,
      "text":"HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions\n\n  Collecting supporting evidence from large corpora of text (e.g., Wikipedia)\nis of great challenge for open-domain Question Answering (QA). Especially, for\nmulti-hop open-domain QA, scattered evidence pieces are required to be gathered\ntogether to support the answer extraction. In this paper, we propose a new\nretrieval target, hop, to collect the hidden reasoning evidence from Wikipedia\nfor complex question answering. Specifically, the hop in this paper is defined\nas the combination of a hyperlink and the corresponding outbound link document.\nThe hyperlink is encoded as the mention embedding which models the structured\nknowledge of how the outbound link entity is mentioned in the textual context,\nand the corresponding outbound link document is encoded as the document\nembedding representing the unstructured knowledge within it. Accordingly, we\nbuild HopRetriever which retrieves hops over Wikipedia to answer complex\nquestions. Experiments on the HotpotQA dataset demonstrate that HopRetriever\noutperforms previously published evidence retrieval methods by large margins.\nMoreover, our approach also yields quantifiable interpretations of the evidence\ncollection process.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.03201,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000028809,
      "text":"A Two-Systems Perspective for Computational Thinking\n\n  Computational Thinking (CT) has emerged as one of the vital thinking skills\nin recent times, especially for Science, Technology, Engineering and Management\n(STEM) graduates. Educators are in search of underlying cognitive models\nagainst which CT can be analyzed and evaluated. This paper suggests adopting\nKahneman's two-systems model as a framework to understand the computational\nthought process. Kahneman's two-systems model postulates that human thinking\nhappens at two levels, i.e. fast and slow thinking. This paper illustrates\nthrough examples that CT activities can be represented and analyzed using\nKahneman's two-systems model. The potential benefits of adopting Kahneman's\ntwo-systems perspective are that it helps us to fix the biases that cause\nerrors in our reasoning. Further, it also provides a set of heuristics to speed\nup reasoning activities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.06971,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000063247,
      "text":"Syntactic representation learning for neural network based TTS with\n  syntactic parse tree traversal\n\n  Syntactic structure of a sentence text is correlated with the prosodic\nstructure of the speech that is crucial for improving the prosody and\nnaturalness of a text-to-speech (TTS) system. Nowadays TTS systems usually try\nto incorporate syntactic structure information with manually designed features\nbased on expert knowledge. In this paper, we propose a syntactic representation\nlearning method based on syntactic parse tree traversal to automatically\nutilize the syntactic structure information. Two constituent label sequences\nare linearized through left-first and right-first traversals from constituent\nparse tree. Syntactic representations are then extracted at word level from\neach constituent label sequence by a corresponding uni-directional gated\nrecurrent unit (GRU) network. Meanwhile, nuclear-norm maximization loss is\nintroduced to enhance the discriminability and diversity of the embeddings of\nconstituent labels. Upsampled syntactic representations and phoneme embeddings\nare concatenated to serve as the encoder input of Tacotron2. Experimental\nresults demonstrate the effectiveness of our proposed approach, with mean\nopinion score (MOS) increasing from 3.70 to 3.82 and ABX preference exceeding\nby 17% compared with the baseline. In addition, for sentences with multiple\nsyntactic parse trees, prosodic differences can be clearly perceived from the\nsynthesized speeches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.11204,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000114905,
      "text":"Leveraging ParsBERT and Pretrained mT5 for Persian Abstractive Text\n  Summarization\n\n  Text summarization is one of the most critical Natural Language Processing\n(NLP) tasks. More and more researches are conducted in this field every day.\nPre-trained transformer-based encoder-decoder models have begun to gain\npopularity for these tasks. This paper proposes two methods to address this\ntask and introduces a novel dataset named pn-summary for Persian abstractive\ntext summarization. The models employed in this paper are mT5 and an\nencoder-decoder version of the ParsBERT model (i.e., a monolingual BERT model\nfor Persian). These models are fine-tuned on the pn-summary dataset. The\ncurrent work is the first of its kind and, by achieving promising results, can\nserve as a baseline for any future work.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.05716,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"SICKNL: A Dataset for Dutch Natural Language Inference\n\n  We present SICK-NL (read: signal), a dataset targeting Natural Language\nInference in Dutch. SICK-NL is obtained by translating the SICK dataset of\nMarelli et al. (2014)from English into Dutch. Having a parallel inference\ndataset allows us to compare both monolingual and multilingual NLP models for\nEnglish and Dutch on the two tasks. In the paper, we motivate and detail the\ntranslation process, perform a baseline evaluation on both the original SICK\ndataset and its Dutch incarnation SICK-NL, taking inspiration from Dutch\nskipgram embeddings and contextualised embedding models. In addition, we\nencapsulate two phenomena encountered in the translation to formulate stress\ntests and verify how well the Dutch models capture syntactic restructurings\nthat do not affect semantics. Our main finding is all models perform worse on\nSICK-NL than on SICK, indicating that the Dutch dataset is more challenging\nthan the English original. Results on the stress tests show that models don't\nfully capture word order freedom in Dutch, warranting future systematic\nstudies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.08201,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000024504,
      "text":"Can Taxonomy Help? Improving Semantic Question Matching using Question\n  Taxonomy\n\n  In this paper, we propose a hybrid technique for semantic question matching.\nIt uses our proposed two-layered taxonomy for English questions by augmenting\nstate-of-the-art deep learning models with question classes obtained from a\ndeep learning based question classifier. Experiments performed on three\nopen-domain datasets demonstrate the effectiveness of our proposed approach. We\nachieve state-of-the-art results on partial ordering question ranking (POQR)\nbenchmark dataset. Our empirical analysis shows that coupling standard\ndistributional features (provided by the question encoder) with knowledge from\ntaxonomy is more effective than either deep learning (DL) or taxonomy-based\nknowledge alone.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.06949,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"HinFlair: pre-trained contextual string embeddings for pos tagging and\n  text classification in the Hindi language\n\n  Recent advancements in language models based on recurrent neural networks and\ntransformers architecture have achieved state-of-the-art results on a wide\nrange of natural language processing tasks such as pos tagging, named entity\nrecognition, and text classification. However, most of these language models\nare pre-trained in high resource languages like English, German, Spanish.\nMulti-lingual language models include Indian languages like Hindi, Telugu,\nBengali in their training corpus, but they often fail to represent the\nlinguistic features of these languages as they are not the primary language of\nthe study. We introduce HinFlair, which is a language representation model\n(contextual string embeddings) pre-trained on a large monolingual Hindi corpus.\nExperiments were conducted on 6 text classification datasets and a Hindi\ndependency treebank to analyze the performance of these contextualized string\nembeddings for the Hindi language. Results show that HinFlair outperforms\nprevious state-of-the-art publicly available pre-trained embeddings for\ndownstream tasks like text classification and pos tagging. Also, HinFlair when\ncombined with FastText embeddings outperforms many transformers-based language\nmodels trained particularly for the Hindi language.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.10096,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"With Measured Words: Simple Sentence Selection for Black-Box\n  Optimization of Sentence Compression Algorithms\n\n  Sentence Compression is the task of generating a shorter, yet grammatical\nversion of a given sentence, preserving the essence of the original sentence.\nThis paper proposes a Black-Box Optimizer for Compression (B-BOC): given a\nblack-box compression algorithm and assuming not all sentences need be\ncompressed -- find the best candidates for compression in order to maximize\nboth compression rate and quality. Given a required compression ratio, we\nconsider two scenarios: (i) single-sentence compression, and (ii)\nsentences-sequence compression. In the first scenario, our optimizer is trained\nto predict how well each sentence could be compressed while meeting the\nspecified ratio requirement. In the latter, the desired compression ratio is\napplied to a sequence of sentences (e.g., a paragraph) as a whole, rather than\non each individual sentence. To achieve that, we use B-BOC to assign an optimal\ncompression ratio to each sentence, then cast it as a Knapsack problem, which\nwe solve using bounded dynamic programming. We evaluate B-BOC on both scenarios\non three datasets, demonstrating that our optimizer improves both accuracy and\nRouge-F1-score compared to direct application of other compression algorithms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.10196,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000048677,
      "text":"A Hybrid Approach to Measure Semantic Relatedness in Biomedical Concepts\n\n  Objective: This work aimed to demonstrate the effectiveness of a hybrid\napproach based on Sentence BERT model and retrofitting algorithm to compute\nrelatedness between any two biomedical concepts. Materials and Methods: We\ngenerated concept vectors by encoding concept preferred terms using ELMo, BERT,\nand Sentence BERT models. We used BioELMo and Clinical ELMo. We used Ontology\nKnowledge Free (OKF) models like PubMedBERT, BioBERT, BioClinicalBERT, and\nOntology Knowledge Injected (OKI) models like SapBERT, CoderBERT, KbBERT, and\nUmlsBERT. We trained all the BERT models using Siamese network on SNLI and STSb\ndatasets to allow the models to learn more semantic information at the phrase\nor sentence level so that they can represent multi-word concepts better.\nFinally, to inject ontology relationship knowledge into concept vectors, we\nused retrofitting algorithm and concepts from various UMLS relationships. We\nevaluated our hybrid approach on four publicly available datasets which also\nincludes the recently released EHR-RelB dataset. EHR-RelB is the largest\npublicly available relatedness dataset in which 89% of terms are multi-word\nwhich makes it more challenging. Results: Sentence BERT models mostly\noutperformed corresponding BERT models. The concept vectors generated using the\nSentence BERT model based on SapBERT and retrofitted using UMLS-related\nconcepts achieved the best results on all four datasets. Conclusions: Sentence\nBERT models are more effective compared to BERT models in computing relatedness\nscores in most of the cases. Injecting ontology knowledge into concept vectors\nfurther enhances their quality and contributes to better relatedness scores.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.10573,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.00000904,
      "text":"Representations for Question Answering from Documents with Tables and\n  Text\n\n  Tables in Web documents are pervasive and can be directly used to answer many\nof the queries searched on the Web, motivating their integration in question\nanswering. Very often information presented in tables is succinct and hard to\ninterpret with standard language representations. On the other hand, tables\noften appear within textual context, such as an article describing the table.\nUsing the information from an article as additional context can potentially\nenrich table representations. In this work we aim to improve question answering\nfrom tables by refining table representations based on information from\nsurrounding text. We also present an effective method to combine text and\ntable-based predictions for question answering from full documents, obtaining\nsignificant improvements on the Natural Questions dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.01213,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Improving Portuguese Semantic Role Labeling with Transformers and\n  Transfer Learning\n\n  The Natural Language Processing task of determining \"Who did what to whom\" is\ncalled Semantic Role Labeling. For English, recent methods based on Transformer\nmodels have allowed for major improvements in this task over the previous state\nof the art. However, for low resource languages, like Portuguese, currently\navailable semantic role labeling models are hindered by scarce training data.\nIn this paper, we explore a model architecture with only a pre-trained\nTransformer-based model, a linear layer, softmax and Viterbi decoding. We\nsubstantially improve the state-of-the-art performance in Portuguese by over 15\nF1. Additionally, we improve semantic role labeling results in Portuguese\ncorpora by exploiting cross-lingual transfer learning using multilingual\npre-trained models, and transfer learning from dependency parsing in\nPortuguese, evaluating the various proposed approaches empirically.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.11492,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000089738,
      "text":"On the Evolution of Syntactic Information Encoded by BERT's\n  Contextualized Representations\n\n  The adaptation of pretrained language models to solve supervised tasks has\nbecome a baseline in NLP, and many recent works have focused on studying how\nlinguistic information is encoded in the pretrained sentence representations.\nAmong other information, it has been shown that entire syntax trees are\nimplicitly embedded in the geometry of such models. As these models are often\nfine-tuned, it becomes increasingly important to understand how the encoded\nknowledge evolves along the fine-tuning. In this paper, we analyze the\nevolution of the embedded syntax trees along the fine-tuning process of BERT\nfor six different tasks, covering all levels of the linguistic structure.\nExperimental results show that the encoded syntactic information is forgotten\n(PoS tagging), reinforced (dependency and constituency parsing) or preserved\n(semantics-related tasks) in different ways along the fine-tuning process\ndepending on the task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.00416,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000031458,
      "text":"Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting\n\n  In this paper, we generalize text infilling (e.g., masked language models) by\nproposing Sequence Span Rewriting (SSR) as a self-supervised\nsequence-to-sequence (seq2seq) pre-training objective. SSR provides more\nfine-grained learning signals for text representations by supervising the model\nto rewrite imperfect spans to ground truth, and it is more consistent than text\ninfilling with many downstream seq2seq tasks that rewrite a source sentences\ninto a target sentence. Our experiments with T5 models on various seq2seq tasks\nshow that SSR can substantially improve seq2seq pre-training. Moreover, we\nobserve SSR is especially helpful to improve pre-training a small-size seq2seq\nmodel with a powerful imperfect span generator, which indicates a new\nperspective of transferring knowledge from a large model to a smaller model for\nseq2seq pre-training.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.09914,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000056293,
      "text":"EGFI: Drug-Drug Interaction Extraction and Generation with Fusion of\n  Enriched Entity and Sentence Information\n\n  The rapid growth in literature accumulates diverse and yet comprehensive\nbiomedical knowledge hidden to be mined such as drug interactions. However, it\nis difficult to extract the heterogeneous knowledge to retrieve or even\ndiscover the latest and novel knowledge in an efficient manner. To address such\na problem, we propose EGFI for extracting and consolidating drug interactions\nfrom large-scale medical literature text data. Specifically, EGFI consists of\ntwo parts: classification and generation. In the classification part, EGFI\nencompasses the language model BioBERT which has been comprehensively\npre-trained on biomedical corpus. In particular, we propose the multi-head\nattention mechanism and pack BiGRU to fuse multiple semantic information for\nrigorous context modeling. In the generation part, EGFI utilizes another\npre-trained language model BioGPT-2 where the generation sentences are selected\nbased on filtering rules. We evaluated the classification part on \"DDIs 2013\"\ndataset and \"DTIs\" dataset, achieving the FI score of 0.842 and 0.720\nrespectively. Moreover, we applied the classification part to distinguish\nhigh-quality generated sentences and verified with the exiting growth truth to\nconfirm the filtered sentences. The generated sentences that are not recorded\nin DrugBank and DDIs 2013 dataset also demonstrate the potential of EGFI to\nidentify novel drug relationships.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.09009,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000040068,
      "text":"Does a Hybrid Neural Network based Feature Selection Model Improve Text\n  Classification?\n\n  Text classification is a fundamental problem in the field of natural language\nprocessing. Text classification mainly focuses on giving more importance to all\nthe relevant features that help classify the textual data. Apart from these,\nthe text can have redundant or highly correlated features. These features\nincrease the complexity of the classification algorithm. Thus, many\ndimensionality reduction methods were proposed with the traditional machine\nlearning classifiers. The use of dimensionality reduction methods with machine\nlearning classifiers has achieved good results. In this paper, we propose a\nhybrid feature selection method for obtaining relevant features by combining\nvarious filter-based feature selection methods and fastText classifier. We then\npresent three ways of implementing a feature selection and neural network\npipeline. We observed a reduction in training time when feature selection\nmethods are used along with neural networks. We also observed a slight increase\nin accuracy on some datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.02157,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"EfficientQA : a RoBERTa Based Phrase-Indexed Question-Answering System\n\n  State-of-the-art extractive question-answering models achieve superhuman\nperformances on the SQuAD benchmark. Yet, they are unreasonably heavy and need\nexpensive GPU computing to answer questions in a reasonable time. Thus, they\ncannot be used in the open-domain question-answering paradigm for real-world\nqueries on hundreds of thousands of documents. In this paper, we explore the\npossibility of transferring the natural language understanding of language\nmodels into dense vectors representing questions and answer candidates to make\nquestion-answering compatible with a simple nearest neighbor search task. This\nnew model, which we call EfficientQA, takes advantage of the pair of sequences\nkind of input of BERT-based models to build meaningful, dense representations\nof candidate answers. These latter are extracted from the context in a\nquestion-agnostic fashion. Our model achieves state-of-the-art results in\nPhrase-Indexed Question Answering (PIQA), beating the previous state-of-art by\n1.3 points in exact-match and 1.4 points in f1-score. These results show that\ndense vectors can embed rich semantic representations of sequences, although\nthese were built from language models not originally trained for the use case.\nThus, to build more resource-efficient NLP systems in the future, training\nlanguage models better adapted to build dense representations of phrases is one\nof the possibilities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.10448,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000044703,
      "text":"PolyLM: Learning about Polysemy through Language Modeling\n\n  To avoid the \"meaning conflation deficiency\" of word embeddings, a number of\nmodels have aimed to embed individual word senses. These methods at one time\nperformed well on tasks such as word sense induction (WSI), but they have since\nbeen overtaken by task-specific techniques which exploit contextualized\nembeddings. However, sense embeddings and contextualization need not be\nmutually exclusive. We introduce PolyLM, a method which formulates the task of\nlearning sense embeddings as a language modeling problem, allowing\ncontextualization techniques to be applied. PolyLM is based on two underlying\nassumptions about word senses: firstly, that the probability of a word\noccurring in a given context is equal to the sum of the probabilities of its\nindividual senses occurring; and secondly, that for a given occurrence of a\nword, one of its senses tends to be much more plausible in the context than the\nothers. We evaluate PolyLM on WSI, showing that it performs considerably better\nthan previous sense embedding techniques, and matches the current\nstate-of-the-art specialized WSI method despite having six times fewer\nparameters. Code and pre-trained models are available at\nhttps:\/\/github.com\/AlanAnsell\/PolyLM.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.06514,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"Linguistically-Enriched and Context-Aware Zero-shot Slot Filling\n\n  Slot filling is identifying contiguous spans of words in an utterance that\ncorrespond to certain parameters (i.e., slots) of a user request\/query. Slot\nfilling is one of the most important challenges in modern task-oriented dialog\nsystems. Supervised learning approaches have proven effective at tackling this\nchallenge, but they need a significant amount of labeled training data in a\ngiven domain. However, new domains (i.e., unseen in training) may emerge after\ndeployment. Thus, it is imperative that these models seamlessly adapt and fill\nslots from both seen and unseen domains -- unseen domains contain unseen slot\ntypes with no training data, and even seen slots in unseen domains are\ntypically presented in different contexts. This setting is commonly referred to\nas zero-shot slot filling. Little work has focused on this setting, with\nlimited experimental evaluation. Existing models that mainly rely on\ncontext-independent embedding-based similarity measures fail to detect slot\nvalues in unseen domains or do so only partially. We propose a new zero-shot\nslot filling neural model, LEONA, which works in three steps. Step one acquires\ndomain-oblivious, context-aware representations of the utterance word by\nexploiting (a) linguistic features; (b) named entity recognition cues; (c)\ncontextual embeddings from pre-trained language models. Step two fine-tunes\nthese rich representations and produces slot-independent tags for each word.\nStep three exploits generalizable context-aware utterance-slot similarity\nfeatures at the word level, uses slot-independent tags, and contextualizes them\nto produce slot-specific predictions for each word. Our thorough evaluation on\nfour diverse public datasets demonstrates that our approach consistently\noutperforms the SOTA models by 17.52%, 22.15%, 17.42%, and 17.95% on average\nfor unseen domains on SNIPS, ATIS, MultiWOZ, and SGD datasets, respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.00838,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Fine-tuning BERT-based models for Plant Health Bulletin Classification\n\n  In the era of digitization, different actors in agriculture produce numerous\ndata. Such data contains already latent historical knowledge in the domain.\nThis knowledge enables us to precisely study natural hazards within global or\nlocal aspects, and then improve the risk prevention tasks and augment the\nyield, which helps to tackle the challenge of growing population and changing\nalimentary habits. In particular, French Plants Health Bulletins (BSV, for its\nname in French Bulletin de Sant{\\'e} du V{\\'e}g{\\'e}tal) give information about\nthe development stages of phytosanitary risks in agricultural production.\nHowever, they are written in natural language, thus, machines and human cannot\nexploit them as efficiently as it could be. Natural language processing (NLP)\ntechnologies aim to automatically process and analyze large amounts of natural\nlanguage data. Since the 2010s, with the increases in computational power and\nparallelization, representation learning and deep learning methods became\nwidespread in NLP. Recent advancements Bidirectional Encoder Representations\nfrom Transformers (BERT) inspire us to rethink of knowledge representation and\nnatural language understanding in plant health management domain. The goal in\nthis work is to propose a BERT-based approach to automatically classify the BSV\nto make their data easily indexable. We sampled 200 BSV to finetune the\npretrained BERT language models and classify them as pest or\/and disease and we\nshow preliminary results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.10213,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000036094,
      "text":"A Trigger-Sense Memory Flow Framework for Joint Entity and Relation\n  Extraction\n\n  Joint entity and relation extraction framework constructs a unified model to\nperform entity recognition and relation extraction simultaneously, which can\nexploit the dependency between the two tasks to mitigate the error propagation\nproblem suffered by the pipeline model. Current efforts on joint entity and\nrelation extraction focus on enhancing the interaction between entity\nrecognition and relation extraction through parameter sharing, joint decoding,\nor other ad-hoc tricks (e.g., modeled as a semi-Markov decision process, cast\nas a multi-round reading comprehension task). However, there are still two\nissues on the table. First, the interaction utilized by most methods is still\nweak and uni-directional, which is unable to model the mutual dependency\nbetween the two tasks. Second, relation triggers are ignored by most methods,\nwhich can help explain why humans would extract a relation in the sentence.\nThey're essential for relation extraction but overlooked. To this end, we\npresent a Trigger-Sense Memory Flow Framework (TriMF) for joint entity and\nrelation extraction. We build a memory module to remember category\nrepresentations learned in entity recognition and relation extraction tasks.\nAnd based on it, we design a multi-level memory flow attention mechanism to\nenhance the bi-directional interaction between entity recognition and relation\nextraction. Moreover, without any human annotations, our model can enhance\nrelation trigger information in a sentence through a trigger sensor module,\nwhich improves the model performance and makes model predictions with better\ninterpretation. Experiment results show that our proposed framework achieves\nstate-of-the-art results by improves the relation F1 to 52.44% (+3.2%) on\nSciERC, 66.49% (+4.9%) on ACE05, 72.35% (+0.6%) on CoNLL04 and 80.66% (+2.3%)\non ADE.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.0076,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000168218,
      "text":"Benchmarking Knowledge-Enhanced Commonsense Question Answering via\n  Knowledge-to-Text Transformation\n\n  A fundamental ability of humans is to utilize commonsense knowledge in\nlanguage understanding and question answering. In recent years, many\nknowledge-enhanced Commonsense Question Answering (CQA) approaches have been\nproposed. However, it remains unclear: (1) How far can we get by exploiting\nexternal knowledge for CQA? (2) How much potential of knowledge has been\nexploited in current CQA models? (3) Which are the most promising directions\nfor future CQA? To answer these questions, we benchmark knowledge-enhanced CQA\nby conducting extensive experiments on multiple standard CQA datasets using a\nsimple and effective knowledge-to-text transformation framework. Experiments\nshow that: (1) Our knowledge-to-text framework is effective and achieves\nstate-of-the-art performance on CommonsenseQA dataset, providing a simple and\nstrong knowledge-enhanced baseline for CQA; (2) The potential of knowledge is\nstill far from being fully exploited in CQA -- there is a significant\nperformance gap from current models to our models with golden knowledge; and\n(3) Context-sensitive knowledge selection, heterogeneous knowledge\nexploitation, and commonsense-rich language models are promising CQA\ndirections.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.10376,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"The Power of Language: Understanding Sentiment Towards the Climate\n  Emergency using Twitter Data\n\n  Understanding how attitudes towards the Climate Emergency vary can hold the\nkey to driving policy changes for effective action to mitigate climate related\nrisk. The Oil and Gas industry account for a significant proportion of global\nemissions and so it could be speculated that there is a relationship between\nCrude Oil Futures and sentiment towards the Climate Emergency. Using Latent\nDirichlet Allocation for Topic Modelling on a bespoke Twitter dataset, this\nstudy shows that it is possible to split the conversation surrounding the\nClimate Emergency into 3 distinct topics. Forecasting Crude Oil Futures using\nSeasonal AutoRegressive Integrated Moving Average Modelling gives promising\nresults with a root mean squared error of 0.196 and 0.209 on the training and\ntesting data respectively. Understanding variation in attitudes towards climate\nemergency provides inconclusive results which could be improved using\nspatial-temporal analysis methods such as Density Based Clustering (DBSCAN).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.09012,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000236101,
      "text":"Multilingual Pre-Trained Transformers and Convolutional NN\n  Classification Models for Technical Domain Identification\n\n  In this paper, we present a transfer learning system to perform technical\ndomain identification on multilingual text data. We have submitted two runs,\none uses the transformer model BERT, and the other uses XLM-ROBERTa with the\nCNN model for text classification. These models allowed us to identify the\ndomain of the given sentences for the ICON 2020 shared Task, TechDOfication:\nTechnical Domain Identification. Our system ranked the best for the subtasks\n1d, 1g for the given TechDOfication dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.03634,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000003775,
      "text":"The Logic for a Mildly Context-Sensitive Fragment of the Lambek-Grishin\n  Calculus\n\n  While context-free grammars are characterized by a simple proof-theoretic\ngrammatical formalism namely categorial grammar and its logic the Lambek\ncalculus, no such characterizations were known for tree-adjoining grammars, and\neven for any mildly context-sensitive languages classes in the last forty years\ndespite some efforts. We settle this problem in this paper. On the basis of the\nexisting fragment of the Lambek-Grishin calculus which captures tree-adjoining\nlanguages, we present a logic called HLG: a proof-theoretic characterization of\ntree-adjoining languages based on the Lambek-Grishin calculus restricted to\nHyperedge-replacement grammar with rank two studied by Moot. HLG is defined in\ndisplay calculus with cut-admissibility. Several new techniques are introduced\nfor the proofs, such as purely structural connectives, usefulness, and a\ngraph-theoretic argument on proof nets for HLG.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.01051,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000483129,
      "text":"SJ_AJ@DravidianLangTech-EACL2021: Task-Adaptive Pre-Training of\n  Multilingual BERT models for Offensive Language Identification\n\n  In this paper we present our submission for the EACL 2021-Shared Task on\nOffensive Language Identification in Dravidian languages. Our final system is\nan ensemble of mBERT and XLM-RoBERTa models which leverage task-adaptive\npre-training of multilingual BERT models with a masked language modeling\nobjective. Our system was ranked 1st for Kannada, 2nd for Malayalam and 3rd for\nTamil.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.05007,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Bootstrapping Relation Extractors using Syntactic Search by Examples\n\n  The advent of neural-networks in NLP brought with it substantial improvements\nin supervised relation extraction. However, obtaining a sufficient quantity of\ntraining data remains a key challenge. In this work we propose a process for\nbootstrapping training datasets which can be performed quickly by\nnon-NLP-experts. We take advantage of search engines over syntactic-graphs\n(Such as Shlain et al. (2020)) which expose a friendly by-example syntax. We\nuse these to obtain positive examples by searching for sentences that are\nsyntactically similar to user input examples. We apply this technique to\nrelations from TACRED and DocRED and show that the resulting models are\ncompetitive with models trained on manually annotated data and on data obtained\nfrom distant supervision. The models also outperform models trained using NLG\ndata augmentation techniques. Extending the search-based approach with the NLG\nmethod further improves the results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.09708,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000043048,
      "text":"Back Translation Survey for Improving Text Augmentation\n\n  Natural Language Processing (NLP) relies heavily on training data.\nTransformers, as they have gotten bigger, have required massive amounts of\ntraining data. To satisfy this requirement, text augmentation should be looked\nat as a way to expand your current dataset and to generalize your models. One\ntext augmentation we will look at is translation augmentation. We take an\nEnglish sentence and translate it to another language before translating it\nback to English. In this paper, we look at the effect of 108 different language\nback translations on various metrics and text embeddings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.09687,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0,
      "text":"Moroccan Dialect -Darija- Open Dataset\n\n  Darija Open Dataset (DODa) is an open-source project for the Moroccan\ndialect. With more than 10,000 entries DODa is arguably the largest open-source\ncollaborative project for Darija-English translation built for Natural Language\nProcessing purposes. In fact, besides semantic categorization, DODa also adopts\na syntactic one, presents words under different spellings, offers verb-to-noun\nand masculine-to-feminine correspondences, contains the conjugation of hundreds\nof verbs in different tenses, and many other subsets to help researchers better\nunderstand and study Moroccan dialect. This data paper presents a description\nof DODa, its features, how it was collected, as well as a first application in\nImage Classification using ImageNet labels translated to Darija. This\ncollaborative project is hosted on GitHub platform under MIT's Open-Source\nlicense and aims to be a standard resource for researchers, students, and\nanyone who is interested in Moroccan Dialect\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.06578,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000028478,
      "text":"Improving Zero-shot Neural Machine Translation on Language-specific\n  Encoders-Decoders\n\n  Recently, universal neural machine translation (NMT) with shared\nencoder-decoder gained good performance on zero-shot translation. Unlike\nuniversal NMT, jointly trained language-specific encoders-decoders aim to\nachieve universal representation across non-shared modules, each of which is\nfor a language or language family. The non-shared architecture has the\nadvantage of mitigating internal language competition, especially when the\nshared vocabulary and model parameters are restricted in their size. However,\nthe performance of using multiple encoders and decoders on zero-shot\ntranslation still lags behind universal NMT. In this work, we study zero-shot\ntranslation using language-specific encoders-decoders. We propose to generalize\nthe non-shared architecture and universal NMT by differentiating the\nTransformer layers between language-specific and interlingua. By selectively\nsharing parameters and applying cross-attentions, we explore maximizing the\nrepresentation universality and realizing the best alignment of\nlanguage-agnostic information. We also introduce a denoising auto-encoding\n(DAE) objective to jointly train the model with the translation task in a\nmulti-task manner. Experiments on two public multilingual parallel datasets\nshow that our proposed model achieves a competitive or better results than\nuniversal NMT and strong pivot baseline. Moreover, we experiment incrementally\nadding new language to the trained model by only updating the new model\nparameters. With this little effort, the zero-shot translation between this\nnewly added language and existing languages achieves a comparable result with\nthe model trained jointly from scratch on all languages.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.01826,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000082122,
      "text":"A Computational Framework for Slang Generation\n\n  Slang is a common type of informal language, but its flexible nature and\npaucity of data resources present challenges for existing natural language\nsystems. We take an initial step toward machine generation of slang by\ndeveloping a framework that models the speaker's word choice in slang context.\nOur framework encodes novel slang meaning by relating the conventional and\nslang senses of a word while incorporating syntactic and contextual knowledge\nin slang usage. We construct the framework using a combination of probabilistic\ninference and neural contrastive learning. We perform rigorous evaluations on\nthree slang dictionaries and show that our approach not only outperforms\nstate-of-the-art language models, but also better predicts the historical\nemergence of slang word usages from 1960s to 2000s. We interpret the proposed\nmodels and find that the contrastively learned semantic space is sensitive to\nthe similarities between slang and conventional senses of words. Our work\ncreates opportunities for the automated generation and interpretation of\ninformal language.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.09777,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000036756,
      "text":"Progressive Transformer-Based Generation of Radiology Reports\n\n  Inspired by Curriculum Learning, we propose a consecutive (i.e.,\nimage-to-text-to-text) generation framework where we divide the problem of\nradiology report generation into two steps. Contrary to generating the full\nradiology report from the image at once, the model generates global concepts\nfrom the image in the first step and then reforms them into finer and coherent\ntexts using a transformer architecture. We follow the transformer-based\nsequence-to-sequence paradigm at each step. We improve upon the\nstate-of-the-art on two benchmark datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.0999,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Analyzing Curriculum Learning for Sentiment Analysis along Task\n  Difficulty, Pacing and Visualization Axes\n\n  While Curriculum Learning (CL) has recently gained traction in Natural\nlanguage Processing Tasks, it is still not adequately analyzed. Previous works\nonly show their effectiveness but fail short to explain and interpret the\ninternal workings fully. In this paper, we analyze curriculum learning in\nsentiment analysis along multiple axes. Some of these axes have been proposed\nby earlier works that need more in-depth study. Such analysis requires\nunderstanding where curriculum learning works and where it does not. Our axes\nof analysis include Task difficulty on CL, comparing CL pacing techniques, and\nqualitative analysis by visualizing the movement of attention scores in the\nmodel as curriculum phases progress. We find that curriculum learning works\nbest for difficult tasks and may even lead to a decrement in performance for\ntasks with higher performance without curriculum learning. We see that One-Pass\ncurriculum strategies suffer from catastrophic forgetting and attention\nmovement visualization within curriculum pacing. This shows that curriculum\nlearning breaks down the challenging main task into easier sub-tasks solved\nsequentially.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.12777,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000074837,
      "text":"IIE-NLP-Eyas at SemEval-2021 Task 4: Enhancing PLM for ReCAM with\n  Special Tokens, Re-Ranking, Siamese Encoders and Back Translation\n\n  This paper introduces our systems for all three subtasks of SemEval-2021 Task\n4: Reading Comprehension of Abstract Meaning. To help our model better\nrepresent and understand abstract concepts in natural language, we well-design\nmany simple and effective approaches adapted to the backbone model (RoBERTa).\nSpecifically, we formalize the subtasks into the multiple-choice question\nanswering format and add special tokens to abstract concepts, then, the final\nprediction of question answering is considered as the result of subtasks.\nAdditionally, we employ many finetuning tricks to improve the performance.\nExperimental results show that our approaches achieve significant performance\ncompared with the baseline systems. Our approaches achieve eighth rank on\nsubtask-1 and tenth rank on subtask-2.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.04895,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000028809,
      "text":"Leveraging cross-platform data to improve automated hate speech\n  detection\n\n  Hate speech is increasingly prevalent online, and its negative outcomes\ninclude increased prejudice, extremism, and even offline hate crime. Automatic\ndetection of online hate speech can help us to better understand these impacts.\nHowever, while the field has recently progressed through advances in natural\nlanguage processing, challenges still remain. In particular, most existing\napproaches for hate speech detection focus on a single social media platform in\nisolation. This limits both the use of these models and their validity, as the\nnature of language varies from platform to platform. Here we propose a new\ncross-platform approach to detect hate speech which leverages multiple datasets\nand classification models from different platforms and trains a superlearner\nthat can combine existing and novel training data to improve detection and\nincrease model applicability. We demonstrate how this approach outperforms\nexisting models, and achieves good performance when tested on messages from\nnovel social media platforms not included in the original training data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.09749,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000091725,
      "text":"Dialect Identification in Nuanced Arabic Tweets Using Farasa\n  Segmentation and AraBERT\n\n  This paper presents our approach to address the EACL WANLP-2021 Shared Task\n1: Nuanced Arabic Dialect Identification (NADI). The task is aimed at\ndeveloping a system that identifies the geographical location(country\/province)\nfrom where an Arabic tweet in the form of modern standard Arabic or dialect\ncomes from. We solve the task in two parts. The first part involves\npre-processing the provided dataset by cleaning, adding and segmenting various\nparts of the text. This is followed by carrying out experiments with different\nversions of two Transformer based models, AraBERT and AraELECTRA. Our final\napproach achieved macro F1-scores of 0.216, 0.235, 0.054, and 0.043 in the four\nsubtasks, and we were ranked second in MSA identification subtasks and fourth\nin DA identification subtasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.00464,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"NLP-CUET@LT-EDI-EACL2021: Multilingual Code-Mixed Hope Speech Detection\n  using Cross-lingual Representation Learner\n\n  In recent years, several systems have been developed to regulate the spread\nof negativity and eliminate aggressive, offensive or abusive contents from the\nonline platforms. Nevertheless, a limited number of researches carried out to\nidentify positive, encouraging and supportive contents. In this work, our goal\nis to identify whether a social media post\/comment contains hope speech or not.\nWe propose three distinct models to identify hope speech in English, Tamil and\nMalayalam language to serve this purpose. To attain this goal, we employed\nvarious machine learning (support vector machine, logistic regression,\nensemble), deep learning (convolutional neural network + long short term\nmemory) and transformer (m-BERT, Indic-BERT, XLNet, XLM-Roberta) based methods.\nResults indicate that XLM-Roberta outdoes all other techniques by gaining a\nweighted $f_1$-score of $0.93$, $0.60$ and $0.85$ respectively for English,\nTamil and Malayalam language. Our team has achieved $1^{st}$, $2^{nd}$ and\n$1^{st}$ rank in these three tasks respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.06788,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000053313,
      "text":"They, Them, Theirs: Rewriting with Gender-Neutral English\n\n  Responsible development of technology involves applications being inclusive\nof the diverse set of users they hope to support. An important part of this is\nunderstanding the many ways to refer to a person and being able to fluently\nchange between the different forms as needed. We perform a case study on the\nsingular they, a common way to promote gender inclusion in English. We define a\nre-writing task, create an evaluation benchmark, and show how a model can be\ntrained to produce gender-neutral English with <1% word error rate with no\nhuman-labeled data. We discuss the practical applications and ethical\nconsiderations of the task, providing direction for future work into inclusive\nnatural language systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.12082,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000029471,
      "text":"Hopeful_Men@LT-EDI-EACL2021: Hope Speech Detection Using Indic\n  Transliteration and Transformers\n\n  This paper aims to describe the approach we used to detect hope speech in the\nHopeEDI dataset. We experimented with two approaches. In the first approach, we\nused contextual embeddings to train classifiers using logistic regression,\nrandom forest, SVM, and LSTM based models.The second approach involved using a\nmajority voting ensemble of 11 models which were obtained by fine-tuning\npre-trained transformer models (BERT, ALBERT, RoBERTa, IndicBERT) after adding\nan output layer. We found that the second approach was superior for English,\nTamil and Malayalam. Our solution got a weighted F1 score of 0.93, 0.75 and\n0.49 for English,Malayalam and Tamil respectively. Our solution ranked first in\nEnglish, eighth in Malayalam and eleventh in Tamil.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.00492,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"RoBERTa-wwm-ext Fine-Tuning for Chinese Text Classification\n\n  Bidirectional Encoder Representations from Transformers (BERT) have shown to\nbe a promising way to dramatically improve the performance across various\nNatural Language Processing tasks [Devlin et al., 2019]. Meanwhile, progress\nmade over the past few years by various Neural Net-work has also proved the\neffectiveness of Neural Network in the field of Natural Language Processing. In\nthis project, RoBERTa-wwm-ext [Cui et al., 2019] pre-train language model was\nadopted and fine-tuned for Chinese text classification. The models were able to\nclassify Chinese texts into two categories, containing descriptions of legal\nbehavior and descriptions of illegal behavior. Four different models are also\nproposed in the paper. Those models will use RoBERTa-wwm-extas their embedding\nlayer and feed the embedding into different neural networks. The motivation\nbe-hind proposing these models is straightforward. By introducing complex\noutput layer architecture, the overall performance of the models could be\nimproved. All the models were trained on a data set derived from Chinese public\ncourt records, and the performance of different models were compared.The\nexperiment shows that the performance of pro-posed models failed to beat the\noriginal RoBERTa-wwm-ext model in terms of accuracy and training efficiency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.12266,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding\n  Learning\n\n  Word embedding learning methods require a large number of occurrences of a\nword to accurately learn its embedding. However, out-of-vocabulary (OOV) words\nwhich do not appear in the training corpus emerge frequently in the smaller\ndownstream data. Recent work formulated OOV embedding learning as a few-shot\nregression problem and demonstrated that meta-learning can improve results\nobtained. However, the algorithm used, model-agnostic meta-learning (MAML) is\nknown to be unstable and perform worse when a large number of gradient steps\nare used for parameter updates. In this work, we propose the use of Leap, a\nmeta-learning algorithm which leverages the entire trajectory of the learning\nprocess instead of just the beginning and the end points, and thus ameliorates\nthese two issues. In our experiments on a benchmark OOV embedding learning\ndataset and in an extrinsic evaluation, Leap performs comparably or better than\nMAML. We go on to examine which contexts are most beneficial to learn an OOV\nembedding from, and propose that the choice of contexts may matter more than\nthe meta-learning employed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.09991,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Using Transformer based Ensemble Learning to classify Scientific\n  Articles\n\n  Many time reviewers fail to appreciate novel ideas of a researcher and\nprovide generic feedback. Thus, proper assignment of reviewers based on their\narea of expertise is necessary. Moreover, reading each and every paper from\nend-to-end for assigning it to a reviewer is a tedious task. In this paper, we\ndescribe a system which our team FideLIPI submitted in the shared task of\nSDPRA-2021 [14]. It comprises four independent sub-systems capable of\nclassifying abstracts of scientific literature to one of the given seven\nclasses. The first one is a RoBERTa [10] based model built over these\nabstracts. Adding topic models \/ Latent dirichlet allocation (LDA) [2] based\nfeatures to the first model results in the second sub-system. The third one is\na sentence level RoBERTa [10] model. The fourth one is a Logistic Regression\nmodel built using Term Frequency Inverse Document Frequency (TF-IDF) features.\nWe ensemble predictions of these four sub-systems using majority voting to\ndevelop the final system which gives a F1 score of 0.93 on the test and\nvalidation set. This outperforms the existing State Of The Art (SOTA) model\nSciBERT's [1] in terms of F1 score on the validation set.Our codebase is\navailable at https:\/\/github.com\/SDPRA-2021\/shared-task\/tree\/main\/FideLIPI\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.04643,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000026491,
      "text":"Efficient Retrieval Augmented Generation from Unstructured Knowledge for\n  Task-Oriented Dialog\n\n  This paper summarizes our work on the first track of the ninth Dialog System\nTechnology Challenge (DSTC 9), \"Beyond Domain APIs: Task-oriented\nConversational Modeling with Unstructured Knowledge Access\". The goal of the\ntask is to generate responses to user turns in a task-oriented dialog that\nrequire knowledge from unstructured documents. The task is divided into three\nsubtasks: detection, selection and generation. In order to be compute\nefficient, we formulate the selection problem in terms of hierarchical\nclassification steps. We achieve our best results with this model.\nAlternatively, we employ siamese sequence embedding models, referred to as\nDense Knowledge Retrieval, to retrieve relevant documents. This method further\nreduces the computation time by a factor of more than 100x at the cost of\ndegradation in R@1 of 5-6% compared to the first model. Then for either\napproach, we use Retrieval Augmented Generation to generate responses based on\nmultiple selected snippets and we show how the method can be used to fine-tune\ntrained embeddings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.02557,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000028478,
      "text":"Adaptive Semiparametric Language Models\n\n  We present a language model that combines a large parametric neural network\n(i.e., a transformer) with a non-parametric episodic memory component in an\nintegrated architecture. Our model uses extended short-term context by caching\nlocal hidden states -- similar to transformer-XL -- and global long-term memory\nby retrieving a set of nearest neighbor tokens at each timestep. We design a\ngating function to adaptively combine multiple information sources to make a\nprediction. This mechanism allows the model to use either local context,\nshort-term memory, or long-term memory (or any combination of them) on an ad\nhoc basis depending on the context. Experiments on word-based and\ncharacter-based language modeling datasets demonstrate the efficacy of our\nproposed method compared to strong baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.01417,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000028478,
      "text":"Two Demonstrations of the Machine Translation Applications to Historical\n  Documents\n\n  We present our demonstration of two machine translation applications to\nhistorical documents. The first task consists in generating a new version of a\nhistorical document, written in the modern version of its original language.\nThe second application is limited to a document's orthography. It adapts the\ndocument's spelling to modern standards in order to achieve an orthography\nconsistency and accounting for the lack of spelling conventions. We followed an\ninteractive, adaptive framework that allows the user to introduce corrections\nto the system's hypothesis. The system reacts to these corrections by\ngenerating a new hypothesis that takes them into account. Once the user is\nsatisfied with the system's hypothesis and validates it, the system adapts its\nmodel following an online learning strategy. This system is implemented\nfollowing a client-server architecture. We developed a website which\ncommunicates with the neural models. All code is open-source and publicly\navailable. The demonstration is hosted at http:\/\/demosmt.prhlt.upv.es\/mthd\/.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.0435,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000055631,
      "text":"Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees\n\n  Pre-trained language models like BERT achieve superior performances in\nvarious NLP tasks without explicit consideration of syntactic information.\nMeanwhile, syntactic information has been proved to be crucial for the success\nof NLP applications. However, how to incorporate the syntax trees effectively\nand efficiently into pre-trained Transformers is still unsettled. In this\npaper, we address this problem by proposing a novel framework named\nSyntax-BERT. This framework works in a plug-and-play mode and is applicable to\nan arbitrary pre-trained checkpoint based on Transformer architecture.\nExperiments on various datasets of natural language understanding verify the\neffectiveness of syntax trees and achieve consistent improvement over multiple\npre-trained models, including BERT, RoBERTa, and T5.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.01328,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"ToxCCIn: Toxic Content Classification with Interpretability\n\n  Despite the recent successes of transformer-based models in terms of\neffectiveness on a variety of tasks, their decisions often remain opaque to\nhumans. Explanations are particularly important for tasks like offensive\nlanguage or toxicity detection on social media because a manual appeal process\nis often in place to dispute automatically flagged content. In this work, we\npropose a technique to improve the interpretability of these models, based on a\nsimple and powerful assumption: a post is at least as toxic as its most toxic\nspan. We incorporate this assumption into transformer models by scoring a post\nbased on the maximum toxicity of its spans and augmenting the training process\nto identify correct spans. We find this approach effective and can produce\nexplanations that exceed the quality of those provided by Logistic Regression\nanalysis (often regarded as a highly-interpretable model), according to a human\nstudy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.11643,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000071194,
      "text":"Complementary Evidence Identification in Open-Domain Question Answering\n\n  This paper proposes a new problem of complementary evidence identification\nfor open-domain question answering (QA). The problem aims to efficiently find a\nsmall set of passages that covers full evidence from multiple aspects as to\nanswer a complex question. To this end, we proposes a method that learns vector\nrepresentations of passages and models the sufficiency and diversity within the\nselected set, in addition to the relevance between the question and passages.\nOur experiments demonstrate that our method considers the dependence within the\nsupporting evidence and significantly improves the accuracy of complementary\nevidence selection in QA domain.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.03095,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"A Survey on Spoken Language Understanding: Recent Advances and New\n  Frontiers\n\n  Spoken Language Understanding (SLU) aims to extract the semantics frame of\nuser queries, which is a core component in a task-oriented dialog system. With\nthe burst of deep neural networks and the evolution of pre-trained language\nmodels, the research of SLU has obtained significant breakthroughs. However,\nthere remains a lack of a comprehensive survey summarizing existing approaches\nand recent trends, which motivated the work presented in this article. In this\npaper, we survey recent advances and new frontiers in SLU. Specifically, we\ngive a thorough review of this research field, covering different aspects\nincluding (1) new taxonomy: we provide a new perspective for SLU filed,\nincluding single model vs. joint model, implicit joint modeling vs. explicit\njoint modeling in joint model, non pre-trained paradigm vs. pre-trained\nparadigm;(2) new frontiers: some emerging areas in complex SLU as well as the\ncorresponding challenges; (3) abundant open-source resources: to help the\ncommunity, we have collected, organized the related papers, baseline projects\nand leaderboard on a public website where SLU researchers could directly access\nto the recent progress. We hope that this survey can shed a light on future\nresearch in SLU field.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.15335,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000038412,
      "text":"Changing the Mind of Transformers for Topically-Controllable Language\n  Generation\n\n  Large Transformer-based language models can aid human authors by suggesting\nplausible continuations of text written so far. However, current interactive\nwriting assistants do not allow authors to guide text generation in desired\ntopical directions. To address this limitation, we design a framework that\ndisplays multiple candidate upcoming topics, of which a user can select a\nsubset to guide the generation. Our framework consists of two components: (1) a\nmethod that produces a set of candidate topics by predicting the centers of\nword clusters in the possible continuations, and (2) a text generation model\nwhose output adheres to the chosen topics. The training of both components is\nself-supervised, using only unlabeled text. Our experiments demonstrate that\nour topic options are better than those of standard clustering approaches, and\nour framework often generates fluent sentences related to the chosen topics, as\njudged by automated metrics and crowdsourced workers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.12412,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Leveraging Multi-domain, Heterogeneous Data using Deep Multitask\n  Learning for Hate Speech Detection\n\n  With the exponential rise in user-generated web content on social media, the\nproliferation of abusive languages towards an individual or a group across the\ndifferent sections of the internet is also rapidly increasing. It is very\nchallenging for human moderators to identify the offensive contents and filter\nthose out. Deep neural networks have shown promise with reasonable accuracy for\nhate speech detection and allied applications. However, the classifiers are\nheavily dependent on the size and quality of the training data. Such a\nhigh-quality large data set is not easy to obtain. Moreover, the existing data\nsets that have emerged in recent times are not created following the same\nannotation guidelines and are often concerned with different types and\nsub-types related to hate. To solve this data sparsity problem, and to obtain\nmore global representative features, we propose a Convolution Neural Network\n(CNN) based multi-task learning models (MTLs)\\footnote{code is available at\nhttps:\/\/github.com\/imprasshant\/STL-MTL} to leverage information from multiple\nsources. Empirical analysis performed on three benchmark datasets shows the\nefficacy of the proposed approach with the significant improvement in accuracy\nand F-score to obtain state-of-the-art performance with respect to the existing\nsystems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.12838,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000026822,
      "text":"Repairing Pronouns in Translation with BERT-Based Post-Editing\n\n  Pronouns are important determinants of a text's meaning but difficult to\ntranslate. This is because pronoun choice can depend on entities described in\nprevious sentences, and in some languages pronouns may be dropped when the\nreferent is inferrable from the context. These issues can lead Neural Machine\nTranslation (NMT) systems to make critical errors on pronouns that impair\nintelligibility and even reinforce gender bias. We investigate the severity of\nthis pronoun issue, showing that (1) in some domains, pronoun choice can\naccount for more than half of a NMT systems' errors, and (2) pronouns have a\ndisproportionately large impact on perceived translation quality. We then\ninvestigate a possible solution: fine-tuning BERT on a pronoun prediction task\nusing chunks of source-side sentences, then using the resulting classifier to\nrepair the translations of an existing NMT model. We offer an initial case\nstudy of this approach for the Japanese-English language pair, observing that a\nsmall number of translations are significantly improved according to human\nevaluators.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.06779,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding\n\n  Generating metaphors is a challenging task as it requires a proper\nunderstanding of abstract concepts, making connections between unrelated\nconcepts, and deviating from the literal meaning. In this paper, we aim to\ngenerate a metaphoric sentence given a literal expression by replacing relevant\nverbs. Based on a theoretically-grounded connection between metaphors and\nsymbols, we propose a method to automatically construct a parallel corpus by\ntransforming a large number of metaphorical sentences from the Gutenberg Poetry\ncorpus (Jacobs, 2018) to their literal counterpart using recent advances in\nmasked language modeling coupled with commonsense inference. For the generation\ntask, we incorporate a metaphor discriminator to guide the decoding of a\nsequence to sequence model fine-tuned on our parallel data to generate\nhigh-quality metaphors. Human evaluation on an independent test set of literal\nstatements shows that our best model generates metaphors better than three\nwell-crafted baselines 66% of the time on average. A task-based evaluation\nshows that human-written poems enhanced with metaphors proposed by our model\nare preferred 68% of the time compared to poems without metaphors.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.15022,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000019206,
      "text":"'Just because you are right, doesn't mean I am wrong': Overcoming a\n  Bottleneck in the Development and Evaluation of Open-Ended Visual Question\n  Answering (VQA) Tasks\n\n  GQA~\\citep{hudson2019gqa} is a dataset for real-world visual reasoning and\ncompositional question answering. We found that many answers predicted by the\nbest vision-language models on the GQA dataset do not match the ground-truth\nanswer but still are semantically meaningful and correct in the given context.\nIn fact, this is the case with most existing visual question answering (VQA)\ndatasets where they assume only one ground-truth answer for each question. We\npropose Alternative Answer Sets (AAS) of ground-truth answers to address this\nlimitation, which is created automatically using off-the-shelf NLP tools. We\nintroduce a semantic metric based on AAS and modify top VQA solvers to support\nmultiple plausible answers for a question. We implement this approach on the\nGQA dataset and show the performance improvements. Code and data are available\nin this link \\url{https:\/\/github.com\/luomancs\/alternative_answer_set.git}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.06944,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000060598,
      "text":"Preregistering NLP Research\n\n  Preregistration refers to the practice of specifying what you are going to\ndo, and what you expect to find in your study, before carrying out the study.\nThis practice is increasingly common in medicine and psychology, but is rarely\ndiscussed in NLP. This paper discusses preregistration in more detail, explores\nhow NLP researchers could preregister their work, and presents several\npreregistration questions for different kinds of studies. Finally, we argue in\nfavour of registered reports, which could provide firmer grounds for slow\nscience in NLP research. The goal of this paper is to elicit a discussion in\nthe NLP community, which we hope to synthesise into a general NLP\npreregistration form in future research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.16789,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"An Exploration of Data Augmentation Techniques for Improving English to\n  Tigrinya Translation\n\n  It has been shown that the performance of neural machine translation (NMT)\ndrops starkly in low-resource conditions, often requiring large amounts of\nauxiliary data to achieve competitive results. An effective method of\ngenerating auxiliary data is back-translation of target language sentences. In\nthis work, we present a case study of Tigrinya where we investigate several\nback-translation methods to generate synthetic source sentences. We find that\nin low-resource conditions, back-translation by pivoting through a\nhigher-resource language related to the target language proves most effective\nresulting in substantial improvements over baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.11367,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000086758,
      "text":"ROSITA: Refined BERT cOmpreSsion with InTegrAted techniques\n\n  Pre-trained language models of the BERT family have defined the\nstate-of-the-arts in a wide range of NLP tasks. However, the performance of\nBERT-based models is mainly driven by the enormous amount of parameters, which\nhinders their application to resource-limited scenarios. Faced with this\nproblem, recent studies have been attempting to compress BERT into a\nsmall-scale model. However, most previous work primarily focuses on a single\nkind of compression technique, and few attention has been paid to the\ncombination of different methods. When BERT is compressed with integrated\ntechniques, a critical question is how to design the entire compression\nframework to obtain the optimal performance. In response to this question, we\nintegrate three kinds of compression methods (weight pruning, low-rank\nfactorization and knowledge distillation (KD)) and explore a range of designs\nconcerning model architecture, KD strategy, pruning frequency and learning rate\nschedule. We find that a careful choice of the designs is crucial to the\nperformance of the compressed model. Based on the empirical findings, our best\ncompressed model, dubbed Refined BERT cOmpreSsion with InTegrAted techniques\n(ROSITA), is $7.5 \\times$ smaller than BERT while maintains $98.5\\%$ of the\nperformance on five tasks of the GLUE benchmark, outperforming the previous\nBERT compression methods with similar parameter budget. The code is available\nat https:\/\/github.com\/llyx97\/Rosita.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.06352,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0001139111,
      "text":"Identifying ARDS using the Hierarchical Attention Network with Sentence\n  Objectives Framework\n\n  Acute respiratory distress syndrome (ARDS) is a life-threatening condition\nthat is often undiagnosed or diagnosed late. ARDS is especially prominent in\nthose infected with COVID-19. We explore the automatic identification of ARDS\nindicators and confounding factors in free-text chest radiograph reports. We\npresent a new annotated corpus of chest radiograph reports and introduce the\nHierarchical Attention Network with Sentence Objectives (HANSO) text\nclassification framework. HANSO utilizes fine-grained annotations to improve\ndocument classification performance. HANSO can extract ARDS-related information\nwith high performance by leveraging relation annotations, even if the annotated\nspans are noisy. Using annotated chest radiograph images as a gold standard,\nHANSO identifies bilateral infiltrates, an indicator of ARDS, in chest\nradiograph reports with performance (0.87 F1) comparable to human annotations\n(0.84 F1). This algorithm could facilitate more efficient and expeditious\nidentification of ARDS by clinicians and researchers and contribute to the\ndevelopment of new therapies to improve patient care.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.08466,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000520547,
      "text":"NADI 2021: The Second Nuanced Arabic Dialect Identification Shared Task\n\n  We present the findings and results of the Second Nuanced Arabic Dialect\nIdentification Shared Task (NADI 2021). This Shared Task includes four\nsubtasks: country-level Modern Standard Arabic (MSA) identification (Subtask\n1.1), country-level dialect identification (Subtask 1.2), province-level MSA\nidentification (Subtask 2.1), and province-level sub-dialect identification\n(Subtask 2.2). The shared task dataset covers a total of 100 provinces from 21\nArab countries, collected from the Twitter domain. A total of 53 teams from 23\ncountries registered to participate in the tasks, thus reflecting the interest\nof the community in this area. We received 16 submissions for Subtask 1.1 from\nfive teams, 27 submissions for Subtask 1.2 from eight teams, 12 submissions for\nSubtask 2.1 from four teams, and 13 Submissions for subtask 2.2 from four\nteams.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.0971,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.00000904,
      "text":"The Human Evaluation Datasheet 1.0: A Template for Recording Details of\n  Human Evaluation Experiments in NLP\n\n  This paper introduces the Human Evaluation Datasheet, a template for\nrecording the details of individual human evaluation experiments in Natural\nLanguage Processing (NLP). Originally taking inspiration from seminal papers by\nBender and Friedman (2018), Mitchell et al. (2019), and Gebru et al. (2020),\nthe Human Evaluation Datasheet is intended to facilitate the recording of\nproperties of human evaluations in sufficient detail, and with sufficient\nstandardisation, to support comparability, meta-evaluation, and reproducibility\ntests.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.06648,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000363257,
      "text":"Domain State Tracking for a Simplified Dialogue System\n\n  Task-oriented dialogue systems aim to help users achieve their goals in\nspecific domains. Recent neural dialogue systems use the entire dialogue\nhistory for abundant contextual information accumulated over multiple\nconversational turns. However, the dialogue history becomes increasingly longer\nas the number of turns increases, thereby increasing memory usage and\ncomputational costs. In this paper, we present DoTS (Domain State Tracking for\na Simplified Dialogue System), a task-oriented dialogue system that uses a\nsimplified input context instead of the entire dialogue history. However,\nneglecting the dialogue history can result in a loss of contextual information\nfrom previous conversational turns. To address this issue, DoTS tracks the\ndomain state in addition to the belief state and uses it for the input context.\nUsing this simplified input, DoTS improves the inform rate and success rate by\n1.09 points and 1.24 points, respectively, compared to the previous\nstate-of-the-art model on MultiWOZ, which is a well-known benchmark.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.05345,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000007583,
      "text":"Detecting Inappropriate Messages on Sensitive Topics that Could Harm a\n  Company's Reputation\n\n  Not all topics are equally \"flammable\" in terms of toxicity: a calm\ndiscussion of turtles or fishing less often fuels inappropriate toxic dialogues\nthan a discussion of politics or sexual minorities. We define a set of\nsensitive topics that can yield inappropriate and toxic messages and describe\nthe methodology of collecting and labeling a dataset for appropriateness. While\ntoxicity in user-generated data is well-studied, we aim at defining a more\nfine-grained notion of inappropriateness. The core of inappropriateness is that\nit can harm the reputation of a speaker. This is different from toxicity in two\nrespects: (i) inappropriateness is topic-related, and (ii) inappropriate\nmessage is not toxic but still unacceptable. We collect and release two\ndatasets for Russian: a topic-labeled dataset and an appropriateness-labeled\ndataset. We also release pre-trained classification models trained on this\ndata.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.05135,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000004371,
      "text":"A Topological Approach to Compare Document Semantics Based on a New\n  Variant of Syntactic N-grams\n\n  This paper delivers a new perspective of thinking and utilizing syntactic\nn-grams (sn-grams). Sn-grams are a type of non-linear n-grams which have been\nplaying a critical role in many NLP tasks. Introducing sn-grams to comparing\ndocument semantics thus is an appealing application, and few studies have\nreported progress at this. However, when proceeding on this application, we\nfound three major issues of sn-grams: lack of significance, being sensitive to\nword orders and failing on capture indirect syntactic relations. To address\nthese issues, we propose a new variant of sn-grams named generalized phrases\n(GPs). Then based on GPs we propose a topological approach, named DSCoH, to\ncompute document semantic similarities. DSCoH has been extensively tested on\nthe document semantics comparison and the document clustering tasks. The\nexperimental results show that DSCoH can outperform state-of-the-art\nembedding-based methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.1132,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000036094,
      "text":"Lawyers are Dishonest? Quantifying Representational Harms in Commonsense\n  Knowledge Resources\n\n  Warning: this paper contains content that may be offensive or upsetting.\n  Numerous natural language processing models have tried injecting commonsense\nby using the ConceptNet knowledge base to improve performance on different\ntasks. ConceptNet, however, is mostly crowdsourced from humans and may reflect\nhuman biases such as \"lawyers are dishonest.\" It is important that these biases\nare not conflated with the notion of commonsense. We study this missing yet\nimportant problem by first defining and quantifying biases in ConceptNet as two\ntypes of representational harms: overgeneralization of polarized perceptions\nand representation disparity. We find that ConceptNet contains severe biases\nand disparities across four demographic categories. In addition, we analyze two\ndownstream models that use ConceptNet as a source for commonsense knowledge and\nfind the existence of biases in those models as well. We further propose a\nfiltered-based bias-mitigation approach and examine its effectiveness. We show\nthat our mitigation approach can reduce the issues in both resource and models\nbut leads to a performance drop, leaving room for future work to build fairer\nand stronger commonsense models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.07472,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000040399,
      "text":"Fabula Entropy Indexing: Objective Measures of Story Coherence\n\n  Automated story generation remains a difficult area of research because it\nlacks strong objective measures. Generated stories may be linguistically sound,\nbut in many cases suffer poor narrative coherence required for a compelling,\nlogically-sound story. To address this, we present Fabula Entropy Indexing\n(FEI), an evaluation method to assess story coherence by measuring the degree\nto which human participants agree with each other when answering true\/false\nquestions about stories. We devise two theoretically grounded measures of\nreader question-answering entropy, the entropy of world coherence (EWC), and\nthe entropy of transitional coherence (ETC), focusing on global and local\ncoherence, respectively. We evaluate these metrics by testing them on\nhuman-written stories and comparing against the same stories that have been\ncorrupted to introduce incoherencies. We show that in these controlled studies,\nour entropy indices provide a reliable objective measure of story coherence.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.07179,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Does Putting a Linguist in the Loop Improve NLU Data Collection?\n\n  Many crowdsourced NLP datasets contain systematic gaps and biases that are\nidentified only after data collection is complete. Identifying these issues\nfrom early data samples during crowdsourcing should make mitigation more\nefficient, especially when done iteratively. We take natural language inference\nas a test case and ask whether it is beneficial to put a linguist `in the loop'\nduring data collection to dynamically identify and address gaps in the data by\nintroducing novel constraints on the task. We directly compare three data\ncollection protocols: (i) a baseline protocol, (ii) a linguist-in-the-loop\nintervention with iteratively-updated constraints on the task, and (iii) an\nextension of linguist-in-the-loop that provides direct interaction between\nlinguists and crowdworkers via a chatroom. The datasets collected with linguist\ninvolvement are more reliably challenging than baseline, without loss of\nquality. But we see no evidence that using this data in training leads to\nbetter out-of-domain model performance, and the addition of a chat platform has\nno measurable effect on the resulting dataset. We suggest integrating expert\nanalysis \\textit{during} data collection so that the expert can dynamically\naddress gaps and biases in the dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.03764,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000061923,
      "text":"Languages for Smart and Computable Contracts\n\n  Smart Contracts use computer technology to automate the performance of\naspects of commercial agreements. Yet how can there be confidence that the\ncomputer code is faithful to the intentions of the parties? To understand the\ndepth and subtlety of this question requires an exploration of natural and\ncomputer languages, of the semantics of expressions in those languages, and of\nthe gap that exists between the disciplines of law and computer science. Here\nwe provide a perspective on some of the key issues, explore some current\nresearch directions, and explain the importance of language design in the\ndevelopment of reliable Smart Contracts, including the specific methodology of\nComputable Contracts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.0231,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000094374,
      "text":"SERRANT: a syntactic classifier for English Grammatical Error Types\n\n  SERRANT is a system and code for automatic classification of English\ngrammatical errors that combines SErCl and ERRANT. SERRANT uses ERRANT's\nannotations when they are informative and those provided by SErCl otherwise.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.09106,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition\n\n  Subword units are commonly used for end-to-end automatic speech recognition\n(ASR), while a fully acoustic-oriented subword modeling approach is somewhat\nmissing. We propose an acoustic data-driven subword modeling (ADSM) approach\nthat adapts the advantages of several text-based and acoustic-based subword\nmethods into one pipeline. With a fully acoustic-oriented label design and\nlearning process, ADSM produces acoustic-structured subword units and\nacoustic-matched target sequence for further ASR training. The obtained ADSM\nlabels are evaluated with different end-to-end ASR approaches including CTC,\nRNN-Transducer and attention models. Experiments on the LibriSpeech corpus show\nthat ADSM clearly outperforms both byte pair encoding (BPE) and\npronunciation-assisted subword modeling (PASM) in all cases. Detailed analysis\nshows that ADSM achieves acoustically more logical word segmentation and more\nbalanced sequence length, and thus, is suitable for both time-synchronous and\nlabel-synchronous models. We also briefly describe how to apply acoustic-based\nsubword regularization and unseen text segmentation using ADSM.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.07623,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000044372,
      "text":"Sometimes We Want Translationese\n\n  Rapid progress in Neural Machine Translation (NMT) systems over the last few\nyears has been driven primarily towards improving translation quality, and as a\nsecondary focus, improved robustness to input perturbations (e.g. spelling and\ngrammatical mistakes). While performance and robustness are important\nobjectives, by over-focusing on these, we risk overlooking other important\nproperties. In this paper, we draw attention to the fact that for some\napplications, faithfulness to the original (input) text is important to\npreserve, even if it means introducing unusual language patterns in the\n(output) translation. We propose a simple, novel way to quantify whether an NMT\nsystem exhibits robustness and faithfulness, focusing on the case of word-order\nperturbations. We explore a suite of functions to perturb the word order of\nsource sentences without deleting or injecting tokens, and measure the effects\non the target side in terms of both robustness and faithfulness. Across several\nexperimental conditions, we observe a strong tendency towards robustness rather\nthan faithfulness. These results allow us to better understand the trade-off\nbetween faithfulness and robustness in NMT, and opens up the possibility of\ndeveloping systems where users have more autonomy and control in selecting\nwhich property is best suited for their use case.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.07885,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"Probing Across Time: What Does RoBERTa Know and When?\n\n  Models of language trained on very large corpora have been demonstrated\nuseful for NLP. As fixed artifacts, they have become the object of intense\nstudy, with many researchers \"probing\" the extent to which linguistic\nabstractions, factual and commonsense knowledge, and reasoning abilities they\nacquire and readily demonstrate. Building on this line of work, we consider a\nnew question: for types of knowledge a language model learns, when during\n(pre)training are they acquired? We plot probing performance across iterations,\nusing RoBERTa as a case study. Among our findings: linguistic knowledge is\nacquired fast, stably, and robustly across domains. Facts and commonsense are\nslower and more domain-sensitive. Reasoning abilities are, in general, not\nstably acquired. As new datasets, pretraining protocols, and probes emerge, we\nbelieve that probing-across-time analyses can help researchers understand the\ncomplex, intermingled learning that these models undergo and guide us toward\nmore efficient approaches that accomplish necessary learning faster.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.10339,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000356966,
      "text":"Discriminative Self-training for Punctuation Prediction\n\n  Punctuation prediction for automatic speech recognition (ASR) output\ntranscripts plays a crucial role for improving the readability of the ASR\ntranscripts and for improving the performance of downstream natural language\nprocessing applications. However, achieving good performance on punctuation\nprediction often requires large amounts of labeled speech transcripts, which is\nexpensive and laborious. In this paper, we propose a Discriminative\nSelf-Training approach with weighted loss and discriminative label smoothing to\nexploit unlabeled speech transcripts. Experimental results on the English\nIWSLT2011 benchmark test set and an internal Chinese spoken language dataset\ndemonstrate that the proposed approach achieves significant improvement on\npunctuation prediction accuracy over strong baselines including BERT, RoBERTa,\nand ELECTRA models. The proposed Discriminative Self-Training approach\noutperforms the vanilla self-training approach. We establish a new\nstate-of-the-art (SOTA) on the IWSLT2011 test set, outperforming the current\nSOTA model by 1.3% absolute gain on F$_1$.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.11394,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"BERT-CoQAC: BERT-based Conversational Question Answering in Context\n\n  As one promising way to inquire about any particular information through a\ndialog with the bot, question answering dialog systems have gained increasing\nresearch interests recently. Designing interactive QA systems has always been a\nchallenging task in natural language processing and used as a benchmark to\nevaluate a machine's ability of natural language understanding. However, such\nsystems often struggle when the question answering is carried out in multiple\nturns by the users to seek more information based on what they have already\nlearned, thus, giving rise to another complicated form called Conversational\nQuestion Answering (CQA). CQA systems are often criticized for not\nunderstanding or utilizing the previous context of the conversation when\nanswering the questions. To address the research gap, in this paper, we explore\nhow to integrate conversational history into the neural machine comprehension\nsystem. On one hand, we introduce a framework based on a publically available\npre-trained language model called BERT for incorporating history turns into the\nsystem. On the other hand, we propose a history selection mechanism that\nselects the turns that are relevant and contributes the most to answer the\ncurrent question. Experimentation results revealed that our framework is\ncomparable in performance with the state-of-the-art models on the QuAC leader\nboard. We also conduct a number of experiments to show the side effects of\nusing entire context information which brings unnecessary information and noise\nsignals resulting in a decline in the model's performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.07,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000267559,
      "text":"IGA : An Intent-Guided Authoring Assistant\n\n  While large-scale pretrained language models have significantly improved\nwriting assistance functionalities such as autocomplete, more complex and\ncontrollable writing assistants have yet to be explored. We leverage advances\nin language modeling to build an interactive writing assistant that generates\nand rephrases text according to fine-grained author specifications. Users\nprovide input to our Intent-Guided Assistant (IGA) in the form of text\ninterspersed with tags that correspond to specific rhetorical directives (e.g.,\nadding description or contrast, or rephrasing a particular sentence). We\nfine-tune a language model on a dataset heuristically-labeled with author\nintent, which allows IGA to fill in these tags with generated text that users\ncan subsequently edit to their liking. A series of automatic and crowdsourced\nevaluations confirm the quality of IGA's generated outputs, while a small-scale\nuser study demonstrates author preference for IGA over baseline methods in a\ncreative writing task. We release our dataset, code, and demo to spur further\nresearch into AI-assisted writing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.08478,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000120203,
      "text":"Sentence Concatenation Approach to Data Augmentation for Neural Machine\n  Translation\n\n  Neural machine translation (NMT) has recently gained widespread attention\nbecause of its high translation accuracy. However, it shows poor performance in\nthe translation of long sentences, which is a major issue in low-resource\nlanguages. It is assumed that this issue is caused by insufficient number of\nlong sentences in the training data. Therefore, this study proposes a simple\ndata augmentation method to handle long sentences. In this method, we use only\nthe given parallel corpora as the training data and generate long sentences by\nconcatenating two sentences. Based on the experimental results, we confirm\nimprovements in long sentence translation by the proposed data augmentation\nmethod, despite its simplicity. Moreover, the translation quality is further\nimproved by the proposed method, when combined with back-translation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.08481,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Revisiting Few-shot Relation Classification: Evaluation Data and\n  Classification Schemes\n\n  We explore Few-Shot Learning (FSL) for Relation Classification (RC). Focusing\non the realistic scenario of FSL, in which a test instance might not belong to\nany of the target categories (none-of-the-above, aka NOTA), we first revisit\nthe recent popular dataset structure for FSL, pointing out its unrealistic data\ndistribution. To remedy this, we propose a novel methodology for deriving more\nrealistic few-shot test data from available datasets for supervised RC, and\napply it to the TACRED dataset. This yields a new challenging benchmark for FSL\nRC, on which state of the art models show poor performance. Next, we analyze\nclassification schemes within the popular embedding-based nearest-neighbor\napproach for FSL, with respect to constraints they impose on the embedding\nspace. Triggered by this analysis we propose a novel classification scheme, in\nwhich the NOTA category is represented as learned vectors, shown empirically to\nbe an appealing option for FSL.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.08744,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Generative Context Pair Selection for Multi-hop Question Answering\n\n  Compositional reasoning tasks like multi-hop question answering, require\nmaking latent decisions to get the final answer, given a question. However,\ncrowdsourced datasets often capture only a slice of the underlying task\ndistribution, which can induce unanticipated biases in models performing\ncompositional reasoning. Furthermore, discriminatively trained models exploit\nsuch biases to get a better held-out performance, without learning the right\nway to reason, as they do not necessitate paying attention to the question\nrepresentation (conditioning variable) in its entirety, to estimate the answer\nlikelihood. In this work, we propose a generative context selection model for\nmulti-hop question answering that reasons about how the given question could\nhave been generated given a context pair. While being comparable to the\nstate-of-the-art answering performance, our proposed generative passage\nselection model has a better performance (4.9% higher than baseline) on\nadversarial held-out set which tests robustness of model's multi-hop reasoning\ncapabilities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.13559,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000038743,
      "text":"AraStance: A Multi-Country and Multi-Domain Dataset of Arabic Stance\n  Detection for Fact Checking\n\n  With the continuing spread of misinformation and disinformation online, it is\nof increasing importance to develop combating mechanisms at scale in the form\nof automated systems that support multiple languages. One task of interest is\nclaim veracity prediction, which can be addressed using stance detection with\nrespect to relevant documents retrieved online. To this end, we present our new\nArabic Stance Detection dataset (AraStance) of 4,063 claim--article pairs from\na diverse set of sources comprising three fact-checking websites and one news\nwebsite. AraStance covers false and true claims from multiple domains (e.g.,\npolitics, sports, health) and several Arab countries, and it is well-balanced\nbetween related and unrelated documents with respect to the claims. We\nbenchmark AraStance, along with two other stance detection datasets, using a\nnumber of BERT-based models. Our best model achieves an accuracy of 85\\% and a\nmacro F1 score of 78\\%, which leaves room for improvement and reflects the\nchallenging nature of AraStance and the task of stance detection in general.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.10813,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Finding Fuzziness in Neural Network Models of Language Processing\n\n  Humans often communicate by using imprecise language, suggesting that fuzzy\nconcepts with unclear boundaries are prevalent in language use. In this paper,\nwe test the extent to which models trained to capture the distributional\nstatistics of language show correspondence to fuzzy-membership patterns. Using\nthe task of natural language inference, we test a recent state of the art model\non the classical case of temperature, by examining its mapping of temperature\ndata to fuzzy-perceptions such as \"cool\", \"hot\", etc. We find the model to show\npatterns that are similar to classical fuzzy-set theoretic formulations of\nlinguistic hedges, albeit with a substantial amount of noise, suggesting that\nmodels trained solely on language show promise in encoding fuzziness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.02258,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.00001106,
      "text":"Non-autoregressive Mandarin-English Code-switching Speech Recognition\n\n  Mandarin-English code-switching (CS) is frequently used among East and\nSoutheast Asian people. However, the intra-sentence language switching of the\ntwo very different languages makes recognizing CS speech challenging.\nMeanwhile, the recent successful non-autoregressive (NAR) ASR models remove the\nneed for left-to-right beam decoding in autoregressive (AR) models and achieved\noutstanding performance and fast inference speed, but it has not been applied\nto Mandarin-English CS speech recognition. This paper takes advantage of the\nMask-CTC NAR ASR framework to tackle the CS speech recognition issue. We\nfurther propose to change the Mandarin output target of the encoder to Pinyin\nfor faster encoder training and introduce the Pinyin-to-Mandarin decoder to\nlearn contextualized information. Moreover, we use word embedding label\nsmoothing to regularize the decoder with contextualized information and\nprojection matrix regularization to bridge that gap between the encoder and\ndecoder. We evaluate these methods on the SEAME corpus and achieved exciting\nresults.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.05745,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000356966,
      "text":"Fighting the COVID-19 Infodemic with a Holistic BERT Ensemble\n\n  This paper describes the TOKOFOU system, an ensemble model for misinformation\ndetection tasks based on six different transformer-based pre-trained encoders,\nimplemented in the context of the COVID-19 Infodemic Shared Task for English.\nWe fine tune each model on each of the task's questions and aggregate their\nprediction scores using a majority voting approach. TOKOFOU obtains an overall\nF1 score of 89.7%, ranking first.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.07642,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000037087,
      "text":"Bilingual alignment transfers to multilingual alignment for unsupervised\n  parallel text mining\n\n  This work presents methods for learning cross-lingual sentence\nrepresentations using paired or unpaired bilingual texts. We hypothesize that\nthe cross-lingual alignment strategy is transferable, and therefore a model\ntrained to align only two languages can encode multilingually more aligned\nrepresentations. We thus introduce dual-pivot transfer: training on one\nlanguage pair and evaluating on other pairs. To study this theory, we design\nunsupervised models trained on unpaired sentences and single-pair supervised\nmodels trained on bitexts, both based on the unsupervised language model XLM-R\nwith its parameters frozen. The experiments evaluate the models as universal\nsentence encoders on the task of unsupervised bitext mining on two datasets,\nwhere the unsupervised model reaches the state of the art of unsupervised\nretrieval, and the alternative single-pair supervised model approaches the\nperformance of multilingually supervised models. The results suggest that\nbilingual training techniques as proposed can be applied to get sentence\nrepresentations with multilingual alignment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.00369,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"FeTaQA: Free-form Table Question Answering\n\n  Existing table question answering datasets contain abundant factual questions\nthat primarily evaluate the query and schema comprehension capability of a\nsystem, but they fail to include questions that require complex reasoning and\nintegration of information due to the constraint of the associated short-form\nanswers. To address these issues and to demonstrate the full challenge of table\nquestion answering, we introduce FeTaQA, a new dataset with 10K Wikipedia-based\n{table, question, free-form answer, supporting table cells} pairs. FeTaQA\nyields a more challenging table question answering setting because it requires\ngenerating free-form text answers after retrieval, inference, and integration\nof multiple discontinuous facts from a structured knowledge source. Unlike\ndatasets of generative QA over text in which answers are prevalent with copies\nof short text spans from the source, answers in our dataset are human-generated\nexplanations involving entities and their high-level relations. We provide two\nbenchmark methods for the proposed task: a pipeline method based on\nsemantic-parsing-based QA systems and an end-to-end method based on large\npretrained text generation models, and show that FeTaQA poses a challenge for\nboth methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.00789,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Do RNN States Encode Abstract Phonological Processes?\n\n  Sequence-to-sequence models have delivered impressive results in word\nformation tasks such as morphological inflection, often learning to model\nsubtle morphophonological details with limited training data. Despite the\nperformance, the opacity of neural models makes it difficult to determine\nwhether complex generalizations are learned, or whether a kind of separate rote\nmemorization of each morphophonological process takes place. To investigate\nwhether complex alternations are simply memorized or whether there is some\nlevel of generalization across related sound changes in a sequence-to-sequence\nmodel, we perform several experiments on Finnish consonant gradation -- a\ncomplex set of sound changes triggered in some words by certain suffixes. We\nfind that our models often -- though not always -- encode 17 different\nconsonant gradation processes in a handful of dimensions in the RNN. We also\nshow that by scaling the activations in these dimensions we can control whether\nconsonant gradation occurs and the direction of the gradation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.12977,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000038081,
      "text":"SE-DAE: Style-Enhanced Denoising Auto-Encoder for Unsupervised Text\n  Style Transfer\n\n  Text style transfer aims to change the style of sentences while preserving\nthe semantic meanings. Due to the lack of parallel data, the Denoising\nAuto-Encoder (DAE) is widely used in this task to model distributions of\ndifferent sentence styles. However, because of the conflict between the target\nof the conventional denoising procedure and the target of style transfer task,\nthe vanilla DAE can not produce satisfying enough results. To improve the\ntransferability of the model, most of the existing works combine DAE with\nvarious complicated unsupervised networks, which makes the whole system become\nover-complex. In this work, we design a novel DAE model named Style-Enhanced\nDAE (SE-DAE), which is specifically designed for the text style transfer task.\nCompared with previous complicated style-transfer models, our model do not\nconsist of any complicated unsupervised networks, but only relies on the\nhigh-quality pseudo-parallel data generated by a novel data refinement\nmechanism. Moreover, to alleviate the conflict between the targets of the\nconventional denoising procedure and the style transfer task, we propose\nanother novel style denoising mechanism, which is more compatible with the\ntarget of the style transfer task. We validate the effectiveness of our model\non two style benchmark datasets. Both automatic evaluation and human evaluation\nshow that our proposed model is highly competitive compared with previous\nstrong the state of the art (SOTA) approaches and greatly outperforms the\nvanilla DAE.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.02573,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000501672,
      "text":"Assessing Dialogue Systems with Distribution Distances\n\n  An important aspect of developing dialogue systems is how to evaluate and\ncompare the performance of different systems. Existing automatic evaluation\nmetrics are based on turn-level quality evaluation and use average scores for\nsystem-level comparison. In this paper, we propose to measure the performance\nof a dialogue system by computing the distribution-wise distance between its\ngenerated conversations and real-world conversations. Specifically, two\ndistribution-wise metrics, FBD and PRD, are developed and evaluated.\nExperiments on several dialogue corpora show that our proposed metrics\ncorrelate better with human judgments than existing metrics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.05457,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Incorporating Commonsense Knowledge Graph in Pretrained Models for\n  Social Commonsense Tasks\n\n  Pretrained language models have excelled at many NLP tasks recently; however,\ntheir social intelligence is still unsatisfactory. To enable this, machines\nneed to have a more general understanding of our complicated world and develop\nthe ability to perform commonsense reasoning besides fitting the specific\ndownstream tasks. External commonsense knowledge graphs (KGs), such as\nConceptNet, provide rich information about words and their relationships. Thus,\ntowards general commonsense learning, we propose two approaches to\n\\emph{implicitly} and \\emph{explicitly} infuse such KGs into pretrained\nlanguage models. We demonstrate our proposed methods perform well on SocialIQA,\na social commonsense reasoning task, in both limited and full training data\nregimes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.06027,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000074175,
      "text":"Towards Human-Free Automatic Quality Evaluation of German Summarization\n\n  Evaluating large summarization corpora using humans has proven to be\nexpensive from both the organizational and the financial perspective.\nTherefore, many automatic evaluation metrics have been developed to measure the\nsummarization quality in a fast and reproducible way. However, most of the\nmetrics still rely on humans and need gold standard summaries generated by\nlinguistic experts. Since BLANC does not require golden summaries and\nsupposedly can use any underlying language model, we consider its application\nto the evaluation of summarization in German. This work demonstrates how to\nadjust the BLANC metric to a language other than English. We compare BLANC\nscores with the crowd and expert ratings, as well as with commonly used\nautomatic metrics on a German summarization data set. Our results show that\nBLANC in German is especially good in evaluating informativeness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.05887,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Black or White but never neutral: How readers perceive identity from\n  yellow or skin-toned emoji\n\n  Research in sociology and linguistics shows that people use language not only\nto express their own identity but to understand the identity of others. Recent\nwork established a connection between expression of identity and emoji usage on\nsocial media, through use of emoji skin tone modifiers. Motivated by that\nfinding, this work asks if, as with language, readers are sensitive to such\nacts of self-expression and use them to understand the identity of authors. In\nbehavioral experiments (n=488), where text and emoji content of social media\nposts were carefully controlled before being presented to participants, we find\nin the affirmative -- emoji are a salient signal of author identity. That\nsignal is distinct from, and complementary to, the one encoded in language.\nParticipant groups (based on self-identified ethnicity) showed no differences\nin how they perceive this signal, except in the case of the default yellow\nemoji. While both groups associate this with a White identity, the effect was\nstronger in White participants. Our finding that emoji can index social\nvariables will have experimental applications for researchers but also\nimplications for designers: supposedly ``neutral`` defaults may be more\nrepresentative of some users than others.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.13318,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000031127,
      "text":"Synthetic Data Generation for Grammatical Error Correction with Tagged\n  Corruption Models\n\n  Synthetic data generation is widely known to boost the accuracy of neural\ngrammatical error correction (GEC) systems, but existing methods often lack\ndiversity or are too simplistic to generate the broad range of grammatical\nerrors made by human writers. In this work, we use error type tags from\nautomatic annotation tools such as ERRANT to guide synthetic data generation.\nWe compare several models that can produce an ungrammatical sentence given a\nclean sentence and an error type tag. We use these models to build a new, large\nsynthetic pre-training data set with error tag frequency distributions matching\na given development set. Our synthetic data set yields large and consistent\ngains, improving the state-of-the-art on the BEA-19 and CoNLL-14 test sets. We\nalso show that our approach is particularly effective in adapting a GEC system,\ntrained on mixed native and non-native English, to a native English test set,\neven surpassing real training data consisting of high-quality sentence pairs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.00896,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000037087,
      "text":"Pseudo Siamese Network for Few-shot Intent Generation\n\n  Few-shot intent detection is a challenging task due to the scare annotation\nproblem. In this paper, we propose a Pseudo Siamese Network (PSN) to generate\nlabeled data for few-shot intents and alleviate this problem. PSN consists of\ntwo identical subnetworks with the same structure but different weights: an\naction network and an object network. Each subnetwork is a transformer-based\nvariational autoencoder that tries to model the latent distribution of\ndifferent components in the sentence. The action network is learned to\nunderstand action tokens and the object network focuses on object-related\nexpressions. It provides an interpretable framework for generating an utterance\nwith an action and an object existing in a given intent. Experiments on two\nreal-world datasets show that PSN achieves state-of-the-art performance for the\ngeneralized few shot intent detection task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.14781,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0001074208,
      "text":"A Semantic-based Method for Unsupervised Commonsense Question Answering\n\n  Unsupervised commonsense question answering is appealing since it does not\nrely on any labeled task data. Among existing work, a popular solution is to\nuse pre-trained language models to score candidate choices directly conditioned\non the question or context. However, such scores from language models can be\neasily affected by irrelevant factors, such as word frequencies, sentence\nstructures, etc. These distracting factors may not only mislead the model to\nchoose a wrong answer but also make it oversensitive to lexical perturbations\nin candidate answers.\n  In this paper, we present a novel SEmantic-based Question Answering method\n(SEQA) for unsupervised commonsense question answering. Instead of directly\nscoring each answer choice, our method first generates a set of plausible\nanswers with generative models (e.g., GPT-2), and then uses these plausible\nanswers to select the correct choice by considering the semantic similarity\nbetween each plausible answer and each choice. We devise a simple, yet sound\nformalism for this idea and verify its effectiveness and robustness with\nextensive experiments. We evaluate the proposed method on four benchmark\ndatasets, and our method achieves the best results in unsupervised settings.\nMoreover, when attacked by TextFooler with synonym replacement, SEQA\ndemonstrates much less performance drops than baselines, thereby indicating\nstronger robustness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.03664,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000024835,
      "text":"D2S: Document-to-Slide Generation Via Query-Based Text Summarization\n\n  Presentations are critical for communication in all areas of our lives, yet\nthe creation of slide decks is often tedious and time-consuming. There has been\nlimited research aiming to automate the document-to-slides generation process\nand all face a critical challenge: no publicly available dataset for training\nand benchmarking. In this work, we first contribute a new dataset, SciDuet,\nconsisting of pairs of papers and their corresponding slides decks from recent\nyears' NLP and ML conferences (e.g., ACL). Secondly, we present D2S, a novel\nsystem that tackles the document-to-slides task with a two-step approach: 1)\nUse slide titles to retrieve relevant and engaging text, figures, and tables;\n2) Summarize the retrieved context into bullet points with long-form question\nanswering. Our evaluation suggests that long-form QA outperforms\nstate-of-the-art summarization baselines on both automated ROUGE metrics and\nqualitative human evaluation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.12667,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"Prosodic segmentation for parsing spoken dialogue\n\n  Parsing spoken dialogue poses unique difficulties, including disfluencies and\nunmarked boundaries between sentence-like units. Previous work has shown that\nprosody can help with parsing disfluent speech (Tran et al. 2018), but has\nassumed that the input to the parser is already segmented into sentence-like\nunits (SUs), which isn't true in existing speech applications. We investigate\nhow prosody affects a parser that receives an entire dialogue turn as input (a\nturn-based model), instead of gold standard pre-segmented SUs (an SU-based\nmodel). In experiments on the English Switchboard corpus, we find that when\nusing transcripts alone, the turn-based model has trouble segmenting SUs,\nleading to worse parse performance than the SU-based model. However, prosody\ncan effectively replace gold standard SU boundaries: with prosody, the\nturn-based model performs as well as the SU-based model (90.79 vs. 90.65 F1\nscore, respectively), despite performing two tasks (SU segmentation and\nparsing) rather than one (parsing alone). Analysis shows that pitch and\nintensity features are the most important for this corpus, since they allow the\nmodel to correctly distinguish an SU boundary from a speech disfluency -- a\ndistinction that the model otherwise struggles to make.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.14313,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000127488,
      "text":"Novel Slot Detection: A Benchmark for Discovering Unknown Slot Types in\n  the Task-Oriented Dialogue System\n\n  Existing slot filling models can only recognize pre-defined in-domain slot\ntypes from a limited slot set. In the practical application, a reliable\ndialogue system should know what it does not know. In this paper, we introduce\na new task, Novel Slot Detection (NSD), in the task-oriented dialogue system.\nNSD aims to discover unknown or out-of-domain slot types to strengthen the\ncapability of a dialogue system based on in-domain training data. Besides, we\nconstruct two public NSD datasets, propose several strong NSD baselines, and\nestablish a benchmark for future work. Finally, we conduct exhaustive\nexperiments and qualitative analysis to comprehend key challenges and provide\nnew guidance for future directions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.07109,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000053313,
      "text":"The Low-Dimensional Linear Geometry of Contextualized Word\n  Representations\n\n  Black-box probing models can reliably extract linguistic features like tense,\nnumber, and syntactic role from pretrained word representations. However, the\nmanner in which these features are encoded in representations remains poorly\nunderstood. We present a systematic study of the linear geometry of\ncontextualized word representations in ELMO and BERT. We show that a variety of\nlinguistic features (including structured dependency relationships) are encoded\nin low-dimensional subspaces. We then refine this geometric picture, showing\nthat there are hierarchical relations between the subspaces encoding general\nlinguistic categories and more specific ones, and that low-dimensional feature\nencodings are distributed rather than aligned to individual neurons. Finally,\nwe demonstrate that these linear subspaces are causally related to model\nbehavior, and can be used to perform fine-grained manipulation of BERT's output\ndistribution.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.11136,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000216232,
      "text":"Using Adversarial Attacks to Reveal the Statistical Bias in Machine\n  Reading Comprehension Models\n\n  Pre-trained language models have achieved human-level performance on many\nMachine Reading Comprehension (MRC) tasks, but it remains unclear whether these\nmodels truly understand language or answer questions by exploiting statistical\nbiases in datasets. Here, we demonstrate a simple yet effective method to\nattack MRC models and reveal the statistical biases in these models. We apply\nthe method to the RACE dataset, for which the answer to each MRC question is\nselected from 4 options. It is found that several pre-trained language models,\nincluding BERT, ALBERT, and RoBERTa, show consistent preference to some\noptions, even when these options are irrelevant to the question. When\ninterfered by these irrelevant options, the performance of MRC models can be\nreduced from human-level performance to the chance-level performance. Human\nreaders, however, are not clearly affected by these irrelevant options.\nFinally, we propose an augmented training method that can greatly reduce\nmodels' statistical biases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.06097,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Thematic Fit Bits: Annotation Quality and Quantity Interplay for Event\n  Participant Representation\n\n  Modeling thematic fit (a verb--argument compositional semantics task)\ncurrently requires a very large burden of labeled data. We take a\nlinguistically machine-annotated large corpus and replace corpus layers with\noutput from higher-quality, more modern taggers. We compare the old and new\ncorpus versions' impact on a verb--argument fit modeling task, using a\nhigh-performing neural approach. We discover that higher annotation quality\ndramatically reduces our data requirement while demonstrating better supervised\npredicate-argument classification. But in applying the model to\npsycholinguistic tasks outside the training objective, we see clear gains at\nscale, but only in one of two thematic fit estimation tasks, and no clear gains\non the other. We also see that quality improves with training size, but perhaps\nplateauing or even declining in one task. Last, we tested the effect of role\nset size. All this suggests that the quality\/quantity interplay is not all you\nneed. We replicate previous studies while modifying certain role representation\ndetails and set a new state-of-the-art in event modeling, using a fraction of\nthe data. We make the new corpus version public.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.00572,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000054638,
      "text":"Larger-Scale Transformers for Multilingual Masked Language Modeling\n\n  Recent work has demonstrated the effectiveness of cross-lingual language\nmodel pretraining for cross-lingual understanding. In this study, we present\nthe results of two larger multilingual masked language models, with 3.5B and\n10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform\nXLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the\nRoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on\naverage while handling 99 more languages. This suggests pretrained models with\nlarger capacity may obtain both strong performance on high-resource languages\nwhile greatly improving low-resource languages. We make our code and models\npublicly available.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.02732,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0001498063,
      "text":"What's in the Box? A Preliminary Analysis of Undesirable Content in the\n  Common Crawl Corpus\n\n  Whereas much of the success of the current generation of neural language\nmodels has been driven by increasingly large training corpora, relatively\nlittle research has been dedicated to analyzing these massive sources of\ntextual data. In this exploratory analysis, we delve deeper into the Common\nCrawl, a colossal web corpus that is extensively used for training language\nmodels. We find that it contains a significant amount of undesirable content,\nincluding hate speech and sexually explicit content, even after filtering\nprocedures. We discuss the potential impacts of this content on language models\nand conclude with future research directions and a more mindful approach to\ncorpus collection and analysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.08901,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000200007,
      "text":"A Sequence-to-Set Network for Nested Named Entity Recognition\n\n  Named entity recognition (NER) is a widely studied task in natural language\nprocessing. Recently, a growing number of studies have focused on the nested\nNER. The span-based methods, considering the entity recognition as a span\nclassification task, can deal with nested entities naturally. But they suffer\nfrom the huge search space and the lack of interactions between entities. To\naddress these issues, we propose a novel sequence-to-set neural network for\nnested NER. Instead of specifying candidate spans in advance, we provide a\nfixed set of learnable vectors to learn the patterns of the valuable spans. We\nutilize a non-autoregressive decoder to predict the final set of entities in\none pass, in which we are able to capture dependencies between entities.\nCompared with the sequence-to-sequence method, our model is more suitable for\nsuch unordered recognition task as it is insensitive to the label order. In\naddition, we utilize the loss function based on bipartite matching to compute\nthe overall training loss. Experimental results show that our proposed model\nachieves state-of-the-art on three nested NER corpora: ACE 2004, ACE 2005 and\nKBP 2017. The code is available at\nhttps:\/\/github.com\/zqtan1024\/sequence-to-set.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.04688,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000019537,
      "text":"Assessing the Syntactic Capabilities of Transformer-based Multilingual\n  Language Models\n\n  Multilingual Transformer-based language models, usually pretrained on more\nthan 100 languages, have been shown to achieve outstanding results in a wide\nrange of cross-lingual transfer tasks. However, it remains unknown whether the\noptimization for different languages conditions the capacity of the models to\ngeneralize over syntactic structures, and how languages with syntactic\nphenomena of different complexity are affected. In this work, we explore the\nsyntactic generalization capabilities of the monolingual and multilingual\nversions of BERT and RoBERTa. More specifically, we evaluate the syntactic\ngeneralization potential of the models on English and Spanish tests, comparing\nthe syntactic abilities of monolingual and multilingual models on the same\nlanguage (English), and of multilingual models on two different languages\n(English and Spanish). For English, we use the available SyntaxGym test suite;\nfor Spanish, we introduce SyntaxGymES, a novel ensemble of targeted syntactic\ntests in Spanish, designed to evaluate the syntactic generalization\ncapabilities of language models through the SyntaxGym online platform.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.13635,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"Noised Consistency Training for Text Summarization\n\n  Neural abstractive summarization methods often require large quantities of\nlabeled training data. However, labeling large amounts of summarization data is\noften prohibitive due to time, financial, and expertise constraints, which has\nlimited the usefulness of summarization systems to practical applications. In\nthis paper, we argue that this limitation can be overcome by a semi-supervised\napproach: consistency training which is to leverage large amounts of unlabeled\ndata to improve the performance of supervised learning over a small corpus. The\nconsistency regularization semi-supervised learning can regularize model\npredictions to be invariant to small noise applied to input articles. By adding\nnoised unlabeled corpus to help regularize consistency training, this framework\nobtains comparative performance without using the full dataset. In particular,\nwe have verified that leveraging large amounts of unlabeled data decently\nimproves the performance of supervised learning over an insufficient labeled\ndataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.07316,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000095367,
      "text":"From Masked Language Modeling to Translation: Non-English Auxiliary\n  Tasks Improve Zero-shot Spoken Language Understanding\n\n  The lack of publicly available evaluation data for low-resource languages\nlimits progress in Spoken Language Understanding (SLU). As key tasks like\nintent classification and slot filling require abundant training data, it is\ndesirable to reuse existing data in high-resource languages to develop models\nfor low-resource scenarios. We introduce xSID, a new benchmark for\ncross-lingual Slot and Intent Detection in 13 languages from 6 language\nfamilies, including a very low-resource dialect. To tackle the challenge, we\npropose a joint learning approach, with English SLU training data and\nnon-English auxiliary tasks from raw text, syntax and translation for transfer.\nWe study two setups which differ by type and language coverage of the\npre-trained embeddings. Our results show that jointly learning the main tasks\nwith masked language modeling is effective for slots, while machine translation\ntransfer works best for intent classification.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.14445,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"Modeling Text-visual Mutual Dependency for Multi-modal Dialog Generation\n\n  Multi-modal dialog modeling is of growing interest. In this work, we propose\nframeworks to resolve a specific case of multi-modal dialog generation that\nbetter mimics multi-modal dialog generation in the real world, where each\ndialog turn is associated with the visual context in which it takes place.\nSpecifically, we propose to model the mutual dependency between text-visual\nfeatures, where the model not only needs to learn the probability of generating\nthe next dialog utterance given preceding dialog utterances and visual\ncontexts, but also the probability of predicting the visual features in which a\ndialog utterance takes place, leading the generated dialog utterance specific\nto the visual context. We observe significant performance boosts over vanilla\nmodels when the mutual dependency between text and visual features is modeled.\nCode is available at https:\/\/github.com\/ShannonAI\/OpenViDial.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.06719,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000258287,
      "text":"Improving Unsupervised Dialogue Topic Segmentation with Utterance-Pair\n  Coherence Scoring\n\n  Dialogue topic segmentation is critical in several dialogue modeling\nproblems. However, popular unsupervised approaches only exploit surface\nfeatures in assessing topical coherence among utterances. In this work, we\naddress this limitation by leveraging supervisory signals from the\nutterance-pair coherence scoring task. First, we present a simple yet effective\nstrategy to generate a training corpus for utterance-pair coherence scoring.\nThen, we train a BERT-based neural utterance-pair coherence model with the\nobtained training corpus. Finally, such model is used to measure the topical\nrelevance between utterances, acting as the basis of the segmentation\ninference. Experiments on three public datasets in English and Chinese\ndemonstrate that our proposal outperforms the state-of-the-art baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.00641,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"SpanNER: Named Entity Re-\/Recognition as Span Prediction\n\n  Recent years have seen the paradigm shift of Named Entity Recognition (NER)\nsystems from sequence labeling to span prediction. Despite its preliminary\neffectiveness, the span prediction model's architectural bias has not been\nfully understood. In this paper, we first investigate the strengths and\nweaknesses when the span prediction model is used for named entity recognition\ncompared with the sequence labeling framework and how to further improve it,\nwhich motivates us to make complementary advantages of systems based on\ndifferent paradigms. We then reveal that span prediction, simultaneously, can\nserve as a system combiner to re-recognize named entities from different\nsystems' outputs. We experimentally implement 154 systems on 11 datasets,\ncovering three languages, comprehensive results show the effectiveness of span\nprediction models that both serve as base NER systems and system combiners. We\nmake all code and datasets available: \\url{https:\/\/github.com\/neulab\/spanner},\nas well as an online system demo: \\url{http:\/\/spanner.sh}. Our model also has\nbeen deployed into the ExplainaBoard platform, which allows users to flexibly\nperform a system combination of top-scoring systems in an interactive way:\n\\url{http:\/\/explainaboard.nlpedia.ai\/leaderboard\/task-ner\/}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.12398,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000054306,
      "text":"End-to-End Lexically Constrained Machine Translation for Morphologically\n  Rich Languages\n\n  Lexically constrained machine translation allows the user to manipulate the\noutput sentence by enforcing the presence or absence of certain words and\nphrases. Although current approaches can enforce terms to appear in the\ntranslation, they often struggle to make the constraint word form agree with\nthe rest of the generated output. Our manual analysis shows that 46% of the\nerrors in the output of a baseline constrained model for English to Czech\ntranslation are related to agreement. We investigate mechanisms to allow neural\nmachine translation to infer the correct word inflection given lemmatized\nconstraints. In particular, we focus on methods based on training the model\nwith constraints provided as part of the input sequence. Our experiments on the\nEnglish-Czech language pair show that this approach improves the translation of\nconstrained terms in both automatic and manual evaluation by reducing errors in\nagreement. Our approach thus eliminates inflection errors, without introducing\nnew errors or decreasing the overall quality of the translation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.15872,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Genre determining prediction: Non-standard TAM marking in football\n  language\n\n  German and French football language display tense-aspect-mood (TAM) forms\nwhich differ from the TAM use in other genres. In German football talk, the\npresent indicative may replace the pluperfect subjunctive. In French reports of\nfootball matches, the imperfective past may occur instead of a perfective past\ntense-aspect form. We argue that the two phenomena share a functional core and\nare licensed in the same way, which is a direct result of the genre they occur\nin. More precisely, football match reports adhere to a precise script and\nspecific events are temporally determined in terms of objective time. This\nallows speakers to exploit a secondary function of TAM forms, namely, they\nshift the temporal perspective. We argue that it is on the grounds of the genre\nthat comprehenders predict the deviating forms and are also able to decode\nthem. We present various corpus studies where we explore the functioning of\nthese phenomena in order to gain insights into their distribution,\ngrammaticalization and their functioning in discourse. Relevant factors are\nAktionsart properties, rhetorical relations and their interaction with other\nTAM forms. This allows us to discuss coping mechanisms on the part of the\ncomprehender. We broaden our understanding of the phenomena, which have only\nbeen partly covered for French and up to now seem to have been ignored in\nGerman.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.08582,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000351999,
      "text":"Alternated Training with Synthetic and Authentic Data for Neural Machine\n  Translation\n\n  While synthetic bilingual corpora have demonstrated their effectiveness in\nlow-resource neural machine translation (NMT), adding more synthetic data often\ndeteriorates translation performance. In this work, we propose alternated\ntraining with synthetic and authentic data for NMT. The basic idea is to\nalternate synthetic and authentic corpora iteratively during training. Compared\nwith previous work, we introduce authentic data as guidance to prevent the\ntraining of NMT models from being disturbed by noisy synthetic data.\nExperiments on Chinese-English and German-English translation tasks show that\nour approach improves the performance over several strong baselines. We\nvisualize the BLEU landscape to further investigate the role of authentic and\nsynthetic data during alternated training. From the visualization, we find that\nauthentic data helps to direct the NMT model parameters towards points with\nhigher BLEU scores and leads to consistent translation performance improvement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.03376,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000049671,
      "text":"A Globally Normalized Neural Model for Semantic Parsing\n\n  In this paper, we propose a globally normalized model for context-free\ngrammar (CFG)-based semantic parsing. Instead of predicting a probability, our\nmodel predicts a real-valued score at each step and does not suffer from the\nlabel bias problem. Experiments show that our approach outperforms locally\nnormalized models on small datasets, but it does not yield improvement on a\nlarge dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.04102,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000003212,
      "text":"Swords: A Benchmark for Lexical Substitution with Improved Data Coverage\n  and Quality\n\n  We release a new benchmark for lexical substitution, the task of finding\nappropriate substitutes for a target word in a context. To assist humans with\nwriting, lexical substitution systems can suggest words that humans cannot\neasily think of. However, existing benchmarks depend on human recall as the\nonly source of data, and therefore lack coverage of the substitutes that would\nbe most helpful to humans. Furthermore, annotators often provide substitutes of\nlow quality, which are not actually appropriate in the given context. We\ncollect higher-coverage and higher-quality data by framing lexical substitution\nas a classification problem, guided by the intuition that it is easier for\nhumans to judge the appropriateness of candidate substitutes than conjure them\nfrom memory. To this end, we use a context-free thesaurus to produce candidates\nand rely on human judgement to determine contextual appropriateness. Compared\nto the previous largest benchmark, our Swords benchmark has 4.1x more\nsubstitutes per target word for the same level of quality, and its substitutes\nare 1.5x more appropriate (based on human judgement) for the same number of\nsubstitutes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.04647,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000050995,
      "text":"Compacter: Efficient Low-Rank Hypercomplex Adapter Layers\n\n  Adapting large-scale pretrained language models to downstream tasks via\nfine-tuning is the standard method for achieving state-of-the-art performance\non NLP benchmarks. However, fine-tuning all weights of models with millions or\nbillions of parameters is sample-inefficient, unstable in low-resource\nsettings, and wasteful as it requires storing a separate copy of the model for\neach task. Recent work has developed parameter-efficient fine-tuning methods,\nbut these approaches either still require a relatively large number of\nparameters or underperform standard fine-tuning. In this work, we propose\nCompacter, a method for fine-tuning large-scale language models with a better\ntrade-off between task performance and the number of trainable parameters than\nprior work. Compacter accomplishes this by building on top of ideas from\nadapters, low-rank optimization, and parameterized hypercomplex multiplication\nlayers. Specifically, Compacter inserts task-specific weight matrices into a\npretrained model's weights, which are computed efficiently as a sum of\nKronecker products between shared \"slow\" weights and \"fast\" rank-one matrices\ndefined per Compacter layer. By only training 0.047% of a pretrained model's\nparameters, Compacter performs on par with standard fine-tuning on GLUE and\noutperforms standard fine-tuning on SuperGLUE and low-resource settings. Our\ncode is publicly available at~\\url{https:\/\/github.com\/rabeehk\/compacter}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.08364,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"Unsupervised Enrichment of Persona-grounded Dialog with Background\n  Stories\n\n  Humans often refer to personal narratives, life experiences, and events to\nmake a conversation more engaging and rich. While persona-grounded dialog\nmodels are able to generate responses that follow a given persona, they often\nmiss out on stating detailed experiences or events related to a persona, often\nleaving conversations shallow and dull. In this work, we equip dialog models\nwith 'background stories' related to a persona by leveraging fictional\nnarratives from existing story datasets (e.g. ROCStories). Since current dialog\ndatasets do not contain such narratives as responses, we perform an\nunsupervised adaptation of a retrieved story for generating a dialog response\nusing a gradient-based rewriting technique. Our proposed method encourages the\ngenerated response to be fluent (i.e., highly likely) with the dialog history,\nminimally different from the retrieved story to preserve event ordering and\nconsistent with the original persona. We demonstrate that our method can\ngenerate responses that are more diverse, and are rated more engaging and\nhuman-like by human evaluators, compared to outputs from existing dialog\nmodels.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.0104,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000964933,
      "text":"Hi-Transformer: Hierarchical Interactive Transformer for Efficient and\n  Effective Long Document Modeling\n\n  Transformer is important for text modeling. However, it has difficulty in\nhandling long documents due to the quadratic complexity with input text length.\nIn order to handle this problem, we propose a hierarchical interactive\nTransformer (Hi-Transformer) for efficient and effective long document\nmodeling. Hi-Transformer models documents in a hierarchical way, i.e., first\nlearns sentence representations and then learns document representations. It\ncan effectively reduce the complexity and meanwhile capture global document\ncontext in the modeling of each sentence. More specifically, we first use a\nsentence Transformer to learn the representations of each sentence. Then we use\na document Transformer to model the global document context from these sentence\nrepresentations. Next, we use another sentence Transformer to enhance sentence\nmodeling using the global document context. Finally, we use hierarchical\npooling method to obtain document embedding. Extensive experiments on three\nbenchmark datasets validate the efficiency and effectiveness of Hi-Transformer\nin long document modeling.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.04437,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000149674,
      "text":"Adversarial Training for Machine Reading Comprehension with Virtual\n  Embeddings\n\n  Adversarial training (AT) as a regularization method has proved its\neffectiveness on various tasks. Though there are successful applications of AT\non some NLP tasks, the distinguishing characteristics of NLP tasks have not\nbeen exploited. In this paper, we aim to apply AT on machine reading\ncomprehension (MRC) tasks. Furthermore, we adapt AT for MRC tasks by proposing\na novel adversarial training method called PQAT that perturbs the embedding\nmatrix instead of word vectors. To differentiate the roles of passages and\nquestions, PQAT uses additional virtual P\/Q-embedding matrices to gather the\nglobal perturbations of words from passages and questions separately. We test\nthe method on a wide range of MRC tasks, including span-based extractive RC and\nmultiple-choice RC. The results show that adversarial training is effective\nuniversally, and PQAT further improves the performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.03297,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000011656,
      "text":"On the Language Coverage Bias for Neural Machine Translation\n\n  Language coverage bias, which indicates the content-dependent differences\nbetween sentence pairs originating from the source and target languages, is\nimportant for neural machine translation (NMT) because the target-original\ntraining data is not well exploited in current practice. By carefully designing\nexperiments, we provide comprehensive analyses of the language coverage bias in\nthe training data, and find that using only the source-original data achieves\ncomparable performance with using full training data. Based on these\nobservations, we further propose two simple and effective approaches to\nalleviate the language coverage bias problem through explicitly distinguishing\nbetween the source- and target-original training data, which consistently\nimprove the performance over strong baselines on six WMT20 translation tasks.\nComplementary to the translationese effect, language coverage bias provides\nanother explanation for the performance drop caused by back-translation. We\nalso apply our approach to both back- and forward-translation and find that\nmitigating the language coverage bias can improve the performance of both the\ntwo representative data augmentation methods and their tagged variants.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.06905,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000046359,
      "text":"InfoBehavior: Self-supervised Representation Learning for Ultra-long\n  Behavior Sequence via Hierarchical Grouping\n\n  E-commerce companies have to face abnormal sellers who sell potentially-risky\nproducts. Typically, the risk can be identified by jointly considering product\ncontent (e.g., title and image) and seller behavior. This work focuses on\nbehavior feature extraction as behavior sequences can provide valuable clues\nfor the risk discovery by reflecting the sellers' operation habits. Traditional\nfeature extraction techniques heavily depend on domain experts and adapt poorly\nto new tasks. In this paper, we propose a self-supervised method InfoBehavior\nto automatically extract meaningful representations from ultra-long raw\nbehavior sequences instead of the costly feature selection procedure.\nInfoBehavior utilizes Bidirectional Transformer as feature encoder due to its\nexcellent capability in modeling long-term dependency. However, it is\nintractable for commodity GPUs because the time and memory required by\nTransformer grow quadratically with the increase of sequence length. Thus, we\npropose a hierarchical grouping strategy to aggregate ultra-long raw behavior\nsequences to length-processable high-level embedding sequences. Moreover, we\nintroduce two types of pretext tasks. Sequence-related pretext task defines a\ncontrastive-based training objective to correctly select the masked-out\ncoarse-grained\/fine-grained behavior sequences against other \"distractor\"\nbehavior sequences; Domain-related pretext task designs a classification\ntraining objective to correctly predict the domain-specific statistical results\nof anomalous behavior. We show that behavior representations from the\npre-trained InfoBehavior can be directly used or integrated with features from\nother side information to support a wide range of downstream tasks.\nExperimental results demonstrate that InfoBehavior significantly improves the\nperformance of Product Risk Management and Intellectual Property Protection.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.01649,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"LearnDA: Learnable Knowledge-Guided Data Augmentation for Event\n  Causality Identification\n\n  Modern models for event causality identification (ECI) are mainly based on\nsupervised learning, which are prone to the data lacking problem.\nUnfortunately, the existing NLP-related augmentation methods cannot directly\nproduce the available data required for this task. To solve the data lacking\nproblem, we introduce a new approach to augment training data for event\ncausality identification, by iteratively generating new examples and\nclassifying event causality in a dual learning framework. On the one hand, our\napproach is knowledge-guided, which can leverage existing knowledge bases to\ngenerate well-formed new sentences. On the other hand, our approach employs a\ndual mechanism, which is a learnable augmentation framework and can\ninteractively adjust the generation process to generate task-related sentences.\nExperimental results on two benchmarks EventStoryLine and Causal-TimeBank show\nthat 1) our method can augment suitable task-related training data for ECI; 2)\nour method outperforms previous methods on EventStoryLine and Causal-TimeBank\n(+2.5 and +2.1 points on F1 value respectively).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.01654,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000170536,
      "text":"Improving Event Causality Identification via Self-Supervised\n  Representation Learning on External Causal Statement\n\n  Current models for event causality identification (ECI) mainly adopt a\nsupervised framework, which heavily rely on labeled data for training.\nUnfortunately, the scale of current annotated datasets is relatively limited,\nwhich cannot provide sufficient support for models to capture useful indicators\nfrom causal statements, especially for handing those new, unseen cases. To\nalleviate this problem, we propose a novel approach, shortly named CauSeRL,\nwhich leverages external causal statements for event causality identification.\nFirst of all, we design a self-supervised framework to learn context-specific\ncausal patterns from external causal statements. Then, we adopt a contrastive\ntransfer strategy to incorporate the learned context-specific causal patterns\ninto the target ECI model. Experimental results show that our method\nsignificantly outperforms previous methods on EventStoryLine and\nCausal-TimeBank (+2.0 and +3.4 points on F1 value respectively).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.07349,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Using Integrated Gradients and Constituency Parse Trees to explain\n  Linguistic Acceptability learnt by BERT\n\n  Linguistic Acceptability is the task of determining whether a sentence is\ngrammatical or ungrammatical. It has applications in several use cases like\nQuestion-Answering, Natural Language Generation, Neural Machine Translation,\nwhere grammatical correctness is crucial. In this paper we aim to understand\nthe decision-making process of BERT (Devlin et al., 2019) in distinguishing\nbetween Linguistically Acceptable sentences (LA) and Linguistically\nUnacceptable sentences (LUA). We leverage Layer Integrated Gradients\nAttribution Scores (LIG) to explain the Linguistic Acceptability criteria that\nare learnt by BERT on the Corpus of Linguistic Acceptability (CoLA) (Warstadt\net al., 2018) benchmark dataset. Our experiments on 5 categories of sentences\nlead to the following interesting findings: 1) LIG for LA are significantly\nsmaller in comparison to LUA, 2) There are specific subtrees of the\nConstituency Parse Tree (CPT) for LA and LUA which contribute larger LIG, 3)\nAcross the different categories of sentences we observed around 88% to 100% of\nthe Correctly classified sentences had positive LIG, indicating a strong\npositive relationship to the prediction confidence of the model, and 4) Around\n43% of the Misclassified sentences had negative LIG, which we believe can\nbecome correctly classified sentences if the LIG are parameterized in the loss\nfunction of the model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.14433,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000121858,
      "text":"Efficient Dialogue State Tracking by Masked Hierarchical Transformer\n\n  This paper describes our approach to DSTC 9 Track 2: Cross-lingual\nMulti-domain Dialog State Tracking, the task goal is to build a Cross-lingual\ndialog state tracker with a training set in rich resource language and a\ntesting set in low resource language. We formulate a method for joint learning\nof slot operation classification task and state tracking task respectively.\nFurthermore, we design a novel mask mechanism for fusing contextual information\nabout dialogue, the results show the proposed model achieves excellent\nperformance on DSTC Challenge II with a joint accuracy of 62.37% and 23.96% in\nMultiWOZ(en - zh) dataset and CrossWOZ(zh - en) dataset, respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.0558,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000041061,
      "text":"AGGGEN: Ordering and Aggregating while Generating\n\n  We present AGGGEN (pronounced 'again'), a data-to-text model which\nre-introduces two explicit sentence planning stages into neural data-to-text\nsystems: input ordering and input aggregation. In contrast to previous work\nusing sentence planning, our model is still end-to-end: AGGGEN performs\nsentence planning at the same time as generating text by learning latent\nalignments (via semantic facts) between input representation and target text.\nExperiments on the WebNLG and E2E challenge data show that by using fact-based\nalignments our approach is more interpretable, expressive, robust to noise, and\neasier to control, while retaining the advantages of end-to-end systems in\nterms of fluency. Our code is available at https:\/\/github.com\/XinnuoXu\/AggGen.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.05723,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000028478,
      "text":"Automatic Construction of Context-Aware Sentiment Lexicon in the\n  Financial Domain Using Direction-Dependent Words\n\n  Increasing attention has been drawn to the sentiment analysis of financial\ndocuments. The most popular examples of such documents include analyst reports\nand economic news, the analysis of which is frequently used to capture the\ntrends in market sentiments. On the other hand, the significance of the role\nsentiment analysis plays in the financial domain has given rise to the efforts\nto construct a financial domain-specific sentiment lexicon. Sentiment lexicons\nlend a hand for solving various text mining tasks, such as unsupervised\nclassification of text data, while alleviating the arduous human labor required\nfor manual labeling. One of the challenges in the construction of an effective\nsentiment lexicon is that the semantic orientation of a word may change\ndepending on the context in which it appears. For instance, the word ``profit\"\nusually conveys positive sentiments; however, when the word is juxtaposed with\nanother word ``decrease,\" the sentiment associated with the phrase ``profit\ndecreases\" now becomes negative. Hence, the sentiment of a given word may shift\nas one begins to consider the context surrounding the word. In this paper, we\naddress this issue by incorporating context when building sentiment lexicon\nfrom a given corpus. Specifically, we construct a lexicon named Senti-DD for\nthe Sentiment lexicon composed of Direction-Dependent words, which expresses\neach term a pair of a directional word and a direction-dependent word.\nExperiment results show that higher classification performance is achieved with\nSenti-DD, proving the effectiveness of our method for automatically\nconstructing a context-aware sentiment lexicon in the financial domain.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.03484,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000106957,
      "text":"BERTGEN: Multi-task Generation through BERT\n\n  We present BERTGEN, a novel generative, decoder-only model which extends BERT\nby fusing multimodal and multilingual pretrained models VL-BERT and M-BERT,\nrespectively. BERTGEN is auto-regressively trained for language generation\ntasks, namely image captioning, machine translation and multimodal machine\ntranslation, under a multitask setting. With a comprehensive set of\nevaluations, we show that BERTGEN outperforms many strong baselines across the\ntasks explored. We also show BERTGEN's ability for zero-shot language\ngeneration, where it exhibits competitive performance to supervised\ncounterparts. Finally, we conduct ablation studies which demonstrate that\nBERTGEN substantially benefits from multi-tasking and effectively transfers\nrelevant inductive biases from the pre-trained models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.03625,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000025498,
      "text":"Using CollGram to Compare Formulaic Language in Human and Neural Machine\n  Translation\n\n  A comparison of formulaic sequences in human and neural machine translation\nof quality newspaper articles shows that neural machine translations contain\nless lower-frequency, but strongly-associated formulaic sequences, and more\nhigh-frequency formulaic sequences. These differences were statistically\nsignificant and the effect sizes were almost always medium or large. These\nobservations can be related to the differences between second language learners\nof various levels and between translated and untranslated texts. The comparison\nbetween the neural machine translation systems indicates that some systems\nproduce more formulaic sequences of both types than other systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.02444,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0001317263,
      "text":"The NiuTrans End-to-End Speech Translation System for IWSLT 2021 Offline\n  Task\n\n  This paper describes the submission of the NiuTrans end-to-end speech\ntranslation system for the IWSLT 2021 offline task, which translates from the\nEnglish audio to German text directly without intermediate transcription. We\nuse the Transformer-based model architecture and enhance it by Conformer,\nrelative position encoding, and stacked acoustic and textual encoding. To\naugment the training data, the English transcriptions are translated to German\ntranslations. Finally, we employ ensemble decoding to integrate the predictions\nfrom several models trained with the different datasets. Combining these\ntechniques, we achieve 33.84 BLEU points on the MuST-C En-De test set, which\nshows the enormous potential of the end-to-end model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.00596,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Multimodal Graph-based Transformer Framework for Biomedical Relation\n  Extraction\n\n  The recent advancement of pre-trained Transformer models has propelled the\ndevelopment of effective text mining models across various biomedical tasks.\nHowever, these models are primarily learned on the textual data and often lack\nthe domain knowledge of the entities to capture the context beyond the\nsentence. In this study, we introduced a novel framework that enables the model\nto learn multi-omnics biological information about entities (proteins) with the\nhelp of additional multi-modal cues like molecular structure. Towards this,\nrather developing modality-specific architectures, we devise a generalized and\noptimized graph based multi-modal learning mechanism that utilizes the\nGraphBERT model to encode the textual and molecular structure information and\nexploit the underlying features of various modalities to enable end-to-end\nlearning. We evaluated our proposed method on ProteinProtein Interaction task\nfrom the biomedical corpus, where our proposed generalized approach is observed\nto be benefited by the additional domain-specific modality.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.08807,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"Simultaneous Speech Translation for Live Subtitling: from Delay to\n  Display\n\n  With the increased audiovisualisation of communication, the need for live\nsubtitles in multilingual events is more relevant than ever. In an attempt to\nautomatise the process, we aim at exploring the feasibility of simultaneous\nspeech translation (SimulST) for live subtitling. However, the word-for-word\nrate of generation of SimulST systems is not optimal for displaying the\nsubtitles in a comprehensible and readable way. In this work, we adapt SimulST\nsystems to predict subtitle breaks along with the translation. We then propose\na display mode that exploits the predicted break structure by presenting the\nsubtitles in scrolling lines. We compare our proposed mode with a display 1)\nword-for-word and 2) in blocks, in terms of reading speed and delay.\nExperiments on three language pairs (en$\\rightarrow$it, de, fr) show that\nscrolling lines is the only mode achieving an acceptable reading speed while\nkeeping delay close to a 4-second threshold. We argue that simultaneous\ntranslation for readable live subtitles still faces challenges, the main one\nbeing poor translation quality, and propose directions for steering future\nresearch.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.12203,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000140733,
      "text":"Revisiting Negation in Neural Machine Translation\n\n  In this paper, we evaluate the translation of negation both automatically and\nmanually, in English--German (EN--DE) and English--Chinese (EN--ZH). We show\nthat the ability of neural machine translation (NMT) models to translate\nnegation has improved with deeper and more advanced networks, although the\nperformance varies between language pairs and translation directions. The\naccuracy of manual evaluation in EN-DE, DE-EN, EN-ZH, and ZH-EN is 95.7%,\n94.8%, 93.4%, and 91.7%, respectively. In addition, we show that\nunder-translation is the most significant error type in NMT, which contrasts\nwith the more diverse error profile previously observed for statistical machine\ntranslation. To better understand the root of the under-translation of\nnegation, we study the model's information flow and training data. While our\ninformation flow analysis does not reveal any deficiencies that could be used\nto detect or fix the under-translation of negation, we find that negation is\noften rephrased during training, which could make it more difficult for the\nmodel to learn a reliable link between source and target negation. We finally\nconduct intrinsic analysis and extrinsic probing tasks on negation, showing\nthat NMT models can distinguish negation and non-negation tokens very well and\nencode a lot of information about negation in hidden states but nevertheless\nleave room for improvement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.1161,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Context-aware Adversarial Training for Name Regularity Bias in Named\n  Entity Recognition\n\n  In this work, we examine the ability of NER models to use contextual\ninformation when predicting the type of an ambiguous entity. We introduce NRB,\na new testbed carefully designed to diagnose Name Regularity Bias of NER\nmodels. Our results indicate that all state-of-the-art models we tested show\nsuch a bias; BERT fine-tuned models significantly outperforming feature-based\n(LSTM-CRF) ones on NRB, despite having comparable (sometimes lower) performance\non standard benchmarks.\n  To mitigate this bias, we propose a novel model-agnostic training method that\nadds learnable adversarial noise to some entity mentions, thus enforcing models\nto focus more strongly on the contextual signal, leading to significant gains\non NRB. Combining it with two other training strategies, data augmentation and\nparameter freezing, leads to further gains.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.08772,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"Integrating Unsupervised Data Generation into Self-Supervised Neural\n  Machine Translation for Low-Resource Languages\n\n  For most language combinations, parallel data is either scarce or simply\nunavailable. To address this, unsupervised machine translation (UMT) exploits\nlarge amounts of monolingual data by using synthetic data generation techniques\nsuch as back-translation and noising, while self-supervised NMT (SSNMT)\nidentifies parallel sentences in smaller comparable data and trains on them. To\ndate, the inclusion of UMT data generation techniques in SSNMT has not been\ninvestigated. We show that including UMT techniques into SSNMT significantly\noutperforms SSNMT and UMT on all tested language pairs, with improvements of up\nto +4.3 BLEU, +50.8 BLEU, +51.5 over SSNMT, statistical UMT and hybrid UMT,\nrespectively, on Afrikaans to English. We further show that the combination of\nmultilingual denoising autoencoding, SSNMT with backtranslation and bilingual\nfinetuning enables us to learn machine translation even for distant language\npairs for which only small amounts of monolingual data are available, e.g.\nyielding BLEU scores of 11.6 (English to Swahili).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.02875,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000069208,
      "text":"Kosp2e: Korean Speech to English Translation Corpus\n\n  Most speech-to-text (S2T) translation studies use English speech as a source,\nwhich makes it difficult for non-English speakers to take advantage of the S2T\ntechnologies. For some languages, this problem was tackled through corpus\nconstruction, but the farther linguistically from English or the more\nunder-resourced, this deficiency and underrepresentedness becomes more\nsignificant. In this paper, we introduce kosp2e (read as `kospi'), a corpus\nthat allows Korean speech to be translated into English text in an end-to-end\nmanner. We adopt open license speech recognition corpus, translation corpus,\nand spoken language corpora to make our dataset freely available to the public,\nand check the performance through the pipeline and training-based approaches.\nUsing pipeline and various end-to-end schemes, we obtain the highest BLEU of\n21.3 and 18.0 for each based on the English hypothesis, validating the\nfeasibility of our data. We plan to supplement annotations for other target\nlanguages through community contributions in the future.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.00368,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Ensemble Learning-Based Approach for Improving Generalization Capability\n  of Machine Reading Comprehension Systems\n\n  Machine Reading Comprehension (MRC) is an active field in natural language\nprocessing with many successful developed models in recent years. Despite their\nhigh in-distribution accuracy, these models suffer from two issues: high\ntraining cost and low out-of-distribution accuracy. Even though some approaches\nhave been presented to tackle the generalization problem, they have high,\nintolerable training costs. In this paper, we investigate the effect of\nensemble learning approach to improve generalization of MRC systems without\nretraining a big model. After separately training the base models with\ndifferent structures on different datasets, they are ensembled using weighting\nand stacking approaches in probabilistic and non-probabilistic settings. Three\nconfigurations are investigated including heterogeneous, homogeneous, and\nhybrid on eight datasets and six state-of-the-art models. We identify the\nimportant factors in the effectiveness of ensemble methods. Also, we compare\nthe robustness of ensemble and fine-tuned models against data distribution\nshifts. The experimental results show the effectiveness and robustness of the\nensemble approach in improving the out-of-distribution accuracy of MRC systems,\nespecially when the base models are similar in accuracies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.04268,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000175503,
      "text":"Learning Syntactic Dense Embedding with Correlation Graph for Automatic\n  Readability Assessment\n\n  Deep learning models for automatic readability assessment generally discard\nlinguistic features traditionally used in machine learning models for the task.\nWe propose to incorporate linguistic features into neural network models by\nlearning syntactic dense embeddings based on linguistic features. To cope with\nthe relationships between the features, we form a correlation graph among\nfeatures and use it to learn their embeddings so that similar features will be\nrepresented by similar embeddings. Experiments with six data sets of two\nproficiency levels demonstrate that our proposed methodology can complement\nBERT-only model to achieve significantly better performances for automatic\nreadability assessment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.10922,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Did the Cat Drink the Coffee? Challenging Transformers with Generalized\n  Event Knowledge\n\n  Prior research has explored the ability of computational models to predict a\nword semantic fit with a given predicate. While much work has been devoted to\nmodeling the typicality relation between verbs and arguments in isolation, in\nthis paper we take a broader perspective by assessing whether and to what\nextent computational approaches have access to the information about the\ntypicality of entire events and situations described in language (Generalized\nEvent Knowledge). Given the recent success of Transformers Language Models\n(TLMs), we decided to test them on a benchmark for the \\textit{dynamic\nestimation of thematic fit}. The evaluation of these models was performed in\ncomparison with SDM, a framework specifically designed to integrate events in\nsentence meaning representations, and we conducted a detailed error analysis to\ninvestigate which factors affect their behavior. Our results show that TLMs can\nreach performances that are comparable to those achieved by SDM. However,\nadditional analysis consistently suggests that TLMs do not capture important\naspects of event knowledge, and their predictions often depend on surface\nlinguistic features, such as frequent words, collocations and syntactic\npatterns, thereby showing sub-optimal generalization abilities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.08188,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000078148,
      "text":"Overview and Insights from the SciVer Shared Task on Scientific Claim\n  Verification\n\n  We present an overview of the SciVer shared task, presented at the 2nd\nScholarly Document Processing (SDP) workshop at NAACL 2021. In this shared\ntask, systems were provided a scientific claim and a corpus of research\nabstracts, and asked to identify which articles SUPPORT or REFUTE the claim as\nwell as provide evidentiary sentences justifying those labels. 11 teams made a\ntotal of 14 submissions to the shared task leaderboard, leading to an\nimprovement of more than +23 F1 on the primary task evaluation metric. In\naddition to surveying the participating systems, we provide several insights\ninto modeling approaches to support continued progress and future research on\nthe important and challenging task of scientific claim verification.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.12866,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Unsupervised Domain Adaptation for Hate Speech Detection Using a Data\n  Augmentation Approach\n\n  Online harassment in the form of hate speech has been on the rise in recent\nyears. Addressing the issue requires a combination of content moderation by\npeople, aided by automatic detection methods. As content moderation is itself\nharmful to the people doing it, we desire to reduce the burden by improving the\nautomatic detection of hate speech. Hate speech presents a challenge as it is\ndirected at different target groups using a completely different vocabulary.\nFurther the authors of the hate speech are incentivized to disguise their\nbehavior to avoid being removed from a platform. This makes it difficult to\ndevelop a comprehensive data set for training and evaluating hate speech\ndetection models because the examples that represent one hate speech domain do\nnot typically represent others, even within the same language or culture. We\npropose an unsupervised domain adaptation approach to augment labeled data for\nhate speech detection. We evaluate the approach with three different models\n(character CNNs, BiLSTMs and BERT) on three different collections. We show our\napproach improves Area under the Precision\/Recall curve by as much as 42% and\nrecall by as much as 278%, with no loss (and in some cases a significant gain)\nin precision.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.00676,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"A Primer on Pretrained Multilingual Language Models\n\n  Multilingual Language Models (\\MLLMs) such as mBERT, XLM, XLM-R,\n\\textit{etc.} have emerged as a viable option for bringing the power of\npretraining to a large number of languages. Given their success in zero-shot\ntransfer learning, there has emerged a large body of work in (i) building\nbigger \\MLLMs~covering a large number of languages (ii) creating exhaustive\nbenchmarks covering a wider variety of tasks and languages for evaluating\n\\MLLMs~ (iii) analysing the performance of \\MLLMs~on monolingual, zero-shot\ncross-lingual and bilingual tasks (iv) understanding the universal language\npatterns (if any) learnt by \\MLLMs~ and (v) augmenting the (often) limited\ncapacity of \\MLLMs~ to improve their performance on seen or even unseen\nlanguages. In this survey, we review the existing literature covering the above\nbroad areas of research pertaining to \\MLLMs. Based on our survey, we recommend\nsome promising directions of future research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.1292,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Emotion Stimulus Detection in German News Headlines\n\n  Emotion stimulus extraction is a fine-grained subtask of emotion analysis\nthat focuses on identifying the description of the cause behind an emotion\nexpression from a text passage (e.g., in the sentence \"I am happy that I passed\nmy exam\" the phrase \"passed my exam\" corresponds to the stimulus.). Previous\nwork mainly focused on Mandarin and English, with no resources or models for\nGerman. We fill this research gap by developing a corpus of 2006 German news\nheadlines annotated with emotions and 811 instances with annotations of\nstimulus phrases. Given that such corpus creation efforts are time-consuming\nand expensive, we additionally work on an approach for projecting the existing\nEnglish GoodNewsEveryone (GNE) corpus to a machine-translated German version.\nWe compare the performance of a conditional random field (CRF) model (trained\nmonolingually on German and cross-lingually via projection) with a multilingual\nXLM-RoBERTa (XLM-R) model. Our results show that training with the German\ncorpus achieves higher F1 scores than projection. Experiments with XLM-R\noutperform their respective CRF counterparts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.00807,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000123514,
      "text":"He Thinks He Knows Better than the Doctors: BERT for Event Factuality\n  Fails on Pragmatics\n\n  We investigate how well BERT performs on predicting factuality in several\nexisting English datasets, encompassing various linguistic constructions.\nAlthough BERT obtains a strong performance on most datasets, it does so by\nexploiting common surface patterns that correlate with certain factuality\nlabels, and it fails on instances where pragmatic reasoning is necessary.\nContrary to what the high performance suggests, we are still far from having a\nrobust system for factuality prediction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.0345,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000027153,
      "text":"Handling Heavily Abbreviated Manuscripts: HTR engines vs text\n  normalisation approaches\n\n  Although abbreviations are fairly common in handwritten sources, particularly\nin medieval and modern Western manuscripts, previous research dealing with\ncomputational approaches to their expansion is scarce. Yet abbreviations\npresent particular challenges to computational approaches such as handwritten\ntext recognition and natural language processing tasks. Often, pre-processing\nultimately aims to lead from a digitised image of the source to a normalised\ntext, which includes expansion of the abbreviations. We explore different\nsetups to obtain such a normalised text, either directly, by training HTR\nengines on normalised (i.e., expanded, disabbreviated) text, or by decomposing\nthe process into discrete steps, each making use of specialist models for\nrecognition, word segmentation and normalisation. The case studies considered\nhere are drawn from the medieval Latin tradition.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.08128,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"The Law of Large Documents: Understanding the Structure of Legal\n  Contracts Using Visual Cues\n\n  Large, pre-trained transformer models like BERT have achieved\nstate-of-the-art results on document understanding tasks, but most\nimplementations can only consider 512 tokens at a time. For many real-world\napplications, documents can be much longer, and the segmentation strategies\ntypically used on longer documents miss out on document structure and\ncontextual information, hurting their results on downstream tasks. In our work\non legal agreements, we find that visual cues such as layout, style, and\nplacement of text in a document are strong features that are crucial to\nachieving an acceptable level of accuracy on long documents. We measure the\nimpact of incorporating such visual cues, obtained via computer vision methods,\non the accuracy of document understanding tasks including document\nsegmentation, entity extraction, and attribute classification. Our method of\nsegmenting documents based on structural metadata out-performs existing methods\non four long-document understanding tasks as measured on the Contract\nUnderstanding Atticus Dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.14638,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"An automated domain-independent text reading, interpreting and\n  extracting approach for reviewing the scientific literature\n\n  It is presented here a machine learning-based (ML) natural language\nprocessing (NLP) approach capable to automatically recognize and extract\ncategorical and numerical parameters from a corpus of articles. The approach\n(named a.RIX) operates with a concomitant\/interchangeable use of ML models such\nas neuron networks (NNs), latent semantic analysis (LSA), naive-Bayes\nclassifiers (NBC), and a pattern recognition model using regular expression\n(REGEX). A corpus of 7,873 scientific articles dealing with natural products\n(NPs) was used to demonstrate the efficiency of the a.RIX engine. The engine\nautomatically extracts categorical and numerical parameters such as (i) the\nplant species from which active molecules are extracted, (ii) the\nmicroorganisms species for which active molecules can act against, and (iii)\nthe values of minimum inhibitory concentration (MIC) against these\nmicroorganisms. The parameters are extracted without part-of-speech tagging\n(POS) and named entity recognition (NER) approaches (i.e. without the need of\ntext annotation), and the models training is performed with unsupervised\napproaches. In this way, a.RIX can be essentially used on articles from any\nscientific field. Finally, it can potentially make obsolete the current article\nreviewing process in some areas, especially those in which machine learning\nmodels capture texts structure, text semantics, and latent knowledge.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.05833,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Enforcing Consistency in Weakly Supervised Semantic Parsing\n\n  The predominant challenge in weakly supervised semantic parsing is that of\nspurious programs that evaluate to correct answers for the wrong reasons. Prior\nwork uses elaborate search strategies to mitigate the prevalence of spurious\nprograms; however, they typically consider only one input at a time. In this\nwork we explore the use of consistency between the output programs for related\ninputs to reduce the impact of spurious programs. We bias the program search\n(and thus the model's training signal) towards programs that map the same\nphrase in related inputs to the same sub-parts in their respective programs.\nAdditionally, we study the importance of designing logical formalisms that\nfacilitate this kind of consAistency-based training. We find that a more\nconsistent formalism leads to improved model performance even without\nconsistency-based training. When combined together, these two insights lead to\na 10% absolute improvement over the best prior result on the Natural Language\nVisual Reasoning dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.0499,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Perturbing Inputs for Fragile Interpretations in Deep Natural Language\n  Processing\n\n  Interpretability methods like Integrated Gradient and LIME are popular\nchoices for explaining natural language model predictions with relative word\nimportance scores. These interpretations need to be robust for trustworthy NLP\napplications in high-stake areas like medicine or finance. Our paper\ndemonstrates how interpretations can be manipulated by making simple word\nperturbations on an input text. Via a small portion of word-level swaps, these\nadversarial perturbations aim to make the resulting text semantically and\nspatially similar to its seed input (therefore sharing similar\ninterpretations). Simultaneously, the generated examples achieve the same\nprediction label as the seed yet are given a substantially different\nexplanation by the interpretation methods. Our experiments generate fragile\ninterpretations to attack two SOTA interpretation methods, across three popular\nTransformer models and on two different NLP datasets. We observe that the rank\norder correlation drops by over 20% when less than 10% of words are perturbed\non average. Further, rank-order correlation keeps decreasing as more words get\nperturbed. Furthermore, we demonstrate that candidates generated from our\nmethod have good quality metrics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.08877,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000048677,
      "text":"Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text\n  Models\n\n  We provide the first exploration of sentence embeddings from text-to-text\ntransformers (T5). Sentence embeddings are broadly useful for language\nprocessing tasks. While T5 achieves impressive performance on language tasks\ncast as sequence-to-sequence mapping problems, it is unclear how to produce\nsentence embeddings from encoder-decoder models. We investigate three methods\nfor extracting T5 sentence embeddings: two utilize only the T5 encoder and one\nuses the full T5 encoder-decoder model. To support our investigation, we\nestablish a new sentence representation transfer benchmark, SentGLUE, which\nextends the SentEval toolkit to nine tasks from the GLUE benchmark. Our\nencoder-only models outperforms Sentence-BERT and SimCSE sentence embeddings on\nboth SentEval and SentGLUE transfer tasks, including semantic textual\nsimilarity (STS). Scaling up T5 from millions to billions of parameters is\nfound to produce consistent further improvements. Finally, our encoder-decoder\nmethod achieves a new state-of-the-art on STS when using sentence embeddings.\nOur models are released at https:\/\/tfhub.dev\/google\/collections\/sentence-t5\/1.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.01861,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000274181,
      "text":"Quality Evaluation of the Low-Resource Synthetically Generated\n  Code-Mixed Hinglish Text\n\n  In this shared task, we seek the participating teams to investigate the\nfactors influencing the quality of the code-mixed text generation systems. We\nsynthetically generate code-mixed Hinglish sentences using two distinct\napproaches and employ human annotators to rate the generation quality. We\npropose two subtasks, quality rating prediction and annotators' disagreement\nprediction of the synthetic Hinglish dataset. The proposed subtasks will put\nforward the reasoning and explanation of the factors influencing the quality\nand human perception of the code-mixed text.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.00513,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"Attention-based Aspect Reasoning for Knowledge Base Question Answering\n  on Clinical Notes\n\n  Question Answering (QA) in clinical notes has gained a lot of attention in\nthe past few years. Existing machine reading comprehension approaches in\nclinical domain can only handle questions about a single block of clinical\ntexts and fail to retrieve information about multiple patients and their\nclinical notes. To handle more complex questions, we aim at creating knowledge\nbase from clinical notes to link different patients and clinical notes, and\nperforming knowledge base question answering (KBQA). Based on the expert\nannotations available in the n2c2 dataset, we first created the ClinicalKBQA\ndataset that includes around 9K QA pairs and covers questions about seven\nmedical topics using more than 300 question templates. Then, we investigated an\nattention-based aspect reasoning (AAR) method for KBQA and analyzed the impact\nof different aspects of answers (e.g., entity, type, path, and context) for\nprediction. The AAR method achieves better performance due to the well-designed\nencoder and attention mechanism. From our experiments, we find that both\naspects, type and path, enable the model to identify answers satisfying the\ngeneral conditions and produce lower precision and higher recall. On the other\nhand, the aspects, entity and context, limit the answers by node-specific\ninformation and lead to higher precision and lower recall.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.11792,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0014739566,
      "text":"A Statutory Article Retrieval Dataset in French\n\n  Statutory article retrieval is the task of automatically retrieving law\narticles relevant to a legal question. While recent advances in natural\nlanguage processing have sparked considerable interest in many legal tasks,\nstatutory article retrieval remains primarily untouched due to the scarcity of\nlarge-scale and high-quality annotated datasets. To address this bottleneck, we\nintroduce the Belgian Statutory Article Retrieval Dataset (BSARD), which\nconsists of 1,100+ French native legal questions labeled by experienced jurists\nwith relevant articles from a corpus of 22,600+ Belgian law articles. Using\nBSARD, we benchmark several state-of-the-art retrieval approaches, including\nlexical and dense architectures, both in zero-shot and supervised setups. We\nfind that fine-tuned dense retrieval models significantly outperform other\nsystems. Our best performing baseline achieves 74.8% R@100, which is promising\nfor the feasibility of the task and indicates there is still room for\nimprovement. By the specificity of the domain and addressed task, BSARD\npresents a unique challenge problem for future research on legal information\nretrieval. Our dataset and source code are publicly available.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.03509,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000137091,
      "text":"Compositional Generalization in Multilingual Semantic Parsing over\n  Wikidata\n\n  Semantic parsing (SP) allows humans to leverage vast knowledge resources\nthrough natural interaction. However, parsers are mostly designed for and\nevaluated on English resources, such as CFQ (Keysers et al., 2020), the current\nstandard benchmark based on English data generated from grammar rules and\noriented towards Freebase, an outdated knowledge base. We propose a method for\ncreating a multilingual, parallel dataset of question-query pairs, grounded in\nWikidata. We introduce such a dataset, which we call Multilingual Compositional\nWikidata Questions (MCWQ), and use it to analyze the compositional\ngeneralization of semantic parsers in Hebrew, Kannada, Chinese and English.\nWhile within-language generalization is comparable across languages,\nexperiments on zero-shot cross-lingual transfer demonstrate that cross-lingual\ncompositional generalization fails, even with state-of-the-art pretrained\nmultilingual encoders. Furthermore, our methodology, dataset and results will\nfacilitate future research on SP in more realistic and diverse settings than\nhas been possible with existing resources.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.10986,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000125501,
      "text":"Using BERT Encoding and Sentence-Level Language Model for Sentence\n  Ordering\n\n  Discovering the logical sequence of events is one of the cornerstones in\nNatural Language Understanding. One approach to learn the sequence of events is\nto study the order of sentences in a coherent text. Sentence ordering can be\napplied in various tasks such as retrieval-based Question Answering, document\nsummarization, storytelling, text generation, and dialogue systems.\nFurthermore, we can learn to model text coherence by learning how to order a\nset of shuffled sentences. Previous research has relied on RNN, LSTM, and\nBiLSTM architecture for learning text language models. However, these networks\nhave performed poorly due to the lack of attention mechanisms. We propose an\nalgorithm for sentence ordering in a corpus of short stories. Our proposed\nmethod uses a language model based on Universal Transformers (UT) that captures\nsentences' dependencies by employing an attention mechanism. Our method\nimproves the previous state-of-the-art in terms of Perfect Match Ratio (PMR)\nscore in the ROCStories dataset, a corpus of nearly 100K short human-made\nstories. The proposed model includes three components: Sentence Encoder,\nLanguage Model, and Sentence Arrangement with Brute Force Search. The first\ncomponent generates sentence embeddings using SBERT-WK pre-trained model\nfine-tuned on the ROCStories data. Then a Universal Transformer network\ngenerates a sentence-level language model. For decoding, the network generates\na candidate sentence as the following sentence of the current sentence. We use\ncosine similarity as a scoring function to assign scores to the candidate\nembedding and the embeddings of other sentences in the shuffled set. Then a\nBrute Force Search is employed to maximize the sum of similarities between\npairs of consecutive sentences.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.00577,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000061591,
      "text":"Logic-Consistency Text Generation from Semantic Parses\n\n  Text generation from semantic parses is to generate textual descriptions for\nformal representation inputs such as logic forms and SQL queries. This is\nchallenging due to two reasons: (1) the complex and intensive inner logic with\nthe data scarcity constraint, (2) the lack of automatic evaluation metrics for\nlogic consistency. To address these two challenges, this paper first proposes\nSNOWBALL, a framework for logic consistent text generation from semantic parses\nthat employs an iterative training procedure by recursively augmenting the\ntraining set with quality control. Second, we propose a novel automatic metric,\nBLEC, for evaluating the logical consistency between the semantic parses and\ngenerated texts. The experimental results on two benchmark datasets, Logic2Text\nand Spider, demonstrate the SNOWBALL framework enhances the logic consistency\non both BLEC and human evaluation. Furthermore, our statistical analysis\nreveals that BLEC is more logically consistent with human evaluation than\ngeneral-purpose automatic metrics including BLEU, ROUGE and, BLEURT. Our data\nand code are available at https:\/\/github.com\/Ciaranshu\/relogic.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.11696,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000069539,
      "text":"Rethinking Why Intermediate-Task Fine-Tuning Works\n\n  Supplementary Training on Intermediate Labeled-data Tasks (STILTs) is a\nwidely applied technique, which first fine-tunes the pretrained language models\non an intermediate task before on the target task of interest. While STILTs is\nable to further improve the performance of pretrained language models, it is\nstill unclear why and when it works. Previous research shows that those\nintermediate tasks involving complex inference, such as commonsense reasoning,\nwork especially well for RoBERTa. In this paper, we discover that the\nimprovement from an intermediate task could be orthogonal to it containing\nreasoning or other complex skills -- a simple real-fake discrimination task\nsynthesized by GPT2 can benefit diverse target tasks. We conduct extensive\nexperiments to study the impact of different factors on STILTs. These findings\nsuggest rethinking the role of intermediate fine-tuning in the STILTs pipeline.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.13139,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"CSDS: A Fine-Grained Chinese Dataset for Customer Service Dialogue\n  Summarization\n\n  Dialogue summarization has drawn much attention recently. Especially in the\ncustomer service domain, agents could use dialogue summaries to help boost\ntheir works by quickly knowing customer's issues and service progress. These\napplications require summaries to contain the perspective of a single speaker\nand have a clear topic flow structure, while neither are available in existing\ndatasets. Therefore, in this paper, we introduce a novel Chinese dataset for\nCustomer Service Dialogue Summarization (CSDS). CSDS improves the abstractive\nsummaries in two aspects: (1) In addition to the overall summary for the whole\ndialogue, role-oriented summaries are also provided to acquire different\nspeakers' viewpoints. (2) All the summaries sum up each topic separately, thus\ncontaining the topic-level structure of the dialogue. We define tasks in CSDS\nas generating the overall summary and different role-oriented summaries for a\ngiven dialogue. Next, we compare various summarization methods on CSDS, and\nexperiment results show that existing methods are prone to generate redundant\nand incoherent summaries. Besides, the performance becomes much worse when\nanalyzing the performance on role-oriented summaries and topic structures. We\nhope that this study could benchmark Chinese dialogue summarization and benefit\nfurther studies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.07127,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Active Learning for Massively Parallel Translation of Constrained Text\n  into Low Resource Languages\n\n  We translate a closed text that is known in advance and available in many\nlanguages into a new and severely low resource language. Most human translation\nefforts adopt a portion-based approach to translate consecutive pages\/chapters\nin order, which may not suit machine translation. We compare the portion-based\napproach that optimizes coherence of the text locally with the random sampling\napproach that increases coverage of the text globally. Our results show that\nthe random sampling approach performs better. When training on a seed corpus of\n~1,000 lines from the Bible and testing on the rest of the Bible (~30,000\nlines), random sampling gives a performance gain of +11.0 BLEU using English as\na simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a\nMayan language. Furthermore, we compare three ways of updating machine\ntranslation models with increasing amount of human post-edited data through\niterations. We find that adding newly post-edited data to training after\nvocabulary update without self-supervision performs the best. We propose an\nalgorithm for human and machine to work together seamlessly to translate a\nclosed text into a severely low resource language.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.12409,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000117222,
      "text":"Train Short, Test Long: Attention with Linear Biases Enables Input\n  Length Extrapolation\n\n  Since the introduction of the transformer model by Vaswani et al. (2017), a\nfundamental question has yet to be answered: how does a model achieve\nextrapolation at inference time for sequences that are longer than it saw\nduring training? We first show that extrapolation can be enabled by simply\nchanging the position representation method, though we find that current\nmethods do not allow for efficient extrapolation. We therefore introduce a\nsimpler and more efficient position method, Attention with Linear Biases\n(ALiBi). ALiBi does not add positional embeddings to word embeddings; instead,\nit biases query-key attention scores with a penalty that is proportional to\ntheir distance. We show that this method trains a 1.3 billion parameter model\non input sequences of length 1024 that extrapolates to input sequences of\nlength 2048, achieving the same perplexity as a sinusoidal position embedding\nmodel trained on inputs of length 2048 but training 11% faster and using 11%\nless memory. ALiBi's inductive bias towards recency also leads it to outperform\nmultiple strong position methods on the WikiText-103 benchmark.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.14006,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"A Generative Approach for Mitigating Structural Biases in Natural\n  Language Inference\n\n  Many natural language inference (NLI) datasets contain biases that allow\nmodels to perform well by only using a biased subset of the input, without\nconsidering the remainder features. For instance, models are able to make a\nclassification decision by only using the hypothesis, without learning the true\nrelationship between it and the premise. These structural biases lead\ndiscriminative models to learn unintended superficial features and to\ngeneralize poorly out of the training distribution. In this work, we\nreformulate the NLI task as a generative task, where a model is conditioned on\nthe biased subset of the input and the label and generates the remaining subset\nof the input. We show that by imposing a uniform prior, we obtain a provably\nunbiased model. Through synthetic experiments, we find that this approach is\nhighly robust to large amounts of bias. We then demonstrate empirically on two\ntypes of natural bias that this approach leads to fully unbiased models in\npractice. However, we find that generative models are difficult to train and\nthey generally perform worse than discriminative baselines. We highlight the\ndifficulty of the generative modeling task in the context of NLI as a cause for\nthis worse performance. Finally, by fine-tuning the generative model with a\ndiscriminative objective, we reduce the performance gap between the generative\nmodel and the discriminative baseline, while allowing for a small amount of\nbias.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.08759,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000033445,
      "text":"DESYR: Definition and Syntactic Representation Based Claim Detection on\n  the Web\n\n  The formulation of a claim rests at the core of argument mining. To demarcate\nbetween a claim and a non-claim is arduous for both humans and machines, owing\nto latent linguistic variance between the two and the inadequacy of extensive\ndefinition-based formalization. Furthermore, the increase in the usage of\nonline social media has resulted in an explosion of unsolicited information on\nthe web presented as informal text. To account for the aforementioned, in this\npaper, we proposed DESYR. It is a framework that intends on annulling the said\nissues for informal web-based text by leveraging a combination of hierarchical\nrepresentation learning (dependency-inspired Poincare embedding),\ndefinition-based alignment, and feature projection. We do away with fine-tuning\ncomputer-heavy language models in favor of fabricating a more domain-centric\nbut lighter approach. Experimental results indicate that DESYR builds upon the\nstate-of-the-art system across four benchmark claim datasets, most of which\nwere constructed with informal texts. We see an increase of 3 claim-F1 points\non the LESA-Twitter dataset, an increase of 1 claim-F1 point and 9 macro-F1\npoints on the Online Comments(OC) dataset, an increase of 24 claim-F1 points\nand 17 macro-F1 points on the Web Discourse(WD) dataset, and an increase of 8\nclaim-F1 points and 5 macro-F1 points on the Micro Texts(MT) dataset. We also\nperform an extensive analysis of the results. We make a 100-D pre-trained\nversion of our Poincare-variant along with the source code.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.10949,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000094374,
      "text":"Robustness Evaluation of Entity Disambiguation Using Prior Probes:the\n  Case of Entity Overshadowing\n\n  Entity disambiguation (ED) is the last step of entity linking (EL), when\ncandidate entities are reranked according to the context they appear in. All\ndatasets for training and evaluating models for EL consist of convenience\nsamples, such as news articles and tweets, that propagate the prior probability\nbias of the entity distribution towards more frequently occurring entities. It\nwas previously shown that the performance of the EL systems on such datasets is\noverestimated since it is possible to obtain higher accuracy scores by merely\nlearning the prior. To provide a more adequate evaluation benchmark, we\nintroduce the ShadowLink dataset, which includes 16K short text snippets\nannotated with entity mentions. We evaluate and report the performance of\npopular EL systems on the ShadowLink benchmark. The results show a considerable\ndifference in accuracy between more and less common entities for all of the EL\nsystems under evaluation, demonstrating the effects of prior probability bias\nand entity overshadowing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.03973,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000057949,
      "text":"BERT-based distractor generation for Swedish reading comprehension\n  questions using a small-scale dataset\n\n  An important part when constructing multiple-choice questions (MCQs) for\nreading comprehension assessment are the distractors, the incorrect but\npreferably plausible answer options. In this paper, we present a new BERT-based\nmethod for automatically generating distractors using only a small-scale\ndataset. We also release a new such dataset of Swedish MCQs (used for training\nthe model), and propose a methodology for assessing the generated distractors.\nEvaluation shows that from a student's perspective, our method generated one or\nmore plausible distractors for more than 50% of the MCQs in our test set. From\na teacher's perspective, about 50% of the generated distractors were deemed\nappropriate. We also do a thorough analysis of the results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.10939,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000066559,
      "text":"Towards Offensive Language Identification for Tamil Code-Mixed YouTube\n  Comments and Posts\n\n  Offensive Language detection in social media platforms has been an active\nfield of research over the past years. In non-native English spoken countries,\nsocial media users mostly use a code-mixed form of text in their\nposts\/comments. This poses several challenges in the offensive content\nidentification tasks, and considering the low resources available for Tamil,\nthe task becomes much harder. The current study presents extensive experiments\nusing multiple deep learning, and transfer learning models to detect offensive\ncontent on YouTube. We propose a novel and flexible approach of selective\ntranslation and transliteration techniques to reap better results from\nfine-tuning and ensembling multilingual transformer networks like BERT, Distil-\nBERT, and XLM-RoBERTa. The experimental results showed that ULMFiT is the best\nmodel for this task. The best performing models were ULMFiT and mBERTBiLSTM for\nthis Tamil code-mix dataset instead of more popular transfer learning models\nsuch as Distil- BERT and XLM-RoBERTa and hybrid deep learning models. The\nproposed model ULMFiT and mBERTBiLSTM yielded good results and are promising\nfor effective offensive speech identification in low-resourced languages.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.07708,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000024504,
      "text":"A Game Interface to Study Semantic Grounding in Text-Based Models\n\n  Can language models learn grounded representations from text distribution\nalone? This question is both central and recurrent in natural language\nprocessing; authors generally agree that grounding requires more than textual\ndistribution. We propose to experimentally test this claim: if any two words\nhave different meanings and yet cannot be distinguished from distribution\nalone, then grounding is out of the reach of text-based models. To that end, we\npresent early work on an online game for the collection of human judgments on\nthe distributional similarity of word pairs in five languages. We further\nreport early results of our data collection campaign.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.02684,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000130799,
      "text":"Exploring Out-of-Distribution Generalization in Text Classifiers Trained\n  on Tobacco-3482 and RVL-CDIP\n\n  To be robust enough for widespread adoption, document analysis systems\ninvolving machine learning models must be able to respond correctly to inputs\nthat fall outside of the data distribution that was used to generate the data\non which the models were trained. This paper explores the ability of text\nclassifiers trained on standard document classification datasets to generalize\nto out-of-distribution documents at inference time. We take the Tobacco-3482\nand RVL-CDIP datasets as a starting point and generate new out-of-distribution\nevaluation datasets in order to analyze the generalization performance of\nmodels trained on these standard datasets. We find that models trained on the\nsmaller Tobacco-3482 dataset perform poorly on our new out-of-distribution\ndata, while text classification models trained on the larger RVL-CDIP exhibit\nsmaller performance drops.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.12009,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000030465,
      "text":"EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa\n\n  We present EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with\nRoBERTa, a simple yet expressive scheme of solving the ERC (emotion recognition\nin conversation) task. By simply prepending speaker names to utterances and\ninserting separation tokens between the utterances in a dialogue, EmoBERTa can\nlearn intra- and inter- speaker states and context to predict the emotion of a\ncurrent speaker, in an end-to-end manner. Our experiments show that we reach a\nnew state of the art on the two popular ERC datasets using a basic and\nstraight-forward approach. We've open sourced our code and models at\nhttps:\/\/github.com\/tae898\/erc.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.13105,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000015563,
      "text":"Does referent predictability affect the choice of referential form? A\n  computational approach using masked coreference resolution\n\n  It is often posited that more predictable parts of a speaker's meaning tend\nto be made less explicit, for instance using shorter, less informative words.\nStudying these dynamics in the domain of referring expressions has proven\ndifficult, with existing studies, both psycholinguistic and corpus-based,\nproviding contradictory results. We test the hypothesis that speakers produce\nless informative referring expressions (e.g., pronouns vs. full noun phrases)\nwhen the context is more informative about the referent, using novel\ncomputational estimates of referent predictability. We obtain these estimates\ntraining an existing coreference resolution system for English on a new task,\nmasked coreference resolution, giving us a probability distribution over\nreferents that is conditioned on the context but not the referring expression.\nThe resulting system retains standard coreference resolution performance while\nyielding a better estimate of human-derived referent predictability than\nprevious attempts. A statistical analysis of the relationship between model\noutput and mention form supports the hypothesis that predictability affects the\nform of a mention, both its morphosyntactic type and its length.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.08565,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"Exploring Multitask Learning for Low-Resource AbstractiveSummarization\n\n  This paper explores the effect of using multitask learning for abstractive\nsummarization in the context of small training corpora. In particular, we\nincorporate four different tasks (extractive summarization, language modeling,\nconcept detection, and paraphrase detection) both individually and in\ncombination, with the goal of enhancing the target task of abstractive\nsummarization via multitask learning. We show that for many task combinations,\na model trained in a multitask setting outperforms a model trained only for\nabstractive summarization, with no additional summarization data introduced.\nAdditionally, we do a comprehensive search and find that certain tasks (e.g.\nparaphrase detection) consistently benefit abstractive summarization, not only\nwhen combined with other tasks but also when using different architectures and\ntraining corpora.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.06605,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000028478,
      "text":"MDAPT: Multilingual Domain Adaptive Pretraining in a Single Model\n\n  Domain adaptive pretraining, i.e. the continued unsupervised pretraining of a\nlanguage model on domain-specific text, improves the modelling of text for\ndownstream tasks within the domain. Numerous real-world applications are based\non domain-specific text, e.g. working with financial or biomedical documents,\nand these applications often need to support multiple languages. However,\nlarge-scale domain-specific multilingual pretraining data for such scenarios\ncan be difficult to obtain, due to regulations, legislation, or simply a lack\nof language- and domain-specific text. One solution is to train a single\nmultilingual model, taking advantage of the data available in as many languages\nas possible. In this work, we explore the benefits of domain adaptive\npretraining with a focus on adapting to multiple languages within a specific\ndomain. We propose different techniques to compose pretraining corpora that\nenable a language model to both become domain-specific and multilingual.\nEvaluation on nine domain-specific datasets-for biomedical named entity\nrecognition and financial sentence classification-covering seven different\nlanguages show that a single multilingual domain-specific model can outperform\nthe general multilingual model, and performs close to its monolingual\ncounterpart. This finding holds across two different pretraining methods,\nadapter-based pretraining and full model pretraining.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.10227,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000035101,
      "text":"Blindness to Modality Helps Entailment Graph Mining\n\n  Understanding linguistic modality is widely seen as important for downstream\ntasks such as Question Answering and Knowledge Graph Population. Entailment\nGraph learning might also be expected to benefit from attention to modality. We\nbuild Entailment Graphs using a news corpus filtered with a modality parser,\nand show that stripping modal modifiers from predicates in fact increases\nperformance. This suggests that for some tasks, the pragmatics of modal\nmodification of predicates allows them to contribute as evidence of entailment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.05438,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000004371,
      "text":"\"Let Your Characters Tell Their Story\": A Dataset for Character-Centric\n  Narrative Understanding\n\n  When reading a literary piece, readers often make inferences about various\ncharacters' roles, personalities, relationships, intents, actions, etc. While\nhumans can readily draw upon their past experiences to build such a\ncharacter-centric view of the narrative, understanding characters in narratives\ncan be a challenging task for machines. To encourage research in this field of\ncharacter-centric narrative understanding, we present LiSCU -- a new dataset of\nliterary pieces and their summaries paired with descriptions of characters that\nappear in them. We also introduce two new tasks on LiSCU: Character\nIdentification and Character Description Generation. Our experiments with\nseveral pre-trained language models adapted for these tasks demonstrate that\nthere is a need for better models of narrative comprehension.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.08068,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000049671,
      "text":"Automatic Error Type Annotation for Arabic\n\n  We present ARETA, an automatic error type annotation system for Modern\nStandard Arabic. We design ARETA to address Arabic's morphological richness and\northographic ambiguity. We base our error taxonomy on the Arabic Learner Corpus\n(ALC) Error Tagset with some modifications. ARETA achieves a performance of\n85.8% (micro average F1 score) on a manually annotated blind test portion of\nALC. We also demonstrate ARETA's usability by applying it to a number of\nsubmissions from the QALB 2014 shared task for Arabic grammatical error\ncorrection. The resulting analyses give helpful insights on the strengths and\nweaknesses of different submissions, which is more useful than the opaque M2\nscoring metrics used in the shared task. ARETA employs a large Arabic\nmorphological analyzer, but is completely unsupervised otherwise. We make ARETA\npublicly available.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.10194,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000078148,
      "text":"TranslateLocally: Blazing-fast translation running on the local CPU\n\n  Every day, millions of people sacrifice their privacy and browsing habits in\nexchange for online machine translation. Companies and governments with\nconfidentiality requirements often ban online translation or pay a premium to\ndisable logging. To bring control back to the end user and demonstrate speed,\nwe developed translateLocally. Running locally on a desktop or laptop CPU,\ntranslateLocally delivers cloud-like translation speed and quality even on 10\nyear old hardware. The open-source software is based on Marian and runs on\nLinux, Windows, and macOS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.14934,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000018875,
      "text":"Prose2Poem: The Blessing of Transformers in Translating Prose to Persian\n  Poetry\n\n  Persian Poetry has consistently expressed its philosophy, wisdom, speech, and\nrationale on the basis of its couplets, making it an enigmatic language on its\nown to both native and non-native speakers. Nevertheless, the notice able gap\nbetween Persian prose and poem has left the two pieces of literature\nmedium-less. Having curated a parallel corpus of prose and their equivalent\npoems, we introduce a novel Neural Machine Translation (NMT) approach to\ntranslate prose to ancient Persian poetry using transformer-based Language\nModels in an extremely low-resource setting. More specifically, we trained a\nTransformer model from scratch to obtain initial translations and pretrained\ndifferent variations of BERT to obtain final translations. To address the\nchallenge of using masked language modelling under poeticness criteria, we\nheuristically joined the two models and generated valid poems in terms of\nautomatic and human assessments. Final results demonstrate the eligibility and\ncreativity of our novel heuristically aided approach among Literature\nprofessionals and non-professionals in generating novel Persian poems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.05747,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000058611,
      "text":"Honey or Poison? Solving the Trigger Curse in Few-shot Event Detection\n  via Causal Intervention\n\n  Event detection has long been troubled by the \\emph{trigger curse}:\noverfitting the trigger will harm the generalization ability while underfitting\nit will hurt the detection performance. This problem is even more severe in\nfew-shot scenario. In this paper, we identify and solve the trigger curse\nproblem in few-shot event detection (FSED) from a causal view. By formulating\nFSED with a structural causal model (SCM), we found that the trigger is a\nconfounder of the context and the result, which makes previous FSED methods\nmuch easier to overfit triggers. To resolve this problem, we propose to\nintervene on the context via backdoor adjustment during training. Experiments\nshow that our method significantly improves the FSED on ACE05, MAVEN and KBP17\ndatasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.10013,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000030796,
      "text":"Negation-Instance Based Evaluation of End-to-End Negation Resolution\n\n  In this paper, we revisit the task of negation resolution, which includes the\nsubtasks of cue detection (e.g. \"not\", \"never\") and scope resolution. In the\ncontext of previous shared tasks, a variety of evaluation metrics have been\nproposed. Subsequent works usually use different subsets of these, including\nvariations and custom implementations, rendering meaningful comparisons between\nsystems difficult. Examining the problem both from a linguistic perspective and\nfrom a downstream viewpoint, we here argue for a negation-instance based\napproach to evaluating negation resolution. Our proposed metrics correspond to\nexpectations over per-instance scores and hence are intuitively interpretable.\nTo render research comparable and to foster future work, we provide results for\na set of current state-of-the-art systems for negation resolution on three\nEnglish corpora, and make our implementation of the evaluation scripts publicly\navailable.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.07971,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000072188,
      "text":"Do Language Models Know the Way to Rome?\n\n  The global geometry of language models is important for a range of\napplications, but language model probes tend to evaluate rather local\nrelations, for which ground truths are easily obtained. In this paper we\nexploit the fact that in geography, ground truths are available beyond local\nrelations. In a series of experiments, we evaluate the extent to which language\nmodel representations of city and country names are isomorphic to real-world\ngeography, e.g., if you tell a language model where Paris and Berlin are, does\nit know the way to Rome? We find that language models generally encode limited\ngeographic information, but with larger models performing the best, suggesting\nthat geographic knowledge can be induced from higher-order co-occurrence\nstatistics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.11237,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000027153,
      "text":"Pregroup Grammars, their Syntax and Semantics\n\n  Pregroup grammars were developed in 1999 and stayed Lambek's preferred\nalgebraic model of grammar. The set-theoretic semantics of pregroups, however,\nfaces an ambiguity problem. In his latest book, Lambek suggests that this\nproblem might be overcome using finite dimensional vector spaces rather than\nsets. What is the right notion of composition in this setting, direct sum or\ntensor product of spaces?\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.05252,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000095367,
      "text":"XCoref: Cross-document Coreference Resolution in the Wild\n\n  Datasets and methods for cross-document coreference resolution (CDCR) focus\non events or entities with strict coreference relations. They lack, however,\nannotating and resolving coreference mentions with more abstract or loose\nrelations that may occur when news articles report about controversial and\npolarized events. Bridging and loose coreference relations trigger associations\nthat may lead to exposing news readers to bias by word choice and labeling. For\nexample, coreferential mentions of \"direct talks between U.S. President Donald\nTrump and Kim\" such as \"an extraordinary meeting following months of heated\nrhetoric\" or \"great chance to solve a world problem\" form a more positive\nperception of this event. A step towards bringing awareness of bias by word\nchoice and labeling is the reliable resolution of coreferences with high\nlexical diversity. We propose an unsupervised method named XCoref, which is a\nCDCR method that capably resolves not only previously prevalent entities, such\nas persons, e.g., \"Donald Trump,\" but also abstractly defined concepts, such as\ngroups of persons, \"caravan of immigrants,\" events and actions, e.g., \"marching\nto the U.S. border.\" In an extensive evaluation, we compare the proposed XCoref\nto a state-of-the-art CDCR method and a previous method TCA that resolves such\ncomplex coreference relations and find that XCoref outperforms these methods.\nOutperforming an established CDCR model shows that the new CDCR models need to\nbe evaluated on semantically complex mentions with more loose coreference\nrelations to indicate their applicability of models to resolve mentions in the\n\"wild\" of political news articles.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.0666,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000222524,
      "text":"An MRC Framework for Semantic Role Labeling\n\n  Semantic Role Labeling (SRL) aims at recognizing the predicate-argument\nstructure of a sentence and can be decomposed into two subtasks: predicate\ndisambiguation and argument labeling. Prior work deals with these two tasks\nindependently, which ignores the semantic connection between the two tasks. In\nthis paper, we propose to use the machine reading comprehension (MRC) framework\nto bridge this gap. We formalize predicate disambiguation as multiple-choice\nmachine reading comprehension, where the descriptions of candidate senses of a\ngiven predicate are used as options to select the correct sense. The chosen\npredicate sense is then used to determine the semantic roles for that\npredicate, and these semantic roles are used to construct the query for another\nMRC model for argument labeling. In this way, we are able to leverage both the\npredicate semantics and the semantic role semantics for argument labeling. We\nalso propose to select a subset of all the possible semantic roles for\ncomputational efficiency. Experiments show that the proposed framework achieves\nstate-of-the-art or comparable results to previous work. Code is available at\n\\url{https:\/\/github.com\/ShannonAI\/MRC-SRL}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.04832,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000163913,
      "text":"Asking It All: Generating Contextualized Questions for any Semantic Role\n\n  Asking questions about a situation is an inherent step towards understanding\nit. To this end, we introduce the task of role question generation, which,\ngiven a predicate mention and a passage, requires producing a set of questions\nasking about all possible semantic roles of the predicate. We develop a\ntwo-stage model for this task, which first produces a context-independent\nquestion prototype for each role and then revises it to be contextually\nappropriate for the passage. Unlike most existing approaches to question\ngeneration, our approach does not require conditioning on existing answers in\nthe text. Instead, we condition on the type of information to inquire about,\nregardless of whether the answer appears explicitly in the text, could be\ninferred from it, or should be sought elsewhere. Our evaluation demonstrates\nthat we generate diverse and well-formed questions for a large, broad-coverage\nontology of predicates and roles.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.00301,
    "paper_type":"other",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000025498,
      "text":"$\\infty$-former: Infinite Memory Transformer\n\n  Transformers are unable to model long-term memories effectively, since the\namount of computation they need to perform grows with the context length. While\nvariations of efficient transformers have been proposed, they all have a finite\nmemory capacity and are forced to drop old information. In this paper, we\npropose the $\\infty$-former, which extends the vanilla transformer with an\nunbounded long-term memory. By making use of a continuous-space attention\nmechanism to attend over the long-term memory, the $\\infty$-former's attention\ncomplexity becomes independent of the context length, trading off memory length\nwith precision. In order to control where precision is more important,\n$\\infty$-former maintains \"sticky memories\" being able to model arbitrarily\nlong contexts while keeping the computation budget fixed. Experiments on a\nsynthetic sorting task, language modeling, and document grounded dialogue\ngeneration demonstrate the $\\infty$-former's ability to retain information from\nlong sequences.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.11227,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000027153,
      "text":"Fuzzy Generalised Quantifiers for Natural Language in Categorical\n  Compositional Distributional Semantics\n\n  Recent work on compositional distributional models shows that bialgebras over\nfinite dimensional vector spaces can be applied to treat generalised\nquantifiers for natural language. That technique requires one to construct the\nvector space over powersets, and therefore is computationally costly. In this\npaper, we overcome this problem by considering fuzzy versions of quantifiers\nalong the lines of Zadeh, within the category of many valued relations. We show\nthat this category is a concrete instantiation of the compositional\ndistributional model. We show that the semantics obtained in this model is\nequivalent to the semantics of the fuzzy quantifiers of Zadeh. As a result, we\nare now able to treat fuzzy quantification without requiring a powerset\nconstruction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.14895,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by\n  Machine Translation Systems\n\n  In translating text where sentiment is the main message, human translators\ngive particular attention to sentiment-carrying words. The reason is that an\nincorrect translation of such words would miss the fundamental aspect of the\nsource text, i.e. the author's sentiment. In the online world, MT systems are\nextensively used to translate User-Generated Content (UGC) such as reviews,\ntweets, and social media posts, where the main message is often the author's\npositive or negative attitude towards the topic of the text. It is important in\nsuch scenarios to accurately measure how far an MT system can be a reliable\nreal-life utility in transferring the correct affect message. This paper\ntackles an under-recognised problem in the field of machine translation\nevaluation which is judging to what extent automatic metrics concur with the\ngold standard of human evaluation for a correct translation of sentiment. We\nevaluate the efficacy of conventional quality metrics in spotting a\nmistranslation of sentiment, especially when it is the sole error in the MT\noutput. We propose a numerical `sentiment-closeness' measure appropriate for\nassessing the accuracy of a translated affect message in UGC text by an MT\nsystem. We will show that incorporating this sentiment-aware measure can\nsignificantly enhance the correlation of some available quality metrics with\nthe human judgement of an accurate translation of sentiment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.08925,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000027153,
      "text":"Benchmarking the Combinatorial Generalizability of Complex Query\n  Answering on Knowledge Graphs\n\n  Complex Query Answering (CQA) is an important reasoning task on knowledge\ngraphs. Current CQA learning models have been shown to be able to generalize\nfrom atomic operators to more complex formulas, which can be regarded as the\ncombinatorial generalizability. In this paper, we present EFO-1-QA, a new\ndataset to benchmark the combinatorial generalizability of CQA models by\nincluding 301 different queries types, which is 20 times larger than existing\ndatasets. Besides, our work, for the first time, provides a benchmark to\nevaluate and analyze the impact of different operators and normal forms by\nusing (a) 7 choices of the operator systems and (b) 9 forms of complex queries.\nSpecifically, we provide the detailed study of the combinatorial\ngeneralizability of two commonly used operators, i.e., projection and\nintersection, and justify the impact of the forms of queries given the\ncanonical choice of operators. Our code and data can provide an effective\npipeline to benchmark CQA models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.06327,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Evaluating Transferability of BERT Models on Uralic Languages\n\n  Transformer-based language models such as BERT have outperformed previous\nmodels on a large number of English benchmarks, but their evaluation is often\nlimited to English or a small number of well-resourced languages. In this work,\nwe evaluate monolingual, multilingual, and randomly initialized language models\nfrom the BERT family on a variety of Uralic languages including Estonian,\nFinnish, Hungarian, Erzya, Moksha, Karelian, Livvi, Komi Permyak, Komi Zyrian,\nNorthern S\\'ami, and Skolt S\\'ami. When monolingual models are available\n(currently only et, fi, hu), these perform better on their native language, but\nin general they transfer worse than multilingual models or models of\ngenetically unrelated languages that share the same character set. Remarkably,\nstraightforward transfer of high-resource models, even without special efforts\ntoward hyperparameter optimization, yields what appear to be state of the art\nPOS and NER tools for the minority Uralic languages where there is sufficient\ndata for finetuning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.00621,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000046028,
      "text":"Self-Attentive Constituency Parsing for UCCA-based Semantic Parsing\n\n  Semantic parsing provides a way to extract the semantic structure of a text\nthat could be understood by machines. It is utilized in various NLP\napplications that require text comprehension such as summarization and question\nanswering. Graph-based representation is one of the semantic representation\napproaches to express the semantic structure of a text. Such representations\ngenerate expressive and adequate graph-based target structures. In this paper,\nwe focus primarily on UCCA graph-based semantic representation. The paper not\nonly presents the existing approaches proposed for UCCA representation, but\nalso proposes a novel self-attentive neural parsing model for the UCCA\nrepresentation. We present the results for both single-lingual and\ncross-lingual tasks using zero-shot and few-shot learning for low-resource\nlanguages.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.08246,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000054638,
      "text":"Tricks for Training Sparse Translation Models\n\n  Multi-task learning with an unbalanced data distribution skews model learning\ntowards high resource tasks, especially when model capacity is fixed and fully\nshared across all tasks. Sparse scaling architectures, such as BASELayers,\nprovide flexible mechanisms for different tasks to have a variable number of\nparameters, which can be useful to counterbalance skewed data distributions. We\nfind that that sparse architectures for multilingual machine translation can\nperform poorly out of the box, and propose two straightforward techniques to\nmitigate this - a temperature heating mechanism and dense pre-training.\nOverall, these methods improve performance on two multilingual translation\nbenchmarks compared to standard BASELayers and Dense scaling baselines, and in\ncombination, more than 2x model convergence speed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.09179,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Analysis of French Phonetic Idiosyncrasies for Accent Recognition\n\n  Speech recognition systems have made tremendous progress since the last few\ndecades. They have developed significantly in identifying the speech of the\nspeaker. However, there is a scope of improvement in speech recognition systems\nin identifying the nuances and accents of a speaker. It is known that any\nspecific natural language may possess at least one accent. Despite the\nidentical word phonemic composition, if it is pronounced in different accents,\nwe will have sound waves, which are different from each other. Differences in\npronunciation, in accent and intonation of speech in general, create one of the\nmost common problems of speech recognition. If there are a lot of accents in\nlanguage we should create the acoustic model for each separately. We carry out\na systematic analysis of the problem in the accurate classification of accents.\nWe use traditional machine learning techniques and convolutional neural\nnetworks, and show that the classical techniques are not sufficiently efficient\nto solve this problem. Using spectrograms of speech signals, we propose a\nmulti-class classification framework for accent recognition. In this paper, we\nfocus our attention on the French accent. We also identify its limitation by\nunderstanding the impact of French idiosyncrasies on its spectrograms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.07586,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"Can Explanations Be Useful for Calibrating Black Box Models?\n\n  NLP practitioners often want to take existing trained models and apply them\nto data from new domains. While fine-tuning or few-shot learning can be used to\nadapt a base model, there is no single recipe for making these techniques work;\nmoreover, one may not have access to the original model weights if it is\ndeployed as a black box. We study how to improve a black box model's\nperformance on a new domain by leveraging explanations of the model's behavior.\nOur approach first extracts a set of features combining human intuition about\nthe task with model attributions generated by black box interpretation\ntechniques, then uses a simple calibrator, in the form of a classifier, to\npredict whether the base model was correct or not. We experiment with our\nmethod on two tasks, extractive question answering and natural language\ninference, covering adaptation from several pairs of domains with limited\ntarget-domain data. The experimental results across all the domain pairs show\nthat explanations are useful for calibrating these models, boosting accuracy\nwhen predictions do not have to be returned on every example. We further show\nthat the calibration model transfers to some extent between tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.10704,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000175834,
      "text":"A Self-Explainable Stylish Image Captioning Framework via\n  Multi-References\n\n  In this paper, we propose to build a stylish image captioning model through a\nMulti-style Multi modality mechanism (2M). We demonstrate that with 2M, we can\nbuild an effective stylish captioner and that multi-references produced by the\nmodel can also support explaining the model through identifying erroneous input\nfeatures on faulty examples. We show how this 2M mechanism can be used to build\nstylish captioning models and show how these models can be utilized to provide\nexplanations of likely errors in the models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.01349,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000100666,
      "text":"Protagonists' Tagger in Literary Domain -- New Datasets and a Method for\n  Person Entity Linkage\n\n  Semantic annotation of long texts, such as novels, remains an open challenge\nin Natural Language Processing (NLP). This research investigates the problem of\ndetecting person entities and assigning them unique identities, i.e.,\nrecognizing people (especially main characters) in novels. We prepared a method\nfor person entity linkage (named entity recognition and disambiguation) and new\ntesting datasets. The datasets comprise 1,300 sentences from 13 classic novels\nof different genres that a novel reader had manually annotated. Our process of\nidentifying literary characters in a text, implemented in protagonistTagger,\ncomprises two stages: (1) named entity recognition (NER) of persons, (2) named\nentity disambiguation (NED) - matching each recognized person with the literary\ncharacter's full name, based on approximate text matching. The\nprotagonistTagger achieves both precision and recall of above 83% on the\nprepared testing sets. Finally, we gathered a corpus of 13 full-text novels\ntagged with protagonistTagger that comprises more than 35,000 mentions of\nliterary characters.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.08458,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Controllable Semantic Parsing via Retrieval Augmentation\n\n  In practical applications of semantic parsing, we often want to rapidly\nchange the behavior of the parser, such as enabling it to handle queries in a\nnew domain, or changing its predictions on certain targeted queries. While we\ncan introduce new training examples exhibiting the target behavior, a mechanism\nfor enacting such behavior changes without expensive model re-training would be\npreferable. To this end, we propose ControllAble Semantic Parser via Exemplar\nRetrieval (CASPER). Given an input query, the parser retrieves related\nexemplars from a retrieval index, augments them to the query, and then applies\na generative seq2seq model to produce an output parse. The exemplars act as a\ncontrol mechanism over the generic generative model: by manipulating the\nretrieval index or how the augmented query is constructed, we can manipulate\nthe behavior of the parser. On the MTOP dataset, in addition to achieving\nstate-of-the-art on the standard setup, we show that CASPER can parse queries\nin a new domain, adapt the prediction toward the specified patterns, or adapt\nto new semantic schemas without having to further re-train the model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.12374,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"Transliterating Kurdish texts in Latin into Persian-Arabic script\n\n  Kurdish is written in different scripts. The two most popular scripts are\nLatin and Persian-Arabic. However, not all Kurdish readers are familiar with\nboth mentioned scripts that could be resolved by automatic transliterators. So\nfar, the developed tools mostly transliterate Persian-Arabic scripts into\nLatin. We present a transliterator to transliterate Kurdish texts in Latin into\nPersian-Arabic script. We also discuss the issues that should be considered in\nthe transliteration process. The tool is a part of Kurdish BLARK, and it is\npublicly available for non-commercial use\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.08454,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"Good Examples Make A Faster Learner: Simple Demonstration-based Learning\n  for Low-resource NER\n\n  Recent advances in prompt-based learning have shown strong results on\nfew-shot text classification by using cloze-style templates. Similar attempts\nhave been made on named entity recognition (NER) which manually design\ntemplates to predict entity types for every text span in a sentence. However,\nsuch methods may suffer from error propagation induced by entity span\ndetection, high cost due to enumeration of all possible text spans, and\nomission of inter-dependencies among token labels in a sentence. Here we\npresent a simple demonstration-based learning method for NER, which lets the\ninput be prefaced by task demonstrations for in-context learning. We perform a\nsystematic study on demonstration strategy regarding what to include (entity\nexamples, with or without surrounding context), how to select the examples, and\nwhat templates to use. Results on in-domain learning and domain adaptation show\nthat the model's performance in low-resource settings can be largely improved\nwith a suitable demonstration strategy (e.g., a 4-17% improvement on 25 train\ninstances). We also find that good demonstration can save many labeled examples\nand consistency in demonstration contributes to better performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.05172,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000011358,
      "text":"K-Wav2vec 2.0: Automatic Speech Recognition based on Joint Decoding of\n  Graphemes and Syllables\n\n  Wav2vec 2.0 is an end-to-end framework of self-supervised learning for speech\nrepresentation that is successful in automatic speech recognition (ASR), but\nmost of the work on the topic has been developed with a single language:\nEnglish. Therefore, it is unclear whether the self-supervised framework is\neffective in recognizing other languages with different writing systems, such\nas Korean which uses the Hangul having a unique writing system. In this paper,\nwe present K-Wav2Vec 2.0, which is a modified version of Wav2vec 2.0 designed\nfor Korean automatic speech recognition by exploring and optimizing various\nfactors of the original Wav2vec 2.0. In fine-tuning, we propose a multi-task\nhierarchical architecture to reflect the Korean writing structure. Moreover, a\njoint decoder is applied to alleviate the problem of words existing outside of\nthe vocabulary. In pre-training, we attempted the cross-lingual transfer of the\npre-trained model by further pre-training the English Wav2vec 2.0 on a Korean\ndataset, considering limited resources. Our experimental results demonstrate\nthat the proposed method yields the best performance on both Korean ASR\ndatasets: Ksponspeech (a large-scale Korean speech corpus) and Clovacall (a\ncall-based dialog corpus). Further pre-training is also effective in language\nadaptation, leading to large improvements without additional data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.15235,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000021855,
      "text":"Multi-stage Clarification in Conversational AI: The case of\n  Question-Answering Dialogue Systems\n\n  Clarification resolution plays an important role in various information\nretrieval tasks such as interactive question answering and conversational\nsearch. In such context, the user often formulates their information needs as\nshort and ambiguous queries, some popular search interfaces then prompt the\nuser to confirm her intent (e.g. \"Did you mean ... ?\") or to rephrase if\nneeded. When it comes to dialogue systems, having fluid user-bot exchanges is\nkey to good user experience. In the absence of such clarification mechanism,\none of the following responses is given to the user: 1) A direct answer, which\ncan potentially be non-relevant if the intent was not clear, 2) a generic\nfallback message informing the user that the retrieval tool is incapable of\nhandling the query. Both scenarios might raise frustration and degrade the user\nexperience. To this end, we propose a multi-stage clarification mechanism for\nprompting clarification and query selection in the context of a question\nanswering dialogue system. We show that our proposed mechanism improves the\noverall user experience and outperforms competitive baselines with two\ndatasets, namely the public in-scope out-of-scope dataset and a commercial\ndataset based on real user logs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.11938,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"A Framework for Learning Assessment through Multimodal Analysis of\n  Reading Behaviour and Language Comprehension\n\n  Reading comprehension, which has been defined as gaining an understanding of\nwritten text through a process of translating grapheme into meaning, is an\nimportant academic skill. Other language learning skills - writing, speaking\nand listening, all are connected to reading comprehension. There have been\nseveral measures proposed by researchers to automate the assessment of\ncomprehension skills for second language (L2) learners, especially English as\nSecond Language (ESL) and English as Foreign Language (EFL) learners. However,\ncurrent methods measure particular skills without analysing the impact of\nreading frequency on comprehension skills. In this dissertation, we show how\ndifferent skills could be measured and scored automatically. We also\ndemonstrate, using example experiments on multiple forms of learners'\nresponses, how frequent reading practices could impact on the variables of\nmultimodal skills (reading pattern, writing, and oral fluency).\n  This thesis comprises of five studies. The first and second studies are based\non eye-tracking data collected from EFL readers in repeated reading (RR)\nsessions. The third and fourth studies are to evaluate free-text summary\nwritten by EFL readers in repeated reading sessions. The fifth and last study,\ndescribed in the sixth chapter of the thesis, is to evaluate recorded oral\nsummaries recited by EFL readers in repeated reading sessions.\n  In a nutshell, through this dissertation, we show that multimodal skills of\nlearners could be assessed to measure their comprehension skills as well as to\nmeasure the effect of repeated readings on these skills in the course of time,\nby finding significant features and by applying machine learning techniques\nwith a combination of statistical models such as LMER.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.13505,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000049339,
      "text":"Part & Whole Extraction: Towards A Deep Understanding of Quantitative\n  Facts for Percentages in Text\n\n  We study the problem of quantitative facts extraction for text with\npercentages. For example, given the sentence \"30 percent of Americans like\nwatching football, while 20% prefer to watch NBA.\", our goal is to obtain a\ndeep understanding of the percentage numbers (\"30 percent\" and \"20%\") by\nextracting their quantitative facts: part (\"like watching football\" and \"prefer\nto watch NBA\") and whole (\"Americans). These quantitative facts can empower new\napplications like automated infographic generation. We formulate part and whole\nextraction as a sequence tagging problem. Due to the large gap between\npart\/whole and its corresponding percentage, we introduce skip mechanism in\nsequence modeling, and achieved improved performance on both our task and the\nCoNLL-2003 named entity recognition task. Experimental results demonstrate that\nlearning to skip in sequence tagging is promising.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.15149,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000234114,
      "text":"Diversity-Driven Combination for Grammatical Error Correction\n\n  Grammatical error correction (GEC) is the task of detecting and correcting\nerrors in a written text. The idea of combining multiple system outputs has\nbeen successfully used in GEC. To achieve successful system combination,\nmultiple component systems need to produce corrected sentences that are both\ndiverse and of comparable quality. However, most existing state-of-the-art GEC\napproaches are based on similar sequence-to-sequence neural networks, so the\ngains are limited from combining the outputs of component systems similar to\none another. In this paper, we present Diversity-Driven Combination (DDC) for\nGEC, a system combination strategy that encourages diversity among component\nsystems. We evaluate our system combination strategy on the CoNLL-2014 shared\ntask and the BEA-2019 shared task. On both benchmarks, DDC achieves significant\nperformance gain with a small number of training examples and outperforms the\ncomponent systems by a large margin. Our source code is available at\nhttps:\/\/github.com\/nusnlp\/gec-ddc.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.06733,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000034107,
      "text":"Systematic Inequalities in Language Technology Performance across the\n  World's Languages\n\n  Natural language processing (NLP) systems have become a central technology in\ncommunication, education, medicine, artificial intelligence, and many other\ndomains of research and development. While the performance of NLP methods has\ngrown enormously over the last decade, this progress has been restricted to a\nminuscule subset of the world's 6,500 languages. We introduce a framework for\nestimating the global utility of language technologies as revealed in a\ncomprehensive snapshot of recent publications in NLP. Our analyses involve the\nfield at large, but also more in-depth studies on both user-facing technologies\n(machine translation, language understanding, question answering,\ntext-to-speech synthesis) as well as more linguistic NLP tasks (dependency\nparsing, morphological inflection). In the process, we (1) quantify disparities\nin the current state of NLP research, (2) explore some of its associated\nsocietal and academic factors, and (3) produce tailored recommendations for\nevidence-based policy making aimed at promoting more global and equitable\nlanguage technologies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.00613,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"Expected Validation Performance and Estimation of a Random Variable's\n  Maximum\n\n  Research in NLP is often supported by experimental results, and improved\nreporting of such results can lead to better understanding and more\nreproducible science. In this paper we analyze three statistical estimators for\nexpected validation performance, a tool used for reporting performance (e.g.,\naccuracy) as a function of computational budget (e.g., number of hyperparameter\ntuning experiments). Where previous work analyzing such estimators focused on\nthe bias, we also examine the variance and mean squared error (MSE). In both\nsynthetic and realistic scenarios, we evaluate three estimators and find the\nunbiased estimator has the highest variance, and the estimator with the\nsmallest variance has the largest bias; the estimator with the smallest MSE\nstrikes a balance between bias and variance, displaying a classic bias-variance\ntradeoff. We use expected validation performance to compare between different\nmodels, and analyze how frequently each estimator leads to drawing incorrect\nconclusions about which of two models performs best. We find that the two\nbiased estimators lead to the fewest incorrect conclusions, which hints at the\nimportance of minimizing variance and MSE.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.0724,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Causal Transformers Perform Below Chance on Recursive Nested\n  Constructions, Unlike Humans\n\n  Recursive processing is considered a hallmark of human linguistic abilities.\nA recent study evaluated recursive processing in recurrent neural language\nmodels (RNN-LMs) and showed that such models perform below chance level on\nembedded dependencies within nested constructions -- a prototypical example of\nrecursion in natural language. Here, we study if state-of-the-art Transformer\nLMs do any better. We test four different Transformer LMs on two different\ntypes of nested constructions, which differ in whether the embedded (inner)\ndependency is short or long range. We find that Transformers achieve\nnear-perfect performance on short-range embedded dependencies, significantly\nbetter than previous results reported for RNN-LMs and humans. However, on\nlong-range embedded dependencies, Transformers' performance sharply drops below\nchance level. Remarkably, the addition of only three words to the embedded\ndependency caused Transformers to fall from near-perfect to below-chance\nperformance. Taken together, our results reveal Transformers' shortcoming when\nit comes to recursive, structure-based, processing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.08193,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"BBQ: A Hand-Built Bias Benchmark for Question Answering\n\n  It is well documented that NLP models learn social biases, but little work\nhas been done on how these biases manifest in model outputs for applied tasks\nlike question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a\ndataset of question sets constructed by the authors that highlight attested\nsocial biases against people belonging to protected classes along nine social\ndimensions relevant for U.S. English-speaking contexts. Our task evaluates\nmodel responses at two levels: (i) given an under-informative context, we test\nhow strongly responses reflect social biases, and (ii) given an adequately\ninformative context, we test whether the model's biases override a correct\nanswer choice. We find that models often rely on stereotypes when the context\nis under-informative, meaning the model's outputs consistently reproduce\nharmful biases in this setting. Though models are more accurate when the\ncontext provides an informative answer, they still rely on stereotypes and\naverage up to 3.4 percentage points higher accuracy when the correct answer\naligns with a social bias than when it conflicts, with this difference widening\nto over 5 points on examples targeting gender for most models tested.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.00276,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"EventNarrative: A large-scale Event-centric Dataset for Knowledge\n  Graph-to-Text Generation\n\n  We introduce EventNarrative, a knowledge graph-to-text dataset from publicly\navailable open-world knowledge graphs. Given the recent advances in\nevent-driven Information Extraction (IE), and that prior research on\ngraph-to-text only focused on entity-driven KGs, this paper focuses on\nevent-centric data. However, our data generation system can still be adapted to\nother other types of KG data. Existing large-scale datasets in the\ngraph-to-text area are non-parallel, meaning there is a large disconnect\nbetween the KGs and text. The datasets that have a paired KG and text, are\nsmall scale and manually generated or generated without a rich ontology, making\nthe corresponding graphs sparse. Furthermore, these datasets contain many\nunlinked entities between their KG and text pairs. EventNarrative consists of\napproximately 230,000 graphs and their corresponding natural language text, 6\ntimes larger than the current largest parallel dataset. It makes use of a rich\nontology, all of the KGs entities are linked to the text, and our manual\nannotations confirm a high data quality. Our aim is two-fold: help break new\nground in event-centric research where data is lacking, and to give researchers\na well-defined, large-scale dataset in order to better evaluate existing and\nfuture knowledge graph-to-text models. We also evaluate two types of baseline\non EventNarrative: a graph-to-text specific model and two state-of-the-art\nlanguage models, which previous work has shown to be adaptable to the knowledge\ngraph-to-text domain.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.08415,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Multilingual unsupervised sequence segmentation transfers to extremely\n  low-resource languages\n\n  We show that unsupervised sequence-segmentation performance can be\ntransferred to extremely low-resource languages by pre-training a Masked\nSegmental Language Model (Downey et al., 2021) multilingually. Further, we show\nthat this transfer can be achieved by training over a collection of\nlow-resource languages that are typologically similar (but phylogenetically\nunrelated) to the target language. In our experiments, we transfer from a\ncollection of 10 Indigenous American languages (AmericasNLP, Mager et al.,\n2021) to K'iche', a Mayan language. We compare our multilingual model to a\nmonolingual (from-scratch) baseline, as well as a model pre-trained on Quechua\nonly. We show that the multilingual pre-trained approach yields consistent\nsegmentation quality across target dataset sizes, exceeding the monolingual\nbaseline in 6\/10 experimental settings. Our model yields especially strong\nresults at small target sizes, including a zero-shot performance of 20.6 F1.\nThese results have promising implications for low-resource NLP pipelines\ninvolving human-like linguistic units, such as the sparse transcription\nframework proposed by Bird (2020).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.03375,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000555317,
      "text":"Developing Successful Shared Tasks on Offensive Language Identification\n  for Dravidian Languages\n\n  With the fast growth of mobile computing and Web technologies, offensive\nlanguage has become more prevalent on social networking platforms. Since\noffensive language identification in local languages is essential to moderate\nthe social media content, in this paper we work with three Dravidian languages,\nnamely Malayalam, Tamil, and Kannada, that are under-resourced. We present an\nevaluation task at FIRE 2020- HASOC-DravidianCodeMix and DravidianLangTech at\nEACL 2021, designed to provide a framework for comparing different approaches\nto this problem. This paper describes the data creation, defines the task,\nlists the participating systems, and discusses various methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.14066,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000076161,
      "text":"Natural Language and Spatial Rules\n\n  We develop a system that formally represents spatial semantics concepts\nwithin natural language descriptions of spatial arrangements. The system builds\non a model of spatial semantics representation according to which words in a\nsentence are assigned spatial roles and the relations among these roles are\nrepresented with spatial relations. We combine our system with the shape\ngrammar formalism that uses shape rules to generate languages (sets) of\ntwo-dimensional shapes. Our proposed system consists of pairs of shape rules\nand verbal rules where the verbal rules describe in English the action of the\nassociated shape rule. We present various types of natural language\ndescriptions of shapes that are successfully parsed by our system and we\ndiscuss open questions and challenges we see at the interface of language and\nperception.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.0594,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000078148,
      "text":"A Novel Corpus of Discourse Structure in Humans and Computers\n\n  We present a novel corpus of 445 human- and computer-generated documents,\ncomprising about 27,000 clauses, annotated for semantic clause types and\ncoherence relations that allow for nuanced comparison of artificial and natural\ndiscourse modes. The corpus covers both formal and informal discourse, and\ncontains documents generated using fine-tuned GPT-2 (Zellers et al., 2019) and\nGPT-3(Brown et al., 2020). We showcase the usefulness of this corpus for\ndetailed discourse analysis of text generation by providing preliminary\nevidence that less numerous, shorter and more often incoherent clause relations\nare associated with lower perceived quality of computer-generated narratives\nand arguments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.09486,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000035763,
      "text":"Linking-Enhanced Pre-Training for Table Semantic Parsing\n\n  Recently pre-training models have significantly improved the performance of\nvarious NLP tasks by leveraging large-scale text corpora to improve the\ncontextual representation ability of the neural network. The large pre-training\nlanguage model has also been applied in the area of table semantic parsing.\nHowever, existing pre-training approaches have not carefully explored explicit\ninteraction relationships between a question and the corresponding database\nschema, which is a key ingredient for uncovering their semantic and structural\ncorrespondence. Furthermore, the question-aware representation learning in the\nschema grounding context has received less attention in pre-training\nobjective.To alleviate these issues, this paper designs two novel pre-training\nobjectives to impose the desired inductive bias into the learned\nrepresentations for table pre-training. We further propose a schema-aware\ncurriculum learning approach to mitigate the impact of noise and learn\neffectively from the pre-training data in an easy-to-hard manner. We evaluate\nour pre-trained framework by fine-tuning it on two benchmarks, Spider and\nSQUALL. The results demonstrate the effectiveness of our pre-training objective\nand curriculum compared to a variety of baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.07993,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000032451,
      "text":"CoLLIE: Continual Learning of Language Grounding from Language-Image\n  Embeddings\n\n  This paper presents CoLLIE: a simple, yet effective model for continual\nlearning of how language is grounded in vision. Given a pre-trained multimodal\nembedding model, where language and images are projected in the same semantic\nspace (in this case CLIP by OpenAI), CoLLIE learns a transformation function\nthat adjusts the language embeddings when needed to accommodate new language\nuse. This is done by predicting the difference vector that needs to be applied,\nas well as a scaling factor for this vector, so that the adjustment is only\napplied when needed. Unlike traditional few-shot learning, the model does not\njust learn new classes and labels, but can also generalize to similar language\nuse and leverage semantic compositionality. We verify the model's performance\non two different tasks of identifying the targets of referring expressions,\nwhere it has to learn new language use. The results show that the model can\nefficiently learn and generalize from only a few examples, with little\ninterference with the model's original zero-shot performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.0821,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Meeting Summarization with Pre-training and Clustering Methods\n\n  Automatic meeting summarization is becoming increasingly popular these days.\nThe ability to automatically summarize meetings and to extract key information\ncould greatly increase the efficiency of our work and life. In this paper, we\nexperiment with different approaches to improve the performance of query-based\nmeeting summarization. We started with HMNet\\cite{hmnet}, a hierarchical\nnetwork that employs both a word-level transformer and a turn-level\ntransformer, as the baseline. We explore the effectiveness of pre-training the\nmodel with a large news-summarization dataset. We investigate adding the\nembeddings of queries as a part of the input vectors for query-based\nsummarization. Furthermore, we experiment with extending the\nlocate-then-summarize approach of QMSum\\cite{qmsum} with an intermediate\nclustering step. Lastly, we compare the performance of our baseline models with\nBART, a state-of-the-art language model that is effective for summarization. We\nachieved improved performance by adding query embeddings to the input of the\nmodel, by using BART as an alternative language model, and by using clustering\nmethods to extract key information at utterance level before feeding the text\ninto summarization models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.04198,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000200669,
      "text":"TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning\n\n  Masked language models (MLMs) such as BERT and RoBERTa have revolutionized\nthe field of Natural Language Understanding in the past few years. However,\nexisting pre-trained MLMs often output an anisotropic distribution of token\nrepresentations that occupies a narrow subset of the entire representation\nspace. Such token representations are not ideal, especially for tasks that\ndemand discriminative semantic meanings of distinct tokens. In this work, we\npropose TaCL (Token-aware Contrastive Learning), a novel continual pre-training\napproach that encourages BERT to learn an isotropic and discriminative\ndistribution of token representations. TaCL is fully unsupervised and requires\nno additional data. We extensively test our approach on a wide range of English\nand Chinese benchmarks. The results show that TaCL brings consistent and\nnotable improvements over the original BERT model. Furthermore, we conduct\ndetailed analysis to reveal the merits and inner-workings of our approach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.14445,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000151992,
      "text":"Action based Network for Conversation Question Reformulation\n\n  Conversation question answering requires the ability to interpret a question\ncorrectly. Current models, however, are still unsatisfactory due to the\ndifficulty of understanding the co-references and ellipsis in daily\nconversation. Even though generative approaches achieved remarkable progress,\nthey are still trapped by semantic incompleteness. This paper presents an\naction-based approach to recover the complete expression of the question.\nSpecifically, we first locate the positions of co-reference or ellipsis in the\nquestion while assigning the corresponding action to each candidate span. We\nthen look for matching phrases related to the candidate clues in the\nconversation context. Finally, according to the predicted action, we decide\nwhether to replace the co-reference or supplement the ellipsis with the matched\ninformation. We demonstrate the effectiveness of our method on both English and\nChinese utterance rewrite tasks, improving the state-of-the-art EM (exact\nmatch) by 3.9\\% and ROUGE-L by 1.0\\% respectively on the Restoration-200K\ndataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.03916,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Distinguishing Commercial from Editorial Content in News\n\n  How can we distinguish commercial from editorial content in news, or more\nspecifically, differentiate between advertorials and regular news articles? An\nadvertorial is a commercial message written and formatted as an article, making\nit harder for readers to recognize these as advertising, despite the use of\ndisclaimers. In our research we aim to differentiate the two using a machine\nlearning model, and a lexicon derived from it. This was accomplished by\nscraping 1.000 articles and 1.000 advertorials from four different Dutch news\nsources and classifying these based on textual features. With this setup our\nmost successful machine learning model had an accuracy of just over $90\\%$. To\ngenerate additional insights into differences between news and advertorial\nlanguage, we also analyzed model coefficients and explored the corpus through\nco-occurrence networks and t-SNE graphs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.13972,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000289745,
      "text":"Tapping BERT for Preposition Sense Disambiguation\n\n  Prepositions are frequently occurring polysemous words. Disambiguation of\nprepositions is crucial in tasks like semantic role labelling, question\nanswering, text entailment, and noun compound paraphrasing. In this paper, we\npropose a novel methodology for preposition sense disambiguation (PSD), which\ndoes not use any linguistic tools. In a supervised setting, the machine\nlearning model is presented with sentences wherein prepositions have been\nannotated with senses. These senses are IDs in what is called The Preposition\nProject (TPP). We use the hidden layer representations from pre-trained BERT\nand BERT variants. The latent representations are then classified into the\ncorrect sense ID using a Multi Layer Perceptron. The dataset used for this task\nis from SemEval-2007 Task-6. Our methodology gives an accuracy of 86.85% which\nis better than the state-of-the-art.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.13301,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"Simple Contrastive Representation Adversarial Learning for NLP Tasks\n\n  Self-supervised learning approach like contrastive learning is attached great\nattention in natural language processing. It uses pairs of training data\naugmentations to build a classification task for an encoder with well\nrepresentation ability. However, the construction of learning pairs over\ncontrastive learning is much harder in NLP tasks. Previous works generate\nword-level changes to form pairs, but small transforms may cause notable\nchanges on the meaning of sentences as the discrete and sparse nature of\nnatural language. In this paper, adversarial training is performed to generate\nchallenging and harder learning adversarial examples over the embedding space\nof NLP as learning pairs. Using contrastive learning improves the\ngeneralization ability of adversarial training because contrastive loss can\nuniform the sample distribution. And at the same time, adversarial training\nalso enhances the robustness of contrastive learning. Two novel frameworks,\nsupervised contrastive adversarial learning (SCAL) and unsupervised SCAL\n(USCAL), are proposed, which yields learning pairs by utilizing the adversarial\ntraining for contrastive learning. The label-based loss of supervised tasks is\nexploited to generate adversarial examples while unsupervised tasks bring\ncontrastive loss. To validate the effectiveness of the proposed framework, we\nemploy it to Transformer-based models for natural language understanding,\nsentence semantic textual similarity and adversarial learning tasks.\nExperimental results on GLUE benchmark tasks show that our fine-tuned\nsupervised method outperforms BERT$_{base}$ over 1.75\\%. We also evaluate our\nunsupervised method on semantic textual similarity (STS) tasks, and our method\ngets 77.29\\% with BERT$_{base}$. The robustness of our approach conducts\nstate-of-the-art results under multiple adversarial datasets on NLI tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.00767,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"A New Tool for Efficiently Generating Quality Estimation Datasets\n\n  Building of data for quality estimation (QE) training is expensive and\nrequires significant human labor. In this study, we focus on a data-centric\napproach while performing QE, and subsequently propose a fully automatic\npseudo-QE dataset generation tool that generates QE datasets by receiving only\nmonolingual or parallel corpus as the input. Consequently, the QE performance\nis enhanced either by data augmentation or by encouraging multiple language\npairs to exploit the applicability of QE. Further, we intend to publicly\nrelease this user friendly QE dataset generation tool as we believe this tool\nprovides a new, inexpensive method to the community for developing QE datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.10951,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000030465,
      "text":"Can depth-adaptive BERT perform better on binary classification tasks\n\n  In light of the success of transferring language models into NLP tasks, we\nask whether the full BERT model is always the best and does it exist a simple\nbut effective method to find the winning ticket in state-of-the-art deep neural\nnetworks without complex calculations. We construct a series of BERT-based\nmodels with different size and compare their predictions on 8 binary\nclassification tasks. The results show there truly exist smaller sub-networks\nperforming better than the full model. Then we present a further study and\npropose a simple method to shrink BERT appropriately before fine-tuning. Some\nextended experiments indicate that our method could save time and storage\noverhead extraordinarily with little even no accuracy loss.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.06971,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000318885,
      "text":"Exploiting All Samples in Low-Resource Sentence Classification: Early\n  Stopping and Initialization Parameters\n\n  To improve deep-learning performance in low-resource settings, many\nresearchers have redesigned model architectures or applied additional data\n(e.g., external resources, unlabeled samples). However, there have been\nrelatively few discussions on how to make good use of small amounts of labeled\nsamples, although it is potentially beneficial and should be done before\napplying additional data or redesigning models. In this study, we assume a\nlow-resource setting in which only a few labeled samples (i.e., 30-100 per\nclass) are available, and we discuss how to exploit them without additional\ndata or model redesigns. We explore possible approaches in the following three\naspects: training-validation splitting, early stopping, and weight\ninitialization. Extensive experiments are conducted on six public sentence\nclassification datasets. Performance on various evaluation metrics (e.g.,\naccuracy, loss, and calibration error) significantly varied depending on the\napproaches that were combined in the three aspects. Based on the results, we\npropose an integrated method, which is to initialize the model with a weight\naveraging method and use a non-validation stop method to train all samples.\nThis simple integrated method consistently outperforms the competitive methods;\ne.g., the average accuracy of six datasets of this method was 1.8% higher than\nthose of conventional validation-based methods. In addition, the integrated\nmethod further improves the performance when adapted to several\nstate-of-the-art models that use additional data or redesign the network\narchitecture (e.g., self-training and enhanced structural models). Our results\nhighlight the importance of the training strategy and suggest that the\nintegrated method can be the first step in the low-resource setting. This study\nprovides empirical knowledge that will be helpful when dealing with\nlow-resource data in future efforts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.11159,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000047684,
      "text":"Investigating Cross-Linguistic Gender Bias in Hindi-English Across\n  Domains\n\n  Measuring, evaluating and reducing Gender Bias has come to the forefront with\nnewer and improved language embeddings being released every few months. But\ncould this bias vary from domain to domain? We see a lot of work to study these\nbiases in various embedding models but limited work has been done to debias\nIndic languages. We aim to measure and study this bias in Hindi language, which\nis a higher-order language (gendered) with reference to English, a lower-order\nlanguage. To achieve this, we study the variations across domains to quantify\nif domain embeddings allow us some insight into Gender bias for this pair of\nHindi-English model. We will generate embeddings in four different corpora and\ncompare results by implementing different metrics like with pre-trained State\nof the Art Indic-English translation model, which has performed better at many\nNLP tasks than existing models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.01465,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000094374,
      "text":"System Combination for Grammatical Error Correction Based on Integer\n  Programming\n\n  In this paper, we propose a system combination method for grammatical error\ncorrection (GEC), based on nonlinear integer programming (IP). Our method\noptimizes a novel F score objective based on error types, and combines multiple\nend-to-end GEC systems. The proposed IP approach optimizes the selection of a\nsingle best system for each grammatical error type present in the data.\nExperiments of the IP approach on combining state-of-the-art standalone GEC\nsystems show that the combined system outperforms all standalone systems. It\nimproves F0.5 score by 3.61% when combining the two best participating systems\nin the BEA 2019 shared task, and achieves F0.5 score of 73.08%. We also perform\nexperiments to compare our IP approach with another state-of-the-art system\ncombination method for GEC, demonstrating IP's competitive combination\ncapability.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.14709,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Linguistic Knowledge in Data Augmentation for Natural Language\n  Processing: An Example on Chinese Question Matching\n\n  To investigate the role of linguistic knowledge in data augmentation (DA) for\nNatural Language Processing (NLP), we designed two adapted DA programs and\napplied them to LCQMC (a Large-scale Chinese Question Matching Corpus) for a\nbinary Chinese question matching classification task. The two DA programs\nproduce augmented texts by five simple text editing operations (or DA\ntechniques), largely irrespective of language generation rules, but one is\nenhanced with a pre-trained n-gram language model to fuse it with prior\nlinguistic knowledge. We then trained four neural network models (BOW, CNN,\nLSTM, and GRU) and a pre-trained model (ERNIE-Gram) on the LCQMCs train sets of\nvarying size as well as the related augmented train sets produced by the two DA\nprograms. The results show that there are no significant performance\ndifferences between the models trained on the two types of augmented train\nsets, both when the five DA techniques are applied together or separately.\nMoreover, due to the inability of the five DA techniques to make strictly\nparaphrastic augmented texts, the results indicate the need of sufficient\namounts of training examples for the classification models trained on them to\nmediate the negative impact of false matching augmented text pairs and improve\nperformance, a limitation of random text editing perturbations used as a DA\napproach. Similar results were also obtained for English.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.01231,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Switch Point biased Self-Training: Re-purposing Pretrained Models for\n  Code-Switching\n\n  Code-switching (CS), a ubiquitous phenomenon due to the ease of communication\nit offers in multilingual communities still remains an understudied problem in\nlanguage processing. The primary reasons behind this are: (1) minimal efforts\nin leveraging large pretrained multilingual models, and (2) the lack of\nannotated data. The distinguishing case of low performance of multilingual\nmodels in CS is the intra-sentence mixing of languages leading to switch\npoints. We first benchmark two sequence labeling tasks -- POS and NER on 4\ndifferent language pairs with a suite of pretrained models to identify the\nproblems and select the best performing model, char-BERT, among them\n(addressing (1)). We then propose a self training method to repurpose the\nexisting pretrained models using a switch-point bias by leveraging unannotated\ndata (addressing (2)). We finally demonstrate that our approach performs well\non both tasks by reducing the gap between the switch point performance while\nretaining the overall performance on two distinct language pairs in both the\ntasks. Our code is available here:\nhttps:\/\/github.com\/PC09\/EMNLP2021-Switch-Point-biased-Self-Training.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.08284,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Few-Shot Self-Rationalization with Natural Language Prompts\n\n  Self-rationalization models that predict task labels and generate free-text\nelaborations for their predictions could enable more intuitive interaction with\nNLP systems. These models are, however, currently trained with a large amount\nof human-written free-text explanations for each task which hinders their\nbroader usage. We propose to study a more realistic setting of\nself-rationalization using few training examples. We present FEB -- a\nstandardized collection of four existing English-language datasets and\nassociated metrics. We identify the right prompting approach by extensively\nexploring natural language prompts on FEB. Then, by using this prompt and\nscaling the model size, we demonstrate that making progress on few-shot\nself-rationalization is possible. We show there is still ample room for\nimprovement in this task: the average plausibility of generated explanations\nassessed by human annotators is at most 51% (with GPT-3), while plausibility of\nhuman explanations is 76%. We hope that FEB and our proposed approach will spur\nthe community to take on the few-shot self-rationalization challenge.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.09064,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Guiding Generative Language Models for Data Augmentation in Few-Shot\n  Text Classification\n\n  Data augmentation techniques are widely used for enhancing the performance of\nmachine learning models by tackling class imbalance issues and data sparsity.\nState-of-the-art generative language models have been shown to provide\nsignificant gains across different NLP tasks. However, their applicability to\ndata augmentation for text classification tasks in few-shot settings have not\nbeen fully explored, especially for specialised domains. In this paper, we\nleverage GPT-2 (Radford A et al, 2019) for generating artificial training\ninstances in order to improve classification performance. Our aim is to analyse\nthe impact the selection process of seed training examples have over the\nquality of GPT-generated samples and consequently the classifier performance.\nWe perform experiments with several seed selection strategies that, among\nothers, exploit class hierarchical structures and domain expert selection. Our\nresults show that fine-tuning GPT-2 in a handful of label instances leads to\nconsistent classification improvements and outperform competitive baselines.\nFinally, we show that guiding this process through domain expert selection can\nlead to further improvements, which opens up interesting research avenues for\ncombining generative models and active learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.08513,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000346038,
      "text":"DocAMR: Multi-Sentence AMR Representation and Evaluation\n\n  Despite extensive research on parsing of English sentences into Abstraction\nMeaning Representation (AMR) graphs, which are compared to gold graphs via the\nSmatch metric, full-document parsing into a unified graph representation lacks\nwell-defined representation and evaluation. Taking advantage of a\nsuper-sentential level of coreference annotation from previous work, we\nintroduce a simple algorithm for deriving a unified graph representation,\navoiding the pitfalls of information loss from over-merging and lack of\ncoherence from under-merging. Next, we describe improvements to the Smatch\nmetric to make it tractable for comparing document-level graphs, and use it to\nre-evaluate the best published document-level AMR parser. We also present a\npipeline approach combining the top performing AMR parser and coreference\nresolution systems, providing a strong baseline for future research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.08726,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000388424,
      "text":"NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead\n  Heuristics\n\n  The dominant paradigm for neural text generation is left-to-right decoding\nfrom autoregressive language models. Constrained or controllable generation\nunder complex lexical constraints, however, requires foresight to plan ahead\nfeasible future paths.\n  Drawing inspiration from the A* search algorithm, we propose NeuroLogic\nA*esque, a decoding algorithm that incorporates heuristic estimates of future\ncost. We develop efficient lookahead heuristics that are efficient for\nlarge-scale language models, making our method a drop-in replacement for common\ntechniques such as beam search and top-k sampling. To enable constrained\ngeneration, we build on NeuroLogic decoding (Lu et al., 2021), combining its\nflexibility in incorporating logical constraints with A*esque estimates of\nfuture constraint satisfaction.\n  Our approach outperforms competitive baselines on five generation tasks, and\nachieves new state-of-the-art performance on table-to-text generation,\nconstrained machine translation, and keyword-constrained generation. The\nimprovements are particularly notable on tasks that require complex constraint\nsatisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque\nillustrates the power of decoding for improving and enabling new capabilities\nof large-scale language models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.02325,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000040399,
      "text":"A Russian Jeopardy! Data Set for Question-Answering Systems\n\n  Question answering (QA) is one of the most common NLP tasks that relates to\nnamed entity recognition, fact extraction, semantic search and some other\nfields. In industry, it is much appreciated in chatbots and corporate\ninformation systems. It is also a challenging task that attracted the attention\nof a very general audience at the quiz show Jeopardy! In this article we\ndescribe a Jeopardy!-like Russian QA data set collected from the official\nRussian quiz database Chgk (che ge ka). The data set includes 379,284 quiz-like\nquestions with 29,375 from the Russian analogue of Jeopardy! - \"Own Game\". We\nobserve its linguistic features and the related QA-task. We conclude about\nperspectives of a QA competition based on the data set collected from this\ndatabase.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.08532,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000058611,
      "text":"Penn-Helsinki Parsed Corpus of Early Modern English: First Parsing\n  Results and Analysis\n\n  We present the first parsing results on the Penn-Helsinki Parsed Corpus of\nEarly Modern English (PPCEME), a 1.9 million word treebank that is an important\nresource for research in syntactic change. We describe key features of PPCEME\nthat make it challenging for parsing, including a larger and more varied set of\nfunction tags than in the Penn Treebank. We present results for this corpus\nusing a modified version of the Berkeley Neural Parser and the approach to\nfunction tag recovery of Gabbard et al (2006). Despite its simplicity, this\napproach works surprisingly well, suggesting it is possible to recover the\noriginal structure with sufficient accuracy to support linguistic applications\n(e.g., searching for syntactic structures of interest). However, for a subset\nof function tags (e.g., the tag indicating direct speech), additional work is\nneeded, and we discuss some further limits of this approach. The resulting\nparser will be used to parse Early English Books Online, a 1.1 billion word\ncorpus whose utility for the study of syntactic change will be greatly\nincreased with the addition of accurate parse trees.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.05125,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000154641,
      "text":"Rethinking the Authorship Verification Experimental Setups\n\n  One of the main drivers of the recent advances in authorship verification is\nthe PAN large-scale authorship dataset. Despite generating significant progress\nin the field, inconsistent performance differences between the closed and open\ntest sets have been reported. To this end, we improve the experimental setup by\nproposing five new public splits over the PAN dataset, specifically designed to\nisolate and identify biases related to the text topic and to the author's\nwriting style. We evaluate several BERT-like baselines on these splits, showing\nthat such models are competitive with authorship verification state-of-the-art\nmethods. Furthermore, using explainable AI, we find that these baselines are\nbiased towards named entities. We show that models trained without the named\nentities obtain better results and generalize better when tested on DarkReddit,\nour new dataset for authorship verification.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.09488,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000066227,
      "text":"Joint Chinese Word Segmentation and Part-of-speech Tagging via Two-stage\n  Span Labeling\n\n  Chinese word segmentation and part-of-speech tagging are necessary tasks in\nterms of computational linguistics and application of natural language\nprocessing. Many re-searchers still debate the demand for Chinese word\nsegmentation and part-of-speech tagging in the deep learning era. Nevertheless,\nresolving ambiguities and detecting unknown words are challenging problems in\nthis field. Previous studies on joint Chinese word segmentation and\npart-of-speech tagging mainly follow the character-based tagging model focusing\non modeling n-gram features. Unlike previous works, we propose a neural model\nnamed SpanSegTag for joint Chinese word segmentation and part-of-speech tagging\nfollowing the span labeling in which the probability of each n-gram being the\nword and the part-of-speech tag is the main problem. We use the biaffine\noperation over the left and right boundary representations of consecutive\ncharacters to model the n-grams. Our experiments show that our BERT-based model\nSpanSegTag achieved competitive performances on the CTB5, CTB6, and UD, or\nsignificant improvements on CTB7 and CTB9 benchmark datasets compared with the\ncurrent state-of-the-art method using BERT or ZEN encoders.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.08152,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000182788,
      "text":"Faster Nearest Neighbor Machine Translation\n\n  $k$NN based neural machine translation ($k$NN-MT) has achieved\nstate-of-the-art results in a variety of MT tasks. One significant shortcoming\nof $k$NN-MT lies in its inefficiency in identifying the $k$ nearest neighbors\nof the query representation from the entire datastore, which is prohibitively\ntime-intensive when the datastore size is large. In this work, we propose\n\\textbf{Faster $k$NN-MT} to address this issue. The core idea of Faster\n$k$NN-MT is to use a hierarchical clustering strategy to approximate the\ndistance between the query and a data point in the datastore, which is\ndecomposed into two parts: the distance between the query and the center of the\ncluster that the data point belongs to, and the distance between the data point\nand the cluster center. We propose practical ways to compute these two parts in\na significantly faster manner. Through extensive experiments on different MT\nbenchmarks, we show that \\textbf{Faster $k$NN-MT} is faster than Fast $k$NN-MT\n\\citep{meng2021fast} and only slightly (1.2 times) slower than its vanilla\ncounterpart while preserving model performance as $k$NN-MT. Faster $k$NN-MT\nenables the deployment of $k$NN-MT models on real-world MT services.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.02246,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Controllable Response Generation for Assistive Use-cases\n\n  Conversational agents have become an integral part of the general population\nfor simple task enabling situations. However, these systems are yet to have any\nsocial impact on the diverse and minority population, for example, helping\npeople with neurological disorders, for example ALS, and people with speech,\nlanguage and social communication disorders. Language model technology can play\na huge role to help these users carry out daily communication and social\ninteractions. To enable this population, we build a dialog system that can be\ncontrolled by users using cues or keywords. We build models that can suggest\nrelevant cues in the dialog response context which is used to control response\ngeneration and can speed up communication. We also introduce a keyword loss to\nlexically constrain the model output. We show both qualitatively and\nquantitatively that our models can effectively induce the keyword into the\nmodel response without degrading the quality of response. In the context of\nusage of such systems for people with degenerative disorders, we present human\nevaluation of our cue or keyword predictor and the controllable dialog system\nand show that our models perform significantly better than models without\ncontrol. Our study shows that keyword control on end to end response generation\nmodels is powerful and can enable and empower users with degenerative disorders\nto carry out their day to day communication.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.15124,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000065565,
      "text":"Utilizing Wordnets for Cognate Detection among Indian Languages\n\n  Automatic Cognate Detection (ACD) is a challenging task which has been\nutilized to help NLP applications like Machine Translation, Information\nRetrieval and Computational Phylogenetics. Unidentified cognate pairs can pose\na challenge to these applications and result in a degradation of performance.\nIn this paper, we detect cognate word pairs among ten Indian languages with\nHindi and use deep learning methodologies to predict whether a word pair is\ncognate or not. We identify IndoWordnet as a potential resource to detect\ncognate word pairs based on orthographic similarity-based methods and train\nneural network models using the data obtained from it. We identify parallel\ncorpora as another potential resource and perform the same experiments for\nthem. We also validate the contribution of Wordnets through further\nexperimentation and report improved performance of up to 26%. We discuss the\nnuances of cognate detection among closely related Indian languages and release\nthe lists of detected cognates as a dataset. We also observe the behaviour of,\nto an extent, unrelated Indian language pairs and release the lists of detected\ncognates among them as well.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.12489,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000042386,
      "text":"TFW2V: An Enhanced Document Similarity Method for the Morphologically\n  Rich Finnish Language\n\n  Measuring the semantic similarity of different texts has many important\napplications in Digital Humanities research such as information retrieval,\ndocument clustering and text summarization. The performance of different\nmethods depends on the length of the text, the domain and the language. This\nstudy focuses on experimenting with some of the current approaches to Finnish,\nwhich is a morphologically rich language. At the same time, we propose a simple\nmethod, TFW2V, which shows high efficiency in handling both long text documents\nand limited amounts of data. Furthermore, we design an objective evaluation\nmethod which can be used as a framework for benchmarking text similarity\napproaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.13288,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Stance Quantification: Definition of the Problem\n\n  Stance detection is commonly defined as the automatic process of determining\nthe positions of text producers, towards a target. In this paper, we define a\nresearch problem closely related to stance detection, namely, stance\nquantification, for the first time. We define stance quantification on a pair\nincluding (1) a set of natural language text items and (2) a target. At the end\nof the stance quantification process, a triple is obtained which consists of\nthe percentages of the number of text items classified as Favor, Against,\nNeither, respectively, towards the target in the input pair. Also defined in\nthe current paper is a significant subproblem of the stance quantification\nproblem, namely, multi-target stance quantification. We believe that stance\nquantification at the aggregate level can lead to fruitful results in many\napplication settings, and furthermore, stance quantification might be the sole\nstance related analysis alternative in settings where privacy concerns prevent\nresearchers from applying generic stance detection.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.0761,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Improving Compositional Generalization with Latent Structure and Data\n  Augmentation\n\n  Generic unstructured neural networks have been shown to struggle on\nout-of-distribution compositional generalization. Compositional data\naugmentation via example recombination has transferred some prior knowledge\nabout compositionality to such black-box neural models for several semantic\nparsing tasks, but this often required task-specific engineering or provided\nlimited gains.\n  We present a more powerful data recombination method using a model called\nCompositional Structure Learner (CSL). CSL is a generative model with a\nquasi-synchronous context-free grammar backbone, which we induce from the\ntraining data. We sample recombined examples from CSL and add them to the\nfine-tuning data of a pre-trained sequence-to-sequence model (T5). This\nprocedure effectively transfers most of CSL's compositional bias to T5 for\ndiagnostic tasks, and results in a model even stronger than a T5-CSL ensemble\non two real world compositional generalization tasks. This results in new\nstate-of-the-art performance for these challenging semantic parsing tasks\nrequiring generalization to both natural language variation and novel\ncompositions of elements.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.13512,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000050995,
      "text":"Event-based clinical findings extraction from radiology reports with\n  pre-trained language model\n\n  Radiology reports contain a diverse and rich set of clinical abnormalities\ndocumented by radiologists during their interpretation of the images.\nComprehensive semantic representations of radiological findings would enable a\nwide range of secondary use applications to support diagnosis, triage, outcomes\nprediction, and clinical research. In this paper, we present a new corpus of\nradiology reports annotated with clinical findings. Our annotation schema\ncaptures detailed representations of pathologic findings that are observable on\nimaging (\"lesions\") and other types of clinical problems (\"medical problems\").\nThe schema used an event-based representation to capture fine-grained details,\nincluding assertion, anatomy, characteristics, size, count, etc. Our gold\nstandard corpus contained a total of 500 annotated computed tomography (CT)\nreports. We extracted triggers and argument entities using two state-of-the-art\ndeep learning architectures, including BERT. We then predicted the linkages\nbetween trigger and argument entities (referred to as argument roles) using a\nBERT-based relation extraction model. We achieved the best extraction\nperformance using a BERT model pre-trained on 3 million radiology reports from\nour institution: 90.9%-93.4% F1 for finding triggers 72.0%-85.6% F1 for\narguments roles. To assess model generalizability, we used an external\nvalidation set randomly sampled from the MIMIC Chest X-ray (MIMIC-CXR)\ndatabase. The extraction performance on this validation set was 95.6% for\nfinding triggers and 79.1%-89.7% for argument roles, demonstrating that the\nmodel generalized well to the cross-institutional data with a different imaging\nmodality. We extracted the finding events from all the radiology reports in the\nMIMIC-CXR database and provided the extractions to the research community.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.06013,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Efficient Document-level Event Extraction via Pseudo-Trigger-aware\n  Pruned Complete Graph\n\n  Most previous studies of document-level event extraction mainly focus on\nbuilding argument chains in an autoregressive way, which achieves a certain\nsuccess but is inefficient in both training and inference. In contrast to the\nprevious studies, we propose a fast and lightweight model named as PTPCG. In\nour model, we design a novel strategy for event argument combination together\nwith a non-autoregressive decoding algorithm via pruned complete graphs, which\nare constructed under the guidance of the automatically selected pseudo\ntriggers. Compared to the previous systems, our system achieves competitive\nresults with 19.8\\% of parameters and much lower resource consumption, taking\nonly 3.8\\% GPU hours for training and up to 8.5 times faster for inference.\nBesides, our model shows superior compatibility for the datasets with (or\nwithout) triggers and the pseudo triggers can be the supplements for annotated\ntriggers to make further improvements. Codes are available at\nhttps:\/\/github.com\/Spico197\/DocEE .\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.08614,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"KAT: A Knowledge Augmented Transformer for Vision-and-Language\n\n  The primary focus of recent work with largescale transformers has been on\noptimizing the amount of information packed into the model's parameters. In\nthis work, we ask a different question: Can multimodal transformers leverage\nexplicit knowledge in their reasoning? Existing, primarily unimodal, methods\nhave explored approaches under the paradigm of knowledge retrieval followed by\nanswer prediction, but leave open questions about the quality and relevance of\nthe retrieved knowledge used, and how the reasoning processes over implicit and\nexplicit knowledge should be integrated. To address these challenges, we\npropose a novel model - Knowledge Augmented Transformer (KAT) - which achieves\na strong state-of-the-art result (+6 points absolute) on the open-domain\nmultimodal task of OK-VQA. Our approach integrates implicit and explicit\nknowledge in an end to end encoder-decoder architecture, while still jointly\nreasoning over both knowledge sources during answer generation. An additional\nbenefit of explicit knowledge integration is seen in improved interpretability\nof model predictions in our analysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.08578,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"CLICKER: A Computational LInguistics Classification Scheme for\n  Educational Resources\n\n  A classification scheme of a scientific subject gives an overview of its body\nof knowledge. It can also be used to facilitate access to research articles and\nother materials related to the subject. For example, the ACM Computing\nClassification System (CCS) is used in the ACM Digital Library search interface\nand also for indexing computer science papers. We observed that a comprehensive\nclassification system like CCS or Mathematics Subject Classification (MSC) does\nnot exist for Computational Linguistics (CL) and Natural Language Processing\n(NLP). We propose a classification scheme -- CLICKER for CL\/NLP based on the\nanalysis of online lectures from 77 university courses on this subject. The\ncurrently proposed taxonomy includes 334 topics and focuses on educational\naspects of CL\/NLP; it is based primarily, but not exclusively, on lecture notes\nfrom NLP courses. We discuss how such a taxonomy can help in various real-world\napplications, including tutoring platforms, resource retrieval, resource\nrecommendation, prerequisite chain learning, and survey generation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.11941,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000076493,
      "text":"CRASS: A Novel Data Set and Benchmark to Test Counterfactual Reasoning\n  of Large Language Models\n\n  We introduce the CRASS (counterfactual reasoning assessment) data set and\nbenchmark utilizing questionized counterfactual conditionals as a novel and\npowerful tool to evaluate large language models. We present the data set design\nand benchmark that supports scoring against a crowd-validated human baseline.\nWe test six state-of-the-art models against our benchmark. Our results show\nthat it poses a valid challenge for these models and opens up considerable room\nfor their improvement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.15338,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Clustering Vietnamese Conversations From Facebook Page To Build Training\n  Dataset For Chatbot\n\n  The biggest challenge of building chatbots is training data. The required\ndata must be realistic and large enough to train chatbots. We create a tool to\nget actual training data from Facebook messenger of a Facebook page. After text\npreprocessing steps, the newly obtained dataset generates FVnC and Sample\ndataset. We use the Retraining of BERT for Vietnamese (PhoBERT) to extract\nfeatures of our text data. K-Means and DBSCAN clustering algorithms are used\nfor clustering tasks based on output embeddings from PhoBERT$_{base}$. We apply\nV-measure score and Silhouette score to evaluate the performance of clustering\nalgorithms. We also demonstrate the efficiency of PhoBERT compared to other\nmodels in feature extraction on the Sample dataset and wiki dataset. A\nGridSearch algorithm that combines both clustering evaluations is also proposed\nto find optimal parameters. Thanks to clustering such a number of\nconversations, we save a lot of time and effort to build data and storylines\nfor training chatbot.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.03445,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000060267,
      "text":"NILC-Metrix: assessing the complexity of written and spoken language in\n  Brazilian Portuguese\n\n  This paper presents and makes publicly available the NILC-Metrix, a\ncomputational system comprising 200 metrics proposed in studies on discourse,\npsycholinguistics, cognitive and computational linguistics, to assess textual\ncomplexity in Brazilian Portuguese (BP). These metrics are relevant for\ndescriptive analysis and the creation of computational models and can be used\nto extract information from various linguistic levels of written and spoken\nlanguage. The metrics in NILC-Metrix were developed during the last 13 years,\nstarting in 2008 with Coh-Metrix-Port, a tool developed within the scope of the\nPorSimples project. Coh-Metrix-Port adapted some metrics to BP from the\nCoh-Metrix tool that computes metrics related to cohesion and coherence of\ntexts in English. After the end of PorSimples in 2010, new metrics were added\nto the initial 48 metrics of Coh-Metrix-Port. Given the large number of\nmetrics, we present them following an organisation similar to the metrics of\nCoh-Metrix v3.0 to facilitate comparisons made with metrics in Portuguese and\nEnglish. In this paper, we illustrate the potential of NILC-Metrix by\npresenting three applications: (i) a descriptive analysis of the differences\nbetween children's film subtitles and texts written for Elementary School I and\nII (Final Years); (ii) a new predictor of textual complexity for the corpus of\noriginal and simplified texts of the PorSimples project; (iii) a complexity\nprediction model for school grades, using transcripts of children's story\nnarratives told by teenagers. For each application, we evaluate which groups of\nmetrics are more discriminative, showing their contribution for each task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.06204,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"Few-Shot Out-of-Domain Transfer Learning of Natural Language\n  Explanations in a Label-Abundant Setup\n\n  Training a model to provide natural language explanations (NLEs) for its\npredictions usually requires the acquisition of task-specific NLEs, which is\ntime- and resource-consuming. A potential solution is the few-shot\nout-of-domain transfer of NLEs from a parent task with many NLEs to a child\ntask. In this work, we examine the setup in which the child task has few NLEs\nbut abundant labels. We establish four few-shot transfer learning methods that\ncover the possible fine-tuning combinations of the labels and NLEs for the\nparent and child tasks. We transfer explainability from a large natural\nlanguage inference dataset (e-SNLI) separately to two child tasks: (1) hard\ncases of pronoun resolution, where we introduce the small-e-WinoGrande dataset\nof NLEs on top of the WinoGrande dataset, and (2)~commonsense validation\n(ComVE). Our results demonstrate that the parent task helps with NLE generation\nand we establish the best methods for this setup.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.08054,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000023875,
      "text":"VISA: An Ambiguous Subtitles Dataset for Visual Scene-Aware Machine\n  Translation\n\n  Existing multimodal machine translation (MMT) datasets consist of images and\nvideo captions or general subtitles, which rarely contain linguistic ambiguity,\nmaking visual information not so effective to generate appropriate\ntranslations. We introduce VISA, a new dataset that consists of 40k\nJapanese-English parallel sentence pairs and corresponding video clips with the\nfollowing key features: (1) the parallel sentences are subtitles from movies\nand TV episodes; (2) the source subtitles are ambiguous, which means they have\nmultiple possible translations with different meanings; (3) we divide the\ndataset into Polysemy and Omission according to the cause of ambiguity. We show\nthat VISA is challenging for the latest MMT system, and we hope that the\ndataset can facilitate MMT research. The VISA dataset is available at:\nhttps:\/\/github.com\/ku-nlp\/VISA.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.11258,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000033445,
      "text":"Learning How to Translate North Korean through South Korean\n\n  South and North Korea both use the Korean language. However, Korean NLP\nresearch has focused on South Korean only, and existing NLP systems of the\nKorean language, such as neural machine translation (NMT) models, cannot\nproperly handle North Korean inputs. Training a model using North Korean data\nis the most straightforward approach to solving this problem, but there is\ninsufficient data to train NMT models. In this study, we create data for North\nKorean NMT models using a comparable corpus. First, we manually create\nevaluation data for automatic alignment and machine translation. Then, we\ninvestigate automatic alignment methods suitable for North Korean. Finally, we\nverify that a model trained by North Korean bilingual data without human\nannotation can significantly boost North Korean translation accuracy compared\nto existing South Korean models in zero-shot settings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.05891,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000011358,
      "text":"Automatic Correction of Syntactic Dependency Annotation Differences\n\n  Annotation inconsistencies between data sets can cause problems for\nlow-resource NLP, where noisy or inconsistent data cannot be as easily replaced\ncompared with resource-rich languages. In this paper, we propose a method for\nautomatically detecting annotation mismatches between dependency parsing\ncorpora, as well as three related methods for automatically converting the\nmismatches. All three methods rely on comparing an unseen example in a new\ncorpus with similar examples in an existing corpus. These three methods include\na simple lexical replacement using the most frequent tag of the example in the\nexisting corpus, a GloVe embedding-based replacement that considers a wider\npool of examples, and a BERT embedding-based replacement that uses\ncontextualized embeddings to provide examples fine-tuned to our specific data.\nWe then evaluate these conversions by retraining two dependency parsers --\nStanza (Qi et al. 2020) and Parsing as Tagging (PaT) (Vacareanu et al. 2020) --\non the converted and unconverted data. We find that applying our conversions\nyields significantly better performance in many cases. Some differences\nobserved between the two parsers are observed. Stanza has a more complex\narchitecture with a quadratic algorithm, so it takes longer to train, but it\ncan generalize better with less data. The PaT parser has a simpler architecture\nwith a linear algorithm, speeding up training time but requiring more training\ndata to reach comparable or better performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.04356,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Computational analyses of the topics, sentiments, literariness,\n  creativity and beauty of texts in a large Corpus of English Literature\n\n  The Gutenberg Literary English Corpus (GLEC, Jacobs, 2018a) provides a rich\nsource of textual data for research in digital humanities, computational\nlinguistics or neurocognitive poetics. In this study we address differences\namong the different literature categories in GLEC, as well as differences\nbetween authors. We report the results of three studies providing i) topic and\nsentiment analyses for six text categories of GLEC (i.e., children and youth,\nessays, novels, plays, poems, stories) and its >100 authors, ii) novel measures\nof semantic complexity as indices of the literariness, creativity and book\nbeauty of the works in GLEC (e.g., Jane Austen's six novels), and iii) two\nexperiments on text classification and authorship recognition using novel\nfeatures of semantic complexity. The data on two novel measures estimating a\ntext's literariness, intratextual variance and stepwise distance (van\nCranenburgh et al., 2019) revealed that plays are the most literary texts in\nGLEC, followed by poems and novels. Computation of a novel index of text\ncreativity (Gray et al., 2016) revealed poems and plays as the most creative\ncategories with the most creative authors all being poets (Milton, Pope, Keats,\nByron, or Wordsworth). We also computed a novel index of perceived beauty of\nverbal art (Kintsch, 2012) for the works in GLEC and predict that Emma is the\ntheoretically most beautiful of Austen's novels. Finally, we demonstrate that\nthese novel measures of semantic complexity are important features for text\nclassification and authorship recognition with overall predictive accuracies in\nthe range of .75 to .97. Our data pave the way for future computational and\nempirical studies of literature or experiments in reading psychology and offer\nmultiple baselines and benchmarks for analysing and validating other book\ncorpora.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.12507,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"AutoDistil: Few-shot Task-agnostic Neural Architecture Search for\n  Distilling Large Language Models\n\n  Knowledge distillation (KD) methods compress large models into smaller\nstudents with manually-designed student architectures given pre-specified\ncomputational cost. This requires several trials to find a viable student, and\nfurther repeating the process for each student or computational budget change.\nWe use Neural Architecture Search (NAS) to automatically distill several\ncompressed students with variable cost from a large model. Current works train\na single SuperLM consisting of millions of subnetworks with weight-sharing,\nresulting in interference between subnetworks of different sizes. Our framework\nAutoDistil addresses above challenges with the following steps: (a)\nIncorporates inductive bias and heuristics to partition Transformer search\nspace into K compact sub-spaces (K=3 for typical student sizes of base, small\nand tiny); (b) Trains one SuperLM for each sub-space using task-agnostic\nobjective (e.g., self-attention distillation) with weight-sharing of students;\n(c) Lightweight search for the optimal student without re-training. Fully\ntask-agnostic training and search allow students to be reused for fine-tuning\non any downstream task. Experiments on GLUE benchmark against state-of-the-art\nKD and NAS methods demonstrate AutoDistil to outperform leading compression\ntechniques with upto 2.7x reduction in computational cost and negligible loss\nin task performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.05041,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000160601,
      "text":"LARD: Large-scale Artificial Disfluency Generation\n\n  Disfluency detection is a critical task in real-time dialogue systems.\nHowever, despite its importance, it remains a relatively unexplored field,\nmainly due to the lack of appropriate datasets. At the same time, existing\ndatasets suffer from various issues, including class imbalance issues, which\ncan significantly affect the performance of the model on rare classes, as it is\ndemonstrated in this paper. To this end, we propose LARD, a method for\ngenerating complex and realistic artificial disfluencies with little effort.\nThe proposed method can handle three of the most common types of disfluencies:\nrepetitions, replacements and restarts. In addition, we release a new\nlarge-scale dataset with disfluencies that can be used on four different tasks:\ndisfluency detection, classification, extraction and correction. Experimental\nresults on the LARD dataset demonstrate that the data produced by the proposed\nmethod can be effectively used for detecting and removing disfluencies, while\nalso addressing limitations of existing datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.08687,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000186761,
      "text":"A Comparative Study on Language Models for Task-Oriented Dialogue\n  Systems\n\n  The recent development of language models has shown promising results by\nachieving state-of-the-art performance on various natural language tasks by\nfine-tuning pretrained models. In task-oriented dialogue (ToD) systems,\nlanguage models can be used for end-to-end training without relying on dialogue\nstate tracking to track the dialogue history but allowing the language models\nto generate responses according to the context given as input. This paper\nconducts a comparative study to show the effectiveness and strength of using\nrecent pretrained models for fine-tuning, such as BART and T5, on endto-end ToD\nsystems. The experimental results show substantial performance improvements\nafter language model fine-tuning. The models produce more fluent responses\nafter adding knowledge to the context that guides the model to avoid\nhallucination and generate accurate entities in the generated responses.\nFurthermore, we found that BART and T5 outperform GPT-based models in BLEU and\nF1 scores and achieve state-of-the-art performance in a ToD system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.05721,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000019206,
      "text":"Extracting Space Situational Awareness Events from News Text\n\n  Space situational awareness typically makes use of physical measurements from\nradar, telescopes, and other assets to monitor satellites and other spacecraft\nfor operational, navigational, and defense purposes. In this work we explore\nusing textual input for the space situational awareness task. We construct a\ncorpus of 48.5k news articles spanning all known active satellites between 2009\nand 2020. Using a dependency-rule-based extraction system designed to target\nthree high-impact events -- spacecraft launches, failures, and\ndecommissionings, we identify 1,787 space-event sentences that are then\nannotated by humans with 15.9k labels for event slots. We empirically\ndemonstrate a state-of-the-art neural extraction system achieves an overall F1\nbetween 53 and 91 per slot for event extraction in this low-resource,\nhigh-impact domain.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.04275,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000039736,
      "text":"PhysNLU: A Language Resource for Evaluating Natural Language\n  Understanding and Explanation Coherence in Physics\n\n  In order for language models to aid physics research, they must first encode\nrepresentations of mathematical and natural language discourse which lead to\ncoherent explanations, with correct ordering and relevance of statements. We\npresent a collection of datasets developed to evaluate the performance of\nlanguage models in this regard, which measure capabilities with respect to\nsentence ordering, position, section prediction, and discourse coherence.\nAnalysis of the data reveals equations and sub-disciplines which are most\ncommon in physics discourse, as well as the sentence-level frequency of\nequations and expressions. We present baselines that demonstrate how\ncontemporary language models are challenged by coherence related tasks in\nphysics, even when trained on mathematical natural language objectives.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.12093,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000062916,
      "text":"PCL: Peer-Contrastive Learning with Diverse Augmentations for\n  Unsupervised Sentence Embeddings\n\n  Learning sentence embeddings in an unsupervised manner is fundamental in\nnatural language processing. Recent common practice is to couple pre-trained\nlanguage models with unsupervised contrastive learning, whose success relies on\naugmenting a sentence with a semantically-close positive instance to construct\ncontrastive pairs. Nonetheless, existing approaches usually depend on a\nmono-augmenting strategy, which causes learning shortcuts towards the\naugmenting biases and thus corrupts the quality of sentence embeddings. A\nstraightforward solution is resorting to more diverse positives from a\nmulti-augmenting strategy, while an open question remains about how to\nunsupervisedly learn from the diverse positives but with uneven augmenting\nqualities in the text field. As one answer, we propose a novel Peer-Contrastive\nLearning (PCL) with diverse augmentations. PCL constructs diverse contrastive\npositives and negatives at the group level for unsupervised sentence\nembeddings. PCL performs peer-positive contrast as well as peer-network\ncooperation, which offers an inherent anti-bias ability and an effective way to\nlearn from diverse augmentations. Experiments on STS benchmarks verify the\neffectiveness of PCL against its competitors in unsupervised sentence\nembeddings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.10262,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Do Transformers Encode a Foundational Ontology? Probing Abstract Classes\n  in Natural Language\n\n  With the methodological support of probing (or diagnostic classification),\nrecent studies have demonstrated that Transformers encode syntactic and\nsemantic information to some extent. Following this line of research, this\npaper aims at taking semantic probing to an abstraction extreme with the goal\nof answering the following research question: can contemporary\nTransformer-based models reflect an underlying Foundational Ontology? To this\nend, we present a systematic Foundational Ontology (FO) probing methodology to\ninvestigate whether Transformers-based models encode abstract semantic\ninformation. Following different pre-training and fine-tuning regimes, we\npresent an extensive evaluation of a diverse set of large-scale language models\nover three distinct and complementary FO tagging experiments. Specifically, we\npresent and discuss the following conclusions: (1) The probing results indicate\nthat Transformer-based models incidentally encode information related to\nFoundational Ontologies during the pre-training pro-cess; (2) Robust FO taggers\n(accuracy of 90 percent)can be efficiently built leveraging on this knowledge.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.04337,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"PromptBERT: Improving BERT Sentence Embeddings with Prompts\n\n  We propose PromptBERT, a novel contrastive learning method for learning\nbetter sentence representation. We firstly analyze the drawback of current\nsentence embedding from original BERT and find that it is mainly due to the\nstatic token embedding bias and ineffective BERT layers. Then we propose the\nfirst prompt-based sentence embeddings method and discuss two prompt\nrepresenting methods and three prompt searching methods to make BERT achieve\nbetter sentence embeddings. Moreover, we propose a novel unsupervised training\nobjective by the technology of template denoising, which substantially shortens\nthe performance gap between the supervised and unsupervised settings. Extensive\nexperiments show the effectiveness of our method. Compared to SimCSE,\nPromptBert achieves 2.29 and 2.58 points of improvement based on BERT and\nRoBERTa in the unsupervised setting.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.04826,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000104639,
      "text":"Document-level Relation Extraction with Context Guided Mention\n  Integration and Inter-pair Reasoning\n\n  Document-level Relation Extraction (DRE) aims to recognize the relations\nbetween two entities. The entity may correspond to multiple mentions that span\nbeyond sentence boundary. Few previous studies have investigated the mention\nintegration, which may be problematic because coreferential mentions do not\nequally contribute to a specific relation. Moreover, prior efforts mainly focus\non reasoning at entity-level rather than capturing the global interactions\nbetween entity pairs. In this paper, we propose two novel techniques, Context\nGuided Mention Integration and Inter-pair Reasoning (CGM2IR), to improve the\nDRE. Instead of simply applying average pooling, the contexts are utilized to\nguide the integration of coreferential mentions in a weighted sum manner.\nAdditionally, inter-pair reasoning executes an iterative algorithm on the\nentity pair graph, so as to model the interdependency of relations. We evaluate\nour CGM2IR model on three widely used benchmark datasets, namely DocRED, CDR,\nand GDA. Experimental results show that our model outperforms previous\nstate-of-the-art models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.02792,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"Defining maximum acceptable latency of AI-enhanced CAI tools\n\n  Recent years have seen an increasing number of studies around the design of\ncomputer-assisted interpreting tools with integrated automatic speech\nprocessing and their use by trainees and professional interpreters. This paper\ndiscusses the role of system latency of such tools and presents the results of\nan experiment designed to investigate the maximum system latency that is\ncognitively acceptable for interpreters working in the simultaneous modality.\nThe results show that interpreters can cope with a system latency of 3 seconds\nwithout any major impact in the rendition of the original text, both in terms\nof accuracy and fluency. This value is above the typical latency of available\nAI-based CAI tools and paves the way to experiment with larger context-based\nlanguage models and higher latencies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.06723,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Emojis as Anchors to Detect Arabic Offensive Language and Hate Speech\n\n  We introduce a generic, language-independent method to collect a large\npercentage of offensive and hate tweets regardless of their topics or genres.\nWe harness the extralinguistic information embedded in the emojis to collect a\nlarge number of offensive tweets. We apply the proposed method on Arabic tweets\nand compare it with English tweets - analysing key cultural differences. We\nobserved a constant usage of these emojis to represent offensiveness throughout\ndifferent timespans on Twitter. We manually annotate and publicly release the\nlargest Arabic dataset for offensive, fine-grained hate speech, vulgar and\nviolence content. Furthermore, we benchmark the dataset for detecting\noffensiveness and hate speech using different transformer architectures and\nperform in-depth linguistic analysis. We evaluate our models on external\ndatasets - a Twitter dataset collected using a completely different method, and\na multi-platform dataset containing comments from Twitter, YouTube and\nFacebook, for assessing generalization capability. Competitive results on these\ndatasets suggest that the data collected using our method captures universal\ncharacteristics of offensive language. Our findings also highlight the common\nwords used in offensive communications, common targets for hate speech,\nspecific patterns in violence tweets; and pinpoint common classification errors\nthat can be attributed to limitations of NLP models. We observe that even\nstate-of-the-art transformer models may fail to take into account culture,\nbackground and context or understand nuances present in real-world data such as\nsarcasm.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.02977,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000040068,
      "text":"Indian Language Wordnets and their Linkages with Princeton WordNet\n\n  Wordnets are rich lexico-semantic resources. Linked wordnets are extensions\nof wordnets, which link similar concepts in wordnets of different languages.\nSuch resources are extremely useful in many Natural Language Processing (NLP)\napplications, primarily those based on knowledge-based approaches. In such\napproaches, these resources are considered as gold standard\/oracle. Thus, it is\ncrucial that these resources hold correct information. Thereby, they are\ncreated by human experts. However, human experts in multiple languages are hard\nto come by. Thus, the community would benefit from sharing of such manually\ncreated resources. In this paper, we release mappings of 18 Indian language\nwordnets linked with Princeton WordNet. We believe that availability of such\nresources will have a direct impact on the progress in NLP for these languages.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.08495,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"SciBERTSUM: Extractive Summarization for Scientific Documents\n\n  The summarization literature focuses on the summarization of news articles.\nThe news articles in the CNN-DailyMail are relatively short documents with\nabout 30 sentences per document on average. We introduce SciBERTSUM, our\nsummarization framework designed for the summarization of long documents like\nscientific papers with more than 500 sentences. SciBERTSUM extends BERTSUM to\nlong documents by 1) adding a section embedding layer to include section\ninformation in the sentence vector and 2) applying a sparse attention mechanism\nwhere each sentences will attend locally to nearby sentences and only a small\nnumber of sentences attend globally to all other sentences. We used slides\ngenerated by the authors of scientific papers as reference summaries since they\ncontain the technical details from the paper. The results show the superiority\nof our model in terms of ROUGE scores.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.06642,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000025498,
      "text":"Towards a Cleaner Document-Oriented Multilingual Crawled Corpus\n\n  The need for raw large raw corpora has dramatically increased in recent years\nwith the introduction of transfer learning and semi-supervised learning methods\nto Natural Language Processing. And while there have been some recent attempts\nto manually curate the amount of data necessary to train large language models,\nthe main way to obtain this data is still through automatic web crawling. In\nthis paper we take the existing multilingual web corpus OSCAR and its pipeline\nUngoliant that extracts and classifies data from Common Crawl at the line\nlevel, and propose a set of improvements and automatic annotations in order to\nproduce a new document-oriented version of OSCAR that could prove more suitable\nto pre-train large generative language models as well as hopefully other\napplications in Natural Language Processing and Digital Humanities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.08975,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000195371,
      "text":"Chinese Word Segmentation with Heterogeneous Graph Neural Network\n\n  In recent years, deep learning has achieved significant success in the\nChinese word segmentation (CWS) task. Most of these methods improve the\nperformance of CWS by leveraging external information, e.g., words, sub-words,\nsyntax. However, existing approaches fail to effectively integrate the\nmulti-level linguistic information and also ignore the structural feature of\nthe external information. Therefore, in this paper, we proposed a framework to\nimprove CWS, named HGNSeg. It exploits multi-level external information\nsufficiently with the pre-trained language model and heterogeneous graph neural\nnetwork. The experimental results on six benchmark datasets (e.g., Bakeoff\n2005, Bakeoff 2008) validate that our approach can effectively improve the\nperformance of Chinese word segmentation. Importantly, in cross-domain\nscenarios, our method also shows a strong ability to alleviate the\nout-of-vocabulary (OOV) problem.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.03382,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"BERT for Sentiment Analysis: Pre-trained and Fine-Tuned Alternatives\n\n  BERT has revolutionized the NLP field by enabling transfer learning with\nlarge language models that can capture complex textual patterns, reaching the\nstate-of-the-art for an expressive number of NLP applications. For text\nclassification tasks, BERT has already been extensively explored. However,\naspects like how to better cope with the different embeddings provided by the\nBERT output layer and the usage of language-specific instead of multilingual\nmodels are not well studied in the literature, especially for the Brazilian\nPortuguese language. The purpose of this article is to conduct an extensive\nexperimental study regarding different strategies for aggregating the features\nproduced in the BERT output layer, with a focus on the sentiment analysis task.\nThe experiments include BERT models trained with Brazilian Portuguese corpora\nand the multilingual version, contemplating multiple aggregation strategies and\nopen-source datasets with predefined training, validation, and test partitions\nto facilitate the reproducibility of the results. BERT achieved the highest\nROC-AUC values for the majority of cases as compared to TF-IDF. Nonetheless,\nTF-IDF represents a good trade-off between the predictive performance and\ncomputational cost.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.13151,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000155303,
      "text":"COMPASS: a Creative Support System that Alerts Novelists to the\n  Unnoticed Missing Contents\n\n  When humans write, they may unintentionally omit some information.\nComplementing the omitted information using a computer is helpful in providing\nwriting support. Recently, in the field of story understanding and generation,\nstory completion (SC) was proposed to generate the missing parts of an\nincomplete story. Although its applicability is limited because it requires\nthat the user have prior knowledge of the missing part of a story, missing\nposition prediction (MPP) can be used to compensate for this problem. MPP aims\nto predict the position of the missing part, but the prerequisite knowledge\nthat \"one sentence is missing\" is still required. In this study, we propose\nVariable Number MPP (VN-MPP), a new MPP task that removes this restriction;\nthat is, the task to predict multiple missing sentences or to judge whether\nthere are no missing sentences in the first place. We also propose two methods\nfor this new MPP task. Furthermore, based on the novel task and methods, we\ndeveloped a creative writing support system, COMPASS. The results of a user\nexperiment involving professional creators who write texts in Japanese confirm\nthe efficacy and utility of the developed system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.05457,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Hindi\/Bengali Sentiment Analysis Using Transfer Learning and Joint Dual\n  Input Learning with Self Attention\n\n  Sentiment Analysis typically refers to using natural language processing,\ntext analysis and computational linguistics to extract affect and emotion based\ninformation from text data. Our work explores how we can effectively use deep\nneural networks in transfer learning and joint dual input learning settings to\neffectively classify sentiments and detect hate speech in Hindi and Bengali\ndata. We start by training Word2Vec word embeddings for Hindi \\textbf{HASOC\ndataset} and Bengali hate speech and then train LSTM and subsequently, employ\nparameter sharing based transfer learning to Bengali sentiment classifiers by\nreusing and fine-tuning the trained weights of Hindi classifiers with both\nclassifier being used as baseline in our study. Finally, we use BiLSTM with\nself attention in joint dual input learning setting where we train a single\nneural network on Hindi and Bengali dataset simultaneously using their\nrespective embeddings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.12246,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000097023,
      "text":"Neural reality of argument structure constructions\n\n  In lexicalist linguistic theories, argument structure is assumed to be\npredictable from the meaning of verbs. As a result, the verb is the primary\ndeterminant of the meaning of a clause. In contrast, construction grammarians\npropose that argument structure is encoded in constructions (or form-meaning\npairs) that are distinct from verbs. Decades of psycholinguistic research have\nproduced substantial empirical evidence in favor of the construction view. Here\nwe adapt several psycholinguistic studies to probe for the existence of\nargument structure constructions (ASCs) in Transformer-based language models\n(LMs). First, using a sentence sorting experiment, we find that sentences\nsharing the same construction are closer in embedding space than sentences\nsharing the same verb. Furthermore, LMs increasingly prefer grouping by\nconstruction with more input data, mirroring the behaviour of non-native\nlanguage learners. Second, in a \"Jabberwocky\" priming-based experiment, we find\nthat LMs associate ASCs with meaning, even in semantically nonsensical\nsentences. Our work offers the first evidence for ASCs in LMs and highlights\nthe potential to devise novel probing methods grounded in psycholinguistic\nresearch.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.12024,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000011358,
      "text":"NoisyTune: A Little Noise Can Help You Finetune Pretrained Language\n  Models Better\n\n  Effectively finetuning pretrained language models (PLMs) is critical for\ntheir success in downstream tasks. However, PLMs may have risks in overfitting\nthe pretraining tasks and data, which usually have gap with the target\ndownstream tasks. Such gap may be difficult for existing PLM finetuning methods\nto overcome and lead to suboptimal performance. In this paper, we propose a\nvery simple yet effective method named NoisyTune to help better finetune PLMs\non downstream tasks by adding some noise to the parameters of PLMs before\nfine-tuning. More specifically, we propose a matrix-wise perturbing method\nwhich adds different uniform noises to different parameter matrices based on\ntheir standard deviations. In this way, the varied characteristics of different\ntypes of parameters in PLMs can be considered. Extensive experiments on both\nGLUE English benchmark and XTREME multilingual benchmark show NoisyTune can\nconsistently empower the finetuning of different PLMs on different downstream\ntasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.02516,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000033114,
      "text":"A Survey on Automated Sarcasm Detection on Twitter\n\n  Automatic sarcasm detection is a growing field in computer science. Short\ntext messages are increasingly used for communication, especially over social\nmedia platforms such as Twitter. Due to insufficient or missing context,\nunidentified sarcasm in these messages can invert the meaning of a statement,\nleading to confusion and communication failures. This paper covers a variety of\ncurrent methods used for sarcasm detection, including detection by context,\nposting history and machine learning models. Additionally, a shift towards deep\nlearning methods is observable, likely due to the benefit of using a model with\ninduced instead of discrete features combined with the innovation of\ntransformers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.1071,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000033776,
      "text":"Incorporating Constituent Syntax for Coreference Resolution\n\n  Syntax has been shown to benefit Coreference Resolution from incorporating\nlong-range dependencies and structured information captured by syntax trees,\neither in traditional statistical machine learning based systems or recently\nproposed neural models. However, most leading systems use only dependency\ntrees. We argue that constituent trees also encode important information, such\nas explicit span-boundary signals captured by nested multi-word phrases, extra\nlinguistic labels and hierarchical structures useful for detecting anaphora. In\nthis work, we propose a simple yet effective graph-based method to incorporate\nconstituent syntactic structures. Moreover, we also explore to utilise\nhigher-order neighbourhood information to encode rich structures in constituent\ntrees. A novel message propagation mechanism is therefore proposed to enable\ninformation flow among elements in syntax trees. Experiments on the English and\nChinese portions of OntoNotes 5.0 benchmark show that our proposed model either\nbeats a strong baseline or achieves new state-of-the-art performance. (Code is\navailable at https:\/\/github.com\/Fantabulous-J\/Coref-Constituent-Graph)\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.03086,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000042054,
      "text":"Machine Translation from Signed to Spoken Languages: State of the Art\n  and Challenges\n\n  Automatic translation from signed to spoken languages is an interdisciplinary\nresearch domain, lying on the intersection of computer vision, machine\ntranslation and linguistics. Nevertheless, research in this domain is performed\nmostly by computer scientists in isolation. As the domain is becoming\nincreasingly popular - the majority of scientific papers on the topic of sign\nlanguage translation have been published in the past three years - we provide\nan overview of the state of the art as well as some required background in the\ndifferent related disciplines. We give a high-level introduction to sign\nlanguage linguistics and machine translation to illustrate the requirements of\nautomatic sign language translation. We present a systematic literature review\nto illustrate the state of the art in the domain and then, harking back to the\nrequirements, lay out several challenges for future research. We find that\nsignificant advances have been made on the shoulders of spoken language machine\ntranslation research. However, current approaches are often not linguistically\nmotivated or are not adapted to the different input modality of sign languages.\nWe explore challenges related to the representation of sign language data, the\ncollection of datasets, the need for interdisciplinary research and\nrequirements for moving beyond research, towards applications. Based on our\nfindings, we advocate for interdisciplinary research and to base future\nresearch on linguistic analysis of sign languages. Furthermore, the inclusion\nof deaf and hearing end users of sign language translation applications in use\ncase identification, data collection and evaluation is of the utmost importance\nin the creation of useful sign language translation models. We recommend\niterative, human-in-the-loop, design and development of sign language\ntranslation models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.04462,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000029802,
      "text":"Social Media as an Instant Source of Feedback on Water Quality\n\n  This paper focuses on an important environmental challenge; namely, water\nquality by analyzing the potential of social media as an immediate source of\nfeedback. The main goal of the work is to automatically analyze and retrieve\nsocial media posts relevant to water quality with particular attention to posts\ndescribing different aspects of water quality, such as watercolor, smell,\ntaste, and related illnesses. To this aim, we propose a novel framework\nincorporating different preprocessing, data augmentation, and classification\ntechniques. In total, three different Neural Networks (NNs) architectures,\nnamely (i) Bidirectional Encoder Representations from Transformers (BERT), (ii)\nRobustly Optimized BERT Pre-training Approach (XLM-RoBERTa), and (iii) custom\nLong short-term memory (LSTM) model, are employed in a merit-based fusion\nscheme. For merit-based weight assignment to the models, several optimization\nand search techniques are compared including a Particle Swarm Optimization\n(PSO), a Genetic Algorithm (GA), Brute Force (BF), Nelder-Mead, and Powell's\noptimization methods. We also provide an evaluation of the individual models\nwhere the highest F1-score of 0.81 is obtained with the BERT model. In\nmerit-based fusion, overall better results are obtained with BF achieving an\nF1-score score of 0.852.\n  We also provide comparison against existing methods, where a significant\nimprovement for our proposed solutions is obtained. We believe such rigorous\nanalysis of this relatively new topic will provide a baseline for future\nresearch.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.01145,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000005828,
      "text":"Relative Position Prediction as Pre-training for Text Encoders\n\n  Meaning is defined by the company it keeps. However, company is two-fold:\nIt's based on the identity of tokens and also on their position (topology). We\nargue that a position-centric perspective is more general and useful. The\nclassic MLM and CLM objectives in NLP are easily phrased as position\npredictions over the whole vocabulary. Adapting the relative position encoding\nparadigm in NLP to create relative labels for self-supervised learning, we seek\nto show superior pre-training judged by performance on downstream tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.13649,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"GausSetExpander: A Simple Approach for Entity Set Expansion\n\n  Entity Set Expansion is an important NLP task that aims at expanding a small\nset of entities into a larger one with items from a large pool of candidates.\nIn this paper, we propose GausSetExpander, an unsupervised approach based on\noptimal transport techniques. We propose to re-frame the problem as choosing\nthe entity that best completes the seed set. For this, we interpret a set as an\nelliptical distribution with a centroid which represents the mean and a spread\nthat is represented by the scale parameter. The best entity is the one that\nincreases the spread of the set the least. We demonstrate the validity of our\napproach by comparing to state-of-the art approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.13047,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000101328,
      "text":"AugESC: Dialogue Augmentation with Large Language Models for Emotional\n  Support Conversation\n\n  Crowdsourced dialogue corpora are usually limited in scale and topic coverage\ndue to the expensive cost of data curation. This would hinder the\ngeneralization of downstream dialogue models to open-domain topics. In this\nwork, we leverage large language models for dialogue augmentation in the task\nof emotional support conversation (ESC). By treating dialogue augmentation as a\ndialogue completion task, we prompt a fine-tuned language model to complete\nfull dialogues from available dialogue posts of various topics, which are then\npostprocessed based on heuristics. Applying this approach, we construct AugESC,\nan augmented dataset for the ESC task, which largely extends the scale and\ntopic coverage of the crowdsourced ESConv corpus. Through comprehensive human\nevaluation, we demonstrate that our approach is superior to strong baselines of\ndialogue augmentation and that AugESC has comparable dialogue quality to the\ncrowdsourced corpus. We also conduct human interactive evaluation and prove\nthat post-training on AugESC improves downstream dialogue models'\ngeneralization ability to open-domain topics. These results suggest the utility\nof AugESC and highlight the potential of large language models in improving\ndata-scarce dialogue generation tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.03629,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"Survey of Hallucination in Natural Language Generation\n\n  Natural Language Generation (NLG) has improved exponentially in recent years\nthanks to the development of sequence-to-sequence deep learning technologies\nsuch as Transformer-based language models. This advancement has led to more\nfluent and coherent NLG, leading to improved development in downstream tasks\nsuch as abstractive summarization, dialogue generation and data-to-text\ngeneration. However, it is also apparent that deep learning based generation is\nprone to hallucinate unintended text, which degrades the system performance and\nfails to meet user expectations in many real-world scenarios. To address this\nissue, many studies have been presented in measuring and mitigating\nhallucinated texts, but these have never been reviewed in a comprehensive\nmanner before. In this survey, we thus provide a broad overview of the research\nprogress and challenges in the hallucination problem in NLG. The survey is\norganized into two parts: (1) a general overview of metrics, mitigation\nmethods, and future directions; (2) an overview of task-specific research\nprogress on hallucinations in the following downstream tasks, namely\nabstractive summarization, dialogue generation, generative question answering,\ndata-to-text generation, machine translation, and visual-language generation;\nand (3) hallucinations in large language models (LLMs). This survey serves to\nfacilitate collaborative efforts among researchers in tackling the challenge of\nhallucinated texts in NLG.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.13756,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000040399,
      "text":"Data-to-text Generation with Variational Sequential Planning\n\n  We consider the task of data-to-text generation, which aims to create textual\noutput from non-linguistic input. We focus on generating long-form text, i.e.,\ndocuments with multiple paragraphs, and propose a neural model enhanced with a\nplanning component responsible for organizing high-level information in a\ncoherent and meaningful way. We infer latent plans sequentially with a\nstructured variational model, while interleaving the steps of planning and\ngeneration. Text is generated by conditioning on previous variational decisions\nand previously generated text. Experiments on two data-to-text benchmarks\n(RotoWire and MLB) show that our model outperforms strong baselines and is\nsample efficient in the face of limited training data (e.g., a few hundred\ninstances).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.13618,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000023842,
      "text":"Supervised Machine Learning Algorithm for Detecting Consistency between\n  Reported Findings and the Conclusions of Mammography Reports\n\n  Objective. Mammography reports document the diagnosis of patients'\nconditions. However, many reports contain non-standard terms (non-BI-RADS\ndescriptors) and incomplete statements, which can lead to conclusions that are\nnot well-supported by the reported findings. Our aim was to develop a tool to\ndetect such discrepancies by comparing the reported conclusions to those that\nwould be expected based on the reported radiology findings. Materials and\nMethods. A deidentified data set from an academic hospital containing 258\nmammography reports supplemented by 120 reports found on the web was used for\ntraining and evaluation. Spell checking and term normalization was used to\nunambiguously determine the reported BI-RADS descriptors. The resulting data\nwere input into seven classifiers that classify mammography reports, based on\ntheir Findings sections, into seven BI-RADS final assessment categories.\nFinally, the semantic similarity score of a report to each BI-RADS category is\nreported. Results. Our term normalization algorithm correctly identified 97% of\nthe BI-RADS descriptors in mammography reports. Our system provided 76%\nprecision and 83% recall in correctly classifying the reports according to\nBI-RADS final assessment category. Discussion. The strength of our approach\nrelies on providing high importance to BI-RADS terms in the summarization\nphase, on the semantic similarity that considers the complex data\nrepresentation, and on the classification into all seven BI-RADs categories.\nConclusion. BI-RADS descriptors and expected final assessment categories could\nbe automatically detected by our approach with fairly good accuracy, which\ncould be used to make users aware that their reported findings do not match\nwell with their conclusion.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.12801,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000062254,
      "text":"On the data requirements of probing\n\n  As large and powerful neural language models are developed, researchers have\nbeen increasingly interested in developing diagnostic tools to probe them.\nThere are many papers with conclusions of the form \"observation X is found in\nmodel Y\", using their own datasets with varying sizes. Larger probing datasets\nbring more reliability, but are also expensive to collect. There is yet to be a\nquantitative method for estimating reasonable probing dataset sizes. We tackle\nthis omission in the context of comparing two probing configurations: after we\nhave collected a small dataset from a pilot study, how many additional data\nsamples are sufficient to distinguish two different configurations? We present\na novel method to estimate the required number of data samples in such\nexperiments and, across several case studies, we verify that our estimations\nhave sufficient statistical power. Our framework helps to systematically\nconstruct probing datasets to diagnose neural NLP models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.09625,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000070863,
      "text":"CALCS 2021 Shared Task: Machine Translation for Code-Switched Data\n\n  To date, efforts in the code-switching literature have focused for the most\npart on language identification, POS, NER, and syntactic parsing. In this\npaper, we address machine translation for code-switched social media data. We\ncreate a community shared task. We provide two modalities for participation:\nsupervised and unsupervised. For the supervised setting, participants are\nchallenged to translate English into Hindi-English (Eng-Hinglish) in a single\ndirection. For the unsupervised setting, we provide the following language\npairs: English and Spanish-English (Eng-Spanglish), and English and Modern\nStandard Arabic-Egyptian Arabic (Eng-MSAEA) in both directions. We share\ninsights and challenges in curating the \"into\" code-switching language\nevaluation data. Further, we provide baselines for all language pairs in the\nshared task. The leaderboard for the shared task comprises 12 individual system\nsubmissions corresponding to 5 different teams. The best performance achieved\nis 12.67% BLEU score for English to Hinglish and 25.72% BLEU score for MSAEA to\nEnglish.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.12132,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"\"splink\" is happy and \"phrouth\" is scary: Emotion Intensity Analysis for\n  Nonsense Words\n\n  People associate affective meanings to words - \"death\" is scary and sad while\n\"party\" is connotated with surprise and joy. This raises the question if the\nassociation is purely a product of the learned affective imports inherent to\nsemantic meanings, or is also an effect of other features of words, e.g.,\nmorphological and phonological patterns. We approach this question with an\nannotation-based analysis leveraging nonsense words. Specifically, we conduct a\nbest-worst scaling crowdsourcing study in which participants assign intensity\nscores for joy, sadness, anger, disgust, fear, and surprise to 272 non-sense\nwords and, for comparison of the results to previous work, to 68 real words.\nBased on this resource, we develop character-level and phonology-based\nintensity regressors. We evaluate them on both nonsense words and real words\n(making use of the NRC emotion intensity lexicon of 7493 words), across six\nemotion categories. The analysis of our data reveals that some phonetic\npatterns show clear differences between emotion intensities. For instance, s as\na first phoneme contributes to joy, sh to surprise, p as last phoneme more to\ndisgust than to anger and fear. In the modelling experiments, a regressor\ntrained on real words from the NRC emotion intensity lexicon shows a higher\nperformance (r = 0.17) than regressors that aim at learning the emotion\nconnotation purely from nonsense words. We conclude that humans do associate\naffective meaning to words based on surface patterns, but also based on\nsimilarities to existing words (\"juy\" to \"joy\", or \"flike\" to \"like\").\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.13636,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000068545,
      "text":"Improving Lexical Embeddings for Robust Question Answering\n\n  Recent techniques in Question Answering (QA) have gained remarkable\nperformance improvement with some QA models even surpassed human performance.\nHowever, the ability of these models in truly understanding the language still\nremains dubious and the models are revealing limitations when facing\nadversarial examples. To strengthen the robustness of QA models and their\ngeneralization ability, we propose a representation Enhancement via Semantic\nand Context constraints (ESC) approach to improve the robustness of lexical\nembeddings. Specifically, we insert perturbations with semantic constraints and\ntrain enhanced contextual representations via a context-constraint loss to\nbetter distinguish the context clues for the correct answer. Experimental\nresults show that our approach gains significant robustness improvement on four\nadversarial test sets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.13817,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000298354,
      "text":"Robust Textual Embedding against Word-level Adversarial Attacks\n\n  We attribute the vulnerability of natural language processing models to the\nfact that similar inputs are converted to dissimilar representations in the\nembedding space, leading to inconsistent outputs, and we propose a novel robust\ntraining method, termed Fast Triplet Metric Learning (FTML). Specifically, we\nargue that the original sample should have similar representation with its\nadversarial counterparts and distinguish its representation from other samples\nfor better robustness. To this end, we adopt the triplet metric learning into\nthe standard training to pull words closer to their positive samples (i.e.,\nsynonyms) and push away their negative samples (i.e., non-synonyms) in the\nembedding space. Extensive experiments demonstrate that FTML can significantly\npromote the model robustness against various advanced adversarial attacks while\nkeeping competitive classification accuracy on original samples. Besides, our\nmethod is efficient as it only needs to adjust the embedding and introduces\nvery little overhead on the standard training. Our work shows great potential\nof improving the textual robustness through robust word embedding.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.1216,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000065234,
      "text":"Self-Attention for Incomplete Utterance Rewriting\n\n  Incomplete utterance rewriting (IUR) has recently become an essential task in\nNLP, aiming to complement the incomplete utterance with sufficient context\ninformation for comprehension. In this paper, we propose a novel method by\ndirectly extracting the coreference and omission relationship from the\nself-attention weight matrix of the transformer instead of word embeddings and\nedit the original text accordingly to generate the complete utterance.\nBenefiting from the rich information in the self-attention weight matrix, our\nmethod achieved competitive results on public IUR datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.04857,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000057287,
      "text":"Neuro-symbolic Natural Logic with Introspective Revision for Natural\n  Language Inference\n\n  We introduce a neuro-symbolic natural logic framework based on reinforcement\nlearning with introspective revision. The model samples and rewards specific\nreasoning paths through policy gradient, in which the introspective revision\nalgorithm modifies intermediate symbolic reasoning steps to discover\nreward-earning operations as well as leverages external knowledge to alleviate\nspurious reasoning and training inefficiency. The framework is supported by\nproperly designed local relation models to avoid input entangling, which helps\nensure the interpretability of the proof paths. The proposed model has built-in\ninterpretability and shows superior capability in monotonicity inference,\nsystematic generalization, and interpretability, compared to previous models on\nthe existing datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.09278,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000182788,
      "text":"Confidence Calibration for Intent Detection via Hyperspherical Space and\n  Rebalanced Accuracy-Uncertainty Loss\n\n  Data-driven methods have achieved notable performance on intent detection,\nwhich is a task to comprehend user queries. Nonetheless, they are controversial\nfor over-confident predictions. In some scenarios, users do not only care about\nthe accuracy but also the confidence of model. Unfortunately, mainstream neural\nnetworks are poorly calibrated, with a large gap between accuracy and\nconfidence. To handle this problem defined as confidence calibration, we\npropose a model using the hyperspherical space and rebalanced\naccuracy-uncertainty loss. Specifically, we project the label vector onto\nhyperspherical space uniformly to generate a dense label representation matrix,\nwhich mitigates over-confident predictions due to overfitting sparce one-hot\nlabel matrix. Besides, we rebalance samples of different accuracy and\nuncertainty to better guide model training. Experiments on the open datasets\nverify that our model outperforms the existing calibration methods and achieves\na significant improvement on the calibration metric.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.07057,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0,
      "text":"Hate Speech Classification Using SVM and Naive BAYES\n\n  The spread of hatred that was formerly limited to verbal communications has\nrapidly moved over the Internet. Social media and community forums that allow\npeople to discuss and express their opinions are becoming platforms for the\nspreading of hate messages. Many countries have developed laws to avoid online\nhate speech. They hold the companies that run the social media responsible for\ntheir failure to eliminate hate speech. But as online content continues to\ngrow, so does the spread of hate speech However, manual analysis of hate speech\non online platforms is infeasible due to the huge amount of data as it is\nexpensive and time consuming. Thus, it is important to automatically process\nthe online user contents to detect and remove hate speech from online media.\nMany recent approaches suffer from interpretability problem which means that it\ncan be difficult to understand why the systems make the decisions they do.\nThrough this work, some solutions for the problem of automatic detection of\nhate messages were proposed using Support Vector Machine (SVM) and Na\\\"ive\nBayes algorithms. This achieved near state-of-the-art performance while being\nsimpler and producing more easily interpretable decisions than other methods.\nEmpirical evaluation of this technique has resulted in a classification\naccuracy of approximately 99% and 50% for SVM and NB respectively over the test\nset.\n  Keywords: classification; hate speech; feature extraction, algorithm,\nsupervised learning\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.1355,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Modeling Target-Side Morphology in Neural Machine Translation: A\n  Comparison of Strategies\n\n  Morphologically rich languages pose difficulties to machine translation.\nMachine translation engines that rely on statistical learning from parallel\ntraining data, such as state-of-the-art neural systems, face challenges\nespecially with rich morphology on the output language side. Key challenges of\nrich target-side morphology in data-driven machine translation include: (1) A\nlarge amount of differently inflected word surface forms entails a larger\nvocabulary and thus data sparsity. (2) Some inflected forms of infrequent terms\ntypically do not appear in the training corpus, which makes closed-vocabulary\nsystems unable to generate these unobserved variants. (3) Linguistic agreement\nrequires the system to correctly match the grammatical categories between\ninflected word forms in the output sentence, both in terms of target-side\nmorpho-syntactic wellformedness and semantic adequacy with respect to the\ninput.\n  In this paper, we re-investigate two target-side linguistic processing\ntechniques: a lemma-tag strategy and a linguistically informed word\nsegmentation strategy. Our experiments are conducted on a English-German\ntranslation task under three training corpus conditions of different\nmagnitudes. We find that a stronger Transformer baseline leaves less room for\nimprovement than a shallow-RNN encoder-decoder model when translating\nin-domain. However, we find that linguistic modeling of target-side morphology\ndoes benefit the Transformer model when the same system is applied to\nout-of-domain input text. We also successfully apply our approach to English to\nCzech translation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.13528,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000026789,
      "text":"Single Model Ensemble for Subword Regularized Models in Low-Resource\n  Machine Translation\n\n  Subword regularizations use multiple subword segmentations during training to\nimprove the robustness of neural machine translation models. In previous\nsubword regularizations, we use multiple segmentations in the training process\nbut use only one segmentation in the inference. In this study, we propose an\ninference strategy to address this discrepancy. The proposed strategy\napproximates the marginalized likelihood by using multiple segmentations\nincluding the most plausible segmentation and several sampled segmentations.\nBecause the proposed strategy aggregates predictions from several\nsegmentations, we can regard it as a single model ensemble that does not\nrequire any additional cost for training. Experimental results show that the\nproposed strategy improves the performance of models trained with subword\nregularization in low-resource machine translation tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.14563,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"The Moral Debater: A Study on the Computational Generation of Morally\n  Framed Arguments\n\n  An audience's prior beliefs and morals are strong indicators of how likely\nthey will be affected by a given argument. Utilizing such knowledge can help\nfocus on shared values to bring disagreeing parties towards agreement. In\nargumentation technology, however, this is barely exploited so far. This paper\nstudies the feasibility of automatically generating morally framed arguments as\nwell as their effect on different audiences. Following the moral foundation\ntheory, we propose a system that effectively generates arguments focusing on\ndifferent morals. In an in-depth user study, we ask liberals and conservatives\nto evaluate the impact of these arguments. Our results suggest that,\nparticularly when prior beliefs are challenged, an audience becomes more\naffected by morally framed arguments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.08304,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000044041,
      "text":"Hyperdecoders: Instance-specific decoders for multi-task NLP\n\n  We investigate input-conditioned hypernetworks for multi-tasking in NLP,\ngenerating parameter-efficient adaptations for a decoder using a hypernetwork\nconditioned on the output of an encoder. This approach produces a unique\ndecoder adaptation for every input instance, allowing the network a larger\ndegree of flexibility than prior work that only produces one decoder adaptation\nper task. We apply our method to sequence classification tasks, extractive QA,\nand summarisation and find that it surpasses previous parameter efficient\nfine-tuning methods and often outperforms fully finetuning the underlying\nmodel. An analysis of the embeddings used by our hypernetwork shows that they\nare sensitive to output label and type, suggesting that our approach better\nmaps from encoder representations to output labels. Our code is publicly\navailable at https:\/\/github.com\/allenai\/hyperdecoders.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.05355,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000225173,
      "text":"SATLab at SemEval-2022 Task 4: Trying to Detect Patronizing and\n  Condescending Language with only Character and Word N-grams\n\n  A logistic regression model only fed with character and word n-grams is\nproposed for the SemEval-2022 Task 4 on Patronizing and Condescending Language\nDetection (PCL). It obtained an average level of performance, well above the\nperformance of a system that tries to guess without using any knowledge about\nthe task, but much lower than the best teams. As the proposed model is very\nsimilar to the one that performed well on a task requiring to automatically\nidentify hate speech and offensive content, this paper confirms the difficulty\nof PCL detection.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.10752,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000004371,
      "text":"XTREME-S: Evaluating Cross-lingual Speech Representations\n\n  We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual\nspeech representations in many languages. XTREME-S covers four task families:\nspeech recognition, classification, speech-to-text translation and retrieval.\nCovering 102 languages from 10+ language families, 3 different domains and 4\ntask families, XTREME-S aims to simplify multilingual speech representation\nevaluation, as well as catalyze research in \"universal\" speech representation\nlearning. This paper describes the new benchmark and establishes the first\nspeech-only and speech-text baselines using XLS-R and mSLAM on all downstream\ntasks. We motivate the design choices and detail how to use the benchmark.\nDatasets and fine-tuning scripts are made easily accessible at\nhttps:\/\/hf.co\/datasets\/google\/xtreme_s.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.08085,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Measuring the Impact of (Psycho-)Linguistic and Readability Features and\n  Their Spill Over Effects on the Prediction of Eye Movement Patterns\n\n  There is a growing interest in the combined use of NLP and machine learning\nmethods to predict gaze patterns during naturalistic reading. While promising\nresults have been obtained through the use of transformer-based language\nmodels, little work has been undertaken to relate the performance of such\nmodels to general text characteristics. In this paper we report on experiments\nwith two eye-tracking corpora of naturalistic reading and two language models\n(BERT and GPT-2). In all experiments, we test effects of a broad spectrum of\nfeatures for predicting human reading behavior that fall into five categories\n(syntactic complexity, lexical richness, register-based multiword combinations,\nreadability and psycholinguistic word properties). Our experiments show that\nboth the features included and the architecture of the transformer-based\nlanguage models play a role in predicting multiple eye-tracking measures during\nnaturalistic reading. We also report the results of experiments aimed at\ndetermining the relative importance of features from different groups using\nSP-LIME.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.03825,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000008146,
      "text":"Incorporating Hierarchy into Text Encoder: a Contrastive Learning\n  Approach for Hierarchical Text Classification\n\n  Hierarchical text classification is a challenging subtask of multi-label\nclassification due to its complex label hierarchy. Existing methods encode text\nand label hierarchy separately and mix their representations for\nclassification, where the hierarchy remains unchanged for all input text.\nInstead of modeling them separately, in this work, we propose Hierarchy-guided\nContrastive Learning (HGCLR) to directly embed the hierarchy into a text\nencoder. During training, HGCLR constructs positive samples for input text\nunder the guidance of the label hierarchy. By pulling together the input text\nand its positive sample, the text encoder can learn to generate the\nhierarchy-aware text representation independently. Therefore, after training,\nthe HGCLR enhanced text encoder can dispense with the redundant hierarchy.\nExtensive experiments on three benchmark datasets verify the effectiveness of\nHGCLR.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.085,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000066559,
      "text":"HeterMPC: A Heterogeneous Graph Neural Network for Response Generation\n  in Multi-Party Conversations\n\n  Recently, various response generation models for two-party conversations have\nachieved impressive improvements, but less effort has been paid to multi-party\nconversations (MPCs) which are more practical and complicated. Compared with a\ntwo-party conversation where a dialogue context is a sequence of utterances,\nbuilding a response generation model for MPCs is more challenging, since there\nexist complicated context structures and the generated responses heavily rely\non both interlocutors (i.e., speaker and addressee) and history utterances. To\naddress these challenges, we present HeterMPC, a heterogeneous graph-based\nneural network for response generation in MPCs which models the semantics of\nutterances and interlocutors simultaneously with two types of nodes in a graph.\nBesides, we also design six types of meta relations with\nnode-edge-type-dependent parameters to characterize the heterogeneous\ninteractions within the graph. Through multi-hop updating, HeterMPC can\nadequately utilize the structural knowledge of conversations for response\ngeneration. Experimental results on the Ubuntu Internet Relay Chat (IRC)\nchannel benchmark show that HeterMPC outperforms various baseline models for\nresponse generation in MPCs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.15424,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Semantic properties of English nominal pluralization: Insights from word\n  embeddings\n\n  Semantic differentiation of nominal pluralization is grammaticalized in many\nlanguages. For example, plural markers may only be relevant for human nouns.\nEnglish does not appear to make such distinctions. Using distributional\nsemantics, we show that English nominal pluralization exhibits semantic\nclusters. For instance, pluralization of fruit words is more similar to one\nanother and less similar to pluralization of other semantic classes. Therefore,\nreduction of the meaning shift in plural formation to the addition of an\nabstract plural meaning is too simplistic. A semantically informed method,\ncalled CosClassAvg, is introduced that outperforms pluralization methods in\ndistributional semantics which assume plural formation amounts to the addition\nof a fixed plural vector. In comparison with our approach, a method from\ncompositional distributional semantics, called FRACSS, predicted plural vectors\nthat were more similar to the corpus-extracted plural vectors in terms of\ndirection but not vector length. A modeling study reveals that the observed\ndifference between the two predicted semantic spaces by CosClassAvg and FRACSS\ncarries over to how well a computational model of the listener can understand\npreviously unencountered plural forms. Mappings from word forms, represented\nwith triphone vectors, to predicted semantic vectors are more productive when\nCosClassAvg-generated semantic vectors are employed as gold standard vectors\ninstead of FRACSS-generated vectors.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.09632,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Dim Wihl Gat Tun: The Case for Linguistic Expertise in NLP for\n  Underdocumented Languages\n\n  Recent progress in NLP is driven by pretrained models leveraging massive\ndatasets and has predominantly benefited the world's political and economic\nsuperpowers. Technologically underserved languages are left behind because they\nlack such resources. Hundreds of underserved languages, nevertheless, have\navailable data sources in the form of interlinear glossed text (IGT) from\nlanguage documentation efforts. IGT remains underutilized in NLP work, perhaps\nbecause its annotations are only semi-structured and often language-specific.\nWith this paper, we make the case that IGT data can be leveraged successfully\nprovided that target language expertise is available. We specifically advocate\nfor collaboration with documentary linguists. Our paper provides a roadmap for\nsuccessful projects utilizing IGT data: (1) It is essential to define which NLP\ntasks can be accomplished with the given IGT data and how these will benefit\nthe speech community. (2) Great care and target language expertise is required\nwhen converting the data into structured formats commonly employed in NLP. (3)\nTask-specific and user-specific evaluation can help to ascertain that the tools\nwhich are created benefit the target language speech community. We illustrate\neach step through a case study on developing a morphological reinflection\nsystem for the Tsimchianic language Gitksan.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.05147,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000280804,
      "text":"An Accurate Unsupervised Method for Joint Entity Alignment and Dangling\n  Entity Detection\n\n  Knowledge graph integration typically suffers from the widely existing\ndangling entities that cannot find alignment cross knowledge graphs (KGs). The\ndangling entity set is unavailable in most real-world scenarios, and manually\nmining the entity pairs that consist of entities with the same meaning is\nlabor-consuming. In this paper, we propose a novel accurate Unsupervised method\nfor joint Entity alignment (EA) and Dangling entity detection (DED), called\nUED. The UED mines the literal semantic information to generate pseudo entity\npairs and globally guided alignment information for EA and then utilizes the EA\nresults to assist the DED. We construct a medical cross-lingual knowledge graph\ndataset, MedED, providing data for both the EA and DED tasks. Extensive\nexperiments demonstrate that in the EA task, UED achieves EA results comparable\nto those of state-of-the-art supervised EA baselines and outperforms the\ncurrent state-of-the-art EA methods by combining supervised EA data. For the\nDED task, UED obtains high-quality results without supervision.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.08928,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000050002,
      "text":"C-MORE: Pretraining to Answer Open-Domain Questions by Consulting\n  Millions of References\n\n  We consider the problem of pretraining a two-stage open-domain question\nanswering (QA) system (retriever + reader) with strong transfer capabilities.\nThe key challenge is how to construct a large amount of high-quality\nquestion-answer-context triplets without task-specific annotations.\nSpecifically, the triplets should align well with downstream tasks by: (i)\ncovering a wide range of domains (for open-domain applications), (ii) linking a\nquestion to its semantically relevant context with supporting evidence (for\ntraining the retriever), and (iii) identifying the correct answer in the\ncontext (for training the reader). Previous pretraining approaches generally\nfall short of one or more of these requirements. In this work, we automatically\nconstruct a large-scale corpus that meets all three criteria by consulting\nmillions of references cited within Wikipedia. The well-aligned pretraining\nsignals benefit both the retriever and the reader significantly. Our pretrained\nretriever leads to 2%-10% absolute gains in top-20 accuracy. And with our\npretrained reader, the entire system improves by up to 4% in exact match.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.12276,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000057618,
      "text":"ERNIE-SPARSE: Learning Hierarchical Efficient Transformer Through\n  Regularized Self-Attention\n\n  Sparse Transformer has recently attracted a lot of attention since the\nability for reducing the quadratic dependency on the sequence length. We argue\nthat two factors, information bottleneck sensitivity and inconsistency between\ndifferent attention topologies, could affect the performance of the Sparse\nTransformer. This paper proposes a well-designed model named ERNIE-Sparse. It\nconsists of two distinctive parts: (i) Hierarchical Sparse Transformer (HST) to\nsequentially unify local and global information. (ii) Self-Attention\nRegularization (SAR) method, a novel regularization designed to minimize the\ndistance for transformers with different attention topologies. To evaluate the\neffectiveness of ERNIE-Sparse, we perform extensive evaluations. Firstly, we\nperform experiments on a multi-modal long sequence modeling task benchmark,\nLong Range Arena (LRA). Experimental results demonstrate that ERNIE-Sparse\nsignificantly outperforms a variety of strong baseline methods including the\ndense attention and other efficient sparse attention methods and achieves\nimprovements by 2.77% (57.78% vs. 55.01%). Secondly, to further show the\neffectiveness of our method, we pretrain ERNIE-Sparse and verified it on 3 text\nclassification and 2 QA downstream tasks, achieve improvements on\nclassification benchmark by 0.83% (92.46% vs. 91.63%), on QA benchmark by 3.24%\n(74.67% vs. 71.43%). Experimental results continue to demonstrate its superior\nperformance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.06875,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Improved Universal Sentence Embeddings with Prompt-based Contrastive\n  Learning and Energy-based Learning\n\n  Contrastive learning has been demonstrated to be effective in enhancing\npre-trained language models (PLMs) to derive superior universal sentence\nembeddings. However, existing contrastive methods still have two limitations.\nFirstly, previous works may acquire poor performance under domain shift\nsettings, thus hindering the application of sentence representations in\npractice. We attribute this low performance to the over-parameterization of\nPLMs with millions of parameters. To alleviate it, we propose PromCSE\n(Prompt-based Contrastive Learning for Sentence Embeddings), which only trains\nsmall-scale \\emph{Soft Prompt} (i.e., a set of trainable vectors) while keeping\nPLMs fixed. Secondly, the commonly used NT-Xent loss function of contrastive\nlearning does not fully exploit hard negatives in supervised learning settings.\nTo this end, we propose to integrate an Energy-based Hinge loss to enhance the\npairwise discriminative power, inspired by the connection between the NT-Xent\nloss and the Energy-based Learning paradigm. Empirical results on seven\nstandard semantic textual similarity (STS) tasks and a domain-shifted STS task\nboth show the effectiveness of our method compared with the current\nstate-of-the-art sentence embedding models. Our code is publicly avaliable at\nhttps:\/\/github.com\/YJiangcm\/PromCSE\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.0921,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000045366,
      "text":"Universal Conditional Masked Language Pre-training for Neural Machine\n  Translation\n\n  Pre-trained sequence-to-sequence models have significantly improved Neural\nMachine Translation (NMT). Different from prior works where pre-trained models\nusually adopt an unidirectional decoder, this paper demonstrates that\npre-training a sequence-to-sequence model but with a bidirectional decoder can\nproduce notable performance gains for both Autoregressive and\nNon-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked\nlanguage model pre-trained on large-scale bilingual and monolingual corpora in\nmany languages. We also introduce two simple but effective methods to enhance\nthe CeMAT, aligned code-switching & masking and dynamic dual-masking. We\nconduct extensive experiments and show that our CeMAT can achieve significant\nperformance improvement for all scenarios from low- to extremely high-resource\nlanguages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on\naverage for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it\ncan also produce consistent performance gains, i.e., up to +5.3 BLEU. To the\nbest of our knowledge, this is the first work to pre-train a unified model for\nfine-tuning on both NMT tasks. Code, data, and pre-trained models are available\nat https:\/\/github.com\/huawei-noah\/Pretrained-Language-Model\/tree\/master\/CeMAT.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.12254,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Chat-Capsule: A Hierarchical Capsule for Dialog-level Emotion Analysis\n\n  Many studies on dialog emotion analysis focus on utterance-level emotion\nonly. These models hence are not optimized for dialog-level emotion detection,\ni.e. to predict the emotion category of a dialog as a whole. More importantly,\nthese models cannot benefit from the context provided by the whole dialog. In\nreal-world applications, annotations to dialog could fine-grained, including\nboth utterance-level tags (e.g. speaker type, intent category, and emotion\ncategory), and dialog-level tags (e.g. user satisfaction, and emotion curve\ncategory). In this paper, we propose a Context-based Hierarchical Attention\nCapsule~(Chat-Capsule) model, which models both utterance-level and\ndialog-level emotions and their interrelations. On a dialog dataset collected\nfrom customer support of an e-commerce platform, our model is also able to\npredict user satisfaction and emotion curve category. Emotion curve refers to\nthe change of emotions along the development of a conversation. Experiments\nshow that the proposed Chat-Capsule outperform state-of-the-art baselines on\nboth benchmark dataset and proprietary dataset. Source code will be released\nupon acceptance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.13355,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000321203,
      "text":"Neighbors Are Not Strangers: Improving Non-Autoregressive Translation\n  under Low-Frequency Lexical Constraints\n\n  However, current autoregressive approaches suffer from high latency. In this\npaper, we focus on non-autoregressive translation (NAT) for this problem for\nits efficiency advantage. We identify that current constrained NAT models,\nwhich are based on iterative editing, do not handle low-frequency constraints\nwell. To this end, we propose a plug-in algorithm for this line of work, i.e.,\nAligned Constrained Training (ACT), which alleviates this problem by\nfamiliarizing the model with the source-side context of the constraints.\nExperiments on the general and domain datasets show that our model improves\nover the backbone constrained NAT model in constraint preservation and\ntranslation quality, especially for rare constraints.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.11218,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000103977,
      "text":"Learning to Win Lottery Tickets in BERT Transfer via Task-agnostic Mask\n  Training\n\n  Recent studies on the lottery ticket hypothesis (LTH) show that pre-trained\nlanguage models (PLMs) like BERT contain matching subnetworks that have similar\ntransfer learning performance as the original PLM. These subnetworks are found\nusing magnitude-based pruning. In this paper, we find that the BERT subnetworks\nhave even more potential than these studies have shown. Firstly, we discover\nthat the success of magnitude pruning can be attributed to the preserved\npre-training performance, which correlates with the downstream transferability.\nInspired by this, we propose to directly optimize the subnetwork structure\ntowards the pre-training objectives, which can better preserve the pre-training\nperformance. Specifically, we train binary masks over model weights on the\npre-training tasks, with the aim of preserving the universal transferability of\nthe subnetwork, which is agnostic to any specific downstream tasks. We then\nfine-tune the subnetworks on the GLUE benchmark and the SQuAD dataset. The\nresults show that, compared with magnitude pruning, mask training can\neffectively find BERT subnetworks with improved overall performance on\ndownstream tasks. Moreover, our method is also more efficient in searching\nsubnetworks and more advantageous when fine-tuning within a certain range of\ndata scarcity. Our code is available at https:\/\/github.com\/llyx97\/TAMT.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.12639,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000035763,
      "text":"Executive Function: A Contrastive Value Policy for Resampling and\n  Relabeling Perceptions via Hindsight Summarization?\n\n  We develop the few-shot continual learning task from first principles and\nhypothesize an evolutionary motivation and mechanism of action for executive\nfunction as a contrastive value policy which resamples and relabels perception\ndata via hindsight summarization to minimize attended prediction error, similar\nto an online prompt engineering problem. This is made feasible by the use of a\nmemory policy and a pretrained network with inductive biases for a grammar of\nlearning and is trained to maximize evolutionary survival. We show how this\nmodel of executive function can be used to implement hypothesis testing as a\nstream of consciousness and may explain observations of human few-shot learning\nand neuroanatomy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.03235,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000042054,
      "text":"A Review on Text-Based Emotion Detection -- Techniques, Applications,\n  Datasets, and Future Directions\n\n  Artificial Intelligence (AI) has been used for processing data to make\ndecisions, interact with humans, and understand their feelings and emotions.\nWith the advent of the internet, people share and express their thoughts on\nday-to-day activities and global and local events through text messaging\napplications. Hence, it is essential for machines to understand emotions in\nopinions, feedback, and textual dialogues to provide emotionally aware\nresponses to users in today's online world. The field of text-based emotion\ndetection (TBED) is advancing to provide automated solutions to various\napplications, such as businesses, and finances, to name a few. TBED has gained\na lot of attention in recent times. The paper presents a systematic literature\nreview of the existing literature published between 2005 to 2021 in TBED. This\nreview has meticulously examined 63 research papers from IEEE, Science Direct,\nScopus, and Web of Science databases to address four primary research\nquestions. It also reviews the different applications of TBED across various\nresearch domains and highlights its use. An overview of various emotion models,\ntechniques, feature extraction methods, datasets, and research challenges with\nfuture directions has also been represented.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.1207,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0001107322,
      "text":"Symlink: A New Dataset for Scientific Symbol-Description Linking\n\n  Mathematical symbols and descriptions appear in various forms across document\nsection boundaries without explicit markup. In this paper, we present a new\nlarge-scale dataset that emphasizes extracting symbols and descriptions in\nscientific documents. Symlink annotates scientific papers of 5 different\ndomains (i.e., computer science, biology, physics, mathematics, and economics).\nOur experiments on Symlink demonstrate the challenges of the symbol-description\nlinking task for existing models and call for further research effort in this\narea. We will publicly release Symlink to facilitate future research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.02624,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000049008,
      "text":"There Are a Thousand Hamlets in a Thousand People's Eyes: Enhancing\n  Knowledge-grounded Dialogue with Personal Memory\n\n  Knowledge-grounded conversation (KGC) shows great potential in building an\nengaging and knowledgeable chatbot, and knowledge selection is a key ingredient\nin it. However, previous methods for knowledge selection only concentrate on\nthe relevance between knowledge and dialogue context, ignoring the fact that\nage, hobby, education and life experience of an interlocutor have a major\neffect on his or her personal preference over external knowledge. Without\ntaking the personalization issue into account, it is difficult to select the\nproper knowledge and generate persona-consistent responses. In this work, we\nintroduce personal memory into knowledge selection in KGC to address the\npersonalization issue. We propose a variational method to model the underlying\nrelationship between one's personal memory and his or her selection of\nknowledge, and devise a learning scheme in which the forward mapping from\npersonal memory to knowledge and its inverse mapping is included in a closed\nloop so that they could teach each other. Experiment results show that our\nmethod outperforms existing KGC methods significantly on both automatic\nevaluation and human evaluation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.08083,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"AfriWOZ: Corpus for Exploiting Cross-Lingual Transferability for\n  Generation of Dialogues in Low-Resource, African Languages\n\n  Dialogue generation is an important NLP task fraught with many challenges.\nThe challenges become more daunting for low-resource African languages. To\nenable the creation of dialogue agents for African languages, we contribute the\nfirst high-quality dialogue datasets for 6 African languages: Swahili, Wolof,\nHausa, Nigerian Pidgin English, Kinyarwanda & Yor\\`ub\\'a. These datasets\nconsist of 1,500 turns each, which we translate from a portion of the English\nmulti-domain MultiWOZ dataset. Subsequently, we investigate & analyze the\neffectiveness of modelling through transfer learning by utilziing\nstate-of-the-art (SoTA) deep monolingual models: DialoGPT and BlenderBot. We\ncompare the models with a simple seq2seq baseline using perplexity. Besides\nthis, we conduct human evaluation of single-turn conversations by using\nmajority votes and measure inter-annotator agreement (IAA). We find that the\nhypothesis that deep monolingual models learn some abstractions that generalize\nacross languages holds. We observe human-like conversations, to different\ndegrees, in 5 out of the 6 languages. The language with the most transferable\nproperties is the Nigerian Pidgin English, with a human-likeness score of\n78.1%, of which 34.4% are unanimous. We freely provide the datasets and host\nthe model checkpoints\/demos on the HuggingFace hub for public access.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.05751,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000034107,
      "text":"Decomposed Meta-Learning for Few-Shot Named Entity Recognition\n\n  Few-shot named entity recognition (NER) systems aim at recognizing\nnovel-class named entities based on only a few labeled examples. In this paper,\nwe present a decomposed meta-learning approach which addresses the problem of\nfew-shot NER by sequentially tackling few-shot span detection and few-shot\nentity typing using meta-learning. In particular, we take the few-shot span\ndetection as a sequence labeling problem and train the span detector by\nintroducing the model-agnostic meta-learning (MAML) algorithm to find a good\nmodel parameter initialization that could fast adapt to new entity classes. For\nfew-shot entity typing, we propose MAML-ProtoNet, i.e., MAML-enhanced\nprototypical networks to find a good embedding space that can better\ndistinguish text span representations from different entity classes. Extensive\nexperiments on various benchmarks show that our approach achieves superior\nperformance over prior methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.04344,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000053644,
      "text":"Towards Better Chinese-centric Neural Machine Translation for\n  Low-resource Languages\n\n  The last decade has witnessed enormous improvements in science and\ntechnology, stimulating the growing demand for economic and cultural exchanges\nin various countries. Building a neural machine translation (NMT) system has\nbecome an urgent trend, especially in the low-resource setting. However, recent\nwork tends to study NMT systems for low-resource languages centered on English,\nwhile few works focus on low-resource NMT systems centered on other languages\nsuch as Chinese. To achieve this, the low-resource multilingual translation\nchallenge of the 2021 iFLYTEK AI Developer Competition provides the\nChinese-centric multilingual low-resource NMT tasks, where participants are\nrequired to build NMT systems based on the provided low-resource samples. In\nthis paper, we present the winner competition system that leverages monolingual\nword embeddings data enhancement, bilingual curriculum learning, and\ncontrastive re-ranking. In addition, a new Incomplete-Trust (In-trust) loss\nfunction is proposed to replace the traditional cross-entropy loss when\ntraining. The experimental results demonstrate that the implementation of these\nideas leads better performance than other state-of-the-art methods. All the\nexperimental codes are released at:\nhttps:\/\/github.com\/WENGSYX\/Low-resource-text-translation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.03954,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000082718,
      "text":"Are We Really Making Much Progress in Text Classification? A Comparative\n  Review\n\n  We analyze various methods for single-label and multi-label text\nclassification across well-known datasets, categorizing them into bag-of-words,\nsequence-based, graph-based, and hierarchical approaches. Despite the surge in\nmethods like graph-based models, encoder-only pre-trained language models,\nnotably BERT, remain state-of-the-art. However, recent findings suggest simpler\nmodels like logistic regression and trigram-based SVMs outperform newer\ntechniques. While decoder-only generative language models show promise in\nlearning with limited data, they lag behind encoder-only models in performance.\nWe emphasize the superiority of discriminative language models like BERT over\ngenerative models for supervised tasks. Additionally, we highlight the\nliterature's lack of robustness in method comparisons, particularly concerning\nbasic hyperparameter optimizations like learning rate in fine-tuning\nencoder-only language models.\n  Data availability: The source code is available at\nhttps:\/\/github.com\/drndr\/multilabel-text-clf\n  All datasets used for our experiments are publicly available except the NYT\ndataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.03953,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000253651,
      "text":"RubCSG at SemEval-2022 Task 5: Ensemble learning for identifying\n  misogynous MEMEs\n\n  This work presents an ensemble system based on various uni-modal and bi-modal\nmodel architectures developed for the SemEval 2022 Task 5: MAMI-Multimedia\nAutomatic Misogyny Identification. The challenge organizers provide an English\nmeme dataset to develop and train systems for identifying and classifying\nmisogynous memes. More precisely, the competition is separated into two\nsub-tasks: sub-task A asks for a binary decision as to whether a meme expresses\nmisogyny, while sub-task B is to classify misogynous memes into the potentially\noverlapping sub-categories of stereotype, shaming, objectification, and\nviolence. For our submission, we implement a new model fusion network and\nemploy an ensemble learning approach for better performance. With this\nstructure, we achieve a 0.755 macroaverage F1-score (11th) in sub-task A and a\n0.709 weighted-average F1-score (10th) in sub-task B.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.08689,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000581808,
      "text":"Generating Authentic Adversarial Examples beyond Meaning-preserving with\n  Doubly Round-trip Translation\n\n  Generating adversarial examples for Neural Machine Translation (NMT) with\nsingle Round-Trip Translation (RTT) has achieved promising results by releasing\nthe meaning-preserving restriction. However, a potential pitfall for this\napproach is that we cannot decide whether the generated examples are\nadversarial to the target NMT model or the auxiliary backward one, as the\nreconstruction error through the RTT can be related to either. To remedy this\nproblem, we propose a new criterion for NMT adversarial examples based on the\nDoubly Round-Trip Translation (DRTT). Specifically, apart from the\nsource-target-source RTT, we also consider the target-source-target one, which\nis utilized to pick out the authentic adversarial examples for the target NMT\nmodel. Additionally, to enhance the robustness of the NMT model, we introduce\nthe masked language models to construct bilingual adversarial pairs based on\nDRTT, which are used to train the NMT model directly. Extensive experiments on\nboth the clean and noisy test sets (including the artificial and natural noise)\nshow that our approach substantially improves the robustness of NMT models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.134,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Placing M-Phasis on the Plurality of Hate: A Feature-Based Corpus of\n  Hate Online\n\n  Even though hate speech (HS) online has been an important object of research\nin the last decade, most HS-related corpora over-simplify the phenomenon of\nhate by attempting to label user comments as \"hate\" or \"neutral\". This ignores\nthe complex and subjective nature of HS, which limits the real-life\napplicability of classifiers trained on these corpora. In this study, we\npresent the M-Phasis corpus, a corpus of ~9k German and French user comments\ncollected from migration-related news articles. It goes beyond the\n\"hate\"-\"neutral\" dichotomy and is instead annotated with 23 features, which in\ncombination become descriptors of various types of speech, ranging from\ncritical comments to implicit and explicit expressions of hate. The annotations\nare performed by 4 native speakers per language and achieve high (0.77 <= k <=\n1) inter-annotator agreements. Besides describing the corpus creation and\npresenting insights from a content, error and domain analysis, we explore its\ndata characteristics by training several classification baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.0498,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000195371,
      "text":"A Comparative Study of Pre-trained Encoders for Low-Resource Named\n  Entity Recognition\n\n  Pre-trained language models (PLM) are effective components of few-shot named\nentity recognition (NER) approaches when augmented with continued pre-training\non task-specific out-of-domain data or fine-tuning on in-domain data. However,\ntheir performance in low-resource scenarios, where such data is not available,\nremains an open question. We introduce an encoder evaluation framework, and use\nit to systematically compare the performance of state-of-the-art pre-trained\nrepresentations on the task of low-resource NER. We analyze a wide range of\nencoders pre-trained with different strategies, model architectures,\nintermediate-task fine-tuning, and contrastive learning. Our experimental\nresults across ten benchmark NER datasets in English and German show that\nencoder performance varies significantly, suggesting that the choice of encoder\nfor a specific low-resource scenario needs to be carefully evaluated.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.03071,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Urdu Morphology, Orthography and Lexicon Extraction\n\n  Urdu is a challenging language because of, first, its Perso-Arabic script and\nsecond, its morphological system having inherent grammatical forms and\nvocabulary of Arabic, Persian and the native languages of South Asia. This\npaper describes an implementation of the Urdu language as a software API, and\nwe deal with orthography, morphology and the extraction of the lexicon. The\nmorphology is implemented in a toolkit called Functional Morphology (Forsberg &\nRanta, 2004), which is based on the idea of dealing grammars as software\nlibraries. Therefore this implementation could be reused in applications such\nas intelligent search of keywords, language training and infrastructure for\nsyntax. We also present an implementation of a small part of Urdu syntax to\ndemonstrate this reusability.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.09081,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Named Entity Recognition for Partially Annotated Datasets\n\n  The most common Named Entity Recognizers are usually sequence taggers trained\non fully annotated corpora, i.e. the class of all words for all entities is\nknown. Partially annotated corpora, i.e. some but not all entities of some\ntypes are annotated, are too noisy for training sequence taggers since the same\nentity may be annotated one time with its true type but not another time,\nmisleading the tagger. Therefore, we are comparing three training strategies\nfor partially annotated datasets and an approach to derive new datasets for new\nclasses of entities from Wikipedia without time-consuming manual data\nannotation. In order to properly verify that our data acquisition and training\napproaches are plausible, we manually annotated test datasets for two new\nclasses, namely food and drugs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.05717,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Do Not Fire the Linguist: Grammatical Profiles Help Language Models\n  Detect Semantic Change\n\n  Morphological and syntactic changes in word usage (as captured, e.g., by\ngrammatical profiles) have been shown to be good predictors of a word's meaning\nchange. In this work, we explore whether large pre-trained contextualised\nlanguage models, a common tool for lexical semantic change detection, are\nsensitive to such morphosyntactic changes. To this end, we first compare the\nperformance of grammatical profiles against that of a multilingual neural\nlanguage model (XLM-R) on 10 datasets, covering 7 languages, and then combine\nthe two approaches in ensembles to assess their complementarity. Our results\nshow that ensembling grammatical profiles with XLM-R improves semantic change\ndetection performance for most datasets and languages. This indicates that\nlanguage models do not fully cover the fine-grained morphological and syntactic\nsignals that are explicitly represented in grammatical profiles.\n  An interesting exception are the test sets where the time spans under\nanalysis are much longer than the time gap between them (for example,\ncentury-long spans with a one-year gap between them). Morphosyntactic change is\nslow so grammatical profiles do not detect in such cases. In contrast, language\nmodels, thanks to their access to lexical information, are able to detect fast\ntopical changes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.10227,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000400013,
      "text":"Label Anchored Contrastive Learning for Language Understanding\n\n  Contrastive learning (CL) has achieved astonishing progress in computer\nvision, speech, and natural language processing fields recently with\nself-supervised learning. However, CL approach to the supervised setting is not\nfully explored, especially for the natural language understanding\nclassification task. Intuitively, the class label itself has the intrinsic\nability to perform hard positive\/negative mining, which is crucial for CL.\nMotivated by this, we propose a novel label anchored contrastive learning\napproach (denoted as LaCon) for language understanding. Specifically, three\ncontrastive objectives are devised, including a multi-head instance-centered\ncontrastive loss (ICL), a label-centered contrastive loss (LCL), and a label\nembedding regularizer (LER). Our approach does not require any specialized\nnetwork architecture or any extra data augmentation, thus it can be easily\nplugged into existing powerful pre-trained language models. Compared to the\nstate-of-the-art baselines, LaCon obtains up to 4.1% improvement on the popular\ndatasets of GLUE and CLUE benchmarks. Besides, LaCon also demonstrates\nsignificant advantages under the few-shot and data imbalance settings, which\nobtains up to 9.4% improvement on the FewGLUE and FewCLUE benchmarking tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.10878,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000084771,
      "text":"ChapterBreak: A Challenge Dataset for Long-Range Language Models\n\n  While numerous architectures for long-range language models (LRLMs) have\nrecently been proposed, a meaningful evaluation of their discourse-level\nlanguage understanding capabilities has not yet followed. To this end, we\nintroduce ChapterBreak, a challenge dataset that provides an LRLM with a long\nsegment from a narrative that ends at a chapter boundary and asks it to\ndistinguish the beginning of the ground-truth next chapter from a set of\nnegative segments sampled from the same narrative. A fine-grained human\nannotation reveals that our dataset contains many complex types of chapter\ntransitions (e.g., parallel narratives, cliffhanger endings) that require\nprocessing global context to comprehend. Experiments on ChapterBreak show that\nexisting LRLMs fail to effectively leverage long-range context, substantially\nunderperforming a segment-level model trained directly for this task. We\npublicly release our ChapterBreak dataset to spur more principled future\nresearch into LRLMs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.00333,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"PriMock57: A Dataset Of Primary Care Mock Consultations\n\n  Recent advances in Automatic Speech Recognition (ASR) have made it possible\nto reliably produce automatic transcripts of clinician-patient conversations.\nHowever, access to clinical datasets is heavily restricted due to patient\nprivacy, thus slowing down normal research practices. We detail the development\nof a public access, high quality dataset comprising of57 mocked primary care\nconsultations, including audio recordings, their manual utterance-level\ntranscriptions, and the associated consultation notes. Our work illustrates how\nthe dataset can be used as a benchmark for conversational medical ASR as well\nas consultation note generation from transcripts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.13183,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000024504,
      "text":"Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach\n\n  Pre-trained models (PTMs) have lead to great improvements in natural language\ngeneration (NLG). However, it is still unclear how much commonsense knowledge\nthey possess. With the goal of evaluating commonsense knowledge of NLG models,\nrecent work has proposed the problem of generative commonsense reasoning, e.g.,\nto compose a logical sentence given a set of unordered concepts. Existing\napproaches to this problem hypothesize that PTMs lack sufficient parametric\nknowledge for this task, which can be overcome by introducing external\nknowledge or task-specific pre-training objectives. Different from this trend,\nwe argue that PTM's inherent ability for generative commonsense reasoning is\nunderestimated due to the order-agnostic property of its input. In particular,\nwe hypothesize that the order of the input concepts can affect the PTM's\nability to utilize its commonsense knowledge. To this end, we propose a\npre-ordering approach to elaborately manipulate the order of the given concepts\nbefore generation. Experiments show that our approach can outperform the more\nsophisticated models that have access to a lot of external data and resources.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.0404,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000052982,
      "text":"ProQA: Structural Prompt-based Pre-training for Unified Question\n  Answering\n\n  Question Answering (QA) is a longstanding challenge in natural language\nprocessing. Existing QA works mostly focus on specific question types,\nknowledge domains, or reasoning skills. The specialty in QA research hinders\nsystems from modeling commonalities between tasks and generalization for wider\napplications. To address this issue, we present ProQA, a unified QA paradigm\nthat solves various tasks through a single model. ProQA takes a unified\nstructural prompt as the bridge and improves the QA-centric ability by\nstructural prompt-based pre-training. Through a structurally designed\nprompt-based input schema, ProQA concurrently models the knowledge\ngeneralization for all QA tasks while keeping the knowledge customization for\nevery specific QA task. Furthermore, ProQA is pre-trained with structural\nprompt-formatted large-scale synthesized corpus, which empowers the model with\nthe commonly-required QA ability. Experimental results on 11 QA benchmarks\ndemonstrate that ProQA consistently boosts performance on both full data\nfine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore,\nProQA exhibits strong ability in both continual learning and transfer learning\nby taking the advantages of the structural prompt.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.08124,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000024835,
      "text":"When to Use Multi-Task Learning vs Intermediate Fine-Tuning for\n  Pre-Trained Encoder Transfer Learning\n\n  Transfer learning (TL) in natural language processing (NLP) has seen a surge\nof interest in recent years, as pre-trained models have shown an impressive\nability to transfer to novel tasks. Three main strategies have emerged for\nmaking use of multiple supervised datasets during fine-tuning: training on an\nintermediate task before training on the target task (STILTs), using multi-task\nlearning (MTL) to train jointly on a supplementary task and the target task\n(pairwise MTL), or simply using MTL to train jointly on all available datasets\n(MTL-ALL). In this work, we compare all three TL methods in a comprehensive\nanalysis on the GLUE dataset suite. We find that there is a simple heuristic\nfor when to use one of these techniques over the other: pairwise MTL is better\nthan STILTs when the target task has fewer instances than the supporting task\nand vice versa. We show that this holds true in more than 92% of applicable\ncases on the GLUE dataset and validate this hypothesis with experiments varying\ndataset size. The simplicity and effectiveness of this heuristic is surprising\nand warrants additional exploration by the TL community. Furthermore, we find\nthat MTL-ALL is worse than the pairwise methods in almost every case. We hope\nthis study will aid others as they choose between TL methods for NLP tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.0796,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"Meta AI at Arabic Hate Speech 2022: MultiTask Learning with\n  Self-Correction for Hate Speech Classification\n\n  In this paper, we tackle the Arabic Fine-Grained Hate Speech Detection shared\ntask and demonstrate significant improvements over reported baselines for its\nthree subtasks. The tasks are to predict if a tweet contains (1) Offensive\nlanguage; and whether it is considered (2) Hate Speech or not and if so, then\npredict the (3) Fine-Grained Hate Speech label from one of six categories. Our\nfinal solution is an ensemble of models that employs multitask learning and a\nself-consistency correction method yielding 82.7% on the hate speech subtask --\nreflecting a 3.4% relative improvement compared to previous work.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.12507,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000019537,
      "text":"Re-Examining Calibration: The Case of Question Answering\n\n  For users to trust model predictions, they need to understand model outputs,\nparticularly their confidence - calibration aims to adjust (calibrate) models'\nconfidence to match expected accuracy. We argue that the traditional\ncalibration evaluation does not promote effective calibrations: for example, it\ncan encourage always assigning a mediocre confidence score to all predictions,\nwhich does not help users distinguish correct predictions from wrong ones.\nBuilding on those observations, we propose a new calibration metric, MacroCE,\nthat better captures whether the model assigns low confidence to wrong\npredictions and high confidence to correct predictions. Focusing on the\npractical application of open-domain question answering, we examine\nconventional calibration methods applied on the widely-used retriever-reader\npipeline, all of which do not bring significant gains under our new MacroCE\nmetric. Toward better calibration, we propose a new calibration method\n(ConsCal) that uses not just final model predictions but whether multiple model\ncheckpoints make consistent predictions. Altogether, we provide an alternative\nview of calibration along with a new metric, re-evaluation of existing\ncalibration methods on our metric, and proposal of a more effective calibration\nmethod.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.12368,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Medical Scientific Table-to-Text Generation with Human-in-the-Loop under\n  the Data Sparsity Constraint\n\n  Structured (tabular) data in the preclinical and clinical domains contains\nvaluable information about individuals and an efficient table-to-text\nsummarization system can drastically reduce manual efforts to condense this\ndata into reports. However, in practice, the problem is heavily impeded by the\ndata paucity, data sparsity and inability of the state-of-the-art natural\nlanguage generation models (including T5, PEGASUS and GPT-Neo) to produce\naccurate and reliable outputs. In this paper, we propose a novel table-to-text\napproach and tackle these problems with a novel two-step architecture which is\nenhanced by auto-correction, copy mechanism and synthetic data augmentation.\nThe study shows that the proposed approach selects salient biomedical entities\nand values from structured data with improved precision (up to 0.13 absolute\nincrease) of copying the tabular values to generate coherent and accurate text\nfor assay validation reports and toxicology reports. Moreover, we also\ndemonstrate a light-weight adaptation of the proposed system to new datasets by\nfine-tuning with as little as 40\\% training examples. The outputs of our model\nare validated by human experts in the Human-in-the-Loop scenario.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.08891,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"A Scalable Workflow to Build Machine Learning Classifiers with\n  Clinician-in-the-Loop to Identify Patients in Specific Diseases\n\n  Clinicians may rely on medical coding systems such as International\nClassification of Diseases (ICD) to identify patients with diseases from\nElectronic Health Records (EHRs). However, due to the lack of detail and\nspecificity as well as a probability of miscoding, recent studies suggest the\nICD codes often cannot characterise patients accurately for specific diseases\nin real clinical practice, and as a result, using them to find patients for\nstudies or trials can result in high failure rates and missing out on uncoded\npatients. Manual inspection of all patients at scale is not feasible as it is\nhighly costly and slow.\n  This paper proposes a scalable workflow which leverages both structured data\nand unstructured textual notes from EHRs with techniques including NLP, AutoML\nand Clinician-in-the-Loop mechanism to build machine learning classifiers to\nidentify patients at scale with given diseases, especially those who might\ncurrently be miscoded or missed by ICD codes.\n  Case studies in the MIMIC-III dataset were conducted where the proposed\nworkflow demonstrates a higher classification performance in terms of F1 scores\ncompared to simply using ICD codes on gold testing subset to identify patients\nwith Ovarian Cancer (0.901 vs 0.814), Lung Cancer (0.859 vs 0.828), Cancer\nCachexia (0.862 vs 0.650), and Lupus Nephritis (0.959 vs 0.855). Also, the\nproposed workflow that leverages unstructured notes consistently outperforms\nthe baseline that uses structured data only with an increase of F1 (Ovarian\nCancer 0.901 vs 0.719, Lung Cancer 0.859 vs 0.787, Cancer Cachexia 0.862 vs\n0.838 and Lupus Nephritis 0.959 vs 0.785). Experiments on the large testing set\nalso demonstrate the proposed workflow can find more patients who are miscoded\nor missed by ICD codes. Moreover, interpretability studies are also conducted\nto clinically validate the top impact features of the classifiers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.13708,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000041723,
      "text":"HiJoNLP at SemEval-2022 Task 2: Detecting Idiomaticity of Multiword\n  Expressions using Multilingual Pretrained Language Models\n\n  This paper describes an approach to detect idiomaticity only from the\ncontextualized representation of a MWE over multilingual pretrained language\nmodels. Our experiments find that larger models are usually more effective in\nidiomaticity detection. However, using a higher layer of the model may not\nguarantee a better performance. In multilingual scenarios, the convergence of\ndifferent languages are not consistent and rich-resource languages have big\nadvantages over other languages.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.14333,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000058611,
      "text":"One Reference Is Not Enough: Diverse Distillation with Reference\n  Selection for Non-Autoregressive Translation\n\n  Non-autoregressive neural machine translation (NAT) suffers from the\nmulti-modality problem: the source sentence may have multiple correct\ntranslations, but the loss function is calculated only according to the\nreference sentence. Sequence-level knowledge distillation makes the target more\ndeterministic by replacing the target with the output from an autoregressive\nmodel. However, the multi-modality problem in the distilled dataset is still\nnonnegligible. Furthermore, learning from a specific teacher limits the upper\nbound of the model capability, restricting the potential of NAT models. In this\npaper, we argue that one reference is not enough and propose diverse\ndistillation with reference selection (DDRS) for NAT. Specifically, we first\npropose a method called SeedDiv for diverse machine translation, which enables\nus to generate a dataset containing multiple high-quality reference\ntranslations for each source sentence. During the training, we compare the NAT\noutput with all references and select the one that best fits the NAT output to\ntrain the model. Experiments on widely-used machine translation benchmarks\ndemonstrate the effectiveness of DDRS, which achieves 29.82 BLEU with only one\ndecoding pass on WMT14 En-De, improving the state-of-the-art performance for\nNAT by over 1 BLEU. Source code: https:\/\/github.com\/ictnlp\/DDRS-NAT\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.08221,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000073844,
      "text":"Efficient Unsupervised Sentence Compression by Fine-tuning Transformers\n  with Reinforcement Learning\n\n  Sentence compression reduces the length of text by removing non-essential\ncontent while preserving important facts and grammaticality. Unsupervised\nobjective driven methods for sentence compression can be used to create\ncustomized models without the need for ground-truth training data, while\nallowing flexibility in the objective function(s) that are used for learning\nand inference. Recent unsupervised sentence compression approaches use custom\nobjectives to guide discrete search; however, guided search is expensive at\ninference time. In this work, we explore the use of reinforcement learning to\ntrain effective sentence compression models that are also fast when generating\npredictions. In particular, we cast the task as binary sequence labelling and\nfine-tune a pre-trained transformer using a simple policy gradient approach.\nOur approach outperforms other unsupervised models while also being more\nefficient at inference time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.10938,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000002318,
      "text":"Diversity Enhanced Table-to-Text Generation via Type Control\n\n  Generating natural language statements to convey logical inferences from\ntabular data (i.e., Logical NLG) is a process with one input and a variety of\nvalid outputs. This characteristic underscores the need for a method to produce\na diverse set of valid outputs, presenting different perspectives of the input\ndata. We propose a simple yet effective diversity-enhancing scheme that builds\nupon an inherent property of the statements, their logic-types, by using a\ntype-controlled table-to-text generation model. We demonstrate, through\nextensive automatic and human evaluations over the two publicly available\nLogical NLG datasets, that our proposed method both facilitates the ability to\neffectively control the generated statement type, and produces results superior\nto the strongest baselines in terms of quality and factuality-diversity\ntrade-off.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.03766,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000307957,
      "text":"Scheduled Multi-task Learning for Neural Chat Translation\n\n  Neural Chat Translation (NCT) aims to translate conversational text into\ndifferent languages. Existing methods mainly focus on modeling the bilingual\ndialogue characteristics (e.g., coherence) to improve chat translation via\nmulti-task learning on small-scale chat translation data. Although the NCT\nmodels have achieved impressive success, it is still far from satisfactory due\nto insufficient chat translation data and simple joint training manners. To\naddress the above issues, we propose a scheduled multi-task learning framework\nfor NCT. Specifically, we devise a three-stage training framework to\nincorporate the large-scale in-domain chat translation data into training by\nadding a second pre-training stage between the original pre-training and\nfine-tuning stages. Further, we investigate where and how to schedule the\ndialogue-related auxiliary tasks in multiple training stages to effectively\nenhance the main chat translation task. Extensive experiments in four language\ndirections (English-Chinese and English-German) verify the effectiveness and\nsuperiority of the proposed approach. Additionally, we have made the\nlarge-scale in-domain paired bilingual dialogue dataset publicly available to\nthe research community.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.12209,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start\n\n  We present EdiT5 - a novel semi-autoregressive text-editing model designed to\ncombine the strengths of non-autoregressive text-editing and autoregressive\ndecoding. EdiT5 is faster during inference than conventional\nsequence-to-sequence (seq2seq) models, while being capable of modelling\nflexible input-output transformations.\n  This is achieved by decomposing the generation process into three sub-tasks:\n(1) tagging to decide on the subset of input tokens to be preserved in the\noutput, (2) re-ordering to define their order in the output text, and (3)\ninsertion to infill the missing tokens that are not present in the input. The\ntagging and re-ordering steps, which are responsible for generating the largest\nportion of the output, are non-autoregressive, while the insertion step uses an\nautoregressive decoder.\n  Depending on the task, EdiT5 on average requires significantly fewer\nautoregressive steps, demonstrating speedups of up to 25x when compared to\nseq2seq models. Quality-wise, EdiT5 is initialized with a pre-trained T5\ncheckpoint yielding comparable performance to T5 in high-resource settings when\nevaluated on three NLG tasks: Sentence Fusion, Grammatical Error Correction,\nand Decontextualization while clearly outperforming T5 in low-resource\nsettings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.12382,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000195702,
      "text":"Challenges and Opportunities in Information Manipulation Detection: An\n  Examination of Wartime Russian Media\n\n  NLP research on public opinion manipulation campaigns has primarily focused\non detecting overt strategies such as fake news and disinformation. However,\ninformation manipulation in the ongoing Russia-Ukraine war exemplifies how\ngovernments and media also employ more nuanced strategies. We release a new\ndataset, VoynaSlov, containing 38M+ posts from Russian media outlets on Twitter\nand VKontakte, as well as public activity and responses, immediately preceding\nand during the 2022 Russia-Ukraine war. We apply standard and\nrecently-developed NLP models on VoynaSlov to examine agenda setting, framing,\nand priming, several strategies underlying information manipulation, and reveal\nvariation across media outlet control, social media platform, and time. Our\nexamination of these media effects and extensive discussion of current\napproaches' limitations encourage further development of NLP models for\nunderstanding information manipulation in emerging crises, as well as other\nreal-world and interdisciplinary tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.10183,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000092718,
      "text":"Prototypical Calibration for Few-shot Learning of Language Models\n\n  In-context learning of GPT-like models has been recognized as fragile across\ndifferent hand-crafted templates, and demonstration permutations. In this work,\nwe propose prototypical calibration to adaptively learn a more robust decision\nboundary for zero- and few-shot classification, instead of greedy decoding.\nConcretely, our method first adopts Gaussian mixture distribution to estimate\nthe prototypical clusters for all categories. Then we assign each cluster to\nthe corresponding label by solving a weighted bipartite matching problem. Given\nan example, its prediction is calibrated by the likelihood of prototypical\nclusters. Experimental results show that prototypical calibration yields a\nsubstantial improvement on a diverse set of tasks. Extensive analysis across\ndifferent scales also indicates that our method calibrates the decision\nboundary as expected, greatly improving the robustness of GPT to templates,\npermutations, and class imbalance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.11162,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000019206,
      "text":"A Self-Paced Mixed Distillation Method for Non-Autoregressive Generation\n\n  Non-Autoregressive generation is a sequence generation paradigm, which\nremoves the dependency between target tokens. It could efficiently reduce the\ntext generation latency with parallel decoding in place of token-by-token\nsequential decoding. However, due to the known multi-modality problem,\nNon-Autoregressive (NAR) models significantly under-perform Auto-regressive\n(AR) models on various language generation tasks. Among the NAR models, BANG is\nthe first large-scale pre-training model on English un-labeled raw text corpus.\nIt considers different generation paradigms as its pre-training tasks including\nAuto-regressive (AR), Non-Autoregressive (NAR), and semi-Non-Autoregressive\n(semi-NAR) information flow with multi-stream strategy. It achieves\nstate-of-the-art performance without any distillation techniques. However, AR\ndistillation has been shown to be a very effective solution for improving NAR\nperformance. In this paper, we propose a novel self-paced mixed distillation\nmethod to further improve the generation quality of BANG. Firstly, we propose\nthe mixed distillation strategy based on the AR stream knowledge. Secondly, we\nencourage the model to focus on the samples with the same modality by\nself-paced learning. The proposed self-paced mixed distillation algorithm\nimproves the generation quality and has no influence on the inference latency.\nWe carry out extensive experiments on summarization and question generation\ntasks to validate the effectiveness. To further illustrate the commercial value\nof our approach, we conduct experiments on three generation tasks in real-world\nadvertisements applications. Experimental results on commercial data show the\neffectiveness of the proposed model. Compared with BANG, it achieves\nsignificant BLEU score improvement. On the other hand, compared with\nauto-regressive generation method, it achieves more than 7x speedup.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.09634,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000030796,
      "text":"Phylogeny-Inspired Adaptation of Multilingual Models to New Languages\n\n  Large pretrained multilingual models, trained on dozens of languages, have\ndelivered promising results due to cross-lingual learning capabilities on\nvariety of language tasks. Further adapting these models to specific languages,\nespecially ones unseen during pre-training, is an important goal towards\nexpanding the coverage of language technologies. In this study, we show how we\ncan use language phylogenetic information to improve cross-lingual transfer\nleveraging closely related languages in a structured, linguistically-informed\nmanner. We perform adapter-based training on languages from diverse language\nfamilies (Germanic, Uralic, Tupian, Uto-Aztecan) and evaluate on both syntactic\nand semantic tasks, obtaining more than 20% relative performance improvements\nover strong commonly used baselines, especially on languages unseen during\npre-training.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.09314,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"Target-Guided Dialogue Response Generation Using Commonsense and Data\n  Augmentation\n\n  Target-guided response generation enables dialogue systems to smoothly\ntransition a conversation from a dialogue context toward a target sentence.\nSuch control is useful for designing dialogue systems that direct a\nconversation toward specific goals, such as creating non-obtrusive\nrecommendations or introducing new topics in the conversation. In this paper,\nwe introduce a new technique for target-guided response generation, which first\nfinds a bridging path of commonsense knowledge concepts between the source and\nthe target, and then uses the identified bridging path to generate transition\nresponses. Additionally, we propose techniques to re-purpose existing dialogue\ndatasets for target-guided generation. Experiments reveal that the proposed\ntechniques outperform various baselines on this task. Finally, we observe that\nthe existing automated metrics for this task correlate poorly with human\njudgement ratings. We propose a novel evaluation metric that we demonstrate is\nmore reliable for target-guided response evaluation. Our work generally enables\ndialogue system designers to exercise more control over the conversations that\ntheir systems produce.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.03403,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000001457,
      "text":"A Data Cartography based MixUp for Pre-trained Language Models\n\n  MixUp is a data augmentation strategy where additional samples are generated\nduring training by combining random pairs of training samples and their labels.\nHowever, selecting random pairs is not potentially an optimal choice. In this\nwork, we propose TDMixUp, a novel MixUp strategy that leverages Training\nDynamics and allows more informative samples to be combined for generating new\ndata samples. Our proposed TDMixUp first measures confidence, variability,\n(Swayamdipta et al., 2020), and Area Under the Margin (AUM) (Pleiss et al.,\n2020) to identify the characteristics of training samples (e.g., as\neasy-to-learn or ambiguous samples), and then interpolates these characterized\nsamples. We empirically validate that our method not only achieves competitive\nperformance using a smaller subset of the training data compared with strong\nbaselines, but also yields lower expected calibration error on the pre-trained\nlanguage model, BERT, on both in-domain and out-of-domain settings in a wide\nrange of NLP tasks. We publicly release our code.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.1085,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000080135,
      "text":"AFEC: A Knowledge Graph Capturing Social Intelligence in Casual\n  Conversations\n\n  This paper introduces AFEC, an automatically curated knowledge graph based on\npeople's day-to-day casual conversations. The knowledge captured in this graph\nbears potential for conversational systems to understand how people offer\nacknowledgement, consoling, and a wide range of empathetic responses in social\nconversations. For this body of knowledge to be comprehensive and meaningful,\nwe curated a large-scale corpus from the r\/CasualConversation SubReddit. After\ntaking the first two turns of all conversations, we obtained 134K speaker nodes\nand 666K listener nodes. To demonstrate how a chatbot can converse in social\nsettings, we built a retrieval-based chatbot and compared it with existing\nempathetic dialog models. Experiments show that our model is capable of\ngenerating much more diverse responses (at least 15% higher diversity scores in\nhuman evaluation), while still outperforming two out of the four baselines in\nterms of response quality.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.04937,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000452995,
      "text":"Generate, Evaluate, and Select: A Dialogue System with a Response\n  Evaluator for Diversity-Aware Response Generation\n\n  We aim to overcome the lack of diversity in responses of current dialogue\nsystems and to develop a dialogue system that is engaging as a conversational\npartner. We propose a generator-evaluator model that evaluates multiple\nresponses generated by a response generator and selects the best response by an\nevaluator. By generating multiple responses, we obtain diverse responses. We\nconduct human evaluations to compare the output of the proposed system with\nthat of a baseline system. The results of the human evaluations showed that the\nproposed system's responses were often judged to be better than the baseline\nsystem's, and indicated the effectiveness of the proposed method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.11332,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000038081,
      "text":"DP-Parse: Finding Word Boundaries from Raw Speech with an Instance\n  Lexicon\n\n  Finding word boundaries in continuous speech is challenging as there is\nlittle or no equivalent of a 'space' delimiter between words. Popular Bayesian\nnon-parametric models for text segmentation use a Dirichlet process to jointly\nsegment sentences and build a lexicon of word types. We introduce DP-Parse,\nwhich uses similar principles but only relies on an instance lexicon of word\ntokens, avoiding the clustering errors that arise with a lexicon of word types.\nOn the Zero Resource Speech Benchmark 2017, our model sets a new speech\nsegmentation state-of-the-art in 5 languages. The algorithm monotonically\nimproves with better input representations, achieving yet higher scores when\nfed with weakly supervised inputs. Despite lacking a type lexicon, DP-Parse can\nbe pipelined to a language model and learn semantic and syntactic\nrepresentations as assessed by a new spoken word embedding benchmark.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.02608,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"What do tokens know about their characters and how do they know it?\n\n  Pre-trained language models (PLMs) that use subword tokenization schemes can\nsucceed at a variety of language tasks that require character-level\ninformation, despite lacking explicit access to the character composition of\ntokens. Here, studying a range of models (e.g., GPT- J, BERT, RoBERTa, GloVe),\nwe probe what word pieces encode about character-level information by training\nclassifiers to predict the presence or absence of a particular alphabetical\ncharacter in a token, based on its embedding (e.g., probing whether the model\nembedding for \"cat\" encodes that it contains the character \"a\"). We find that\nthese models robustly encode character-level information and, in general,\nlarger models perform better at the task. We show that these results generalize\nto characters from non-Latin alphabets (Arabic, Devanagari, and Cyrillic).\nThen, through a series of experiments and analyses, we investigate the\nmechanisms through which PLMs acquire English-language character information\nduring training and argue that this knowledge is acquired through multiple\nphenomena, including a systematic relationship between particular characters\nand particular parts of speech, as well as natural variability in the\ntokenization of related strings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.04925,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000128481,
      "text":"RuCoCo: a new Russian corpus with coreference annotation\n\n  We present a new corpus with coreference annotation, Russian Coreference\nCorpus (RuCoCo). The goal of RuCoCo is to obtain a large number of annotated\ntexts while maintaining high inter-annotator agreement. RuCoCo contains news\ntexts in Russian, part of which were annotated from scratch, and for the rest\nthe machine-generated annotations were refined by human annotators. The size of\nour corpus is one million words and around 150,000 mentions. We make the corpus\npublicly available.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.0433,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000093712,
      "text":"Predicting Embedding Reliability in Low-Resource Settings Using Corpus\n  Similarity Measures\n\n  This paper simulates a low-resource setting across 17 languages in order to\nevaluate embedding similarity, stability, and reliability under different\nconditions. The goal is to use corpus similarity measures before training to\npredict properties of embeddings after training. The main contribution of the\npaper is to show that it is possible to predict downstream embedding similarity\nusing upstream corpus similarity measures. This finding is then applied to\nlow-resource settings by modelling the reliability of embeddings created from\nvery limited training data. Results show that it is possible to estimate the\nreliability of low-resource embeddings using corpus similarity measures that\nremain robust on small amounts of data. These findings have significant\nimplications for the evaluation of truly low-resource languages in which such\nsystematic downstream validation methods are not possible because of data\nlimitations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.03529,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000071526,
      "text":"How to Dissect a Muppet: The Structure of Transformer Embedding Spaces\n\n  Pretrained embeddings based on the Transformer architecture have taken the\nNLP community by storm. We show that they can mathematically be reframed as a\nsum of vector factors and showcase how to use this reframing to study the\nimpact of each component. We provide evidence that multi-head attentions and\nfeed-forwards are not equally useful in all downstream applications, as well as\na quantitative overview of the effects of finetuning on the overall embedding\nspace. This approach allows us to draw connections to a wide range of previous\nstudies, from vector space anisotropy to attention weights.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.14913,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000077486,
      "text":"GPTs at Factify 2022: Prompt Aided Fact-Verification\n\n  One of the most pressing societal issues is the fight against false news. The\nfalse claims, as difficult as they are to expose, create a lot of damage. To\ntackle the problem, fact verification becomes crucial and thus has been a topic\nof interest among diverse research communities. Using only the textual form of\ndata we propose our solution to the problem and achieve competitive results\nwith other approaches. We present our solution based on two approaches - PLM\n(pre-trained language model) based method and Prompt based method. The\nPLM-based approach uses the traditional supervised learning, where the model is\ntrained to take 'x' as input and output prediction 'y' as P(y|x). Whereas,\nPrompt-based learning reflects the idea to design input to fit the model such\nthat the original objective may be re-framed as a problem of (masked) language\nmodeling. We may further stimulate the rich knowledge provided by PLMs to\nbetter serve downstream tasks by employing extra prompts to fine-tune PLMs. Our\nexperiments showed that the proposed method performs better than just\nfine-tuning PLMs. We achieved an F1 score of 0.6946 on the FACTIFY dataset and\na 7th position on the competition leader-board.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.09249,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0002513991,
      "text":"RuArg-2022: Argument Mining Evaluation\n\n  Argumentation analysis is a field of computational linguistics that studies\nmethods for extracting arguments from texts and the relationships between them,\nas well as building argumentation structure of texts. This paper is a report of\nthe organizers on the first competition of argumentation analysis systems\ndealing with Russian language texts within the framework of the Dialogue\nconference. During the competition, the participants were offered two tasks:\nstance detection and argument classification. A corpus containing 9,550\nsentences (comments on social media posts) on three topics related to the\nCOVID-19 pandemic (vaccination, quarantine, and wearing masks) was prepared,\nannotated, and used for training and testing. The system that won the first\nplace in both tasks used the NLI (Natural Language Inference) variant of the\nBERT architecture, automatic translation into English to apply a specialized\nBERT model, retrained on Twitter posts discussing COVID-19, as well as\nadditional masking of target entities. This system showed the following\nresults: for the stance detection task an F1-score of 0.6968, for the argument\nclassification task an F1-score of 0.7404. We hope that the prepared dataset\nand baselines will help to foster further research on argument mining for the\nRussian language.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.09248,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000060929,
      "text":"Collocation2Text: Controllable Text Generation from Guide Phrases in\n  Russian\n\n  Large pre-trained language models are capable of generating varied and fluent\ntexts. Starting from the prompt, these models generate a narrative that can\ndevelop unpredictably. The existing methods of controllable text generation,\nwhich guide the narrative in the text in the user-specified direction, require\ncreating a training corpus and an additional time-consuming training procedure.\nThe paper proposes and investigates Collocation2Text, a plug-and-play method\nfor automatic controllable text generation in Russian, which does not require\nfine-tuning. The method is based on two interacting models: the autoregressive\nlanguage ruGPT-3 model and the autoencoding language ruRoBERTa model. The idea\nof the method is to shift the output distribution of the autoregressive model\naccording to the output distribution of the autoencoding model in order to\nensure a coherent transition of the narrative in the text towards the guide\nphrase, which can contain single words or collocations. The autoencoding model,\nwhich is able to take into account the left and right contexts of the token,\n\"tells\" the autoregressive model which tokens are the most and least logical at\nthe current generation step, increasing or decreasing the probabilities of the\ncorresponding tokens. The experiments on generating news articles using the\nproposed method showed its effectiveness for automatically generated fluent\ntexts which contain coherent transitions between user-specified phrases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.02014,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Actuarial Applications of Natural Language Processing Using\n  Transformers: Case Studies for Using Text Features in an Actuarial Context\n\n  This tutorial demonstrates workflows to incorporate text data into actuarial\nclassification and regression tasks. The main focus is on methods employing\ntransformer-based models. A dataset of car accident descriptions with an\naverage length of 400 words, available in English and German, and a dataset\nwith short property insurance claims descriptions are used to demonstrate these\ntechniques. The case studies tackle challenges related to a multi-lingual\nsetting and long input sequences. They also show ways to interpret model\noutput, to assess and improve model performance, by fine-tuning the models to\nthe domain of application or to a specific prediction task. Finally, the\ntutorial provides practical approaches to handle classification tasks in\nsituations with no or only few labeled data, including but not limited to\nChatGPT. The results achieved by using the language-understanding skills of\noff-the-shelf natural language processing (NLP) models with only minimal\npre-processing and fine-tuning clearly demonstrate the power of transfer\nlearning for practical applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.08415,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000045035,
      "text":"CS-UM6P at SemEval-2022 Task 6: Transformer-based Models for Intended\n  Sarcasm Detection in English and Arabic\n\n  Sarcasm is a form of figurative language where the intended meaning of a\nsentence differs from its literal meaning. This poses a serious challenge to\nseveral Natural Language Processing (NLP) applications such as Sentiment\nAnalysis, Opinion Mining, and Author Profiling. In this paper, we present our\nparticipating system to the intended sarcasm detection task in English and\nArabic languages. Our system\\footnote{The source code of our system is\navailable at \\url{https:\/\/github.com\/AbdelkaderMH\/iSarcasmEval}} consists of\nthree deep learning-based models leveraging two existing pre-trained language\nmodels for Arabic and English. We have participated in all sub-tasks. Our\nofficial submissions achieve the best performance on sub-task A for Arabic\nlanguage and rank second in sub-task B. For sub-task C, our system is ranked\n7th and 11th on Arabic and English datasets, respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.07296,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000028809,
      "text":"Enhanced Knowledge Selection for Grounded Dialogues via Document\n  Semantic Graphs\n\n  Providing conversation models with background knowledge has been shown to\nmake open-domain dialogues more informative and engaging. Existing models treat\nknowledge selection as a sentence ranking or classification problem where each\nsentence is handled individually, ignoring the internal semantic connection\namong sentences in the background document. In this work, we propose to\nautomatically convert the background knowledge documents into document semantic\ngraphs and then perform knowledge selection over such graphs. Our document\nsemantic graphs preserve sentence-level information through the use of sentence\nnodes and provide concept connections between sentences. We jointly apply\nmulti-task learning for sentence-level and concept-level knowledge selection\nand show that it improves sentence-level selection. Our experiments show that\nour semantic graph-based knowledge selection improves over sentence selection\nbaselines for both the knowledge selection task and the end-to-end response\ngeneration task on HollE and improves generalization on unseen topics in WoW.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.05658,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000015563,
      "text":"Improving Pre-trained Language Model Fine-tuning with Noise Stability\n  Regularization\n\n  The advent of large-scale pre-trained language models has contributed greatly\nto the recent progress in natural language processing. Many state-of-the-art\nlanguage models are first trained on a large text corpus and then fine-tuned on\ndownstream tasks. Despite its recent success and wide adoption, fine-tuning a\npre-trained language model often suffers from overfitting, which leads to poor\ngeneralizability due to the extremely high complexity of the model and the\nlimited training samples from downstream tasks. To address this problem, we\npropose a novel and effective fine-tuning framework, named Layerwise Noise\nStability Regularization (LNSR). Specifically, we propose to inject the\nstandard Gaussian noise or In-manifold noise and regularize hidden\nrepresentations of the fine-tuned model. We first provide theoretical analyses\nto support the efficacy of our method. We then demonstrate the advantages of\nthe proposed method over other state-of-the-art algorithms including L2-SP,\nMixout and SMART. While these previous works only verify the effectiveness of\ntheir methods on relatively simple text classification tasks, we also verify\nthe effectiveness of our method on question answering tasks, where the target\nproblem is much more difficult and more training examples are available.\nFurthermore, extensive experimental results indicate that the proposed\nalgorithm can not only enhance the in-domain performance of the language models\nbut also improve the domain generalization performance on out-of-domain data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.06581,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000446373,
      "text":"CHQ-Summ: A Dataset for Consumer Healthcare Question Summarization\n\n  The quest for seeking health information has swamped the web with consumers'\nhealth-related questions. Generally, consumers use overly descriptive and\nperipheral information to express their medical condition or other healthcare\nneeds, contributing to the challenges of natural language understanding. One\nway to address this challenge is to summarize the questions and distill the key\ninformation of the original question. To address this issue, we introduce a new\ndataset, CHQ-Summ that contains 1507 domain-expert annotated consumer health\nquestions and corresponding summaries. The dataset is derived from the\ncommunity question-answering forum and therefore provides a valuable resource\nfor understanding consumer health-related posts on social media. We benchmark\nthe dataset on multiple state-of-the-art summarization models to show the\neffectiveness of the dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.14774,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"TweetNLP: Cutting-Edge Natural Language Processing for Social Media\n\n  In this paper we present TweetNLP, an integrated platform for Natural\nLanguage Processing (NLP) in social media. TweetNLP supports a diverse set of\nNLP tasks, including generic focus areas such as sentiment analysis and named\nentity recognition, as well as social media-specific tasks such as emoji\nprediction and offensive language identification. Task-specific systems are\npowered by reasonably-sized Transformer-based language models specialized on\nsocial media text (in particular, Twitter) which can be run without the need\nfor dedicated hardware or cloud services. The main contributions of TweetNLP\nare: (1) an integrated Python library for a modern toolkit supporting social\nmedia analysis using our various task-specific models adapted to the social\ndomain; (2) an interactive online demo for codeless experimentation using our\nmodels; and (3) a tutorial covering a wide variety of typical social media\napplications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.08727,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"The ITU Faroese Pairs Dataset\n\n  This article documents a dataset of sentence pairs between Faroese and\nDanish, produced at ITU Copenhagen. The data covers tranlsation from both\nsource languages, and is intended for use as training data for machine\ntranslation systems in this language pair.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.04327,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000039074,
      "text":"Language Identification for Austronesian Languages\n\n  This paper provides language identification models for low- and\nunder-resourced languages in the Pacific region with a focus on previously\nunavailable Austronesian languages. Accurate language identification is an\nimportant part of developing language resources. The approach taken in this\npaper combines 29 Austronesian languages with 171 non-Austronesian languages to\ncreate an evaluation set drawn from eight data sources. After evaluating six\napproaches to language identification, we find that a classifier based on\nskip-gram embeddings reaches a significantly higher performance than alternate\nmethods. We then systematically increase the number of non-Austronesian\nlanguages in the model up to a total of 800 languages to evaluate whether an\nincreased language inventory leads to less precise predictions for the\nAustronesian languages of interest. This evaluation finds that there is only a\nminimal impact on accuracy caused by increasing the inventory of\nnon-Austronesian languages. Further experiments adapt these language\nidentification models for code-switching detection, achieving high accuracy\nacross all 29 languages.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.08723,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000105302,
      "text":"CookDial: A dataset for task-oriented dialogs grounded in procedural\n  documents\n\n  This work presents a new dialog dataset, CookDial, that facilitates research\non task-oriented dialog systems with procedural knowledge understanding. The\ncorpus contains 260 human-to-human task-oriented dialogs in which an agent,\ngiven a recipe document, guides the user to cook a dish. Dialogs in CookDial\nexhibit two unique features: (i) procedural alignment between the dialog flow\nand supporting document; (ii) complex agent decision-making that involves\nsegmenting long sentences, paraphrasing hard instructions and resolving\ncoreference in the dialog context. In addition, we identify three challenging\n(sub)tasks in the assumed task-oriented dialog system: (1) User Question\nUnderstanding, (2) Agent Action Frame Prediction, and (3) Agent Response\nGeneration. For each of these tasks, we develop a neural baseline model, which\nwe evaluate on the CookDial dataset. We publicly release the CookDial dataset,\ncomprising rich annotations of both dialogs and recipe documents, to stimulate\nfurther research on domain-specific document-grounded dialog systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.09158,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000054306,
      "text":"A Double-Graph Based Framework for Frame Semantic Parsing\n\n  Frame semantic parsing is a fundamental NLP task, which consists of three\nsubtasks: frame identification, argument identification and role\nclassification. Most previous studies tend to neglect relations between\ndifferent subtasks and arguments and pay little attention to ontological frame\nknowledge defined in FrameNet. In this paper, we propose a Knowledge-guided\nIncremental semantic parser with Double-graph (KID). We first introduce Frame\nKnowledge Graph (FKG), a heterogeneous graph containing both frames and FEs\n(Frame Elements) built on the frame knowledge so that we can derive\nknowledge-enhanced representations for frames and FEs. Besides, we propose\nFrame Semantic Graph (FSG) to represent frame semantic structures extracted\nfrom the text with graph structures. In this way, we can transform frame\nsemantic parsing into an incremental graph construction problem to strengthen\ninteractions between subtasks and relations between arguments. Our experiments\nshow that KID outperforms the previous state-of-the-art method by up to 1.7\nF1-score on two FrameNet datasets. Our code is availavle at\nhttps:\/\/github.com\/PKUnlp-icler\/KID.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.10573,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"AI Based Chatbot: An Approach of Utilizing On Customer Service\n  Assistance\n\n  Providing the best customer experience is one of the primary concerns for the\nfirms that are based online. The advancement of machine learning is\nrevolutionising the company's attitude towards the client through improving the\nservice quality by implementing chatbot solutions, which gives the user instant\nand satisfactory answers to their enquiries. The acceptance of this technology\nis increasing with the new improvements and efficiency of the chatbot system.\nThis thesis paper will cover the concept of chatbot system for the company, as\na use case we took AK traders Ltd. It involves the research work on various\nchatbot technologies available and based on research, use them to develop a\nchatbot system for the company. This system will work based on the text as a\nconversational agent that can interact with humans by natural language. The\nmain objective project is to develop the chatbot solution that could comply\nwith complex questions and logical output answers in a well-defined approach.\nThe ultimate goal is to give high-quality results (answers) based on user input\n(question). For the successful implementation of this project, we have\nundertaken an in-depth analysis of the various machine learning techniques\navailable and followed well-structured implementation to figure out the best\nsolution for the company. The primary concern of this project includes natural\nlanguage processing (NLP), machine learning and the vector space model (VSM).\nThe outcome of the project shows the problem-solving technique for the\nimplementation of the chatbot system for the company at a reasonable quality\nlevel\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.06729,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000028246,
      "text":"Open Terminology Management and Sharing Toolkit for Federation of\n  Terminology Databases\n\n  Consolidated access to current and reliable terms from different subject\nfields and languages is necessary for content creators and translators.\nTerminology is also needed in AI applications such as machine translation,\nspeech recognition, information extraction, and other natural language\nprocessing tools. In this work, we facilitate standards-based sharing and\nmanagement of terminology resources by providing an open terminology management\nsolution - the EuroTermBank Toolkit. It allows organisations to manage and\nsearch their terms, create term collections, and share them within and outside\nthe organisation by participating in the network of federated databases. The\ndata curated in the federated databases are automatically shared with\nEuroTermBank, the largest multilingual terminology resource in Europe, allowing\ntranslators and language service providers as well as researchers and students\nto access terminology resources in their most current version.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.12406,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000704659,
      "text":"UrduFake@FIRE2020: Shared Track on Fake News Identification in Urdu\n\n  This paper gives the overview of the first shared task at FIRE 2020 on fake\nnews detection in the Urdu language. This is a binary classification task in\nwhich the goal is to identify fake news using a dataset composed of 900\nannotated news articles for training and 400 news articles for testing. The\ndataset contains news in five domains: (i) Health, (ii) Sports, (iii) Showbiz,\n(iv) Technology, and (v) Business. 42 teams from 6 different countries (India,\nChina, Egypt, Germany, Pakistan, and the UK) registered for the task. 9 teams\nsubmitted their experimental results. The participants used various machine\nlearning methods ranging from feature-based traditional machine learning to\nneural network techniques. The best performing system achieved an F-score value\nof 0.90, showing that the BERT-based approach outperforms other machine\nlearning classifiers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.00753,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"An End-to-End Set Transformer for User-Level Classification of\n  Depression and Gambling Disorder\n\n  This work proposes a transformer architecture for user-level classification\nof gambling addiction and depression that is trainable end-to-end. As opposed\nto other methods that operate at the post level, we process a set of social\nmedia posts from a particular individual, to make use of the interactions\nbetween posts and eliminate label noise at the post level. We exploit the fact\nthat, by not injecting positional encodings, multi-head attention is\npermutation invariant and we process randomly sampled sets of texts from a user\nafter being encoded with a modern pretrained sentence encoder (RoBERTa \/\nMiniLM). Moreover, our architecture is interpretable with modern feature\nattribution methods and allows for automatic dataset creation by identifying\ndiscriminating posts in a user's text-set. We perform ablation studies on\nhyper-parameters and evaluate our method for the eRisk 2022 Lab on early\ndetection of signs of pathological gambling and early risk detection of\ndepression. The method proposed by our team BLUE obtained the best ERDE5 score\nof 0.015, and the second-best ERDE50 score of 0.009 for pathological gambling\ndetection. For the early detection of depression, we obtained the second-best\nERDE50 of 0.027.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.00746,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000390742,
      "text":"INSCIT: Information-Seeking Conversations with Mixed-Initiative\n  Interactions\n\n  In an information-seeking conversation, a user may ask questions that are\nunder-specified or unanswerable. An ideal agent would interact by initiating\ndifferent response types according to the available knowledge sources. However,\nmost current studies either fail to or artificially incorporate such agent-side\ninitiative. This work presents InSCIt, a dataset for Information-Seeking\nConversations with mixed-initiative Interactions. It contains 4.7K user-agent\nturns from 805 human-human conversations where the agent searches over\nWikipedia and either directly answers, asks for clarification, or provides\nrelevant information to address user queries. The data supports two subtasks,\nevidence passage identification and response generation, as well as a human\nevaluation protocol to assess model performance. We report results of two\nsystems based on state-of-the-art models of conversational knowledge\nidentification and open-domain question answering. Both systems significantly\nunderperform humans, suggesting ample room for improvement in future studies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.13955,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000077486,
      "text":"Neural Architecture Search on Efficient Transformers and Beyond\n\n  Recently, numerous efficient Transformers have been proposed to reduce the\nquadratic computational complexity of standard Transformers caused by the\nSoftmax attention. However, most of them simply swap Softmax with an efficient\nattention mechanism without considering the customized architectures specially\nfor the efficient attention. In this paper, we argue that the handcrafted\nvanilla Transformer architectures for Softmax attention may not be suitable for\nefficient Transformers. To address this issue, we propose a new framework to\nfind optimal architectures for efficient Transformers with the neural\narchitecture search (NAS) technique. The proposed method is validated on\npopular machine translation and image classification tasks. We observe that the\noptimal architecture of the efficient Transformer has the reduced computation\ncompared with that of the standard Transformer, but the general accuracy is\nless comparable. It indicates that the Softmax attention and efficient\nattention have their own distinctions but neither of them can simultaneously\nbalance the accuracy and efficiency well. This motivates us to mix the two\ntypes of attention to reduce the performance imbalance. Besides the search\nspaces that commonly used in existing NAS Transformer approaches, we propose a\nnew search space that allows the NAS algorithm to automatically search the\nattention variants along with architectures. Extensive experiments on WMT' 14\nEn-De and CIFAR-10 demonstrate that our searched architecture maintains\ncomparable accuracy to the standard Transformer with notably improved\ncomputational efficiency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.10572,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Big Data and Education: using big data analytics in language learning\n\n  Working with big data using data mining tools is rapidly becoming a trend in\neducation industry. The combination of the current capacity to collect, store,\nmanage and process data in a timely manner, and data from online educational\nplatforms represents an unprecedented opportunity for educational institutes,\nlearners, educators, and researchers. In this position paper, we consider some\nbasic concepts as well as most popular tools, methods and techniques regarding\nEducational Data Mining and Learning Analytics, and discuss big data\napplications in language learning, in particular.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.03477,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"VeriDark: A Large-Scale Benchmark for Authorship Verification on the\n  Dark Web\n\n  The DarkWeb represents a hotbed for illicit activity, where users communicate\non different market forums in order to exchange goods and services. Law\nenforcement agencies benefit from forensic tools that perform authorship\nanalysis, in order to identify and profile users based on their textual\ncontent. However, authorship analysis has been traditionally studied using\ncorpora featuring literary texts such as fragments from novels or fan fiction,\nwhich may not be suitable in a cybercrime context. Moreover, the few works that\nemploy authorship analysis tools for cybercrime prevention usually employ\nad-hoc experimental setups and datasets. To address these issues, we release\nVeriDark: a benchmark comprised of three large scale authorship verification\ndatasets and one authorship identification dataset obtained from user activity\nfrom either Dark Web related Reddit communities or popular illicit Dark Web\nmarket forums. We evaluate competitive NLP baselines on the three datasets and\nperform an analysis of the predictions to better understand the limitations of\nsuch approaches. We make the datasets and baselines publicly available at\nhttps:\/\/github.com\/bit-ml\/VeriDark\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.033,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000019206,
      "text":"Win-Win Cooperation: Bundling Sequence and Span Models for Named Entity\n  Recognition\n\n  For Named Entity Recognition (NER), sequence labeling-based and span-based\nparadigms are quite different. Previous research has demonstrated that the two\nparadigms have clear complementary advantages, but few models have attempted to\nleverage these advantages in a single NER model as far as we know. In our\nprevious work, we proposed a paradigm known as Bundling Learning (BL) to\naddress the above problem. The BL paradigm bundles the two NER paradigms,\nenabling NER models to jointly tune their parameters by weighted summing each\nparadigm's training loss. However, three critical issues remain unresolved:\nWhen does BL work? Why does BL work? Can BL enhance the existing\nstate-of-the-art (SOTA) NER models? To address the first two issues, we\nimplement three NER models, involving a sequence labeling-based model--SeqNER,\na span-based NER model--SpanNER, and BL-NER that bundles SeqNER and SpanNER\ntogether. We draw two conclusions regarding the two issues based on the\nexperimental results on eleven NER datasets from five domains. We then apply BL\nto five existing SOTA NER models to investigate the third issue, consisting of\nthree sequence labeling-based models and two span-based models. Experimental\nresults indicate that BL consistently enhances their performance, suggesting\nthat it is possible to construct a new SOTA NER system by incorporating BL into\nthe current SOTA system. Moreover, we find that BL reduces both entity boundary\nand type prediction errors. In addition, we compare two commonly used labeling\ntagging methods as well as three types of span semantic representations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.05948,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.00000904,
      "text":"A General Contextualized Rewriting Framework for Text Summarization\n\n  The rewriting method for text summarization combines extractive and\nabstractive approaches, improving the conciseness and readability of extractive\nsummaries using an abstractive model. Exiting rewriting systems take each\nextractive sentence as the only input, which is relatively focused but can lose\nnecessary background knowledge and discourse context. In this paper, we\ninvestigate contextualized rewriting, which consumes the entire document and\nconsiders the summary context. We formalize contextualized rewriting as a\nseq2seq with group-tag alignments, introducing group-tag as a solution to model\nthe alignments, identifying extractive sentences through content-based\naddressing. Results show that our approach significantly outperforms\nnon-contextualized rewriting systems without requiring reinforcement learning,\nachieving strong improvements on ROUGE scores upon multiple extractors.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.04021,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"ASL-Homework-RGBD Dataset: An annotated dataset of 45 fluent and\n  non-fluent signers performing American Sign Language homeworks\n\n  We are releasing a dataset containing videos of both fluent and non-fluent\nsigners using American Sign Language (ASL), which were collected using a Kinect\nv2 sensor. This dataset was collected as a part of a project to develop and\nevaluate computer vision algorithms to support new technologies for automatic\ndetection of ASL fluency attributes. A total of 45 fluent and non-fluent\nparticipants were asked to perform signing homework assignments that are\nsimilar to the assignments used in introductory or intermediate level ASL\ncourses. The data is annotated to identify several aspects of signing including\ngrammatical features and non-manual markers. Sign language recognition is\ncurrently very data-driven and this dataset can support the design of\nrecognition technologies, especially technologies that can benefit ASL\nlearners. This dataset might also be interesting to ASL education researchers\nwho want to contrast fluent and non-fluent signing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.00758,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000015133,
      "text":"MIA 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question\n  Answering for 16 Diverse Languages\n\n  We present the results of the Workshop on Multilingual Information Access\n(MIA) 2022 Shared Task, evaluating cross-lingual open-retrieval question\nanswering (QA) systems in 16 typologically diverse languages. In this task, we\nadapted two large-scale cross-lingual open-retrieval QA datasets in 14\ntypologically diverse languages, and newly annotated open-retrieval QA data in\n2 underrepresented languages: Tagalog and Tamil. Four teams submitted their\nsystems. The best system leveraging iteratively mined diverse negative examples\nand larger pretrained models achieves 32.2 F1, outperforming our baseline by\n4.5 points. The second best system uses entity-aware contextualized\nrepresentations for document retrieval, and achieves significant improvements\nin Tamil (20.8 F1), whereas most of the other systems yield nearly zero scores.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.03256,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"Part-of-Speech Tagging of Odia Language Using statistical and Deep\n  Learning-Based Approaches\n\n  Automatic Part-of-speech (POS) tagging is a preprocessing step of many\nnatural language processing (NLP) tasks such as name entity recognition (NER),\nspeech processing, information extraction, word sense disambiguation, and\nmachine translation. It has already gained a promising result in English and\nEuropean languages, but in Indian languages, particularly in Odia language, it\nis not yet well explored because of the lack of supporting tools, resources,\nand morphological richness of language. Unfortunately, we were unable to locate\nan open source POS tagger for Odia, and only a handful of attempts have been\nmade to develop POS taggers for Odia language. The main contribution of this\nresearch work is to present a conditional random field (CRF) and deep\nlearning-based approaches (CNN and Bidirectional Long Short-Term Memory) to\ndevelop Odia part-of-speech tagger. We used a publicly accessible corpus and\nthe dataset is annotated with the Bureau of Indian Standards (BIS) tagset.\nHowever, most of the languages around the globe have used the dataset annotated\nwith Universal Dependencies (UD) tagset. Hence, to maintain uniformity Odia\ndataset should use the same tagset. So we have constructed a simple mapping\nfrom BIS tagset to UD tagset. We experimented with various feature set inputs\nto the CRF model, observed the impact of constructed feature set. The deep\nlearning-based model includes Bi-LSTM network, CNN network, CRF layer,\ncharacter sequence information, and pre-trained word vector. Character sequence\ninformation was extracted by using convolutional neural network (CNN) and\nBi-LSTM network. Six different combinations of neural sequence labelling models\nare implemented, and their performance measures are investigated. It has been\nobserved that Bi-LSTM model with character sequence feature and pre-trained\nword vector achieved a significant state-of-the-art result.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.04206,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000162257,
      "text":"A Study of Syntactic Multi-Modality in Non-Autoregressive Machine\n  Translation\n\n  It is difficult for non-autoregressive translation (NAT) models to capture\nthe multi-modal distribution of target translations due to their conditional\nindependence assumption, which is known as the \"multi-modality problem\",\nincluding the lexical multi-modality and the syntactic multi-modality. While\nthe first one has been well studied, the syntactic multi-modality brings severe\nchallenge to the standard cross entropy (XE) loss in NAT and is under studied.\nIn this paper, we conduct a systematic study on the syntactic multi-modality\nproblem. Specifically, we decompose it into short- and long-range syntactic\nmulti-modalities and evaluate several recent NAT algorithms with advanced loss\nfunctions on both carefully designed synthesized datasets and real datasets. We\nfind that the Connectionist Temporal Classification (CTC) loss and the\nOrder-Agnostic Cross Entropy (OAXE) loss can better handle short- and\nlong-range syntactic multi-modalities respectively. Furthermore, we take the\nbest of both and design a new loss function to better handle the complicated\nsyntactic multi-modality in real-world datasets. To facilitate practical usage,\nwe provide a guide to use different loss functions for different kinds of\nsyntactic multi-modality.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.11562,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000029471,
      "text":"Better Reasoning Behind Classification Predictions with BERT for Fake\n  News Detection\n\n  Fake news detection has become a major task to solve as there has been an\nincreasing number of fake news on the internet in recent years. Although many\nclassification models have been proposed based on statistical learning methods\nshowing good results, reasoning behind the classification performances may not\nbe enough. In the self-supervised learning studies, it has been highlighted\nthat a quality of representation (embedding) space matters and directly affects\na downstream task performance. In this study, a quality of the representation\nspace is analyzed visually and analytically in terms of linear separability for\ndifferent classes on a real and fake news dataset. To further add\ninterpretability to a classification model, a modification of Class Activation\nMapping (CAM) is proposed. The modified CAM provides a CAM score for each word\ntoken, where the CAM score on a word token denotes a level of focus on that\nword token to make the prediction. Finally, it is shown that the naive BERT\nmodel topped with a learnable linear layer is enough to achieve robust\nperformance while being compatible with CAM.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.08112,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000095699,
      "text":"United States Politicians' Tone Became More Negative with 2016 Primary\n  Campaigns\n\n  There is a widespread belief that the tone of US political language has\nbecome more negative recently, in particular when Donald Trump entered\npolitics. At the same time, there is disagreement as to whether Trump changed\nor merely continued previous trends. To date, data-driven evidence regarding\nthese questions is scarce, partly due to the difficulty of obtaining a\ncomprehensive, longitudinal record of politicians' utterances. Here we apply\npsycholinguistic tools to a novel, comprehensive corpus of 24 million quotes\nfrom online news attributed to 18,627 US politicians in order to analyze how\nthe tone of US politicians' language evolved between 2008 and 2020. We show\nthat, whereas the frequency of negative emotion words had decreased\ncontinuously during Obama's tenure, it suddenly and lastingly increased with\nthe 2016 primary campaigns, by 1.6 pre-campaign standard deviations, or 8% of\nthe pre-campaign mean, in a pattern that emerges across parties. The effect\nsize drops by 40% when omitting Trump's quotes, and by 50% when averaging over\nspeakers rather than quotes, implying that prominent speakers, and Trump in\nparticular, have disproportionately, though not exclusively, contributed to the\nrise in negative language. This work provides the first large-scale data-driven\nevidence of a drastic shift toward a more negative political tone following\nTrump's campaign start as a catalyst, with important implications for the\ndebate about the state of US politics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.10569,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000014272,
      "text":"A Reinforcement Learning-based Offensive semantics Censorship System for\n  Chatbots\n\n  The rapid development of artificial intelligence (AI) technology has enabled\nlarge-scale AI applications to land in the market and practice. However, while\nAI technology has brought many conveniences to people in the productization\nprocess, it has also exposed many security issues. Especially, attacks against\nonline learning vulnerabilities of chatbots occur frequently. Therefore, this\npaper proposes a semantics censorship chatbot system based on reinforcement\nlearning, which is mainly composed of two parts: the Offensive semantics\ncensorship model and the semantics purification model. Offensive semantics\nreview can combine the context of user input sentences to detect the rapid\nevolution of Offensive semantics and respond to Offensive semantics responses.\nThe semantics purification model For the case of chatting robot models, it has\nbeen contaminated by large numbers of offensive semantics, by strengthening the\noffensive reply learned by the learning algorithm, rather than rolling back to\nthe early versions. In addition, by integrating a once-through learning\napproach, the speed of semantics purification is accelerated while reducing the\nimpact on the quality of replies. The experimental results show that our\nproposed approach reduces the probability of the chat model generating\noffensive replies and that the integration of the few-shot learning algorithm\nimproves the training speed rapidly while effectively slowing down the decline\nin BLEU values.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.01672,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000087751,
      "text":"A Cascade Model for Argument Mining in Japanese Political Discussions:\n  the QA Lab-PoliInfo-3 Case Study\n\n  The rVRAIN team tackled the Budget Argument Mining (BAM) task, consisting of\na combination of classification and information retrieval sub-tasks. For the\nargument classification (AC), the team achieved its best performing results\nwith a five-class BERT-based cascade model complemented with some handcrafted\nrules. The rules were used to determine if the expression was monetary or not.\nThen, each monetary expression was classified as a premise or as a conclusion\nin the first level of the cascade model. Finally, each premise was classified\ninto the three premise classes, and each conclusion into the two conclusion\nclasses. For the information retrieval (i.e., relation ID detection or RID),\nour best results were achieved by a combination of a BERT-based binary\nclassifier, and the cosine similarity of pairs consisting of the monetary\nexpression and budget dense embeddings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.09643,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Integrating Linguistic Theory and Neural Language Models\n\n  Transformer-based language models have recently achieved remarkable results\nin many natural language tasks. However, performance on leaderboards is\ngenerally achieved by leveraging massive amounts of training data, and rarely\nby encoding explicit linguistic knowledge into neural models. This has led many\nto question the relevance of linguistics for modern natural language\nprocessing. In this dissertation, I present several case studies to illustrate\nhow theoretical linguistics and neural language models are still relevant to\neach other. First, language models are useful to linguists by providing an\nobjective tool to measure semantic distance, which is difficult to do using\ntraditional methods. On the other hand, linguistic theory contributes to\nlanguage modelling research by providing frameworks and sources of data to\nprobe our language models for specific aspects of language understanding.\n  This thesis contributes three studies that explore different aspects of the\nsyntax-semantics interface in language models. In the first part of my thesis,\nI apply language models to the problem of word class flexibility. Using mBERT\nas a source of semantic distance measurements, I present evidence in favour of\nanalyzing word class flexibility as a directional process. In the second part\nof my thesis, I propose a method to measure surprisal at intermediate layers of\nlanguage models. My experiments show that sentences containing morphosyntactic\nanomalies trigger surprisals earlier in language models than semantic and\ncommonsense anomalies. Finally, in the third part of my thesis, I adapt several\npsycholinguistic studies to show that language models contain knowledge of\nargument structure constructions. In summary, my thesis develops new\nconnections between natural language processing, linguistic theory, and\npsycholinguistics to provide fresh perspectives for the interpretation of\nlanguage models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.03885,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"A Medical Information Extraction Workbench to Process German Clinical\n  Text\n\n  Background: In the information extraction and natural language processing\ndomain, accessible datasets are crucial to reproduce and compare results.\nPublicly available implementations and tools can serve as benchmark and\nfacilitate the development of more complex applications. However, in the\ncontext of clinical text processing the number of accessible datasets is scarce\n-- and so is the number of existing tools. One of the main reasons is the\nsensitivity of the data. This problem is even more evident for non-English\nlanguages.\n  Approach: In order to address this situation, we introduce a workbench: a\ncollection of German clinical text processing models. The models are trained on\na de-identified corpus of German nephrology reports.\n  Result: The presented models provide promising results on in-domain data.\nMoreover, we show that our models can be also successfully applied to other\nbiomedical text in German. Our workbench is made publicly available so it can\nbe used out of the box, as a benchmark or transferred to related problems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.05008,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000047684,
      "text":"A description of Turkish Discourse Bank 1.2 and an examination of common\n  dependencies in Turkish discourse\n\n  We describe Turkish Discourse Bank 1.2, the latest version of a discourse\ncorpus annotated for explicitly or implicitly conveyed discourse relations,\ntheir constitutive units, and senses in the Penn Discourse Treebank style. We\npresent an evaluation of the recently added tokens and examine three commonly\noccurring dependency patterns that hold among the constitutive units of a pair\nof adjacent discourse relations, namely, shared arguments, full embedding and\npartial containment of a discourse relation. We present three major findings:\n(a) implicitly conveyed relations occur more often than explicitly conveyed\nrelations in the data; (b) it is much more common for two adjacent implicit\ndiscourse relations to share an argument than for two adjacent explicit\nrelations to do so; (c) both full embedding and partial containment of\ndiscourse relations are pervasive in the corpus, which can be partly due to\nsubordinator connectives whose preposed subordinate clause tends to be selected\ntogether with the matrix clause rather than being selected alone. Finally, we\nbriefly discuss the implications of our findings for Turkish discourse parsing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.01912,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000060598,
      "text":"Cross-Lingual Knowledge Transfer for Clinical Phenotyping\n\n  Clinical phenotyping enables the automatic extraction of clinical conditions\nfrom patient records, which can be beneficial to doctors and clinics worldwide.\nHowever, current state-of-the-art models are mostly applicable to clinical\nnotes written in English. We therefore investigate cross-lingual knowledge\ntransfer strategies to execute this task for clinics that do not use the\nEnglish language and have a small amount of in-domain data available. We\nevaluate these strategies for a Greek and a Spanish clinic leveraging clinical\nnotes from different clinical domains such as cardiology, oncology and the ICU.\nOur results reveal two strategies that outperform the state-of-the-art:\nTranslation-based methods in combination with domain-specific encoders and\ncross-lingual encoders plus adapters. We find that these strategies perform\nespecially well for classifying rare phenotypes and we advise on which method\nto prefer in which situation. Our results show that using multilingual data\noverall improves clinical phenotyping models and can compensate for data\nsparseness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.0661,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000020199,
      "text":"MetricBERT: Text Representation Learning via Self-Supervised Triplet\n  Training\n\n  We present MetricBERT, a BERT-based model that learns to embed text under a\nwell-defined similarity metric while simultaneously adhering to the\n``traditional'' masked-language task. We focus on downstream tasks of learning\nsimilarities for recommendations where we show that MetricBERT outperforms\nstate-of-the-art alternatives, sometimes by a substantial margin. We conduct\nextensive evaluations of our method and its different variants, showing that\nour training objective is highly beneficial over a traditional contrastive\nloss, a standard cosine similarity objective, and six other baselines. As an\nadditional contribution, we publish a dataset of video games descriptions along\nwith a test set of similarity annotations crafted by a domain expert.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.11508,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000055631,
      "text":"PSSAT: A Perturbed Semantic Structure Awareness Transferring Method for\n  Perturbation-Robust Slot Filling\n\n  Most existing slot filling models tend to memorize inherent patterns of\nentities and corresponding contexts from training data. However, these models\ncan lead to system failure or undesirable outputs when being exposed to spoken\nlanguage perturbation or variation in practice. We propose a perturbed semantic\nstructure awareness transferring method for training perturbation-robust slot\nfilling models. Specifically, we introduce two MLM-based training strategies to\nrespectively learn contextual semantic structure and word distribution from\nunsupervised language perturbation corpus. Then, we transfer semantic knowledge\nlearned from upstream training procedure into the original samples and filter\ngenerated data by consistency processing. These procedures aim to enhance the\nrobustness of slot filling models. Experimental results show that our method\nconsistently outperforms the previous basic methods and gains strong\ngeneralization while preventing the model from memorizing inherent patterns of\nentities and contexts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.08023,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000264247,
      "text":"Few-shot Named Entity Recognition with Entity-level Prototypical Network\n  Enhanced by Dispersedly Distributed Prototypes\n\n  Few-shot named entity recognition (NER) enables us to build a NER system for\na new domain using very few labeled examples. However, existing prototypical\nnetworks for this task suffer from roughly estimated label dependency and\nclosely distributed prototypes, thus often causing misclassifications. To\naddress the above issues, we propose EP-Net, an Entity-level Prototypical\nNetwork enhanced by dispersedly distributed prototypes. EP-Net builds\nentity-level prototypes and considers text spans to be candidate entities, so\nit no longer requires the label dependency. In addition, EP-Net trains the\nprototypes from scratch to distribute them dispersedly and aligns spans to\nprototypes in the embedding space using a space projection. Experimental\nresults on two evaluation tasks and the Few-NERD settings demonstrate that\nEP-Net consistently outperforms the previous strong models in terms of overall\nperformance. Extensive analyses further validate the effectiveness of EP-Net.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.01371,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000032783,
      "text":"Multi-Module G2P Converter for Persian Focusing on Relations between\n  Words\n\n  In this paper, we investigate the application of end-to-end and multi-module\nframeworks for G2P conversion for the Persian language. The results demonstrate\nthat our proposed multi-module G2P system outperforms our end-to-end systems in\nterms of accuracy and speed. The system consists of a pronunciation dictionary\nas our look-up table, along with separate models to handle homographs, OOVs and\nezafe in Persian created using GRU and Transformer architectures. The system is\nsequence-level rather than word-level, which allows it to effectively capture\nthe unwritten relations between words (cross-word information) necessary for\nhomograph disambiguation and ezafe recognition without the need for any\npre-processing. After evaluation, our system achieved a 94.48% word-level\naccuracy, outperforming the previous G2P systems for Persian.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.14509,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000045035,
      "text":"Text Difficulty Study: Do machines behave the same as humans regarding\n  text difficulty?\n\n  Given a task, human learns from easy to hard, whereas the model learns\nrandomly. Undeniably, difficulty insensitive learning leads to great success in\nNLP, but little attention has been paid to the effect of text difficulty in\nNLP. In this research, we propose the Human Learning Matching Index (HLM Index)\nto investigate the effect of text difficulty. Experiment results show: (1) LSTM\nhas more human-like learning behavior than BERT. (2) UID-SuperLinear gives the\nbest evaluation of text difficulty among four text difficulty criteria. (3)\nAmong nine tasks, some tasks' performance is related to text difficulty,\nwhereas some are not. (4) Model trained on easy data performs best in easy and\nmedium data, whereas trains on a hard level only perform well on hard data. (5)\nTraining the model from easy to hard leads to fast convergence.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.01006,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Multi-Document Summarization with Centroid-Based Pretraining\n\n  In Multi-Document Summarization (MDS), the input can be modeled as a set of\ndocuments, and the output is its summary. In this paper, we focus on\npretraining objectives for MDS. Specifically, we introduce a novel pretraining\nobjective, which involves selecting the ROUGE-based centroid of each document\ncluster as a proxy for its summary. Our objective thus does not require human\nwritten summaries and can be utilized for pretraining on a dataset consisting\nsolely of document sets. Through zero-shot, few-shot, and fully supervised\nexperiments on multiple MDS datasets, we show that our model Centrum is better\nor comparable to a state-of-the-art model. We make the pretrained and\nfine-tuned models freely available to the research community\nhttps:\/\/github.com\/ratishsp\/centrum.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.08386,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000034438,
      "text":"Neural Embeddings for Text\n\n  We propose a new kind of embedding for natural language text that deeply\nrepresents semantic meaning. Standard text embeddings use the outputs from\nhidden layers of a pretrained language model. In our method, we let a language\nmodel learn from the text and then literally pick its brain, taking the actual\nweights of the model's neurons to generate a vector. We call this\nrepresentation of the text a neural embedding. We confirm the ability of this\nrepresentation to reflect semantics of the text by an analysis of its behavior\non several datasets, and by a comparison of neural embedding with state of the\nart sentence embeddings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.03438,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"DeepGen: Diverse Search Ad Generation and Real-Time Customization\n\n  We present DeepGen, a system deployed at web scale for automatically creating\nsponsored search advertisements (ads) for BingAds customers. We leverage\nstate-of-the-art natural language generation (NLG) models to generate fluent\nads from advertiser's web pages in an abstractive fashion and solve practical\nissues such as factuality and inference speed. In addition, our system creates\na customized ad in real-time in response to the user's search query, therefore\nhighlighting different aspects of the same product based on what the user is\nlooking for. To achieve this, our system generates a diverse choice of smaller\npieces of the ad ahead of time and, at query time, selects the most relevant\nones to be stitched into a complete ad. We improve generation diversity by\ntraining a controllable NLG model to generate multiple ads for the same web\npage highlighting different selling points. Our system design further improves\ndiversity horizontally by first running an ensemble of generation models\ntrained with different objectives and then using a diversity sampling algorithm\nto pick a diverse subset of generation results for online selection.\nExperimental results show the effectiveness of our proposed system design. Our\nsystem is currently deployed in production, serving ${\\sim}4\\%$ of global ads\nserved in Bing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.03985,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000076161,
      "text":"Generating Coherent Narratives by Learning Dynamic and Discrete Entity\n  States with a Contrastive Framework\n\n  Despite advances in generating fluent texts, existing pretraining models tend\nto attach incoherent event sequences to involved entities when generating\nnarratives such as stories and news. We conjecture that such issues result from\nrepresenting entities as static embeddings of superficial words, while\nneglecting to model their ever-changing states, i.e., the information they\ncarry, as the text unfolds. Therefore, we extend the Transformer model to\ndynamically conduct entity state updates and sentence realization for narrative\ngeneration. We propose a contrastive framework to learn the state\nrepresentations in a discrete space, and insert additional attention layers\ninto the decoder to better exploit these states. Experiments on two narrative\ndatasets show that our model can generate more coherent and diverse narratives\nthan strong baselines with the guidance of meaningful entity states.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.09316,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000083115,
      "text":"UKP-SQuARE v2: Explainability and Adversarial Attacks for Trustworthy QA\n\n  Question Answering (QA) systems are increasingly deployed in applications\nwhere they support real-world decisions. However, state-of-the-art models rely\non deep neural networks, which are difficult to interpret by humans. Inherently\ninterpretable models or post hoc explainability methods can help users to\ncomprehend how a model arrives at its prediction and, if successful, increase\ntheir trust in the system. Furthermore, researchers can leverage these insights\nto develop new methods that are more accurate and less biased. In this paper,\nwe introduce SQuARE v2, the new version of SQuARE, to provide an explainability\ninfrastructure for comparing models based on methods such as saliency maps and\ngraph-based explanations. While saliency maps are useful to inspect the\nimportance of each input token for the model's prediction, graph-based\nexplanations from external Knowledge Graphs enable the users to verify the\nreasoning behind the model prediction. In addition, we provide multiple\nadversarial attacks to compare the robustness of QA models. With these\nexplainability methods and adversarial attacks, we aim to ease the research on\ntrustworthy QA models. SQuARE is available on https:\/\/square.ukp-lab.de.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.01561,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000309613,
      "text":"Lost in Space Marking\n\n  We look at a decision taken early in training a subword tokenizer, namely\nwhether it should be the word-initial token that carries a special mark, or the\nword-final one. Based on surface-level considerations of efficiency and\ncohesion, as well as morphological coverage, we find that a Unigram LM\ntokenizer trained on pre-tokenized English text is better off marking the\nword-initial token, while one trained on raw text benefits from marking word\nends. Our findings generalize across domains.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.09915,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models\n\n  Protecting NLP models against misspellings whether accidental or adversarial\nhas been the object of research interest for the past few years. Existing\nremediations have typically either compromised accuracy or required full model\nre-training with each new class of attacks. We propose a novel method of\nretroactively adding resilience to misspellings to transformer-based NLP\nmodels. This robustness can be achieved without the need for re-training of the\noriginal NLP model and with only a minimal loss of language understanding\nperformance on inputs without misspellings. Additionally we propose a new\nefficient approximate method of generating adversarial misspellings, which\nsignificantly reduces the cost needed to evaluate a model's resilience to\nadversarial attacks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.09617,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000041061,
      "text":"Pretrained Language Encoders are Natural Tagging Frameworks for Aspect\n  Sentiment Triplet Extraction\n\n  Aspect Sentiment Triplet Extraction (ASTE) aims to extract the spans of\naspect, opinion, and their sentiment relations as sentiment triplets. Existing\nworks usually formulate the span detection as a 1D token tagging problem, and\nmodel the sentiment recognition with a 2D tagging matrix of token pairs.\nMoreover, by leveraging the token representation of Pretrained Language\nEncoders (PLEs) like BERT, they can achieve better performance. However, they\nsimply leverage PLEs as feature extractors to build their modules but never\nhave a deep look at what specific knowledge does PLEs contain. In this paper,\nwe argue that instead of further designing modules to capture the inductive\nbias of ASTE, PLEs themselves contain \"enough\" features for 1D and 2D tagging:\n(1) The token representation contains the contextualized meaning of token\nitself, so this level feature carries necessary information for 1D tagging. (2)\nThe attention matrix of different PLE layers can further capture multi-level\nlinguistic knowledge existing in token pairs, which benefits 2D tagging. (3)\nFurthermore, with simple transformations, these two features can also be easily\nconverted to the 2D tagging matrix and 1D tagging sequence, respectively. That\nwill further boost the tagging results. By doing so, PLEs can be natural\ntagging frameworks and achieve a new state of the art, which is verified by\nextensive experiments and deep analyses.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.01368,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000049008,
      "text":"PyABSA: A Modularized Framework for Reproducible Aspect-based Sentiment\n  Analysis\n\n  The advancement of aspect-based sentiment analysis (ABSA) has urged the lack\nof a user-friendly framework that can largely lower the difficulty of\nreproducing state-of-the-art ABSA performance, especially for beginners. To\nmeet the demand, we present \\our, a modularized framework built on PyTorch for\nreproducible ABSA. To facilitate ABSA research, PyABSA supports several ABSA\nsubtasks, including aspect term extraction, aspect sentiment classification,\nand end-to-end aspect-based sentiment analysis. Concretely, PyABSA integrates\n29 models and 26 datasets. With just a few lines of code, the result of a model\non a specific dataset can be reproduced. With a modularized design, PyABSA can\nalso be flexibly extended to considered models, datasets, and other related\ntasks. Besides, PyABSA highlights its data augmentation and annotation\nfeatures, which significantly address data scarcity. All are welcome to have a\ntry at \\url{https:\/\/github.com\/yangheng95\/PyABSA}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.11922,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000053644,
      "text":"Historical\/temporal necessities\/possibilities, and a logical theory of them in branching time\n\nIn this paper, we do three kinds of work. First, we recognize four notions of necessity and two notions of possibility related to time flow, namely strong\/weak historical\/temporal necessities, as well as historical\/temporal possibilities, which are motivated more from a linguistic perspective than from a philosophical one. Strong\/weak historical necessities and historical possibility typically concern the possible futures of the present world, and strong\/weak temporal necessities and temporal possibility concern possible timelines of alternatives of the present world. Second, we provide our approach to the six notions and present a logical theory of them in branching time. Our approach to the six notions is as follows. The agent has a system of ontic rules that determine expected timelines. She treats some ontic rules as undefeatable, determining accepted timelines. The domains of strong\/weak historical necessities, respectively, consist of accepted and expected timelines passing through the present moment, and historical possibility is the dual of strong historical necessity. The domains of strong\/weak temporal necessities, respectively, consist of accepted and expected timelines, and temporal possibility is the dual of strong temporal necessity. The logical theory has six operators: a last-moment operator, a next-moment operator, and four operators for the four notions of necessity. Formulas' evaluation contexts consist of a tree-like model representing a time flow, a context representing the agent's system of ontic rules, a timeline, and an instant. Third, we offer an axiomatic system for the logical theory and show its soundness and completeness.",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.14111,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"Transformers with Learnable Activation Functions\n\n  Activation functions can have a significant impact on reducing the\ntopological complexity of input data and therefore improve the performance of\nthe model. Selecting a suitable activation function is an essential step in\nneural model design. However, the choice of activation function is seldom\ndiscussed or explored in Transformer-based language models. Their activation\nfunctions are chosen beforehand and then remain fixed from pre-training to\nfine-tuning. As a result, the inductive biases they imposed on models cannot be\nadjusted during this long life cycle. Moreover, subsequently developed models\n(e.g., RoBERTa, BART, and GPT-3) often follow up prior work (e.g., BERT) to use\nthe same activation function without justification. In this paper, we\ninvestigate the effectiveness of using Rational Activation Function (RAF), a\nlearnable activation function, in the Transformer architecture. In contrast to\nconventional, predefined activation functions, RAFs can adaptively learn\noptimal activation functions during training according to input data. Our\nexperiments show the RAF-based Transformer (RAFT) achieves a lower validation\nperplexity than a vanilla BERT with the GELU function. We further evaluate RAFT\non downstream tasks in low- and full-data settings. Our results show that RAFT\noutperforms the counterpart model across the majority of tasks and settings.\nFor instance, RAFT outperforms vanilla BERT on the GLUE benchmark by 5.71\npoints on average in low-data scenario (where 100 training examples are\navailable) and by 2.05 points on SQuAD in full-data setting. Analysis of the\nshapes of learned RAFs further unveils that they substantially vary between\ndifferent layers of the pre-trained model and mostly look very different from\nconventional activation functions. RAFT opens a new research direction for\nanalyzing and interpreting pre-trained models according to the learned\nactivation functions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.06178,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Mining Legal Arguments in Court Decisions\n\n  Identifying, classifying, and analyzing arguments in legal discourse has been\na prominent area of research since the inception of the argument mining field.\nHowever, there has been a major discrepancy between the way natural language\nprocessing (NLP) researchers model and annotate arguments in court decisions\nand the way legal experts understand and analyze legal argumentation. While\ncomputational approaches typically simplify arguments into generic premises and\nclaims, arguments in legal research usually exhibit a rich typology that is\nimportant for gaining insights into the particular case and applications of law\nin general. We address this problem and make several substantial contributions\nto move the field forward. First, we design a new annotation scheme for legal\narguments in proceedings of the European Court of Human Rights (ECHR) that is\ndeeply rooted in the theory and practice of legal argumentation research.\nSecond, we compile and annotate a large corpus of 373 court decisions (2.3M\ntokens and 15k annotated argument spans). Finally, we train an argument mining\nmodel that outperforms state-of-the-art models in the legal NLP domain and\nprovide a thorough expert-based evaluation. All datasets and source codes are\navailable under open lincenses at\nhttps:\/\/github.com\/trusthlt\/mining-legal-arguments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.12496,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000044273,
      "text":"Nearest Neighbor Non-autoregressive Text Generation\n\n  Non-autoregressive (NAR) models can generate sentences with less computation\nthan autoregressive models but sacrifice generation quality. Previous studies\naddressed this issue through iterative decoding. This study proposes using\nnearest neighbors as the initial state of an NAR decoder and editing them\niteratively. We present a novel training strategy to learn the edit operations\non neighbors to improve NAR text generation. Experimental results show that the\nproposed method (NeighborEdit) achieves higher translation quality (1.69 points\nhigher than the vanilla Transformer) with fewer decoding iterations\n(one-eighteenth fewer iterations) on the JRC-Acquis En-De dataset, the common\nbenchmark dataset for machine translation using nearest neighbors. We also\nconfirm the effectiveness of the proposed method on a data-to-text task\n(WikiBio). In addition, the proposed method outperforms an NAR baseline on the\nWMT'14 En-De dataset. We also report analysis on neighbor examples used in the\nproposed method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.01099,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Parsimonious Argument Annotations for Hate Speech Counter-narratives\n\n  We present an enrichment of the Hateval corpus of hate speech tweets (Basile\net. al 2019) aimed to facilitate automated counter-narrative generation.\nComparably to previous work (Chung et. al. 2019), manually written\ncounter-narratives are associated to tweets. However, this information alone\nseems insufficient to obtain satisfactory language models for counter-narrative\ngeneration. That is why we have also annotated tweets with argumentative\ninformation based on Wagemanns (2016), that we believe can help in building\nconvincing and effective counter-narratives for hate speech against particular\ngroups.\n  We discuss adequacies and difficulties of this annotation process and present\nseveral baselines for automatic detection of the annotated elements.\nPreliminary results show that automatic annotators perform close to human\nannotators to detect some aspects of argumentation, while others only reach low\nor moderate level of inter-annotator agreement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.01975,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000036094,
      "text":"Selective Annotation Makes Language Models Better Few-Shot Learners\n\n  Many recent approaches to natural language tasks are built on the remarkable\nabilities of large language models. Large language models can perform\nin-context learning, where they learn a new task from a few task\ndemonstrations, without any parameter updates. This work examines the\nimplications of in-context learning for the creation of datasets for new\nnatural language tasks. Departing from recent in-context learning methods, we\nformulate an annotation-efficient, two-step framework: selective annotation\nthat chooses a pool of examples to annotate from unlabeled data in advance,\nfollowed by prompt retrieval that retrieves task examples from the annotated\npool at test time. Based on this framework, we propose an unsupervised,\ngraph-based selective annotation method, voke-k, to select diverse,\nrepresentative examples to annotate. Extensive experiments on 10 datasets\n(covering classification, commonsense reasoning, dialogue, and text\/code\ngeneration) demonstrate that our selective annotation method improves the task\nperformance by a large margin. On average, vote-k achieves a 12.9%\/11.4%\nrelative gain under an annotation budget of 18\/100, as compared to randomly\nselecting examples to annotate. Compared to state-of-the-art supervised\nfinetuning approaches, it yields similar performance with 10-100x less\nannotation cost across 10 tasks. We further analyze the effectiveness of our\nframework in various scenarios: language models with varying sizes, alternative\nselective annotation methods, and cases where there is a test data domain\nshift. We hope that our studies will serve as a basis for data annotations as\nlarge language models are increasingly applied to new tasks. Our code is\navailable at https:\/\/github.com\/HKUNLP\/icl-selective-annotation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.11429,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000047021,
      "text":"News Category Dataset\n\n  People rely on news to know what is happening around the world and inform\ntheir daily lives. In today's world, when the proliferation of fake news is\nrampant, having a large-scale and high-quality source of authentic news\narticles with the published category information is valuable to learning\nauthentic news' Natural Language syntax and semantics. As part of this work, we\npresent a News Category Dataset that contains around 210k news headlines from\nthe year 2012 to 2022 obtained from HuffPost, along with useful metadata to\nenable various NLP tasks. In this paper, we also produce some novel insights\nfrom the dataset and describe various existing and potential applications of\nour dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.01824,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"A Survey on Measuring and Mitigating Reasoning Shortcuts in Machine\n  Reading Comprehension\n\n  The issue of shortcut learning is widely known in NLP and has been an\nimportant research focus in recent years. Unintended correlations in the data\nenable models to easily solve tasks that were meant to exhibit advanced\nlanguage understanding and reasoning capabilities. In this survey paper, we\nfocus on the field of machine reading comprehension (MRC), an important task\nfor showcasing high-level language understanding that also suffers from a range\nof shortcuts. We summarize the available techniques for measuring and\nmitigating shortcuts and conclude with suggestions for further progress in\nshortcut research. Importantly, we highlight two concerns for shortcut\nmitigation in MRC: (1) the lack of public challenge sets, a necessary component\nfor effective and reusable evaluation, and (2) the lack of certain mitigation\ntechniques that are prominent in other areas.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.13773,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000220868,
      "text":"METS-CoV: A Dataset of Medical Entity and Targeted Sentiment on COVID-19\n  Related Tweets\n\n  The COVID-19 pandemic continues to bring up various topics discussed or\ndebated on social media. In order to explore the impact of pandemics on\npeople's lives, it is crucial to understand the public's concerns and attitudes\ntowards pandemic-related entities (e.g., drugs, vaccines) on social media.\nHowever, models trained on existing named entity recognition (NER) or targeted\nsentiment analysis (TSA) datasets have limited ability to understand\nCOVID-19-related social media texts because these datasets are not designed or\nannotated from a medical perspective. This paper releases METS-CoV, a dataset\ncontaining medical entities and targeted sentiments from COVID-19-related\ntweets. METS-CoV contains 10,000 tweets with 7 types of entities, including 4\nmedical entity types (Disease, Drug, Symptom, and Vaccine) and 3 general entity\ntypes (Person, Location, and Organization). To further investigate tweet users'\nattitudes toward specific entities, 4 types of entities (Person, Organization,\nDrug, and Vaccine) are selected and annotated with user sentiments, resulting\nin a targeted sentiment dataset with 9,101 entities (in 5,278 tweets). To the\nbest of our knowledge, METS-CoV is the first dataset to collect medical\nentities and corresponding sentiments of COVID-19-related tweets. We benchmark\nthe performance of classical machine learning models and state-of-the-art deep\nlearning models on NER and TSA tasks with extensive experiments. Results show\nthat the dataset has vast room for improvement for both NER and TSA tasks.\nMETS-CoV is an important resource for developing better medical social media\ntools and facilitating computational social science research, especially in\nepidemiology. Our data, annotation guidelines, benchmark models, and source\ncode are publicly available (https:\/\/github.com\/YLab-Open\/METS-CoV) to ensure\nreproducibility.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.15099,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"MUG: Interactive Multimodal Grounding on User Interfaces\n\n  We present MUG, a novel interactive task for multimodal grounding where a\nuser and an agent work collaboratively on an interface screen. Prior works\nmodeled multimodal UI grounding in one round: the user gives a command and the\nagent responds to the command. Yet, in a realistic scenario, a user command can\nbe ambiguous when the target action is inherently difficult to articulate in\nnatural language. MUG allows multiple rounds of interactions such that upon\nseeing the agent responses, the user can give further commands for the agent to\nrefine or even correct its actions. Such interaction is critical for improving\ngrounding performances in real-world use cases. To investigate the problem, we\ncreate a new dataset that consists of 77,820 sequences of human user-agent\ninteraction on mobile interfaces in which 20% involves multiple rounds of\ninteractions. To establish our benchmark, we experiment with a range of\nmodeling variants and evaluation strategies, including both offline and online\nevaluation-the online strategy consists of both human evaluation and automatic\nwith simulators. Our experiments show that allowing iterative interaction\nsignificantly improves the absolute task completion by 18% over the entire test\ndataset and 31% over the challenging subset. Our results lay the foundation for\nfurther investigation of the problem.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.12278,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000178814,
      "text":"Neural inhibition during speech planning contributes to contrastive hyperarticulation\n\nPrevious work has demonstrated that words are hyperarticulated on dimensions of speech that differentiate them from a minimal pair competitor. This phenomenon has been termed contrastive hyperarticulation (CH). We present a dynamic neural field (DNF) model of voice onset time (VOT) planning that derives CH from an inhibitory influence of the minimal pair competitor during planning. We test some predictions of the model with a novel experiment investigating CH of voiceless stop consonant VOT in pseudowords. The results demonstrate a CH effect in pseudowords, consistent with a basis for the effect in the real-time planning and production of speech. The scope and magnitude of CH in pseudowords was reduced compared to CH in real words, consistent with a role for interactive activation between lexical and phonological levels of planning. We discuss the potential of our model to unify an apparently disparate set of phenomena, from CH to phonological neighborhood effects to phonetic trace effects in speech errors.",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.0082,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Structural Bias for Aspect Sentiment Triplet Extraction\n\n  Structural bias has recently been exploited for aspect sentiment triplet\nextraction (ASTE) and led to improved performance. On the other hand, it is\nrecognized that explicitly incorporating structural bias would have a negative\nimpact on efficiency, whereas pretrained language models (PLMs) can already\ncapture implicit structures. Thus, a natural question arises: Is structural\nbias still a necessity in the context of PLMs? To answer the question, we\npropose to address the efficiency issues by using an adapter to integrate\nstructural bias in the PLM and using a cheap-to-compute relative position\nstructure in place of the syntactic dependency structure. Benchmarking\nevaluation is conducted on the SemEval datasets. The results show that our\nproposed structural adapter is beneficial to PLMs and achieves state-of-the-art\nperformance over a range of strong baselines, yet with a light parameter demand\nand low latency. Meanwhile, we give rise to the concern that the current\nevaluation default with data of small scale is under-confident. Consequently,\nwe release a large-scale dataset for ASTE. The results on the new dataset hint\nthat the structural adapter is confidently effective and efficient to a large\nscale. Overall, we draw the conclusion that structural bias shall still be a\nnecessity even with PLMs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.07139,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"The Impact of Edge Displacement Vaserstein Distance on UD Parsing\n  Performance\n\n  We contribute to the discussion on parsing performance in NLP by introducing\na measurement that evaluates the differences between the distributions of edge\ndisplacement (the directed distance of edges) seen in training and test data.\nWe hypothesize that this measurement will be related to differences observed in\nparsing performance across treebanks. We motivate this by building upon\nprevious work and then attempt to falsify this hypothesis by using a number of\nstatistical methods. We establish that there is a statistical correlation\nbetween this measurement and parsing performance even when controlling for\npotential covariants. We then use this to establish a sampling technique that\ngives us an adversarial and complementary split. This gives an idea of the\nlower and upper bounds of parsing systems for a given treebank in lieu of\nfreshly sampled data. In a broader sense, the methodology presented here can\nact as a reference for future correlation-based exploratory work in NLP.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.07278,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"\\'UFAL CorPipe at CRAC 2022: Effectivity of Multilingual Models for\n  Coreference Resolution\n\n  We describe the winning submission to the CRAC 2022 Shared Task on\nMultilingual Coreference Resolution. Our system first solves mention detection\nand then coreference linking on the retrieved spans with an\nantecedent-maximization approach, and both tasks are fine-tuned jointly with\nshared Transformer weights. We report results of fine-tuning a wide range of\npretrained models. The center of this contribution are fine-tuned multilingual\nmodels. We found one large multilingual model with sufficiently large encoder\nto increase performance on all datasets across the board, with the benefit not\nlimited only to the underrepresented languages or groups of typologically\nrelative languages. The source code is available at\nhttps:\/\/github.com\/ufal\/crac2022-corpipe.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.05745,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000041723,
      "text":"A virtual reality-based method for examining audiovisual prosody\n  perception\n\n  Prosody plays a vital role in verbal communication. Acoustic cues of prosody\nhave been examined extensively. However, prosodic characteristics are not only\nperceived auditorily, but also visually based on head and facial movements. The\npurpose of this report is to present a method for examining audiovisual prosody\nusing virtual reality. We show that animations based on a virtual human provide\nmotion cues similar to those obtained from video recordings of a real talker.\nThe use of virtual reality opens up new avenues for examining multimodal\neffects of verbal communication. We discuss the method in the framework of\nexamining prosody perception in cochlear implant listeners.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.11977,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000158946,
      "text":"Understanding the Use of Quantifiers in Mandarin\n\n  We introduce a corpus of short texts in Mandarin, in which quantified\nexpressions figure prominently. We illustrate the significance of the corpus by\nexamining the hypothesis (known as Huang's \"coolness\" hypothesis) that speakers\nof East Asian Languages tend to speak more briefly but less informatively than,\nfor example, speakers of West-European languages. The corpus results from an\nelicitation experiment in which participants were asked to describe abstract\nvisual scenes. We compare the resulting corpus, called MQTUNA, with an English\ncorpus that was collected using the same experimental paradigm. The comparison\nreveals that some, though not all, aspects of quantifier use support the\nabove-mentioned hypothesis. Implications of these findings for the generation\nof quantified noun phrases are discussed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.05185,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000212259,
      "text":"Open-Domain Dialog Evaluation using Follow-Ups Likelihood\n\n  Automatic evaluation of open-domain dialogs remains an unsolved problem.\nMoreover, existing methods do not correlate strongly with human annotations.\nThis paper presents a new automated evaluation method using follow-ups: we\nmeasure the probability that a language model will continue the conversation\nwith a fixed set of follow-ups (e.g., not really relevant here, what are you\ntrying to say). When compared against twelve existing methods, our new\nevaluation achieves the highest correlation with human evaluations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.10538,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000019206,
      "text":"Subject Verb Agreement Error Patterns in Meaningless Sentences: Humans\n  vs. BERT\n\n  Both humans and neural language models are able to perform subject-verb\nnumber agreement (SVA). In principle, semantics shouldn't interfere with this\ntask, which only requires syntactic knowledge. In this work we test whether\nmeaning interferes with this type of agreement in English in syntactic\nstructures of various complexities. To do so, we generate both semantically\nwell-formed and nonsensical items. We compare the performance of BERT-base to\nthat of humans, obtained with a psycholinguistic online crowdsourcing\nexperiment. We find that BERT and humans are both sensitive to our semantic\nmanipulation: They fail more often when presented with nonsensical items,\nespecially when their syntactic structure features an attractor (a noun phrase\nbetween the subject and the verb that has not the same number as the subject).\nWe also find that the effect of meaningfulness on SVA errors is stronger for\nBERT than for humans, showing higher lexical sensitivity of the former on this\ntask.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.0679,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Drawing Causal Inferences About Performance Effects in NLP\n\n  This article emphasizes that NLP as a science seeks to make inferences about\nthe performance effects that result from applying one method (compared to\nanother method) in the processing of natural language. Yet NLP research in\npractice usually does not achieve this goal: In NLP research articles,\ntypically only a few models are compared. Each model results from a specific\nprocedural pipeline (here named processing system) that is composed of a\nspecific collection of methods that are used in preprocessing, pretraining,\nhyperparameter tuning, and training on the target task. To make generalizing\ninferences about the performance effect that is caused by applying some method\nA vs. another method B, it is not sufficient to compare a few specific models\nthat are produced by a few specific (probably incomparable) processing systems.\nRather, the following procedure would allow drawing inferences about methods'\nperformance effects: (1) A population of processing systems that researchers\nseek to infer to has to be defined. (2) A random sample of processing systems\nfrom this population is drawn. (The drawn processing systems in the sample will\nvary with regard to the methods they apply along their procedural pipelines and\nalso will vary regarding the compositions of their training and test data sets\nused for training and evaluation.) (3) Each processing system is applied once\nwith method A and once with method B. (4) Based on the sample of applied\nprocessing systems, the expected generalization errors of method A and method B\nare approximated. (5) The difference between the expected generalization errors\nof method A and method B is the estimated average treatment effect due to\napplying method A compared to method B in the population of processing systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.14279,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Causal Proxy Models for Concept-Based Model Explanations\n\n  Explainability methods for NLP systems encounter a version of the fundamental\nproblem of causal inference: for a given ground-truth input text, we never\ntruly observe the counterfactual texts necessary for isolating the causal\neffects of model representations on outputs. In response, many explainability\nmethods make no use of counterfactual texts, assuming they will be unavailable.\nIn this paper, we show that robust causal explainability methods can be created\nusing approximate counterfactuals, which can be written by humans to\napproximate a specific counterfactual or simply sampled using metadata-guided\nheuristics. The core of our proposal is the Causal Proxy Model (CPM). A CPM\nexplains a black-box model $\\mathcal{N}$ because it is trained to have the same\nactual input\/output behavior as $\\mathcal{N}$ while creating neural\nrepresentations that can be intervened upon to simulate the counterfactual\ninput\/output behavior of $\\mathcal{N}$. Furthermore, we show that the best CPM\nfor $\\mathcal{N}$ performs comparably to $\\mathcal{N}$ in making factual\npredictions, which means that the CPM can simply replace $\\mathcal{N}$, leading\nto more explainable deployed models. Our code is available at\nhttps:\/\/github.com\/frankaging\/Causal-Proxy-Model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.07239,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000030299,
      "text":"UBARv2: Towards Mitigating Exposure Bias in Task-Oriented Dialogs\n\n  This paper studies the exposure bias problem in task-oriented dialog systems,\nwhere the model's generated content over multiple turns drives the dialog\ncontext away from the ground-truth distribution at training time, introducing\nerror propagation and damaging the robustness of the TOD system. To bridge the\ngap between training and inference for multi-turn task-oriented dialogs, we\npropose session-level sampling which explicitly exposes the model to sampled\ngenerated content of dialog context during training. Additionally, we employ a\ndropout-based consistency regularization with the masking strategy R-Mask to\nfurther improve the robustness and performance of the model. The proposed\nUBARv2 achieves state-of-the-art performance on the standardized evaluation\nbenchmark MultiWOZ and extensive experiments show the effectiveness of the\nproposed methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.13877,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000717905,
      "text":"YATO: Yet Another deep learning based Text analysis Open toolkit\n\n  We introduce YATO, an open-source, easy-to-use toolkit for text analysis with\ndeep learning. Different from existing heavily engineered toolkits and\nplatforms, YATO is lightweight and user-friendly for researchers from\ncross-disciplinary areas. Designed in a hierarchical structure, YATO supports\nfree combinations of three types of widely used features including 1)\ntraditional neural networks (CNN, RNN, etc.); 2) pre-trained language models\n(BERT, RoBERTa, ELECTRA, etc.); and 3) user-customized neural features via a\nsimple configurable file. Benefiting from the advantages of flexibility and\nease of use, YATO can facilitate fast reproduction and refinement of\nstate-of-the-art NLP models, and promote the cross-disciplinary applications of\nNLP techniques. The code, examples, and documentation are publicly available at\nhttps:\/\/github.com\/jiesutd\/YATO. A demo video is also available at\nhttps:\/\/www.youtube.com\/playlist?list=PLJ0mhzMcRuDUlTkzBfAftOqiJRxYTTjXH.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.07834,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Entity-based Claim Representation Improves Fact-Checking of Medical\n  Content in Tweets\n\n  False medical information on social media poses harm to people's health.\nWhile the need for biomedical fact-checking has been recognized in recent\nyears, user-generated medical content has received comparably little attention.\nAt the same time, models for other text genres might not be reusable, because\nthe claims they have been trained with are substantially different. For\ninstance, claims in the SciFact dataset are short and focused: \"Side effects\nassociated with antidepressants increases risk of stroke\". In contrast, social\nmedia holds naturally-occurring claims, often embedded in additional context:\n\"`If you take antidepressants like SSRIs, you could be at risk of a condition\ncalled serotonin syndrome' Serotonin syndrome nearly killed me in 2010. Had\nsymptoms of stroke and seizure.\" This showcases the mismatch between real-world\nmedical claims and the input that existing fact-checking systems expect. To\nmake user-generated content checkable by existing models, we propose to\nreformulate the social-media input in such a way that the resulting claim\nmimics the claim characteristics in established datasets. To accomplish this,\nour method condenses the claim with the help of relational entity information\nand either compiles the claim out of an entity-relation-entity triple or\nextracts the shortest phrase that contains these elements. We show that the\nreformulated input improves the performance of various fact-checking models as\nopposed to checking the tweet text in its entirety.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.13736,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000086758,
      "text":"An Effective, Performant Named Entity Recognition System for Noisy\n  Business Telephone Conversation Transcripts\n\n  We present a simple yet effective method to train a named entity recognition\n(NER) model that operates on business telephone conversation transcripts that\ncontain noise due to the nature of spoken conversation and artifacts of\nautomatic speech recognition. We first fine-tune LUKE, a state-of-the-art Named\nEntity Recognition (NER) model, on a limited amount of transcripts, then use it\nas the teacher model to teach a smaller DistilBERT-based student model using a\nlarge amount of weakly labeled data and a small amount of human-annotated data.\nThe model achieves high accuracy while also satisfying the practical\nconstraints for inclusion in a commercial telephony product: realtime\nperformance when deployed on cost-effective CPUs rather than GPUs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.08359,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"From Disfluency Detection to Intent Detection and Slot Filling\n\n  We present the first empirical study investigating the influence of\ndisfluency detection on downstream tasks of intent detection and slot filling.\nWe perform this study for Vietnamese -- a low-resource language that has no\nprevious study as well as no public dataset available for disfluency detection.\nFirst, we extend the fluent Vietnamese intent detection and slot filling\ndataset PhoATIS by manually adding contextual disfluencies and annotating them.\nThen, we conduct experiments using strong baselines for disfluency detection\nand joint intent detection and slot filling, which are based on pre-trained\nlanguage models. We find that: (i) disfluencies produce negative effects on the\nperformances of the downstream intent detection and slot filling tasks, and\n(ii) in the disfluency context, the pre-trained multilingual language model\nXLM-R helps produce better intent detection and slot filling performances than\nthe pre-trained monolingual language model PhoBERT, and this is opposite to\nwhat generally found in the fluency context.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.12096,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000054306,
      "text":"Augmenting Multi-Turn Text-to-SQL Datasets with Self-Play\n\n  The task of context-dependent text-to-SQL aims to convert multi-turn user\nutterances to formal SQL queries. This is a challenging task due to both the\nscarcity of training data from which to learn complex contextual dependencies\nand to generalize to unseen databases. In this paper we explore augmenting the\ntraining datasets using self-play, which leverages contextual information to\nsynthesize new interactions to adapt the model to new databases. We first\ndesign a SQL-to-text model conditioned on a sampled goal query, which\nrepresents a user's intent, that then converses with a text-to-SQL semantic\nparser to generate new interactions. We then filter the synthesized\ninteractions and retrain the models with the augmented data. We find that\nself-play improves the accuracy of a strong baseline on SParC and CoSQL, two\nwidely used cross-domain text-to-SQL datasets. Our analysis shows that\nself-play simulates various conversational thematic relations, enhances\ncross-domain generalization and improves beam-search.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.12673,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0,
      "text":"Lexical Generalization Improves with Larger Models and Longer Training\n\n  While fine-tuned language models perform well on many tasks, they were also\nshown to rely on superficial surface features such as lexical overlap.\nExcessive utilization of such heuristics can lead to failure on challenging\ninputs. We analyze the use of lexical overlap heuristics in natural language\ninference, paraphrase detection, and reading comprehension (using a novel\ncontrastive dataset), and find that larger models are much less susceptible to\nadopting lexical overlap heuristics. We also find that longer training leads\nmodels to abandon lexical overlap heuristics. Finally, we provide evidence that\nthe disparity between models size has its source in the pre-trained model\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.08779,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000039074,
      "text":"Towards Summary Candidates Fusion\n\n  Sequence-to-sequence deep neural models fine-tuned for abstractive\nsummarization can achieve great performance on datasets with enough human\nannotations. Yet, it has been shown that they have not reached their full\npotential, with a wide gap between the top beam search output and the oracle\nbeam. Recently, re-ranking methods have been proposed, to learn to select a\nbetter summary candidate. However, such methods are limited by the summary\nquality aspects captured by the first-stage candidates. To bypass this\nlimitation, we propose a new paradigm in second-stage abstractive summarization\ncalled SummaFusion that fuses several summary candidates to produce a novel\nabstractive second-stage summary. Our method works well on several\nsummarization datasets, improving both the ROUGE scores and qualitative\nproperties of fused summaries. It is especially good when the candidates to\nfuse are worse, such as in the few-shot setup where we set a new\nstate-of-the-art. We will make our code and checkpoints available at\nhttps:\/\/github.com\/ntunlp\/SummaFusion\/.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.05921,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000610948,
      "text":"Step out of KG: Knowledge Graph Completion via Knowledgeable Retrieval\n  and Reading Comprehension\n\n  Knowledge graphs, as the cornerstone of many AI applications, usually face\nserious incompleteness problems. In recent years, there have been many efforts\nto study automatic knowledge graph completion (KGC), most of which use existing\nknowledge to infer new knowledge. However, in our experiments, we find that not\nall relations can be obtained by inference, which constrains the performance of\nexisting models. To alleviate this problem, we propose a new model based on\ninformation retrieval and reading comprehension, namely IR4KGC. Specifically,\nwe pre-train a knowledge-based information retrieval module that can retrieve\ndocuments related to the triples to be completed. Then, the retrieved documents\nare handed over to the reading comprehension module to generate the predicted\nanswers. In experiments, we find that our model can well solve relations that\ncannot be inferred from existing knowledge, and achieve good results on KGC\ndatasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.04373,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000041392,
      "text":"Contrastive Representation Learning for Conversational Question\n  Answering over Knowledge Graphs\n\n  This paper addresses the task of conversational question answering (ConvQA)\nover knowledge graphs (KGs). The majority of existing ConvQA methods rely on\nfull supervision signals with a strict assumption of the availability of gold\nlogical forms of queries to extract answers from the KG. However, creating such\na gold logical form is not viable for each potential question in a real-world\nscenario. Hence, in the case of missing gold logical forms, the existing\ninformation retrieval-based approaches use weak supervision via heuristics or\nreinforcement learning, formulating ConvQA as a KG path ranking problem.\nDespite missing gold logical forms, an abundance of conversational contexts,\nsuch as entire dialog history with fluent responses and domain information, can\nbe incorporated to effectively reach the correct KG path. This work proposes a\ncontrastive representation learning-based approach to rank KG paths\neffectively. Our approach solves two key challenges. Firstly, it allows weak\nsupervision-based learning that omits the necessity of gold annotations.\nSecond, it incorporates the conversational context (entire dialog history and\ndomain information) to jointly learn its homogeneous representation with KG\npaths to improve contrastive representations for effective path ranking. We\nevaluate our approach on standard datasets for ConvQA, on which it\nsignificantly outperforms existing baselines on all domains and overall.\nSpecifically, in some cases, the Mean Reciprocal Rank (MRR) and Hit@5 ranking\nmetrics improve by absolute 10 and 18 points, respectively, compared to the\nstate-of-the-art performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.06566,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000043379,
      "text":"Developing a general-purpose clinical language inference model from a\n  large corpus of clinical notes\n\n  Several biomedical language models have already been developed for clinical\nlanguage inference. However, these models typically utilize general\nvocabularies and are trained on relatively small clinical corpora. We sought to\nevaluate the impact of using a domain-specific vocabulary and a large clinical\ntraining corpus on the performance of these language models in clinical\nlanguage inference. We trained a Bidirectional Encoder Decoder from\nTransformers (BERT) model using a diverse, deidentified corpus of 75 million\ndeidentified clinical notes authored at the University of California, San\nFrancisco (UCSF). We evaluated this model on several clinical language\ninference benchmark tasks: clinical and temporal concept recognition, relation\nextraction and medical language inference. We also evaluated our model on two\ntasks using discharge summaries from UCSF: diagnostic code assignment and\ntherapeutic class inference. Our model performs at par with the best publicly\navailable biomedical language models of comparable sizes on the public\nbenchmark tasks, and is significantly better than these models in a\nwithin-system evaluation on the two tasks using UCSF data. The use of in-domain\nvocabulary appears to improve the encoding of longer documents. The use of\nlarge clinical corpora appears to enhance document encoding and inferential\naccuracy. However, further research is needed to improve abbreviation\nresolution, and numerical, temporal, and implicitly causal inference.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.1518,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000408623,
      "text":"Disentangled and Robust Representation Learning for Bragging\n  Classification in Social Media\n\n  Researching bragging behavior on social media arouses interest of\ncomputational (socio) linguists. However, existing bragging classification\ndatasets suffer from a serious data imbalance issue. Because labeling a\ndata-balance dataset is expensive, most methods introduce external knowledge to\nimprove model learning. Nevertheless, such methods inevitably introduce noise\nand non-relevance information from external knowledge. To overcome the\ndrawback, we propose a novel bragging classification method with\ndisentangle-based representation augmentation and domain-aware adversarial\nstrategy. Specifically, model learns to disentangle and reconstruct\nrepresentation and generate augmented features via disentangle-based\nrepresentation augmentation. Moreover, domain-aware adversarial strategy aims\nto constrain domain of augmented features to improve their robustness.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance compared to other methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.06726,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000024835,
      "text":"Explanations from Large Language Models Make Small Reasoners Better\n\n  Integrating free-text explanations to in-context learning of large language\nmodels (LLM) is shown to elicit strong reasoning capabilities along with\nreasonable explanations. In this paper, we consider the problem of leveraging\nthe explanations generated by LLM to improve the training of small reasoners,\nwhich are more favorable in real-production deployment due to their low cost.\nWe systematically explore three explanation generation approaches from LLM and\nutilize a multi-task learning framework to facilitate small models to acquire\nstrong reasoning power together with explanation generation capabilities.\nExperiments on multiple reasoning tasks show that our method can consistently\nand significantly outperform finetuning baselines across different settings,\nand even perform better than finetuning\/prompting a 60x larger GPT-3 (175B)\nmodel by up to 9.5% in accuracy. As a side benefit, human evaluation further\nshows that our method can generate high-quality explanations to justify its\npredictions, moving towards the goal of explainable AI.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.12563,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000084109,
      "text":"On the Limitations of Reference-Free Evaluations of Generated Text\n\n  There is significant interest in developing evaluation metrics which\naccurately estimate the quality of generated text without the aid of a\nhuman-written reference text, which can be time consuming and expensive to\ncollect or entirely unavailable in online applications. However, in this work,\nwe demonstrate that these reference-free metrics are inherently biased and\nlimited in their ability to evaluate generated text, and we argue that they\nshould not be used to measure progress on tasks like machine translation or\nsummarization. We show how reference-free metrics are equivalent to using one\ngeneration model to evaluate another, which has several limitations: (1) the\nmetrics can be optimized at test time to find the approximate best-possible\noutput, (2) they are inherently biased toward models which are more similar to\ntheir own, and (3) they can be biased against higher-quality outputs, including\nthose written by humans. Therefore, we recommend that reference-free metrics\nshould be used as diagnostic tools for analyzing and understanding model\nbehavior instead of measures of how well models perform a task, in which the\ngoal is to achieve as high of a score as possible.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.14328,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Causal Analysis of Syntactic Agreement Neurons in Multilingual Language\n  Models\n\n  Structural probing work has found evidence for latent syntactic information\nin pre-trained language models. However, much of this analysis has focused on\nmonolingual models, and analyses of multilingual models have employed\ncorrelational methods that are confounded by the choice of probing tasks. In\nthis study, we causally probe multilingual language models (XGLM and\nmultilingual BERT) as well as monolingual BERT-based models across various\nlanguages; we do this by performing counterfactual perturbations on neuron\nactivations and observing the effect on models' subject-verb agreement\nprobabilities. We observe where in the model and to what extent syntactic\nagreement is encoded in each language. We find significant neuron overlap\nacross languages in autoregressive multilingual language models, but not masked\nlanguage models. We also find two distinct layer-wise effect patterns and two\ndistinct sets of neurons used for syntactic agreement, depending on whether the\nsubject and verb are separated by other tokens. Finally, we find that\nbehavioral analyses of language models are likely underestimating how sensitive\nmasked language models are to syntactic information.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.06336,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000042054,
      "text":"Synthetic Text Detection: Systemic Literature Review\n\n  Within the text analysis and processing fields, generated text attacks have\nbeen made easier to create than ever before. To combat these attacks open\nsourcing models and datasets have become a major trend to create automated\ndetection algorithms in defense of authenticity. For this purpose, synthetic\ntext detection has become an increasingly viable topic of research. This review\nis written for the purpose of creating a snapshot of the state of current\nliterature and easing the barrier to entry for future authors. Towards that\ngoal, we identified few research trends and challenges in this field.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.03999,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000067221,
      "text":"ngram-OAXE: Phrase-Based Order-Agnostic Cross Entropy for\n  Non-Autoregressive Machine Translation\n\n  Recently, a new training oaxe loss has proven effective to ameliorate the\neffect of multimodality for non-autoregressive translation (NAT), which removes\nthe penalty of word order errors in the standard cross-entropy loss. Starting\nfrom the intuition that reordering generally occurs between phrases, we extend\noaxe by only allowing reordering between ngram phrases and still requiring a\nstrict match of word order within the phrases. Extensive experiments on NAT\nbenchmarks across language pairs and data scales demonstrate the effectiveness\nand universality of our approach. %Further analyses show that the proposed\nngram-oaxe alleviates the multimodality problem with a better modeling of\nphrase translation. Further analyses show that ngram-oaxe indeed improves the\ntranslation of ngram phrases, and produces more fluent translation with a\nbetter modeling of sentence structure.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.13845,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000058611,
      "text":"DialogConv: A Lightweight Fully Convolutional Network for Multi-view\n  Response Selection\n\n  Current end-to-end retrieval-based dialogue systems are mainly based on\nRecurrent Neural Networks or Transformers with attention mechanisms. Although\npromising results have been achieved, these models often suffer from slow\ninference or huge number of parameters. In this paper, we propose a novel\nlightweight fully convolutional architecture, called DialogConv, for response\nselection. DialogConv is exclusively built on top of convolution to extract\nmatching features of context and response. Dialogues are modeled in 3D views,\nwhere DialogConv performs convolution operations on embedding view, word view\nand utterance view to capture richer semantic information from multiple\ncontextual views. On the four benchmark datasets, compared with\nstate-of-the-art baselines, DialogConv is on average about 8.5x smaller in\nsize, and 79.39x and 10.64x faster on CPU and GPU devices, respectively. At the\nsame time, DialogConv achieves the competitive effectiveness of response\nselection.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.03405,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000097023,
      "text":"PARAGEN : A Parallel Generation Toolkit\n\n  PARAGEN is a PyTorch-based NLP toolkit for further development on parallel\ngeneration. PARAGEN provides thirteen types of customizable plugins, helping\nusers to experiment quickly with novel ideas across model architectures,\noptimization, and learning strategies. We implement various features, such as\nunlimited data loading and automatic model selection, to enhance its industrial\nusage. ParaGen is now deployed to support various research and industry\napplications at ByteDance. PARAGEN is available at\nhttps:\/\/github.com\/bytedance\/ParaGen.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.16841,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000008146,
      "text":"Actionable Phrase Detection using NLP\n\n  Actionable sentences are terms that, in the most basic sense, imply the\nnecessity of taking a specific action. In Linguistic terms, they are steps to\nachieve an operation, often through the usage of action verbs. For example, the\nsentence, `Get your homework finished by tomorrow` qualifies as actionable\nsince it demands a specific action (In this case, finishing homework) to be\ntaken. In contrast, a simple sentence such as, `I like to play the guitar` does\nnot qualify as an actionable phrase since it simply states a personal choice of\nthe person instead of demanding a task to be finished.\n  In this paper, the aim is to explore if Actionables can be extracted from raw\ntext using Linguistic filters designed from scratch. These filters are\nspecially catered to identifying actionable text using Transfer Learning as the\nlead role. Actionable Detection can be used in detecting emergency tasks during\na crisis, Instruction accuracy for First aid and can also be used to make\nproductivity tools like automatic ToDo list generators from conferences. To\naccomplish this, we use the Enron Email Dataset and apply our Linguistic\nfilters on the cleaned textual data. We then use Transfer Learning with the\nUniversal Sentence Encoder to train a model to classify whether a given string\nof raw text is actionable or not.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.12461,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000078148,
      "text":"Towards Efficient Dialogue Pre-training with Transferable and\n  Interpretable Latent Structure\n\n  With the availability of massive general-domain dialogue data, pre-trained\ndialogue generation appears to be super appealing to transfer knowledge from\nthe general domain to downstream applications. In most existing work, such\ntransferable ability is mainly obtained by fitting a large model with hundreds\nof millions of parameters on massive data in an exhaustive way, leading to\ninefficient running and poor interpretability. This paper proposes a novel\ndialogue generation model with a latent structure that is easily transferable\nfrom the general domain to downstream tasks in a lightweight and transparent\nway. Experiments on two benchmarks validate the effectiveness of the proposed\nmodel. Thanks to the transferable latent structure, our model is able to yield\nbetter dialogue responses than four strong baselines in terms of both automatic\nand human evaluations, and our model with about 22% parameters particularly\ndelivers a 5x speedup in running time compared with the strongest baseline.\nMoreover, the proposed model is explainable by interpreting the discrete latent\nvariables.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.00553,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000077155,
      "text":"ALT: A software for readability analysis of Portuguese-language texts\n\n  In the initial stage of human life, communication, seen as a process of\nsocial interaction, was always the best way to reach consensus between the\nparties. Understanding and credibility in this process are essential for the\nmutual agreement to be validated. But, how to do it so that this communication\nreaches the great mass? This is the main challenge when what is sought is the\ndissemination of information and its approval. In this context, this study\npresents the ALT software, developed from original readability metrics adapted\nto the Portuguese language, available on the web, to reduce communication\ndifficulties. The development of the software was motivated by the theory of\ncommunicative action of Habermas, which uses a multidisciplinary style to\nmeasure the credibility of the discourse in the communication channels used to\nbuild and maintain a safe and healthy relationship with the public.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.04473,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000037418,
      "text":"Leveraging Key Information Modeling to Improve Less-Data Constrained\n  News Headline Generation via Duality Fine-Tuning\n\n  Recent language generative models are mostly trained on large-scale datasets,\nwhile in some real scenarios, the training datasets are often expensive to\nobtain and would be small-scale. In this paper we investigate the challenging\ntask of less-data constrained generation, especially when the generated news\nheadlines are short yet expected by readers to keep readable and informative\nsimultaneously. We highlight the key information modeling task and propose a\nnovel duality fine-tuning method by formally defining the probabilistic duality\nconstraints between key information prediction and headline generation tasks.\nThe proposed method can capture more information from limited data, build\nconnections between separate tasks, and is suitable for less-data constrained\ngeneration tasks. Furthermore, the method can leverage various pre-trained\ngenerative regimes, e.g., autoregressive and encoder-decoder models. We conduct\nextensive experiments to demonstrate that our method is effective and efficient\nto achieve improved performance in terms of language modeling metric and\ninformativeness correctness metric on two public datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.02574,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000039736,
      "text":"Privacy-Preserving Text Classification on BERT Embeddings with\n  Homomorphic Encryption\n\n  Embeddings, which compress information in raw text into semantics-preserving\nlow-dimensional vectors, have been widely adopted for their efficacy. However,\nrecent research has shown that embeddings can potentially leak private\ninformation about sensitive attributes of the text, and in some cases, can be\ninverted to recover the original input text. To address these growing privacy\nchallenges, we propose a privatization mechanism for embeddings based on\nhomomorphic encryption, to prevent potential leakage of any piece of\ninformation in the process of text classification. In particular, our method\nperforms text classification on the encryption of embeddings from\nstate-of-the-art models like BERT, supported by an efficient GPU implementation\nof CKKS encryption scheme. We show that our method offers encrypted protection\nof BERT embeddings, while largely preserving their utility on downstream text\nclassification tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.04073,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"On Task-Adaptive Pretraining for Dialogue Response Selection\n\n  Recent advancements in dialogue response selection (DRS) are based on the\n\\textit{task-adaptive pre-training (TAP)} approach, by first initializing their\nmodel with BERT~\\cite{devlin-etal-2019-bert}, and adapt to dialogue data with\ndialogue-specific or fine-grained pre-training tasks. However, it is uncertain\nwhether BERT is the best initialization choice, or whether the proposed\ndialogue-specific fine-grained learning tasks are actually better than MLM+NSP.\nThis paper aims to verify assumptions made in previous works and understand the\nsource of improvements for DRS. We show that initializing with RoBERTa achieve\nsimilar performance as BERT, and MLM+NSP can outperform all previously proposed\nTAP tasks, during which we also contribute a new state-of-the-art on the Ubuntu\ncorpus. Additional analyses shows that the main source of improvements comes\nfrom the TAP step, and that the NSP task is crucial to DRS, different from\ncommon NLU tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.1447,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000151992,
      "text":"Towards Better Document-level Relation Extraction via Iterative\n  Inference\n\n  Document-level relation extraction (RE) aims to extract the relations between\nentities from the input document that usually containing many\ndifficultly-predicted entity pairs whose relations can only be predicted\nthrough relational inference. Existing methods usually directly predict the\nrelations of all entity pairs of input document in a one-pass manner, ignoring\nthe fact that predictions of some entity pairs heavily depend on the predicted\nresults of other pairs. To deal with this issue, in this paper, we propose a\nnovel document-level RE model with iterative inference. Our model is mainly\ncomposed of two modules: 1) a base module expected to provide preliminary\nrelation predictions on entity pairs; 2) an inference module introduced to\nrefine these preliminary predictions by iteratively dealing with\ndifficultly-predicted entity pairs depending on other pairs in an easy-to-hard\nmanner. Unlike previous methods which only consider feature information of\nentity pairs, our inference module is equipped with two Extended Cross\nAttention units, allowing it to exploit both feature information and previous\npredictions of entity pairs during relational inference. Furthermore, we adopt\na two-stage strategy to train our model. At the first stage, we only train our\nbase module. During the second stage, we train the whole model, where\ncontrastive learning is introduced to enhance the training of inference module.\nExperimental results on three commonly-used datasets show that our model\nconsistently outperforms other competitive baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.15916,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"BotSIM: An End-to-End Bot Simulation Toolkit for Commercial\n  Task-Oriented Dialog Systems\n\n  We introduce BotSIM, a modular, open-source Bot SIMulation environment with\ndialog generation, user simulation and conversation analytics capabilities.\nBotSIM aims to serve as a one-stop solution for large-scale data-efficient\nend-to-end evaluation, diagnosis and remediation of commercial task-oriented\ndialog (TOD) systems to significantly accelerate commercial bot development and\nevaluation, reduce cost and time-to-market. BotSIM adopts a layered design\ncomprising the infrastructure layer, the adaptor layer and the application\nlayer. The infrastructure layer hosts key models and components to support\nBotSIM's major functionalities via a streamlined\n\"generation-simulation-remediation\" pipeline. The adaptor layer is used to\nextend BotSIM to accommodate new bot platforms. The application layer provides\na suite of command line tools and a Web App to significantly lower the entry\nbarrier for BotSIM users such as bot admins or practitioners. In this report,\nwe focus on the technical designs of various system components. A detailed case\nstudy using Einstein BotBuilder is also presented to show how to apply BotSIM\npipeline for bot evaluation and remediation. The detailed system descriptions\ncan be found in our system demo paper. The toolkit is available at:\nhttps:\/\/github.com\/salesforce\/BotSIM .\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.13892,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000068214,
      "text":"Complementary Explanations for Effective In-Context Learning\n\n  Large language models (LLMs) have exhibited remarkable capabilities in\nlearning from explanations in prompts, but there has been limited understanding\nof exactly how these explanations function or why they are effective. This work\naims to better understand the mechanisms by which explanations are used for\nin-context learning. We first study the impact of two different factors on the\nperformance of prompts with explanations: the computation trace (the way the\nsolution is decomposed) and the natural language used to express the prompt. By\nperturbing explanations on three controlled tasks, we show that both factors\ncontribute to the effectiveness of explanations. We further study how to form\nmaximally effective sets of explanations for solving a given test query. We\nfind that LLMs can benefit from the complementarity of the explanation set:\ndiverse reasoning skills shown by different exemplars can lead to better\nperformance. Therefore, we propose a maximal marginal relevance-based exemplar\nselection approach for constructing exemplar sets that are both relevant as\nwell as complementary, which successfully improves the in-context learning\nperformance across three real-world tasks on multiple LLMs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.13718,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"Emotion-guided Cross-domain Fake News Detection using Adversarial Domain\n  Adaptation\n\n  Recent works on fake news detection have shown the efficacy of using emotions\nas a feature or emotions-based features for improved performance. However, the\nimpact of these emotion-guided features for fake news detection in cross-domain\nsettings, where we face the problem of domain shift, is still largely\nunexplored. In this work, we evaluate the impact of emotion-guided features for\ncross-domain fake news detection, and further propose an emotion-guided,\ndomain-adaptive approach using adversarial learning. We prove the efficacy of\nemotion-guided models in cross-domain settings for various combinations of\nsource and target datasets from FakeNewsAMT, Celeb, Politifact and Gossipcop\ndatasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.04668,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000212921,
      "text":"Zero-Label Prompt Selection\n\n  Natural language prompts have been shown to facilitate cross-task\ngeneralization for large language models. However, with no or limited labeled\nexamples, the cross-task performance is highly sensitive to the choice of\nprompts, while selecting a high-performing prompt is challenging given the\nscarcity of labels. To address the issue, we propose a Zero-Label Prompt\nSelection (ZPS) method that selects prompts without any labeled data or\ngradient update. Specifically, given the candidate human-written prompts for a\ntask, ZPS labels a set of unlabeled data with a prompt ensemble and uses the\npseudo-labels for prompt selection. Experiments show that ZPS improves over\nprior methods by a sizeable margin in zero-label performance. We also extend\nZPS to a few-shot setting and show its advantages over strong baselines such as\nprompt tuning and model tuning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.04699,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000100334,
      "text":"FF2: A Feature Fusion Two-Stream Framework for Punctuation Restoration\n\n  To accomplish punctuation restoration, most existing methods focus on\nintroducing extra information (e.g., part-of-speech) or addressing the class\nimbalance problem. Recently, large-scale transformer-based pre-trained language\nmodels (PLMS) have been utilized widely and obtained remarkable success.\nHowever, the PLMS are trained on the large dataset with marks, which may not\nfit well with the small dataset without marks, causing the convergence to be\nnot ideal. In this study, we propose a Feature Fusion two-stream framework\n(FF2) to bridge the gap. Specifically, one stream leverages a pre-trained\nlanguage model to capture the semantic feature, while another auxiliary module\ncaptures the feature at hand. We also modify the computation of multi-head\nattention to encourage communication among heads. Then, two features with\ndifferent perspectives are aggregated to fuse information and enhance context\nawareness. Without additional data, the experimental results on the popular\nbenchmark IWSLT demonstrate that FF2 achieves new SOTA performance, which\nverifies that our approach is effective.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.11554,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000166562,
      "text":"Programming by Example and Text-to-Code Translation for Conversational\n  Code Generation\n\n  Dialogue systems is an increasingly popular task of natural language\nprocessing. However, the dialogue paths tend to be deterministic, restricted to\nthe system rails, regardless of the given request or input text. Recent\nadvances in program synthesis have led to systems which can synthesize programs\nfrom very general search spaces, e.g. Programming by Example, and to systems\nwith very accessible interfaces for writing programs, e.g. text-to-code\ntranslation, but have not achieved both of these qualities in the same system.\nWe propose Modular Programs for Text-guided Hierarchical Synthesis (MPaTHS), a\nmethod for integrating Programming by Example and text-to-code systems which\noffers an accessible natural language interface for synthesizing general\nprograms. We present a program representation that allows our method to be\napplied to the problem of task-oriented dialogue. Finally, we demo MPaTHS using\nour program representation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.07067,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Retrieval-Augmented Generative Question Answering for Event Argument\n  Extraction\n\n  Event argument extraction has long been studied as a sequential prediction\nproblem with extractive-based methods, tackling each argument in isolation.\nAlthough recent work proposes generation-based methods to capture\ncross-argument dependency, they require generating and post-processing a\ncomplicated target sequence (template). Motivated by these observations and\nrecent pretrained language models' capabilities of learning from\ndemonstrations. We propose a retrieval-augmented generative QA model (R-GQA)\nfor event argument extraction. It retrieves the most similar QA pair and\naugments it as prompt to the current example's context, then decodes the\narguments as answers. Our approach outperforms substantially prior methods\nacross various settings (i.e. fully supervised, domain transfer, and fewshot\nlearning). Finally, we propose a clustering-based sampling strategy (JointEnc)\nand conduct a thorough analysis of how different strategies influence the\nfew-shot learning performance. The implementations are available at https:\/\/\ngithub.com\/xinyadu\/RGQA\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.04698,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000078811,
      "text":"Unsupervised Extractive Summarization with Heterogeneous Graph\n  Embeddings for Chinese Document\n\n  In the scenario of unsupervised extractive summarization, learning\nhigh-quality sentence representations is essential to select salient sentences\nfrom the input document. Previous studies focus more on employing statistical\napproaches or pre-trained language models (PLMs) to extract sentence\nembeddings, while ignoring the rich information inherent in the heterogeneous\ntypes of interaction between words and sentences. In this paper, we are the\nfirst to propose an unsupervised extractive summarizaiton method with\nheterogeneous graph embeddings (HGEs) for Chinese document. A heterogeneous\ntext graph is constructed to capture different granularities of interactions by\nincorporating graph structural information. Moreover, our proposed graph is\ngeneral and flexible where additional nodes such as keywords can be easily\nintegrated. Experimental results demonstrate that our method consistently\noutperforms the strong baseline in three summarization datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.08316,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"FolkScope: Intention Knowledge Graph Construction for E-commerce\n  Commonsense Discovery\n\n  Understanding users' intentions in e-commerce platforms requires commonsense\nknowledge. In this paper, we present FolkScope, an intention knowledge graph\nconstruction framework to reveal the structure of humans' minds about\npurchasing items. As commonsense knowledge is usually ineffable and not\nexpressed explicitly, it is challenging to perform information extraction.\nThus, we propose a new approach that leverages the generation power of large\nlanguage models~(LLMs) and human-in-the-loop annotation to semi-automatically\nconstruct the knowledge graph. LLMs first generate intention assertions via\ne-commerce-specific prompts to explain shopping behaviors, where the intention\ncan be an open reason or a predicate falling into one of 18 categories aligning\nwith ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibility\nand typicality labels of sampled intentions as training data in order to\npopulate human judgments to all automatic generations. Last, to structurize the\nassertions, we propose pattern mining and conceptualization to form more\ncondensed and abstract knowledge. Extensive evaluations and studies demonstrate\nthat our constructed knowledge graph can well model e-commerce knowledge and\nhave many potential applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.10808,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000092387,
      "text":"Combining State-of-the-Art Models with Maximal Marginal Relevance for\n  Few-Shot and Zero-Shot Multi-Document Summarization\n\n  In Natural Language Processing, multi-document summarization (MDS) poses many\nchallenges to researchers above those posed by single-document summarization\n(SDS). These challenges include the increased search space and greater\npotential for the inclusion of redundant information. While advancements in\ndeep learning approaches have led to the development of several advanced\nlanguage models capable of summarization, the variety of training data specific\nto the problem of MDS remains relatively limited. Therefore, MDS approaches\nwhich require little to no pretraining, known as few-shot or zero-shot\napplications, respectively, could be beneficial additions to the current set of\ntools available in summarization. To explore one possible approach, we devise a\nstrategy for combining state-of-the-art models' outputs using maximal marginal\nrelevance (MMR) with a focus on query relevance rather than document diversity.\nOur MMR-based approach shows improvement over some aspects of the current\nstate-of-the-art results in both few-shot and zero-shot MDS applications while\nmaintaining a state-of-the-art standard of output by all available metrics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.00974,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"Processing Long Legal Documents with Pre-trained Transformers: Modding\n  LegalBERT and Longformer\n\n  Pre-trained Transformers currently dominate most NLP tasks. They impose,\nhowever, limits on the maximum input length (512 sub-words in BERT), which are\ntoo restrictive in the legal domain. Even sparse-attention models, such as\nLongformer and BigBird, which increase the maximum input length to 4,096\nsub-words, severely truncate texts in three of the six datasets of LexGLUE.\nSimpler linear classifiers with TF-IDF features can handle texts of any length,\nrequire far less resources to train and deploy, but are usually outperformed by\npre-trained Transformers. We explore two directions to cope with long legal\ntexts: (i) modifying a Longformer warm-started from LegalBERT to handle even\nlonger texts (up to 8,192 sub-words), and (ii) modifying LegalBERT to use\nTF-IDF representations. The first approach is the best in terms of performance,\nsurpassing a hierarchical version of LegalBERT, which was the previous state of\nthe art in LexGLUE. The second approach leads to computationally more efficient\nmodels at the expense of lower performance, but the resulting models still\noutperform overall a linear SVM with TF-IDF features in long legal document\nclassification.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.08369,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency\n  Methods\n\n  A popular approach to unveiling the black box of neural NLP models is to\nleverage saliency methods, which assign scalar importance scores to each input\ncomponent. A common practice for evaluating whether an interpretability method\nis faithful has been to use evaluation-by-agreement -- if multiple methods\nagree on an explanation, its credibility increases. However, recent work has\nfound that saliency methods exhibit weak rank correlations even when applied to\nthe same model instance and advocated for the use of alternative diagnostic\nmethods. In our work, we demonstrate that rank correlation is not a good fit\nfor evaluating agreement and argue that Pearson-$r$ is a better-suited\nalternative. We further show that regularization techniques that increase\nfaithfulness of attention explanations also increase agreement between saliency\nmethods. By connecting our findings to instance categories based on training\ndynamics, we show that the agreement of saliency method explanations is very\nlow for easy-to-learn instances. Finally, we connect the improvement in\nagreement across instance categories to local representation space statistics\nof instances, paving the way for work on analyzing which intrinsic model\nproperties improve their predisposition to interpretability methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.07954,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"An Overview on Controllable Text Generation via Variational\n  Auto-Encoders\n\n  Recent advances in neural-based generative modeling have reignited the hopes\nof having computer systems capable of conversing with humans and able to\nunderstand natural language. The employment of deep neural architectures has\nbeen largely explored in a multitude of context and tasks to fulfill various\nuser needs. On one hand, producing textual content that meets specific\nrequirements is of priority for a model to seamlessly conduct conversations\nwith different groups of people. On the other hand, latent variable models\n(LVM) such as variational auto-encoders (VAEs) as one of the most popular\ngenres of generative models are designed to characterize the distributional\npattern of textual data. Thus they are inherently capable of learning the\nintegral textual features that are worth exploring for controllable pursuits.\n  \\noindent This overview gives an introduction to existing generation schemes,\nproblems associated with text variational auto-encoders, and a review of\nseveral applications about the controllable generation that are instantiations\nof these general formulations,\\footnote{A detailed paper list is available at\n\\url{https:\/\/github.com\/ImKeTT\/CTG-latentAEs}} as well as related datasets,\nmetrics and discussions for future researches. Hopefully, this overview will\nprovide an overview of living questions, popular methodologies and raw thoughts\nfor controllable language generation under the scope of variational\nauto-encoder.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.02011,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000129475,
      "text":"Inverse scaling can become U-shaped\n\n  Scaling up language models has been empirically shown to improve performance\non a wide range of downstream tasks. However, if we were to observe worse\nperformance as a function of scale (\"inverse scaling\") on certain tasks, this\nwould indicate that scaling can also encourage behaviors that are misaligned\nwith human preferences. The Inverse Scaling Prize (McKenzie et al. 2022)\nidentified eleven such inverse scaling tasks, evaluated on models of up to 280B\nparameters and up to 500 zettaFLOPs of training compute. This paper takes a\ncloser look at these inverse scaling tasks. We evaluate models of up to 540B\nparameters, trained on five times more compute than those evaluated in the\nInverse Scaling Prize. With this increased range of model sizes and training\ncompute, only four out of the eleven tasks remain inverse scaling. Six out of\nthe eleven tasks exhibit \"U-shaped scaling\", where performance decreases up to\na certain size, and then increases again up to the largest model evaluated (the\none remaining task displays positive scaling). In addition, we find that 1-shot\nexamples and chain-of-thought can help mitigate undesirable scaling patterns\neven further. U-shaped scaling suggests that the inverse scaling trend observed\nin McKenzie et al. (2022) may not continue to hold for larger models, which we\nattribute to the presence of distractor tasks that only sufficiently large\nmodels can avoid.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.04052,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000040068,
      "text":"What Knowledge Is Needed? Towards Explainable Memory for kNN-MT Domain\n  Adaptation\n\n  kNN-MT presents a new paradigm for domain adaptation by building an external\ndatastore, which usually saves all target language token occurrences in the\nparallel corpus. As a result, the constructed datastore is usually large and\npossibly redundant. In this paper, we investigate the interpretability issue of\nthis approach: what knowledge does the NMT model need? We propose the notion of\nlocal correctness (LAC) as a new angle, which describes the potential\ntranslation correctness for a single entry and for a given neighborhood.\nEmpirical study shows that our investigation successfully finds the conditions\nwhere the NMT model could easily fail and need related knowledge. Experiments\non six diverse target domains and two language-pairs show that pruning\naccording to local correctness brings a light and more explainable memory for\nkNN-MT domain adaptation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.15556,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Attack on Unfair ToS Clause Detection: A Case Study using Universal\n  Adversarial Triggers\n\n  Recent work has demonstrated that natural language processing techniques can\nsupport consumer protection by automatically detecting unfair clauses in the\nTerms of Service (ToS) Agreement. This work demonstrates that transformer-based\nToS analysis systems are vulnerable to adversarial attacks. We conduct\nexperiments attacking an unfair-clause detector with universal adversarial\ntriggers. Experiments show that a minor perturbation of the text can\nconsiderably reduce the detection performance. Moreover, to measure the\ndetectability of the triggers, we conduct a detailed human evaluation study by\ncollecting both answer accuracy and response time from the participants. The\nresults show that the naturalness of the triggers remains key to tricking\nreaders.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.13376,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"InDEX: Indonesian Idiom and Expression Dataset for Cloze Test\n\n  We propose InDEX, an Indonesian Idiom and Expression dataset for cloze test.\nThe dataset contains 10438 unique sentences for 289 idioms and expressions for\nwhich we generate 15 different types of distractors, resulting in a large\ncloze-style corpus. Many baseline models of cloze test reading comprehension\napply BERT with random initialization to learn embedding representation. But\nidioms and fixed expressions are different such that the literal meaning of the\nphrases may or may not be consistent with their contextual meaning. Therefore,\nwe explore different ways to combine static and contextual representations for\na stronger baseline model. Experimentations show that combining definition and\nrandom initialization will better support cloze test model performance for\nidioms whether independently or mixed with fixed expressions. While for fixed\nexpressions with no special meaning, static embedding with random\ninitialization is sufficient for cloze test model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.1033,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000175171,
      "text":"GENIUS: Sketch-based Language Model Pre-training via Extreme and\n  Selective Masking for Text Generation and Augmentation\n\n  We introduce GENIUS: a conditional text generation model using sketches as\ninput, which can fill in the missing contexts for a given sketch (key\ninformation consisting of textual spans, phrases, or words, concatenated by\nmask tokens). GENIUS is pre-trained on a large-scale textual corpus with a\nnovel reconstruction from sketch objective using an extreme and selective\nmasking strategy, enabling it to generate diverse and high-quality texts given\nsketches. Comparison with other competitive conditional language models (CLMs)\nreveals the superiority of GENIUS's text generation quality. We further show\nthat GENIUS can be used as a strong and ready-to-use data augmentation tool for\nvarious natural language processing (NLP) tasks. Most existing textual data\naugmentation methods are either too conservative, by making small changes to\nthe original text, or too aggressive, by creating entirely new samples. With\nGENIUS, we propose GeniusAug, which first extracts the target-aware sketches\nfrom the original training set and then generates new samples based on the\nsketches. Empirical experiments on 6 text classification datasets show that\nGeniusAug significantly improves the models' performance in both\nin-distribution (ID) and out-of-distribution (OOD) settings. We also\ndemonstrate the effectiveness of GeniusAug on named entity recognition (NER)\nand machine reading comprehension (MRC) tasks. (Code and models are publicly\navailable at https:\/\/github.com\/microsoft\/SCGLab and\nhttps:\/\/github.com\/beyondguo\/genius)\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.0399,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000130468,
      "text":"Robust Unstructured Knowledge Access in Conversational Dialogue with ASR\n  Errors\n\n  Performance of spoken language understanding (SLU) can be degraded with\nautomatic speech recognition (ASR) errors. We propose a novel approach to\nimprove SLU robustness by randomly corrupting clean training text with an ASR\nerror simulator, followed by self-correcting the errors and minimizing the\ntarget classification loss in a joint manner. In the proposed error simulator,\nwe leverage confusion networks generated from an ASR decoder without human\ntranscriptions to generate a variety of error patterns for model training. We\nevaluate our approach on the DSTC10 challenge targeted for knowledge-grounded\ntask-oriented conversational dialogues with ASR errors. Experimental results\nshow the effectiveness of our proposed approach, boosting the knowledge-seeking\nturn detection (KTD) F1 significantly from 0.9433 to 0.9904. Knowledge cluster\nclassification is boosted from 0.7924 to 0.9333 in Recall@1. After knowledge\ndocument re-ranking, our approach shows significant improvement in all\nknowledge selection metrics, from 0.7358 to 0.7806 in Recall@1, from 0.8301 to\n0.9333 in Recall@5, and from 0.7798 to 0.8460 in MRR@5 on the test set. In the\nrecent DSTC10 evaluation, our approach demonstrates significant improvement in\nknowledge selection, boosting Recall@1 from 0.495 to 0.7144 compared to the\nofficial baseline. Our source code is released in GitHub\nhttps:\/\/github.com\/yctam\/dstc10_track2_task2.git.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.02762,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000076161,
      "text":"Automated Identification of Eviction Status from Electronic Health\n  Record Notes\n\n  Objective: Evictions are important social and behavioral determinants of\nhealth. Evictions are associated with a cascade of negative events that can\nlead to unemployment, housing insecurity\/homelessness, long-term poverty, and\nmental health problems. In this study, we developed a natural language\nprocessing system to automatically detect eviction status from electronic\nhealth record (EHR) notes.\n  Materials and Methods: We first defined eviction status (eviction presence\nand eviction period) and then annotated eviction status in 5000 EHR notes from\nthe Veterans Health Administration (VHA). We developed a novel model, KIRESH,\nthat has shown to substantially outperform other state-of-the-art models such\nas fine-tuning pre-trained language models like BioBERT and BioClinicalBERT.\nMoreover, we designed a novel prompt to further improve the model performance\nby using the intrinsic connection between the two sub-tasks of eviction\npresence and period prediction. Finally, we used the Temperature Scaling-based\nCalibration on our KIRESH-Prompt method to avoid over-confidence issues arising\nfrom the imbalance dataset.\n  Results: KIRESH-Prompt substantially outperformed strong baseline models\nincluding fine-tuning the BioClinicalBERT model to achieve 0.74672 MCC, 0.71153\nMacro-F1, and 0.83396 Micro-F1 in predicting eviction period and 0.66827 MCC,\n0.62734 Macro-F1, and 0.7863 Micro-F1 in predicting eviction presence. We also\nconducted additional experiments on a benchmark social determinants of health\n(SBDH) dataset to demonstrate the generalizability of our methods.\n  Conclusion and Future Work: KIRESH-Prompt has substantially improved eviction\nstatus classification. We plan to deploy KIRESH-Prompt to the VHA EHRs as an\neviction surveillance system to help address the US Veterans' housing\ninsecurity.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.12799,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000196364,
      "text":"A Comprehensive Study of Gender Bias in Chemical Named Entity\n  Recognition Models\n\n  Chemical named entity recognition (NER) models are used in many downstream\ntasks, from adverse drug reaction identification to pharmacoepidemiology.\nHowever, it is unknown whether these models work the same for everyone.\nPerformance disparities can potentially cause harm rather than the intended\ngood. This paper assesses gender-related performance disparities in chemical\nNER systems. We develop a framework for measuring gender bias in chemical NER\nmodels using synthetic data and a newly annotated corpus of over 92,405 words\nwith self-identified gender information from Reddit. Our evaluation of multiple\nbiomedical NER models reveals evident biases. For instance, synthetic data\nsuggests female-related names are frequently misclassified as chemicals,\nespecially for brand name mentions. Additionally, we observe performance\ndisparities between female- and male-associated data in both datasets. Many\nsystems fail to detect contraceptives such as birth control. Our findings\nemphasize the biases in chemical NER models, urging practitioners to account\nfor these biases in downstream applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.10392,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"Debiasing Stance Detection Models with Counterfactual Reasoning and\n  Adversarial Bias Learning\n\n  Stance detection models may tend to rely on dataset bias in the text part as\na shortcut and thus fail to sufficiently learn the interaction between the\ntargets and texts. Recent debiasing methods usually treated features learned by\nsmall models or big models at earlier steps as bias features and proposed to\nexclude the branch learning those bias features during inference. However, most\nof these methods fail to disentangle the ``good'' stance features and ``bad''\nbias features in the text part. In this paper, we investigate how to mitigate\ndataset bias in stance detection. Motivated by causal effects, we leverage a\nnovel counterfactual inference framework, which enables us to capture the\ndataset bias in the text part as the direct causal effect of the text on\nstances and reduce the dataset bias in the text part by subtracting the direct\ntext effect from the total causal effect. We novelly model bias features as\nfeatures that correlate with the stance labels but fail on intermediate stance\nreasoning subtasks and propose an adversarial bias learning module to model the\nbias more accurately. To verify whether our model could better model the\ninteraction between texts and targets, we test our model on recently proposed\ntest sets to evaluate the understanding of the task from various aspects.\nExperiments demonstrate that our proposed method (1) could better model the\nbias features, and (2) outperforms existing debiasing baselines on both the\noriginal dataset and most of the newly constructed test sets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.1052,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Privacy-Preserving Domain Adaptation of Semantic Parsers\n\n  Task-oriented dialogue systems often assist users with personal or\nconfidential matters. For this reason, the developers of such a system are\ngenerally prohibited from observing actual usage. So how can they know where\nthe system is failing and needs more training data or new functionality? In\nthis work, we study ways in which realistic user utterances can be generated\nsynthetically, to help increase the linguistic and functional coverage of the\nsystem, without compromising the privacy of actual users. To this end, we\npropose a two-stage Differentially Private (DP) generation method which first\ngenerates latent semantic parses, and then generates utterances based on the\nparses. Our proposed approach improves MAUVE by 2.5$\\times$ and parse tree\nfunction type overlap by 1.3$\\times$ relative to current approaches for private\nsynthetic data generation, improving both on fluency and semantic coverage. We\nfurther validate our approach on a realistic domain adaptation task of adding\nnew functionality from private user data to a semantic parser, and show overall\ngains of 8.5% points in accuracy with the new feature.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.05373,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Topic-Aware Response Generation in Task-Oriented Dialogue with\n  Unstructured Knowledge Access\n\n  To alleviate the problem of structured databases' limited coverage, recent\ntask-oriented dialogue systems incorporate external unstructured knowledge to\nguide the generation of system responses. However, these usually use word or\nsentence level similarities to detect the relevant knowledge context, which\nonly partially capture the topical level relevance. In this paper, we examine\nhow to better integrate topical information in knowledge grounded task-oriented\ndialogue and propose ``Topic-Aware Response Generation'' (TARG), an end-to-end\nresponse generation model. TARG incorporates multiple topic-aware attention\nmechanisms to derive the importance weighting scheme over dialogue utterances\nand external knowledge sources towards a better understanding of the dialogue\nhistory. Experimental results indicate that TARG achieves state-of-the-art\nperformance in knowledge selection and response generation, outperforming\nprevious state-of-the-art by 3.2, 3.6, and 4.2 points in EM, F1 and BLEU-4\nrespectively on Doc2Dial, and performing comparably with previous work on\nDSTC9; both being knowledge-grounded task-oriented dialogue datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.09577,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"CiteBench: A benchmark for Scientific Citation Text Generation\n\n  Science progresses by building upon the prior body of knowledge documented in\nscientific publications. The acceleration of research makes it hard to stay\nup-to-date with the recent developments and to summarize the ever-growing body\nof prior work. To address this, the task of citation text generation aims to\nproduce accurate textual summaries given a set of papers-to-cite and the citing\npaper context. Due to otherwise rare explicit anchoring of cited documents in\nthe citing paper, citation text generation provides an excellent opportunity to\nstudy how humans aggregate and synthesize textual knowledge from sources. Yet,\nexisting studies are based upon widely diverging task definitions, which makes\nit hard to study this task systematically. To address this challenge, we\npropose CiteBench: a benchmark for citation text generation that unifies\nmultiple diverse datasets and enables standardized evaluation of citation text\ngeneration models across task designs and domains. Using the new benchmark, we\ninvestigate the performance of multiple strong baselines, test their\ntransferability between the datasets, and deliver new insights into the task\ndefinition and evaluation to guide future research in citation text generation.\nWe make the code for CiteBench publicly available at\nhttps:\/\/github.com\/UKPLab\/citebench.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.01146,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000058942,
      "text":"SumREN: Summarizing Reported Speech about Events in News\n\n  A primary objective of news articles is to establish the factual record for\nan event, frequently achieved by conveying both the details of the specified\nevent (i.e., the 5 Ws; Who, What, Where, When and Why regarding the event) and\nhow people reacted to it (i.e., reported statements). However, existing work on\nnews summarization almost exclusively focuses on the event details. In this\nwork, we propose the novel task of summarizing the reactions of different\nspeakers, as expressed by their reported statements, to a given event. To this\nend, we create a new multi-document summarization benchmark, SUMREN, comprising\n745 summaries of reported statements from various public figures obtained from\n633 news articles discussing 132 events. We propose an automatic silver\ntraining data generation approach for our task, which helps smaller models like\nBART achieve GPT-3 level performance on this task. Finally, we introduce a\npipeline-based framework for summarizing reported speech, which we empirically\nshow to generate summaries that are more abstractive and factual than baseline\nquery-focused summarization approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.09666,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000419882,
      "text":"MultiCoder: Multi-Programming-Lingual Pre-Training for Low-Resource Code\n  Completion\n\n  Code completion is a valuable topic in both academia and industry. Recently,\nlarge-scale mono-programming-lingual (MonoPL) pre-training models have been\nproposed to boost the performance of code completion. However, the code\ncompletion on low-resource programming languages (PL) is difficult for the\ndata-driven paradigm, while there are plenty of developers using low-resource\nPLs. On the other hand, there are few studies exploring the effects of\nmulti-programming-lingual (MultiPL) pre-training for the code completion,\nespecially the impact on low-resource programming languages. To this end, we\npropose the MultiCoder to enhance the low-resource code completion via MultiPL\npre-training and MultiPL Mixture-of-Experts (MoE) layers. We further propose a\nnovel PL-level MoE routing strategy (PL-MoE) for improving the code completion\non all PLs. Experimental results on CodeXGLUE and MultiCC demonstrate that 1)\nthe proposed MultiCoder significantly outperforms the MonoPL baselines on\nlow-resource programming languages, and 2) the PL-MoE module further boosts the\nperformance on six programming languages. In addition, we analyze the effects\nof the proposed method in details and explore the effectiveness of our method\nin a variety of scenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2301.0515,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000056956,
      "text":"Unsupervised Question Duplicate and Related Questions Detection in\n  e-learning platforms\n\n  Online learning platforms provide diverse questions to gauge the learners'\nunderstanding of different concepts. The repository of questions has to be\nconstantly updated to ensure a diverse pool of questions to conduct assessments\nfor learners. However, it is impossible for the academician to manually skim\nthrough the large repository of questions to check for duplicates when\nonboarding new questions from external sources. Hence, we propose a tool QDup\nin this paper that can surface near-duplicate and semantically related\nquestions without any supervised data. The proposed tool follows an\nunsupervised hybrid pipeline of statistical and neural approaches for\nincorporating different nuances in similarity for the task of question\nduplicate detection. We demonstrate that QDup can detect near-duplicate\nquestions and also suggest related questions for practice with remarkable\naccuracy and speed from a large repository of questions. The demo video of the\ntool can be found at https:\/\/www.youtube.com\/watch?v=loh0_-7XLW4.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.05251,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000115567,
      "text":"A Unified Knowledge Graph Augmentation Service for Boosting\n  Domain-specific NLP Tasks\n\n  By focusing the pre-training process on domain-specific corpora, some\ndomain-specific pre-trained language models (PLMs) have achieved\nstate-of-the-art results. However, it is under-investigated to design a unified\nparadigm to inject domain knowledge in the PLM fine-tuning stage. We propose\nKnowledgeDA, a unified domain language model development service to enhance the\ntask-specific training procedure with domain knowledge graphs. Given\ndomain-specific task texts input, KnowledgeDA can automatically generate a\ndomain-specific language model following three steps: (i) localize domain\nknowledge entities in texts via an embedding-similarity approach; (ii) generate\naugmented samples by retrieving replaceable domain entity pairs from two views\nof both knowledge graph and training data; (iii) select high-quality augmented\nsamples for fine-tuning via confidence-based assessment. We implement a\nprototype of KnowledgeDA to learn language models for two domains, healthcare\nand software development. Experiments on domain-specific text classification\nand QA tasks verify the effectiveness and generalizability of KnowledgeDA.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.10152,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Quirk or Palmer: A Comparative Study of Modal Verb Frameworks with\n  Annotated Datasets\n\n  Modal verbs, such as \"can\", \"may\", and \"must\", are commonly used in daily\ncommunication to convey the speaker's perspective related to the likelihood\nand\/or mode of the proposition. They can differ greatly in meaning depending on\nhow they're used and the context of a sentence (e.g. \"They 'must' help each\nother out.\" vs. \"They 'must' have helped each other out.\") Despite their\npractical importance in natural language understanding, linguists have yet to\nagree on a single, prominent framework for the categorization of modal verb\nsenses. This lack of agreement stems from high degrees of flexibility and\npolysemy from the modal verbs, making it more difficult for researchers to\nincorporate insights from this family of words into their work. This work\npresents Moverb dataset, which consists of 27,240 annotations of modal verb\nsenses over 4,540 utterances containing one or more sentences from social\nconversations. Each utterance is annotated by three annotators using two\ndifferent theoretical frameworks (i.e., Quirk and Palmer) of modal verb senses.\nWe observe that both frameworks have similar inter-annotator agreements,\ndespite having different numbers of sense types (8 for Quirk and 3 for Palmer).\nWith the RoBERTa-based classifiers fine-tuned on \\dataset, we achieve F1 scores\nof 82.2 and 78.3 on Quirk and Palmer, respectively, showing that modal verb\nsense disambiguation is not a trivial task. Our dataset will be publicly\navailable with our final version.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.05093,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Plug-and-Play Recipe Generation with Content Planning\n\n  Recent pre-trained language models have shown promising capabilities in\ngenerating fluent and realistic natural language text. However, generating\nmulti-sentence text with global content planning has been a long-existing\nresearch question. Current approaches for controlled text generation can hardly\naddress this issue, as they usually condition on single known control\nattributes. In this study, we propose a low-cost yet effective framework which\nexplicitly models the global content plan of the generated text. Specifically,\nit optimizes the joint distribution of the natural language sequence and the\nglobal content plan in a plug-and-play manner. We conduct extensive experiments\non the well-established Recipe1M+ benchmark. Both automatic and human\nevaluations verify that our model achieves the state-of-the-art performance on\nthe task of recipe generation\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.09746,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000070863,
      "text":"Evaluating Human-Language Model Interaction\n\n  Many real-world applications of language models (LMs), such as writing\nassistance and code autocomplete, involve human-LM interaction. However, most\nbenchmarks are non-interactive in that a model produces output without human\ninvolvement. To evaluate human-LM interaction, we develop a new framework,\nHuman-AI Language-based Interaction Evaluation (HALIE), that defines the\ncomponents of interactive systems and dimensions to consider when designing\nevaluation metrics. Compared to standard, non-interactive evaluation, HALIE\ncaptures (i) the interactive process, not only the final output; (ii) the\nfirst-person subjective experience, not just a third-party assessment; and\n(iii) notions of preference beyond quality (e.g., enjoyment and ownership). We\nthen design five tasks to cover different forms of interaction: social\ndialogue, question answering, crossword puzzles, summarization, and metaphor\ngeneration. With four state-of-the-art LMs (three variants of OpenAI's GPT-3\nand AI21 Labs' Jurassic-1), we find that better non-interactive performance\ndoes not always translate to better human-LM interaction. In particular, we\nhighlight three cases where the results from non-interactive and interactive\nmetrics diverge and underscore the importance of human-LM interaction for LM\nevaluation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.1002,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000094374,
      "text":"On the Blind Spots of Model-Based Evaluation Metrics for Text Generation\n\n  In this work, we explore a useful but often neglected methodology for\nrobustness analysis of text generation evaluation metrics: stress tests with\nsynthetic data. Basically, we design and synthesize a wide range of potential\nerrors and check whether they result in a commensurate drop in the metric\nscores. We examine a range of recently proposed evaluation metrics based on\npretrained language models, for the tasks of open-ended generation,\ntranslation, and summarization. Our experiments reveal interesting\ninsensitivities, biases, or even loopholes in existing metrics. For example, we\nfind that BERTScore is confused by truncation errors in summarization, and\nMAUVE (built on top of GPT-2) is insensitive to errors at the beginning or\nmiddle of generations. Further, we investigate the reasons behind these blind\nspots and suggest practical workarounds for a more reliable evaluation of text\ngeneration. We have released our code and data at\nhttps:\/\/github.com\/cloudygoose\/blindspot_nlg.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.08307,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000040068,
      "text":"Controllable Text Generation via Probability Density Estimation in the\n  Latent Space\n\n  Previous work on controllable text generation has explored the idea of\ncontrol from the latent space, such as optimizing a representation with\nattribute-related classifiers or sampling a representation from relevant\ndiscrete samples. However, they are not effective enough in modeling both the\nlatent space and the control, leaving controlled text with low quality and\ndiversity. In this work, we propose a novel control framework using probability\ndensity estimation in the latent space. Our method utilizes an invertible\ntransformation function, the Normalizing Flow, that maps the complex\ndistributions in the latent space to simple Gaussian distributions in the prior\nspace. Thus, we can perform sophisticated and flexible control in the prior\nspace and feed the control effects back into the latent space owing to the\none-one-mapping property of invertible transformations. Experiments on\nsingle-attribute controls and multi-attribute control reveal that our method\noutperforms several strong baselines on attribute relevance and text quality\nand achieves the SOTA. Further analysis of control strength adjustment\ndemonstrates the flexibility of our control strategy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.09028,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000118216,
      "text":"Neural Coreference Resolution based on Reinforcement Learning\n\n  The target of a coreference resolution system is to cluster all mentions that\nrefer to the same entity in a given context. All coreference resolution systems\nneed to solve two subtasks; one task is to detect all of the potential\nmentions, and the other is to learn the linking of an antecedent for each\npossible mention. In this paper, we propose a reinforcement learning\nactor-critic-based neural coreference resolution system, which can achieve both\nmention detection and mention clustering by leveraging an actor-critic deep\nreinforcement learning technique and a joint training algorithm. We experiment\non the BERT model to generate different input span representations. Our model\nwith the BERT span representation achieves the state-of-the-art performance\namong the models on the CoNLL-2012 Shared Task English Test Set.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.02475,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000062916,
      "text":"Meta-Learning Fast Weight Language Models\n\n  Dynamic evaluation of language models (LMs) adapts model parameters at test\ntime using gradient information from previous tokens and substantially improves\nLM performance. However, it requires over 3x more compute than standard\ninference. We present Fast Weight Layers (FWLs), a neural component that\nprovides the benefits of dynamic evaluation much more efficiently by expressing\ngradient updates as linear attention. A key improvement over dynamic evaluation\nis that FWLs can also be applied at training time so the model learns to make\ngood use of gradient updates. FWLs can easily be added on top of existing\ntransformer models, require relatively little extra compute or memory to run,\nand significantly improve language modeling perplexity.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.05726,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics\n\n  Modern embedding-based metrics for evaluation of generated text generally\nfall into one of two paradigms: discriminative metrics that are trained to\ndirectly predict which outputs are of higher quality according to supervised\nhuman annotations, and generative metrics that are trained to evaluate text\nbased on the probabilities of a generative model. Both have their advantages;\ndiscriminative metrics are able to directly optimize for the problem of\ndistinguishing between good and bad outputs, while generative metrics can be\ntrained using abundant raw text. In this paper, we present a framework that\ncombines the best of both worlds, using both supervised and unsupervised\nsignals from whatever data we have available. We operationalize this idea by\ntraining T5Score, a metric that uses these training signals with mT5 as the\nbackbone. We perform an extensive empirical comparison with other existing\nmetrics on 5 datasets, 19 languages and 280 systems, demonstrating the utility\nof our method. Experimental results show that: T5Score achieves the best\nperformance on all datasets against existing top-scoring metrics at the segment\nlevel. We release our code and models at https:\/\/github.com\/qinyiwei\/T5Score.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.10011,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"PLUE: Language Understanding Evaluation Benchmark for Privacy Policies\n  in English\n\n  Privacy policies provide individuals with information about their rights and\nhow their personal information is handled. Natural language understanding (NLU)\ntechnologies can support individuals and practitioners to understand better\nprivacy practices described in lengthy and complex documents. However, existing\nefforts that use NLU technologies are limited by processing the language in a\nway exclusive to a single task focusing on certain privacy practices. To this\nend, we introduce the Privacy Policy Language Understanding Evaluation (PLUE)\nbenchmark, a multi-task benchmark for evaluating the privacy policy language\nunderstanding across various tasks. We also collect a large corpus of privacy\npolicies to enable privacy policy domain-specific language model pre-training.\nWe evaluate several generic pre-trained language models and continue\npre-training them on the collected corpus. We demonstrate that domain-specific\ncontinual pre-training offers performance improvements across all tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.06385,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000121858,
      "text":"TencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models\n  of Different Modalities\n\n  Recently, the success of pre-training in text domain has been fully extended\nto vision, audio, and cross-modal scenarios. The proposed pre-training models\nof different modalities are showing a rising trend of homogeneity in their\nmodel structures, which brings the opportunity to implement different\npre-training models within a uniform framework. In this paper, we present\nTencentPretrain, a toolkit supporting pre-training models of different\nmodalities. The core feature of TencentPretrain is the modular design. The\ntoolkit uniformly divides pre-training models into 5 components: embedding,\nencoder, target embedding, decoder, and target. As almost all of common modules\nare provided in each component, users can choose the desired modules from\ndifferent components to build a complete pre-training model. The modular design\nenables users to efficiently reproduce existing pre-training models or build\nbrand-new one. We test the toolkit on text, vision, and audio benchmarks and\nshow that it can match the performance of the original implementations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  }
]