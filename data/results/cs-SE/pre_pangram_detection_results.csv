arxiv_id,paper_type,period,year,month,pangram_prediction
2001.06652,review,pre_llm,2020,1,"{'ai_likelihood': 1.2252065870496963e-06, 'text': 'Boundary Value Exploration for Software Analysis\n\n  For software to be reliable and resilient, it is widely accepted that tests\nmust be created and maintained alongside the software itself. One safeguard\nfrom vulnerabilities and failures in code is to ensure correct behavior on the\nboundaries between the input space sub-domains. So-called boundary value\nanalysis (BVA) and boundary value testing (BVT) techniques aim to exercise\nthose boundaries and increase test effectiveness. However, the concepts of BVA\nand BVT themselves are not generally well defined, and it is not clear how to\nidentify relevant sub-domains, and thus the boundaries delineating them, given\na specification. This has limited adoption and hindered automation. We clarify\nBVA and BVT and introduce Boundary Value Exploration (BVE) to describe\ntechniques that support them by helping to detect and identify boundary inputs.\nAdditionally, we propose two concrete BVE techniques based on\ninformation-theoretic distance functions: (i) an algorithm for boundary\ndetection and (ii) the usage of software visualization to explore the behavior\nof the software under test and identify its boundary behavior. As an initial\nevaluation, we apply these techniques on a much used and well-tested date\nhandling library. Our results reveal questionable behavior at boundaries\nhighlighted by our techniques. In conclusion, we argue that the boundary value\nexploration that our techniques enable is a step towards automated boundary\nvalue analysis and testing, fostering their wider use and improving test\neffectiveness and efficiency.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.0963,regular,pre_llm,2020,1,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Ammonia: An Approach for Deriving Project-specific Bug Patterns\n\n  Finding and fixing buggy code is an important and cost-intensive maintenance\ntask, and static analysis (SA) is one of the methods developers use to perform\nit. SA tools warn developers about potential bugs by scanning their source code\nfor commonly occurring bug patterns, thus giving those developers opportunities\nto fix the warnings (potential bugs) before they release the software.\nTypically, SA tools scan for general bug patterns that are common to any\nsoftware project (such as null pointer dereference), and not for project\nspecific patterns. However, past research has pointed to this lack of\ncustomizability as a severe limiting issue in SA. Accordingly, in this paper,\nwe propose an approach called Ammonia, which is based on statically analyzing\nchanges across the development history of a project, as a means to identify\nproject-specific bug patterns. Furthermore, the bug patterns identified by our\ntool do not relate to just one developer or one specific commit, they reflect\nthe project as a whole and compliment the warnings from other SA tools that\nidentify general bug patterns. Herein, we report on the application of our\nimplemented tool and approach to four Java projects: Ant, Camel, POI, and\nWicket. The results obtained show that our tool could detect 19 project\nspecific bug patterns across those four projects. Next, through manual\nanalysis, we determined that six of those change patterns were actual bugs and\nsubmitted pull requests based on those bug patterns. As a result, five of the\npull requests were merged.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.11245,review,pre_llm,2020,1,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'An Automated Framework for the Extraction of Semantic Legal Metadata\n  from Legal Texts\n\n  Semantic legal metadata provides information that helps with understanding\nand interpreting legal provisions. Such metadata is therefore important for the\nsystematic analysis of legal requirements. However, manually enhancing a large\nlegal corpus with semantic metadata is prohibitively expensive. Our work is\nmotivated by two observations: (1) the existing requirements engineering (RE)\nliterature does not provide a harmonized view on the semantic metadata types\nthat are useful for legal requirements analysis; (2) automated support for the\nextraction of semantic legal metadata is scarce, and it does not exploit the\nfull potential of artificial intelligence technologies, notably natural\nlanguage processing (NLP) and machine learning (ML). Our objective is to take\nsteps toward overcoming these limitations. To do so, we review and reconcile\nthe semantic legal metadata types proposed in the RE literature. Subsequently,\nwe devise an automated extraction approach for the identified metadata types\nusing NLP and ML. We evaluate our approach through two case studies over the\nLuxembourgish legislation. Our results indicate a high accuracy in the\ngeneration of metadata annotations. In particular, in the two case studies, we\nwere able to obtain precision scores of 97.2% and 82.4% and recall scores of\n94.9% and 92.4%.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.11962,regular,pre_llm,2020,1,"{'ai_likelihood': 0.0, 'text': 'Modeling Events and Events of Events in Software Engineering\n\n  A model is a simplified representation of portion of reality that hides a\nsystem s nonessential characteristics. It provides a means for reducing\ncomplexity as well as visualization and communication and a basis for building\nit. Most models involve graphic languages during many of the software lifecycle\nphases. A new model, called thinging machine (TM), has recently been developed\nas an extension of the input-process-output framework. The paper focuses on\nevents in a TM, offering a new perspective that captures a system s dynamic\nbehaviors and a means of diagrammatically modeling events. The event notion is\nan important factor in giving semantics to specifications and providing a\nnatural way to specify the interfaces and observable behavior of system\ncomponents. Specifically, five generic TM event processes are analyzed: create,\nprocess, receive, release, and transfer. All events can be mapped (or reduced)\nto the events of these five event processes\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.01598,regular,pre_llm,2020,1,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'Why and How to Balance Alignment and Diversity of Requirements\n  Engineering Practices in Automotive\n\n  In large-scale automotive companies, various requirements engineering (RE)\npractices are used across teams. RE practices manifest in Requirements\nInformation Models (RIM) that define what concepts and information should be\ncaptured for requirements. Collaboration of practitioners from different parts\nof an organization is required to define a suitable RIM that balances support\nfor diverse practices in individual teams with the alignment needed for a\nshared view and team support on system level. There exists no guidance for this\nchallenging task. This paper presents a mixed methods study to examine the role\nof RIMs in balancing alignment and diversity of RE practices in four automotive\ncompanies. Our analysis is based on data from systems engineering tools, 11\nsemi-structured interviews, and a survey to validate findings and suggestions.\nWe found that balancing alignment and diversity of RE practices is important to\nconsider when defining RIMs. We further investigated enablers for this balance\nand actions that practitioners take to achieve it. From these factors, we\nderived and evaluated recommendations for managing RIMs in practice that take\ninto account the lifecycle of requirements and allow for diverse practices\nacross sub-disciplines in early development, while enforcing alignment of\nrequirements that are close to release.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.079,regular,pre_llm,2020,1,"{'ai_likelihood': 4.106097751193576e-06, 'text': 'Model-Based Cloud Resource Management with TOSCA and OCCI\n\n  With the advent of cloud computing, different cloud providers with\nheterogeneous cloud services (compute, storage, network, applications, etc.)\nand their related Application Programming Interfaces (APIs) have emerged. This\nheterogeneity complicates the implementation of an interoperable cloud system.\nSeveral standards have been proposed to address this challenge and provide a\nunified interface to cloud resources. The Open Cloud Computing Interface (OCCI)\nthereby focuses on the standardization of a common API for\nInfrastructure-as-a-Service (IaaS) providers while the Topology and\nOrchestration Specification for Cloud Applications (TOSCA) focuses on the\nstandardization of a template language to enable the proper definition of the\ntopology of cloud applications and their orchestrations on top of a cloud\nsystem. TOSCA thereby does not define how the application topologies are\ncreated on the cloud. Therefore, we analyse the conceptual similarities between\nthe two approaches and we study how we can integrate them to obtain a complete\nstandard-based approach to manage both cloud infrastructure and cloud\napplication layers. We propose an automated extensive mapping between the\nconcepts of the two standards and we provide TOSCA Studio, a model-driven tool\nchain for TOSCA that conforms to OCCI. TOSCA Studio allows to graphically\ndesign cloud applications as well as to deploy and manage them at runtime using\na fully model-driven cloud orchestrator based on the two standards. Our\ncontribution is validated by successfully designing and deploying three cloud\napplications: WordPress, Node Cellar and Multi-Tier.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.04808,regular,pre_llm,2020,1,"{'ai_likelihood': 1.0463926527235244e-05, 'text': 'Testing with Jupyter notebooks: NoteBook VALidation (nbval) plug-in for\n  pytest\n\n  The Notebook validation tool nbval allows to load and execute Python code\nfrom a Jupyter notebook file. While computing outputs from the cells in the\nnotebook, these outputs are compared with the outputs saved in the notebook\nfile, treating each cell as a test. Deviations are reported as test failures,\nwith various configuration options available to control the behaviour.\nApplication use cases include the validation of notebook-based documentation,\ntutorials and textbooks, as well as the use of notebooks as additional unit,\nintegration and system tests for the libraries that are used in the notebook.\nNbval is implemented as a plugin for the pytest testing software.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.00219,review,pre_llm,2020,1,"{'ai_likelihood': 1.953707800971137e-06, 'text': 'Business Value of ITSM. Requirement or Mirage?\n\n  This paper builds on a presentation provided as part of a recent panel\nsession on ITSM (IT Service Management) Business Value at the NYC itSMF\n(Service Management Forum) Local Interest Group meeting. The panel presentation\nexplored the definition of Business Value and how ITSM itself could be measured\nto produce business value. While ITSM and ITIL have been in use for years it\noften remains a challenge to demonstrate the business value of these methods or\neven to understand business value itself. This paper expands on the panel\ndiscussion on what is meant by business value and how it can be found (if at\nall) in the context of ITSM development and process improvement.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.10281,regular,pre_llm,2020,1,"{'ai_likelihood': 8.609559800889757e-07, 'text': ""Efficient Logging for Blockchain Applications\n\n  Second generation blockchain platforms, like Ethereum, can store arbitrary\ndata and execute user-defined smart contracts. Due to the shared nature of\nblockchains, understanding the usage of blockchain-based applications and the\nunderlying network is crucial. Although log analysis is a well-established\nmeans, data extraction from blockchain platforms can be highly inconvenient and\nslow, not least due to the absence of logging libraries. To close the gap, we\nhere introduce the Ethereum Logging Framework (ELF) which is highly\nconfigurable and available as open source. ELF supports users (i) in generating\ncost-efficient logging code readily embeddable into smart contracts and (ii) in\nextracting log analysis data into common formats regardless of whether the code\ngeneration has been used during development. We provide an overview of and\nrationale for the framework's features, outline implementation details, and\ndemonstrate ELF's versatility based on three case studies from the public\nEthereum blockchain.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.06426,regular,pre_llm,2020,1,"{'ai_likelihood': 7.318125830756294e-06, 'text': ""Eye of the Mind: Image Processing for Social Coding\n\n  Developers are increasingly sharing images in social coding environments\nalongside the growth in visual interactions within social networks. The\nanalysis of the ratio between the textual and visual content of Mozilla's\nchange requests and in Q/As of StackOverflow programming revealed a steady\nincrease in sharing images over the past five years. Developers' shared images\nare meaningful and are providing complementary information compared to their\nassociated text. Often, the shared images are essential in understanding the\nchange requests, questions, or the responses submitted. Relying on these\nobservations, we delve into the potential of automatic completion of textual\nsoftware artifacts with visual content.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.02941,regular,pre_llm,2020,1,"{'ai_likelihood': 2.980232238769531e-07, 'text': ""Killing Stubborn Mutants with Symbolic Execution\n\n  We introduce SeMu, a Dynamic Symbolic Execution technique that generates test\ninputs capable of killing stubborn mutants (killable mutants that remain\nundetected after a reasonable amount of testing). SeMu aims at mutant\npropagation (triggering erroneous states to the program output) by\nincrementally searching for divergent program behaviours between the original\nand the mutant versions. We model the mutant killing problem as a symbolic\nexecution search within a specific area in the programs' symbolic tree. In this\nframework, the search area is defined and controlled by parameters that allow\nscalable and cost-effective mutant killing. We integrate SeMu in KLEE and\nexperimented with Coreutils (a benchmark frequently used in symbolic execution\nstudies). Our results show that our modelling plays an important role in mutant\nkilling. Perhaps more importantly, our results also show that, within a\ntwo-hour time limit, SeMu kills 37% of the stubborn mutants, where KLEE kills\nnone and where the mutant infection strategy (strategy suggested by previous\nresearch) kills 17%.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.07808,review,pre_llm,2020,1,"{'ai_likelihood': 1.6887982686360678e-06, 'text': ""A Comprehensive Study of Bloated Dependencies in the Maven Ecosystem\n\n  Build automation tools and package managers have a profound influence on\nsoftware development. They facilitate the reuse of third-party libraries,\nsupport a clear separation between the application's code and its external\ndependencies, and automate several software development tasks. However, the\nwide adoption of these tools introduces new challenges related to dependency\nmanagement. In this paper, we propose an original study of one such challenge:\nthe emergence of bloated dependencies.\n  Bloated dependencies are libraries that the build tool packages with the\napplication's compiled code but that are actually not necessary to build and\nrun the application. This phenomenon artificially grows the size of the built\nbinary and increases maintenance effort. We propose a tool, called DepClean, to\nanalyze the presence of bloated dependencies in Maven artifacts. We analyze\n9,639 Java artifacts hosted on Maven Central, which include a total of 723,444\ndependency relationships. Our key result is that 75.1% of the analyzed\ndependency relationships are bloated. In other words, it is feasible to reduce\nthe number of dependencies of Maven artifacts up to 1/4 of its current count.\nWe also perform a qualitative study with 30 notable open-source projects. Our\nresults indicate that developers pay attention to their dependencies and are\nwilling to remove bloated dependencies: 18/21 answered pull requests were\naccepted and merged by developers, removing 131 dependencies in total.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.03338,regular,pre_llm,2020,1,"{'ai_likelihood': 6.523397233751085e-06, 'text': ""The Effectiveness of Supervised Machine Learning Algorithms in\n  Predicting Software Refactoring\n\n  Refactoring is the process of changing the internal structure of software to\nimprove its quality without modifying its external behavior. Empirical studies\nhave repeatedly shown that refactoring has a positive impact on the\nunderstandability and maintainability of software systems. However, before\ncarrying out refactoring activities, developers need to identify refactoring\nopportunities. Currently, refactoring opportunity identification heavily relies\non developers' expertise and intuition. In this paper, we investigate the\neffectiveness of machine learning algorithms in predicting software\nrefactorings. More specifically, we train six different machine learning\nalgorithms (i.e., Logistic Regression, Naive Bayes, Support Vector Machine,\nDecision Trees, Random Forest, and Neural Network) with a dataset comprising\nover two million refactorings from 11,149 real-world projects from the Apache,\nF-Droid, and GitHub ecosystems. The resulting models predict 20 different\nrefactorings at class, method, and variable-levels with an accuracy often\nhigher than 90%. Our results show that (i) Random Forests are the best models\nfor predicting software refactoring, (ii) process and ownership metrics seem to\nplay a crucial role in the creation of better models, and (iii) models\ngeneralize well in different contexts.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.11367,regular,pre_llm,2020,1,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'Application of Seq2Seq Models on Code Correction\n\n  We apply various seq2seq models on programming language correction tasks on\nJuliet Test Suite for C/C++ and Java of Software Assurance Reference\nDatasets(SARD), and achieve 75\\%(for C/C++) and 56\\%(for Java) repair rates on\nthese tasks. We introduce Pyramid Encoder in these seq2seq models, which\nlargely increases the computational efficiency and memory efficiency, while\nremain similar repair rate to their non-pyramid counterparts. We successfully\ncarry out error type classification task on ITC benchmark examples (with only\n685 code instances) using transfer learning with models pre-trained on Juliet\nTest Suite, pointing out a novel way of processing small programing language\ndatasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.06675,regular,pre_llm,2020,1,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'An Interdisciplinary Guideline for the Production of Videos and Vision\n  Videos by Software Professionals\n\n  Background and Motivation: In recent years, the topic of applying videos in\nrequirements engineering has been discussed and its contributions are of\ninteresting potential. In the last 35 years, several researchers proposed\napproaches for applying videos in requirements engineering due to their\ncommunication richness and effectiveness. However, these approaches mainly use\nvideos but omit the details about how to produce them. This lack of guidance is\none crucial reason why videos are not an established documentation option for\nsuccessful requirements communication and thus shared understanding. Software\nprofessionals are not directors and thus they do not necessarily know what\nconstitutes a good video in general and for an existing approach. Therefore,\nthis lack of knowledge and skills on how to produce and use videos for visual\ncommunication impedes the application of videos by software professionals in\nrequirements engineering.\n  How to Create Effective Videos and Vision Videos?: This technical report\naddresses this lack of knowledge and skills by software professionals. We\nprovide two guidelines that can be used as checklists to avoid frequent flaws\nin the production and use of videos respectively vision videos. Software\nprofessionals without special training should be able to follow these\nguidelines to achieve the basic capabilities to produce (vision) videos that\nare accepted by their stakeholders. These guidelines represent a core set of\nthose capabilities in the preproduction, shooting, postproduction, and viewing\nof (vision) videos. We do not strive for perfection in any of these\ncapabilities, .e.g., technical handling of video equipment, storytelling, or\nvideo editing. Instead, these guidelines support all steps of the (vision)\nvideo production and use process to a balanced way.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.09342,regular,pre_llm,2020,1,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'Open-source Defect Injection Benchmark Testbed for the Evaluation of\n  Testing\n\n  A natural method to evaluate the effectiveness of a testing technique is to\nmeasure the defect detection rate when applying the created test cases. Here,\nreal or artificial software defects can be injected into the source code of\nsoftware. For a more extensive evaluation, the injection of artificial defects\nis usually needed and can be performed via mutation testing using code mutation\noperators. However, to simulate complex defects arising from a misunderstanding\nof design specifications, mutation testing might reach its limit in some cases.\nIn this paper, we present an open-source benchmark testbed application that\nemploys a complement method of artificial defect injection. The application is\ncompiled after artificial defects are injected into its source code from\npredefined building blocks. The majority of the functions and user interface\nelements are covered by creating front-end-based automated test cases that can\nbe used in experiments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.03277,regular,pre_llm,2020,1,"{'ai_likelihood': 4.967053731282552e-07, 'text': ""Searching a Database of Source Codes Using Contextualized Code Search\n\n  Consider the case where a programmer has written some part of a program, but\nhas left part of the program (such as a method or a function body) incomplete.\nThe goal is to use the context surrounding the missing code to automatically\n'figure out' which of the codes in the database would be useful to the\nprogrammer in order to help complete the missing code. The search is\n'contextualized' in the sense that the search engine should use clues in the\npartially-completed code to figure out which database code is most useful. The\nuser should not be required to formulate an explicit query. We cast\ncontextualized code search as a learning problem, where the goal is to learn a\ndistribution function computing the likelihood that each database code\ncompletes the program, and propose a neural model for predicting which database\ncode is likely to be most useful. Because it will be prohibitively expensive to\napply a neural model to each code in a database of millions or billions of\ncodes at search time, one of our key technical concerns is ensuring a speedy\nsearch. We address this by learning a 'reverse encoder' that can be used to\nreduce the problem of evaluating each database code to computing a convolution\nof two normal distributions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.11655,regular,pre_llm,2020,1,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'StackEmo-Towards Enhancing User Experience by Augmenting Stack Overflow\n  with Emojis\n\n  With the increase in acceptance of open source platforms for knowledge\nsharing, Question and Answer (Q\\&A) websites such as Stack Overflow have become\nincreasingly popular in the programming domain. Many novice programmers visit\nStack Overflow for reasons that include posing questions, finding answers for\nissues they come across in the process of programming. Practitioners\nvoluntarily answer questions on Stack Overflow based on their experience or\nprior knowledge. Most of these answers are also accompanied by comments from\nusers of Stack Overflow. Questions, answers and comments on Stack Overflow also\ninclude sentiments of users, which when analysed and presented could motivate\nusers in reading and contributing to the posts. However, the sentiment of these\nposts is not being depicted in the current Stack Overflow platform. There is\nextensive research on analysing sentiments on social networking platforms such\nas twitter. Representing sentiment of a post might motivate users to follow or\nanswer certain posts. While there exist several tools that augment or annotate\nStack Overflow platform for developers, we are not aware of tools that deal\nwith sentiment of the posts. In this paper, we propose StackEmo as a Google\nChrome plugin to augment comments on Stack Overflow with emojis, based on the\nsentiment of the comments posted, with the aim to provide users with visual\ncues that could motivate the users to review and contribute to available\ncomments. We evaluated StackEmo through an in-user likert scale based survey\nwith 30 university students. The results of the survey provided us insights on\nimproving StackEmo, with 83% participants having recommended the plugin to\ntheir peers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.01606,regular,pre_llm,2020,1,"{'ai_likelihood': 8.54333241780599e-06, 'text': 'The SmartSHARK Ecosystem for Software Repository Mining\n\n  Software repository mining is the foundation for many empirical software\nengineering studies. The collection and analysis of detailed data can be\nchallenging, especially if data shall be shared to enable replicable research\nand open science practices. SmartSHARK is an ecosystem that supports replicable\nand reproducible research based on software repository mining.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.05254,review,pre_llm,2020,1,"{'ai_likelihood': 3.245141771104601e-06, 'text': 'Integrating the Common Variability Language with Multilanguage\n  Annotations for Web Engineering\n\n  Web applications development involves managing a high diversity of files and\nresources like code, pages or style sheets, implemented in different languages.\nTo deal with the automatic generation of custom-made configurations of web\napplications, industry usually adopts annotation-based approaches despite the\nmajority of studies encourage the use of composition-based approaches to\nimplement Software Product Lines. Recent work tries to combine both approaches\nto get the complementary benefits. However, technological companies are\nreticent to adopt new development paradigms such as feature-oriented\nprogramming or aspect-oriented programming. Moreover, it is extremely\ndifficult, or even impossible, to apply these programming models to web\napplications, mainly because of their multilingual nature, since their\ndevelopment involves multiple types of source code (Java, Groovy, JavaScript),\ntemplates (HTML, Markdown, XML), style sheet files (CSS and its variants, such\nas SCSS), and other files (JSON, YML, shell scripts). We propose to use the\nCommon Variability Language as a composition-based approach and integrate\nannotations to manage fine grained variability of a Software Product Line for\nweb applications. In this paper, we (i) show that existing composition and\nannotation-based approaches, including some well-known combinations, are not\nappropriate to model and implement the variability of web applications; and\n(ii) present a combined approach that effectively integrates annotations into a\ncomposition-based approach for web applications. We implement our approach and\nshow its applicability with an industrial real-world system.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.02672,review,pre_llm,2020,2,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'How do Quantifiers Affect the Quality of Requirements?\n\n  Context: Requirements quality can have a substantial impact on the\neffectiveness and efficiency of using requirements artifacts in a development\nprocess. Quantifiers such as ""at least"", ""all"", or ""exactly"" are common\nlanguage constructs used to express requirements. Quantifiers can be formulated\nby affirmative phrases (""At least"") or negative phrases (""Not less than"").\nProblem: It is long assumed that negation in quantification negatively affects\nthe readability of requirements, however, empirical research on these topics\nremains sparse. Principal Idea: In a web-based experiment with 51 participants,\nwe compare the impact of negations and quantifiers on readability in terms of\nreading effort, reading error rate and perceived reading difficulty of\nrequirements. Results: For 5 out of 9 quantifiers, our participants performed\nbetter on the affirmative phrase compared to the negative phrase. Only for one\nquantifier, the negative phrase was more effective. Contribution: This research\nfocuses on creating an empirical understanding of the effect of language in\nRequirements Engineering. It furthermore provides concrete advice on how to\nphrase requirements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.00404,regular,pre_llm,2020,2,"{'ai_likelihood': 5.066394805908203e-06, 'text': 'An Automated Testing Framework For Smart TV apps Based on Model\n  Separation\n\n  Smart TV application (app) is a new technological software app that can deal\nwith smart TV devices to add more functionality and features. Despite its\nimportance nowadays, far too little attention has been paid to present a\nsystematic approach to test this kind of app so far. In this paper, we present\na systematic model-based testing approach for smart TV app. We used our new\nnotion of model separation to use sub-models based on the user preference\ninstead of the exhaustive testing to generate the test cases. Based on the\nconstructed model, we generated a set of test cases to assess the selected\npaths to the chosen destination in the app. We also defined new mutation\noperators for smart TV app to assess our testing approach. The evaluation\nresults showed that our approach can generate more comprehensive models of\nsmart TV apps with less time as compared to manual exploratory testing. The\nresults also showed that our approach can generate effective test cases in term\nof fault detection.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.02793,review,pre_llm,2020,2,"{'ai_likelihood': 6.291601392957899e-07, 'text': ""Views on Quality Requirements in Academia and Practice: Commonalities,\n  Differences, and Context-Dependent Grey Areas\n\n  Context: Quality requirements (QRs) are a topic of constant discussions both\nin industry and academia. Debates entwine around the definition of quality\nrequirements, the way how to handle them, or their importance for project\nsuccess. While many academic endeavors contribute to the body of knowledge\nabout QRs, practitioners may have different views. In fact, we still lack a\nconsistent body of knowledge on QRs since much of the discussion around this\ntopic is still dominated by observations that are strongly context-dependent.\nThis holds for both academic and practitioners' views. Our assumption is that,\nin consequence, those views may differ. Objective: We report on a study to\nbetter understand the extent to which available research statements on quality\nrequirements, as found in exemplary peer-reviewed and frequently cited\npublications, are reflected in the perception of practitioners. Our goal is to\nanalyze differences, commonalities, and context-dependent grey areas in the\nviews of academics and practitioners to allow a discussion on potential\nmisconceptions (on either sides) and opportunities for future research. Method:\nWe conducted a survey with 109 practitioners to assess whether they agree with\nresearch statements about QRs reflected in the literature. Based on a\nstatistical model, we evaluate the impact of a set of context factors to the\nperception of research statements. Results: Our results show that a majority of\nthe statements is well respected by practitioners; however, not all of them.\nWhen examining the different groups and backgrounds of respondents, we noticed\ninteresting deviations of perceptions within different groups that may lead to\nnew research questions. Conclusions: Our results help identifying prevalent\ncontext-dependent differences about how academics and practitioners view QRs\nand pinpointing statements where further research might be useful.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.12482,regular,pre_llm,2020,2,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'An Improved Generic ER Schema for Conceptual Modeling of Information\n  Systems\n\n  The Entity-Relationship (ER) model is widely used for creating ER schemas for\nmodeling application domains in the field of Information Systems development.\nHowever, when an ER schema is transformed to a Relational Database Schema\n(RDS), some important information on the ER schema may not be represented\nmeaningfully on the RDS. This causes a loss of information during the\ntransformation process. Although, several previous researches have proposed\nsolutions to remedy the situation, the problem still exists. Thus, in this\non-going research, we wish to improve the proposed solutions and maximize\ninformation preservation in the ER to relational transformation process.\nCardinality ratio constraints, role names, composite attributes, and certain\nrelationship types are among the information frequently lost in the\ntransformation process. Deficiencies in the ER model and the transformation\nmethod seems to cause this situation. We take the view that if the information\nlost is resolved; a one-to-one mapping should exist from the ER schema to its\nRDS. We modified the ER model and the transformation algorithm following a\nheuristic research method with a view to eliminating the deficiencies and\nthereby achieving a one-to-one mapping. We should show that the mapping exists\nfor any real-world application. We create a generic ER schema - an ER schema\nthat represents any phenomena in symbolic form - and use it to show that a\none-to-one mapping exists for any real-world application. In this paper, we\nexplore our generic ER schema and its advantages over its predecessors in view\nof representing any real-world application.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.00257,regular,pre_llm,2020,2,"{'ai_likelihood': 1.321236292521159e-05, 'text': 'ADF-GA: Data Flow Criterion Based Test Case Generation for Ethereum\n  Smart Contracts\n\n  Testing is an important technique to improve the quality of Ethereum smart\ncontract programs. However, current work on testing smart contract only focus\non static problems of smart contract programs. A data flow oriented test case\ngeneration approach for dynamic testing of smart contract programs is still\nmissing. To address this problem, this paper proposes a novel test case\ngeneration approach, called ADF-GA (All-uses Data Flow criterion based test\ncase generation using Genetic Algorithm), for Solidity based Ethereum smart\ncontract programs. ADF-GA aims to efficiently generate a valid set of test\ncases via three stages. First, the corresponding program control flow graph is\nconstructed from the source codes. Second, the generated control flow graph is\nanalyzed to obtain the variable information in the Solidity programs, locate\nthe require statements, and also get the definition-use pairs to be tested.\nFinally, a genetic algorithm is used to generate test cases, in which an\nimproved fitness function is proposed to calculate the definition-use pairs\ncoverage of each test case with program instrumentation. Experimental studies\nare performed on several representative Solidity programs. The results show\nthat ADF-GA can effectively generate test cases, achieve better coverage, and\nreduce the number of iterations in genetic algorithm.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.0265,regular,pre_llm,2020,2,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'What You See is What it Means! Semantic Representation Learning of Code\n  based on Visualization and Transfer Learning\n\n  Recent successes in training word embeddings for NLP tasks have encouraged a\nwave of research on representation learning for source code, which builds on\nsimilar NLP methods. The overall objective is then to produce code embeddings\nthat capture the maximum of program semantics. State-of-the-art approaches\ninvariably rely on a syntactic representation (i.e., raw lexical tokens,\nabstract syntax trees, or intermediate representation tokens) to generate\nembeddings, which are criticized in the literature as non-robust or\nnon-generalizable. In this work, we investigate a novel embedding approach\nbased on the intuition that source code has visual patterns of semantics. We\nfurther use these patterns to address the outstanding challenge of identifying\nsemantic code clones. We propose the WYSIWIM (""What You See Is What It Means"")\napproach where visual representations of source code are fed into powerful\npre-trained image classification neural networks from the field of computer\nvision to benefit from the practical advantages of transfer learning. We\nevaluate the proposed embedding approach on two variations of the task of\nsemantic code clone identification: code clone detection (a binary\nclassification problem), and code classification (a multi-classification\nproblem). We show with experiments on the BigCloneBench (Java) and Open Judge\n(C) datasets that although simple, our WYSIWIM approach performs as effectively\nas state of the art approaches such as ASTNN or TBCNN. We further explore the\ninfluence of different steps in our approach, such as the choice of visual\nrepresentations or the classification algorithm, to eventually discuss the\npromises and limitations of this research direction.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.03968,review,pre_llm,2020,2,"{'ai_likelihood': 6.291601392957899e-06, 'text': 'E-APR: Mapping the Effectiveness of Automated Program Repair\n\n  Automated Program Repair (APR) is a fast growing area with numerous new\ntechniques being developed to tackle one of the most challenging software\nengineering problems. APR techniques have shown promising results, giving us\nhope that one day it will be possible for software to repair itself. In this\npaper, we focus on the problem of objective performance evaluation of APR\ntechniques. We introduce a new approach, Explaining Automated Program Repair\n(E-APR), which identifies features of buggy programs that explain why a\nparticular instance is difficult for an APR technique. E-APR is used to examine\nthe diversity and quality of the buggy programs used by most researchers, and\nanalyse the strengths and weaknesses of existing APR techniques. E-APR\nvisualises an instance space of buggy programs, with each buggy program\nrepresented as a point in the space. The instance space is constructed to\nreveal areas of hard and easy buggy programs, and enables the strengths and\nweaknesses of APR techniques to be identified.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.0708,review,pre_llm,2020,2,"{'ai_likelihood': 2.2186173333062066e-05, 'text': ""The Probabilistic Model Checker Storm\n\n  We present the probabilistic model checker Storm. Storm supports the analysis\nof discrete- and continuous-time variants of both Markov chains and Markov\ndecision processes. Storm has three major distinguishing features. It supports\nmultiple input languages for Markov models, including the JANI and PRISM\nmodeling languages, dynamic fault trees, generalized stochastic Petri nets, and\nthe probabilistic guarded command language. It has a modular set-up in which\nsolvers and symbolic engines can easily be exchanged. Its Python API allows for\nrapid prototyping by encapsulating Storm's fast and scalable algorithms. This\npaper reports on the main features of Storm and explains how to effectively use\nthem. A description is provided of the main distinguishing functionalities of\nStorm. Finally, an empirical evaluation of different configurations of Storm on\nthe QComp 2019 benchmark set is presented.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.01759,review,pre_llm,2020,2,"{'ai_likelihood': 2.384185791015625e-06, 'text': 'Quality Assurance Technologies of Big Data Applications: A Systematic\n  Literature Review\n\n  Big data applications are currently used in many application domains, ranging\nfrom statistical applications to prediction systems and smart cities. However,\nthe quality of these applications is far from perfect, leading to a large\namount of issues and problems. Consequently, assuring the overall quality for\nbig data applications plays an increasingly important role. This paper aims at\nsummarizing and assessing existing quality assurance (QA) technologies\naddressing quality issues in big data applications. We have conducted a\nsystematic literature review (SLR) by searching major scientific databases,\nresulting in 83 primary and relevant studies on QA technologies for big data\napplications. The SLR results reveal the following main findings: 1) the impact\nof the big data attributes of volume, velocity, and variety on the quality of\nbig data applications; 2) the quality attributes that determine the quality for\nbig data applications include correctness, performance, availability,\nscalability, reliability and so on; 3) the existing QA technologies, including\nanalysis, specification, model-driven architecture (MDA), verification, fault\ntolerance, testing, monitoring and fault & failure prediction; 4) existing\nstrengths and limitations of each kind of QA technology; 5) the existing\nempirical evidence of each QA technology. This study provides a solid\nfoundation for research on QA technologies of big data applications. However,\nmany challenges of big data applications regarding quality still remain.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.0576,regular,pre_llm,2020,2,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'An Exploratory Study of Code Smells in Web Games\n\n  With the continuous growth of the internet market, games are becoming more\nand more popular worldwide. However, increased market competition for game\ndemands developers to write more efficient games in terms of performance,\nsecurity, and maintenance. The continuous evolution of software systems and its\nincreasing complexity may result in bad design decisions. Researchers analyzed\nthe cognitive, behavioral and social effects of games. Also, gameplay and game\nmechanics have been a research area to enhance game playing, but to the extent\nof our knowledge, there hardly exists any research work that studies the bad\ncoding practices in game development. Hence, through our study, we try to\nanalyze and identify the presence of bad coding practices called code smells\nthat may cause quality issues in games. To accomplish this, we created a\ndataset of 361 web games written in JavaScript. On this dataset, we run a\nJavaScript code smell detection tool JSNose to find the occurrence and\ndistribution of code smell in web games. Further, we did a manual study on 9\nweb games to find violation of existing game programming patterns. Our results\nshow that existing tools are mostly language-specific and are not enough in the\ncontext of games as they were not able to detect the anti-patterns or bad\ncoding practices that are game-specific, motivating the need of game-specific\ncode smell detection tools.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.08458,review,pre_llm,2020,2,"{'ai_likelihood': 2.3510720994737412e-06, 'text': 'Caveats in Eliciting Mobile App Requirements\n\n  Factors such as app stores or platform choices heavily affect functional and\nnon-functional mobile app requirements. We surveyed 45 companies and\ninterviewed ten experts to explore how factors that impact mobile app\nrequirements are understood by requirements engineers in the mobile app\nindustry.\n  We observed a lack of knowledge in several areas. For instance, we observed\nthat all practitioners were aware of data privacy concerns, however, they did\nnot know that certain third-party libraries, usage aggregators, or advertising\nlibraries also occasionally leak sensitive user data. Similarly, certain\nfunctional requirements may not be implementable in the absence of a\nthird-party library that is either banned from an app store for policy\nviolations or lacks features, for instance, missing desired features in ARKit\nlibrary for iOS made practitioners turn to Android.\n  We conclude that requirements engineers should have adequate technical\nexperience with mobile app development as well as sufficient knowledge in areas\nsuch as privacy, security and law, in order to make informed decisions during\nrequirements elicitation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.06213,regular,pre_llm,2020,2,"{'ai_likelihood': 5.298190646701389e-07, 'text': 'Lightweight Lexical Test Prioritization for Immediate Feedback\n\n  The practice of unit testing enables programmers to obtain automated feedback\non whether a currently edited program is consistent with the expectations\nspecified in test cases. Feedback is most valuable when it happens immediately,\nas defects can be corrected instantly before they become harder to fix. With\ngrowing and longer running test suites, however, feedback is obtained less\nfrequently and lags behind program changes.\n  The objective of test prioritization is to rank tests so that defects, if\npresent, are found as early as possible or with the least costs. While there\nare numerous static approaches that output a ranking of tests solely based on\nthe current version of a program, we focus on change-based test prioritization,\nwhich recommends tests that likely fail in response to the most recent program\nchange. The canonical approach relies on coverage data and prioritizes tests\nthat cover the changed region, but obtaining and updating coverage data is\ncostly. More recently, information retrieval techniques that exploit\noverlapping vocabulary between change and tests have proven to be powerful, yet\nlightweight.\n  In this work, we demonstrate the capabilities of information retrieval for\nprioritizing tests in dynamic programming languages using Python as example. We\ndiscuss and measure previously understudied variation points, including how\ncontextual information around a program change can be used, and design\nalternatives to the widespread \\emph{TF-IDF} retrieval model tailored to\nretrieving failing tests.\n  To obtain program changes with associated test failures, we designed a tool\nthat generates a large set of faulty changes from version history along with\ntheir test results. Using this data set, we compared existing and new lexical\nprioritization strategies using four open-source Python projects, showing large\nimprovements over untreated and random test orders and results consistent with\nrelated work in statically typed languages.\n  We conclude that lightweight IR-based prioritization strategies are effective\ntools to predict failing tests in the absence of coverage data or when static\nanalysis is intractable like in dynamic languages. This knowledge can benefit\nboth individual programmers that rely on fast feedback, as well as operators of\ncontinuous integration infrastructure, where resources can be freed sooner by\ndetecting defects earlier in the build cycle.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.02303,review,pre_llm,2020,2,"{'ai_likelihood': 0.00014848179287380644, 'text': 'Management of quality requirements in agile and rapid software\n  development: A systematic mapping study\n\n  Context:Quality requirements (QRs) describe the desired quality of software,\nand they play an important role in the success of software projects. In agile\nsoftware development (ASD), QRs are often ill-defined and not well addressed\ndue to the focus on quickly delivering functionality. Rapid software\ndevelopment (RSD) approaches (e.g., continuous delivery and continuous\ndeployment), which shorten delivery times, are more prone to neglect QRs.\nDespite the significance of QRs in both ASD and RSD, there is limited\nsynthesized knowledge on their management in those approaches. Objective:This\nstudy aims to synthesize state-of-the-art knowledge about QR management in ASD\nand RSD, focusing on three aspects: bibliometric, strategies, and challenges.\nResearch method:Using a systematic mapping study with a snowballing search\nstrategy, we identified and structured the literature on QR management in ASD\nand RSD. Check the PDF file to see the full abstract and document.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.01699,regular,pre_llm,2020,2,"{'ai_likelihood': 1.2020270029703777e-05, 'text': 'Component-aware Orchestration of Cloud-based Enterprise Applications,\n  from TOSCA to Docker and Kubernetes\n\n  Enterprise IT is currently facing the challenge of coordinating the\nmanagement of complex, multi-component applications across heterogeneous cloud\nplatforms. Containers and container orchestrators provide a valuable solution\nto deploy multi-component applications over cloud platforms, by coupling the\nlifecycle of each application component to that of its hosting container. We\nhereby propose a solution for going beyond such a coupling, based on the OASIS\nstandard TOSCA and on Docker. We indeed propose a novel approach for deploying\nmulti-component applications on top of existing container orchestrators, which\nallows to manage each component independently from the container used to run\nit. We also present prototype tools implementing our approach, and we show how\nwe effectively exploited them to carry out a concrete case study.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.0077,review,pre_llm,2020,2,"{'ai_likelihood': 8.973810407850478e-06, 'text': ""Analyzing the evolution and diversity of SBES Program Committee\n\n  The Brazilian Symposium on Software Engineering (SBES) is one of the most\nimportant Latin American Software Engineering conferences. It was first held in\n1987, and in 2019 marks its 33rd edition. Over these years, many researchers\nhave participated in SBES, attending the conference, submitting, and reviewing\npapers. The researchers who participate in the Program Committee (PC) and\nperform the reviewers' role are fundamentally important to SBES, since their\nevaluations (e.g., deciding whether a paper is accepted or not) have the\npotential of drawing what SBES is now. Knowing that diversity is an important\naspect of any group work, we wanted to understand diversity in the SBES PC\ncommunity. We investigated a number of characteristics of SBES PC members,\nincluding their gender and geographic location. We also analyzed the turnover\nand renovation of the committee. Among the findings, we observed that although\nthe number of participants in the SBES PC has increased over the years, most of\nthem are men (~80%) and from the Southeast and Northeast of Brazil, with very\nfew members from the North region. We also observed that there is a small\nturnover: during the 2010 decade, only 11% of new members were added to the PC.\nFinally, we investigated the participation of the PC members publishing papers\nat SBES. We observed that only 24% of the papers accepted to SBES were authored\nby members who were not committee members of the respective year. Moreover,\ncommittee members usually do not collaborate among themselves: a significant\nnumber of the papers are authored by the PC members and students. This paper\nmay contribute to the SBES community, in particular, its special interest\ngroup, in understanding the needs and challenges of the PC's participants.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.01785,regular,pre_llm,2020,2,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'A Framework for In-Vivo Testing of Mobile Applications\n\n  The ecosystem in which mobile applications run is highly heterogeneous and\nconfigurable. All layers upon which mobile apps are built offer wide\npossibilities of variations, from the device and the hardware, to the operating\nsystem and middleware, up to the user preferences and settings. Testing all\npossible configurations exhaustively, before releasing the app, is\nunaffordable. As a consequence, the app may exhibit different, including\nfaulty, behaviours when executed in the field, under specific configurations.\nIn this paper, we describe a framework that can be instantiated to support\nin-vivo testing of a mobile app. The framework monitors the configuration in\nthe field and triggers in-vivo testing when an untested configuration is\nrecognized. Experimental results show that the overhead introduced by\nmonitoring is unnoticeable to negligible (i.e., 0-6%) depending on the device\nbeing used (high- vs. low-end). In-vivo test execution required on average 3s:\nif performed upon screen lock activation, it introduces just a slight delay\nbefore locking the device.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.058,regular,pre_llm,2020,2,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'On Learning Meaningful Assert Statements for Unit Test Cases\n\n  Software testing is an essential part of the software lifecycle and requires\na substantial amount of time and effort. It has been estimated that software\ndevelopers spend close to 50% of their time on testing the code they write. For\nthese reasons, a long standing goal within the research community is to\n(partially) automate software testing. While several techniques and tools have\nbeen proposed to automatically generate test methods, recent work has\ncriticized the quality and usefulness of the assert statements they generate.\nTherefore, we employ a Neural Machine Translation (NMT) based approach called\nAtlas(AuTomatic Learning of Assert Statements) to automatically generate\nmeaningful assert statements for test methods. Given a test method and a focal\nmethod (i.e.,the main method under test), Atlas can predict a meaningful assert\nstatement to assess the correctness of the focal method. We applied Atlas to\nthousands of test methods from GitHub projects and it was able to predict the\nexact assert statement manually written by developers in 31% of the cases when\nonly considering the top-1 predicted assert. When considering the top-5\npredicted assert statements, Atlas is able to predict exact matches in 50% of\nthe cases. These promising results hint to the potential usefulness ofour\napproach as (i) a complement to automatic test case generation techniques, and\n(ii) a code completion support for developers, whocan benefit from the\nrecommended assert statements while writing test code.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.03121,regular,pre_llm,2020,2,"{'ai_likelihood': 1.447068320380317e-05, 'text': 'PP-ind: Description of a Repository of Industrial Pair Programming\n  Research Data\n\n  PP-ind is a repository of audio-video-recordings of industrial pair\nprogramming sessions. Since 2007, our research group has collected data in 13\ncompanies. A total of 57 developers worked together (mostly in groups of two,\nbut also three or four) in 67 sessions with a mean length of 1:35 hours. In\nthis report, we describe how we collected the data and provide summaries and\ncharacterizations of the sessions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.0683,review,pre_llm,2020,2,"{'ai_likelihood': 1.4901161193847656e-06, 'text': ""GDPR Compliance in the Context of Continuous Integration\n\n  The enactment of the General Data Protection Regulation (GDPR) in 2018 forced\nany organization that collects and/or processes EU-based personal data to\ncomply with stringent privacy regulations. Software organizations have\nstruggled to achieve GDPR compliance both before and after the GDPR deadline.\nWhile some studies have relied on surveys or interviews to find general\nimplications of the GDPR, there is a lack of in-depth studies that investigate\ncompliance practices and compliance challenges of software organizations. In\nparticular, there is no information on small and medium enterprises (SMEs),\nwhich represent the majority of organizations in the EU, nor on organizations\nthat practice continuous integration. Using design science methodology, we\nconducted an in-depth study over the span of 20 months regarding GDPR\ncompliance practices and challenges in collaboration with a small, startup\norganization. We first identified our collaborator's business problems and then\niteratively developed two artifacts to address those problems: a set of\noperationalized GDPR principles, and an automated GDPR tool that tests those\nGDPR-derived privacy requirements. This design science approach resulted in\nfour implications for research and for practice. For example, our research\nreveals that GDPR regulations can be partially operationalized and tested\nthrough automated means, which improves compliance practices, but more research\nis needed to create more efficient and effective means to disseminate and\nmanage GDPR knowledge among software developers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.00032,regular,pre_llm,2020,2,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Declarative Stream Runtime Verification (hLola)\n\n  Stream Runtime Verification is a formal dynamic analysis technique that\ngeneralizes runtime verification algorithms from temporal logics like LTL to\nstream monitoring, allowing to compute richer verdicts than Booleans (including\nquantitative and arbitrary data). In this paper we study the problem of\nimplementing an SRV engine that is truly extensible to arbitrary data theories,\nand we propose a solution as a Haskell embedded domain specific language. In\nspite of the theoretical clean separation in SRV between temporal dependencies\nand data computations, previous engines include ad-hoc implementations of a few\ndata types, requiring complex changes to incorporate new data theories. We\npropose here an SRV language called hLola that borrows general Haskell types\nand embeds them transparently into an eDSL. This novel technique, which we call\nlift deep embedding, allows for example, the use of higher-order functions for\nstatic stream parameterization. We describe the Haskell implementation of hLola\nand illustrate simple extensions implemented using libraries, which require\nlong and error-prone additions in other ad-hoc SRV formalisms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.03001,regular,pre_llm,2020,3,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'The Cost and Benefits of Static Analysis During Development\n\n  Without quantitative data, deciding whether and how to use static analysis in\na development workflow is a matter of expert opinion and guesswork rather than\nan engineering trade-off. Moreover, relevant data collected under real-world\nconditions is scarce. Important but unknown quantitative parameters include,\nbut are not limited to, the effort to apply the techniques, the effectiveness\nof removing defects, where in the workflow the analysis should be applied, and\nhow static analysis interacts with other quality techniques. This study\nexamined the detailed development process data 35 industrial development\nprojects that included static analysis and that were also instrumented with the\nTeam Software Process. We collected data project plans, logs of effort, defect,\nand size and post mortem reports and analyzed performance of their development\nactivities to populate a parameterized performance model. We compared effort\nand defect levels with and without static analysis using a planning model that\nincludes feedback for defect removal effectiveness and fix effort. We found\nevidence that using each tool developers found and removed defects at a higher\nrate than alternative removal techniques. Moreover, the early and inexpensive\nremoval reduced not only final defect density but also total development\neffort. The contributions of this paper include real-world benchmarks of\nprocess data from projects using static analysis tools, a demonstration of a\ncost-effectiveness analysis using this data, and a recommendation these tools\nwere consistently cost effective operationally.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.03201,regular,pre_llm,2020,3,"{'ai_likelihood': 7.616149054633247e-07, 'text': ""Automated Repair of Resource Leaks in Android Applications\n\n  Resource leaks -- a program does not release resources it previously acquired\n-- are a common kind of bug in Android applications. Even with the help of\nexisting techniques to automatically detect leaks, writing a leak-free program\nremains tricky. One of the reasons is Android's event-driven programming model,\nwhich complicates the understanding of an application's overall control flow.\n  In this paper, we present PlumbDroid: a technique to automatically detect and\nfix resource leaks in Android applications. PlumbDroid uses static analysis to\nfind execution traces that may leak a resource. The information built for\ndetection also undergirds automatically building a fix -- consisting of release\noperations performed at appropriate locations -- that removes the leak and does\nnot otherwise affect the application's usage of the resource.\n  An empirical evaluation on resource leaks from the DroidLeaks curated\ncollection demonstrates that PlumbDroid's approach is scalable, precise, and\nproduces correct fixes for a variety of resource leak bugs: PlumbDroid\nautomatically found and repaired 50 leaks that affect 9 widely used resources\nof the Android system, including all those collected by DroidLeaks for those\nresources; on average, it took just 2 minutes to detect and repair a leak.\nPlumbDroid also compares favorably to Relda2/RelFix -- the only other fully\nautomated approach to repair Android resource leaks -- since it usually detects\nmore leaks with higher precision and producing smaller fixes. These results\nindicate that PlumbDroid can provide valuable support to enhance the quality of\nAndroid applications in practice.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.0562,regular,pre_llm,2020,3,"{'ai_likelihood': 3.1458006964789497e-06, 'text': 'CC2Vec: Distributed Representations of Code Changes\n\n  Existing work on software patches often use features specific to a single\ntask. These works often rely on manually identified features, and human effort\nis required to identify these features for each task. In this work, we propose\nCC2Vec, a neural network model that learns a representation of code changes\nguided by their accompanying log messages, which represent the semantic intent\nof the code changes. CC2Vec models the hierarchical structure of a code change\nwith the help of the attention mechanism and uses multiple comparison functions\nto identify the differences between the removed and added code.\n  To evaluate if CC2Vec can produce a distributed representation of code\nchanges that is general and useful for multiple tasks on software patches, we\nuse the vectors produced by CC2Vec for three tasks: log message generation, bug\nfixing patch identification, and just-in-time defect prediction. In all tasks,\nthe models using CC2Vec outperform the state-of-the-art techniques.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.00643,review,pre_llm,2020,3,"{'ai_likelihood': 1.5695889790852867e-05, 'text': 'A Systematic Literature Review of Modern Software Visualization\n\n  We report on the state-of-the-art of software visualization. To ensure\nreproducibility, we adopted the Systematic Literature Review methodology. That\nis, we analyzed 1440 entries from IEEE Xplore and ACM Digital Library\ndatabases. We selected 105 relevant full papers published in 2013-2019, which\nwe classified based on the aspect of the software system that is supported\n(i.e., structure, behavior, and evolution). For each paper, we extracted main\ndimensions that characterize software visualizations, such as software\nengineering tasks, roles of users, information visualization techniques, and\nmedia used to display visualizations. We provide researchers in the field an\noverview of the state-of-the-art in software visualization and highlight\nresearch opportunities. We also help developers to identify suitable\nvisualizations for their particular context by matching software visualizations\nto development concerns and concrete details to obtain available visualization\ntools.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.14377,regular,pre_llm,2020,3,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'DATESSO: Self-Adapting Service Composition with Debt-Aware Two Levels\n  Constraint Reasoning\n\n  The rapidly changing workload of service-based systems can easily cause\nunder-/over-utilization on the component services, which can consequently\naffect the overall Quality of Service (QoS), such as latency. Self-adaptive\nservices composition rectifies this problem, but poses several challenges: (i)\nthe effectiveness of adaptation can deteriorate due to over-optimistic\nassumptions on the latency and utilization constraints, at both local and\nglobal levels; and (ii) the benefits brought by each composition plan is often\nshort term and is not often designed for long-term benefits -- a natural\nprerequisite for sustaining the system. To tackle these issues, we propose a\ntwo levels constraint reasoning framework for sustainable self-adaptive\nservices composition, called DATESSO. In particular, DATESSO consists of a re\nned formulation that differentiates the ""strictness"" for latency/utilization\nconstraints in two levels. To strive for long-term benefits, DATESSO leverages\nthe concept of technical debt and time-series prediction to model the utility\ncontribution of the component services in the composition. The approach embeds\na debt-aware two level constraint reasoning algorithm in DATESSO to improve the\nefficiency, effectiveness and sustainability of self-adaptive service\ncomposition. We evaluate DATESSO on a service-based system with real-world\nWS-DREAM dataset and comparing it with other state-of-the-art approaches. The\nresults demonstrate the superiority of DATESSO over the others on the\nutilization, latency and running time whilst likely to be more sustainable.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.09873,regular,pre_llm,2020,3,"{'ai_likelihood': 4.7021441989474825e-06, 'text': 'Software Effort Estimation from Use Case Diagrams Using Nonlinear\n  Regression Analysis\n\n  Software effort estimation in the early stages of the software life cycle is\none of the most essential and daunting tasks for project managers. In this\nresearch, a new model based on non-linear regression analysis is proposed to\npredict software effort from use case diagrams. It is concluded that, where\nsoftware size is classified from small to very large, one linear or non-linear\nequation for effort estimation cannot be applied. Our model with three\ndifferent non-linear regression equations can incorporate the different ranges\nin software size.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.10707,review,pre_llm,2020,3,"{'ai_likelihood': 2.053048875596788e-06, 'text': 'Quantifying the Re-identification Risk of Event Logs for Process Mining\n\n  Event logs recorded during the execution of business processes constitute a\nvaluable source of information. Applying process mining techniques to them,\nevent logs may reveal the actual process execution and enable reasoning on\nquantitative or qualitative process properties. However, event logs often\ncontain sensitive information that could be related to individual process\nstakeholders through background information and cross-correlation. We therefore\nargue that, when publishing event logs, the risk of such re-identification\nattacks must be considered. In this paper, we show how to quantify the\nre-identification risk with measures for the individual uniqueness in event\nlogs. We also report on a large-scale study that explored the individual\nuniqueness in a collection of publicly available event logs. Our results\nsuggest that potentially up to all of the cases in an event log may be\nre-identified, which highlights the importance of privacy-preserving techniques\nin process mining.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.14015,regular,pre_llm,2020,3,"{'ai_likelihood': 2.317958407931858e-07, 'text': '20-MAD -- 20 Years of Issues and Commits of Mozilla and Apache\n  Development\n\n  Data of long-lived and high profile projects is valuable for research on\nsuccessful software engineering in the wild. Having a dataset with different\nlinked software repositories of such projects, enables deeper diving\ninvestigations. This paper presents 20-MAD, a dataset linking the commit and\nissue data of Mozilla and Apache projects. It includes over 20 years of\ninformation about 765 projects, 3.4M commits, 2.3M issues, and 17.3M issue\ncomments, and its compressed size is over 6 GB. The data contains all the\ntypical information about source code commits (e.g., lines added and removed,\nmessage and commit time) and issues (status, severity, votes, and summary). The\nissue comments have been pre-processed for natural language processing and\nsentiment analysis. This includes emoticons and valence and arousal scores.\nLinking code repository and issue tracker information, allows studying\nindividuals in two types of repositories and provide more accurate time zone\ninformation for issue trackers as well. To our knowledge, this the largest\nlinked dataset in size and in project lifetime that is not based on GitHub.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.0259,review,pre_llm,2020,3,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'Some Aspects of a Software Reliability Problem\n\n  Obviously, the dynamism of software reliability research has speeded up\nsignificantly in the last period, and we can state the fact that its intensity\nis approaching, and in some cases is ahead of the information systems hardware\nreliability research intensity. Reliability of software is much more important\nthan its other characteristics, such as runtime, and although the absolute\nreliability of modern software is apparently unattainable, there is still no\ngenerally accepted measure of reliability of computer programs. The article\nanalyzes the reasons for the situation and offers an approach to solving the\nproblem. The article touches upon the issue of general characteristics of\ninformation systems software life cycle. Considered software application\nreliability questions and use of fail-safe ensuring programming. Also presented\nbasic types of so-called virus programs that lead to abnormal functioning of\ninformation systems. Much attention is given to presenting some known models\nused for software debugging and operating. So, this review paper consists of\nfour sections: information systems software process creation, reliability of\ninformation systems software, using of fail-safe programs and estimation of\nsoftware reliability according the results of adjusting and normal operation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.03799,regular,pre_llm,2020,3,"{'ai_likelihood': 1.457002427842882e-06, 'text': 'Continuous Experimentation for Automotive Software on the Example of a\n  Heavy Commercial Vehicle in Daily Operation\n\n  As the automotive industry focuses its attention more and more towards the\nsoftware functionality of vehicles, techniques to deliver new software value at\na fast pace are needed. Continuous Experimentation, a practice coming from the\nweb-based systems world, is one of such techniques. It enables researchers and\ndevelopers to use real-world data to verify their hypothesis and steer the\nsoftware evolution based on performances and user preferences, reducing the\nreliance on simulations and guesswork. Several challenges prevent the verbatim\nadoption of this practice on automotive cyber-physical systems, e.g., safety\nconcerns and limitations from computational resources; nonetheless, the\nautomotive field is starting to take interest in this technique. This work aims\nat demonstrating and evaluating a prototypical Continuous Experimentation\ninfrastructure, implemented on a distributed computational system housed in a\ncommercial truck tractor that is used in daily operations by a logistic company\non public roads. The system comprises computing units and sensors, and software\ndeployment and data retrieval are only possible remotely via a mobile data\nconnection due to the commercial interests of the logistics company. This study\nshows that the proposed experimentation process resulted in the development\nteam being able to base software development choices on the real-world data\ncollected during the experimental procedure. Additionally, a set of previously\nidentified design criteria to enable Continuous Experimentation on automotive\nsystems was discussed and their validity confirmed in the light of the\npresented work.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.10572,regular,pre_llm,2020,3,"{'ai_likelihood': 0.0, 'text': ""Characterizing the Roles of Contributors in Open-source Scientific\n  Software Projects\n\n  The development of scientific software is, more than ever, critical to the\npractice of science, and this is accompanied by a trend towards more open and\ncollaborative efforts. Unfortunately, there has been little investigation into\nwho is driving the evolution of such scientific software or how the\ncollaboration happens. In this paper, we address this problem. We present an\nextensive analysis of seven open-source scientific software projects in order\nto develop an empirically-informed model of the development process. This\nanalysis was complemented by a survey of 72 scientific software developers. In\nthe majority of the projects, we found senior research staff (e.g. professors)\nto be responsible for half or more of commits (an average commit share of 72%)\nand heavily involved in architectural concerns (seniors were more likely to\ninteract with files related to the build system, project meta-data, and\ndeveloper documentation). Juniors (e.g.graduate students) also contribute\nsubstantially -- in one studied project, juniors made almost 100% of its\ncommits. Still, graduate students had the longest contribution periods among\njuniors (with 1.72 years of commit activity compared to 0.98 years for postdocs\nand 4 months for undergraduates). Moreover, we also found that third-party\ncontributors are scarce, contributing for just one day for the project. The\nresults from this study aim to help scientists to better understand their own\nprojects, communities, and the contributors' behavior, while paving the road\nfor future software engineering research\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.1273,regular,pre_llm,2020,3,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'Why did developers migrate Android applications from Java to Kotlin?\n\n  Currently, the majority of apps running on mobile devices are Android apps\ndeveloped in Java. However, developers can now write Android applications using\na new programming language: Kotlin, which Google adopted in 2017 as an official\nprogramming language for developing Android apps. Since then, Android\ndevelopers have been able to: a) start writing Android applications from\nscratch using Kotlin, b) evolve their existing Android applications written in\nJava by adding Kotlin code (possible thanks to the interoperability between the\ntwo languages), or c) migrate their Android apps from Java to Kotlin. This\npaper aims to study this last case. We conducted a qualitative study to find\nout why Android developers have migrated Java code to Kotlin and to bring\ntogether their experiences about the process, in order to identify the main\ndifficulties they have faced. To execute the study, we first identified commits\nfrom open-source Android projects that have migrated Java code to Kotlin. Then,\nwe emailed the developers that wrote those migrations. We thus obtained\ninformation from 98 developers who had migrated code from Java to Kotlin. This\npaper presents the main reasons identified by the study for performing the\nmigration. We found that developers migrated Java code to Kotlin in order to\naccess programming language features (e.g., extension functions, lambdas, smart\ncasts) that are not available with Java for Android development, and to obtain\nsafer code (i.e., avoid null-pointer exceptions). We also identified research\ndirections that the research community could focus on in order to help\ndevelopers to improve the experience of migrating their Java applications to\nKotlin.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.06108,regular,pre_llm,2020,3,"{'ai_likelihood': 3.6093923780653213e-06, 'text': 'On the Role of Software Architecture in DevOps Transformation: An\n  Industrial Case Study\n\n  Development and Operations (DevOps), a particular type of Continuous Software\nEngineering, has become a popular Software System Engineering paradigm.\nSoftware architecture is critical in succeeding with DevOps. However, there is\nlittle evidence-based knowledge of how software systems are architected in the\nindustry to enable and support DevOps. Since architectural decisions, along\nwith their rationales and implications, are very important in the architecting\nprocess, we performed an industrial case study that has empirically identified\nand synthesized the key architectural decisions considered essential to DevOps\ntransformation by two software development teams. Our study also reveals that\napart from the chosen architecture style, DevOps works best with modular\narchitectures. In addition, we found that the performance of the studied teams\ncan improve in DevOps if operations specialists are added to the teams to\nperform the operations tasks that require advanced expertise. Finally,\ninvestment in testing is inevitable for the teams if they want to release\nsoftware changes faster.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.08116,regular,pre_llm,2020,3,"{'ai_likelihood': 9.602970547146268e-06, 'text': 'Automated synthesis of local time requirement for service composition\n\n  Service composition aims at achieving a business goal by composing existing\nservice-based applications or components. The response time of a service is\ncrucial especially in time critical business environments, which is often\nstated as a clause in service level agreements between service providers and\nservice users. To meet the guaranteed response time requirement of a composite\nservice, it is important to select a feasible set of component services such\nthat their response time will collectively satisfy the response time\nrequirement of the composite service. In this work, we use the BPEL modeling\nlanguage, that aims at specifying Web services. We extend it with timing\nparameters, and equip it with a formal semantics. Then, we propose a fully\nautomated approach to synthesize the response time requirement of component\nservices modeled using BPEL, in the form of a constraint on the local response\ntimes. The synthesized requirement will guarantee the satisfaction of the\nglobal response time requirement, statically or dynamically. We implemented our\nwork into a tool, Selamat, and performed several experiments to evaluate the\nvalidity of our approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.01483,regular,pre_llm,2020,3,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Modeling and Selection of Interdependent Software Requirements using\n  Fuzzy Graphs\n\n  Software requirement selection is to find an optimal set of requirements that\ngives the highest value for a release of software while keeping the cost within\nthe budget. However, value-related dependencies among software requirements may\nimpact the value of an optimal set. Moreover, value-related dependencies can be\nof varying strengths. Hence, it is important to consider both the existence and\nthe strengths of value-related dependencies during a requirement selection. The\nexisting selection models however, either assume that software requirements are\nindependent or they ignore strengths of requirement dependencies. This paper\npresents a cost-value optimization model that considers the impacts of\nvalue-related requirement dependencies on the value of selected requirements\n(optimal set). We have exploited algebraic structure of fuzzy graphs for\nmodeling value-related requirement dependencies and their strengths. Validity\nand practicality of the work are verified through carrying out several\nsimulations and studying a real world software project.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.07914,regular,pre_llm,2020,3,"{'ai_likelihood': 1.8874804178873699e-06, 'text': 'Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code\n\n  Statistical language modeling techniques have successfully been applied to\nlarge source code corpora, yielding a variety of new software development\ntools, such as tools for code suggestion, improving readability, and API\nmigration. A major issue with these techniques is that code introduces new\nvocabulary at a far higher rate than natural language, as new identifier names\nproliferate. Both large vocabularies and out-of-vocabulary issues severely\naffect Neural Language Models (NLMs) of source code, degrading their\nperformance and rendering them unable to scale.\n  In this paper, we address this issue by: 1) studying how various modelling\nchoices impact the resulting vocabulary on a large-scale corpus of 13,362\nprojects; 2) presenting an open vocabulary source code NLM that can scale to\nsuch a corpus, 100 times larger than in previous work; and 3) showing that such\nmodels outperform the state of the art on three distinct code corpora (Java, C,\nPython). To our knowledge, these are the largest NLMs for code that have been\nreported.\n  All datasets, code, and trained models used in this work are publicly\navailable.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.05336,regular,pre_llm,2020,3,"{'ai_likelihood': 1.483493381076389e-05, 'text': ""On Tracking Java Methods with Git Mechanisms\n\n  Method-level historical information is useful in research on mining software\nrepositories such as fault-prone module detection or evolutionary coupling\nidentification. An existing technique named Historage converts a Git repository\nof a Java project to a finer-grained one. In a finer-grained repository, each\nJava method exists as a single file. Treating Java methods as files has an\nadvantage, which is that Java methods can be tracked with Git mechanisms. The\nbiggest benefit of tracking methods with Git mechanisms is that it can easily\nconnect with any other tools and techniques build on Git infrastructure.\nHowever, Historage's tracking has an issue of accuracy, especially on small\nmethods. More concretely, in the case that a small method is renamed or moved\nto another class, Historage has a limited capability to track the method. In\nthis paper, we propose a new technique, FinerGit, to improve the trackability\nof Java methods with Git mechanisms. We implement FinerGit as a system and\napply it to 182 open source software projects, which include 1,768K methods in\ntotal. The experimental results show that our tool has a higher capability of\ntracking methods in the case that methods are renamed or moved to other\nclasses.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.00078,regular,pre_llm,2020,3,"{'ai_likelihood': 5.39753172132704e-06, 'text': 'Modeling the Realization and Execution of Functions and Functional\n  Requirements\n\n  Requirements engineering plays a critical role in developing software\nsystems. One of the most difficult tasks in this process is identifying\nfunctional requirements. A critical problem in many projects is missing\nrequirements until late in the development cycle. In this paper, our core\ninterest is function modeling, which refers to building models of systems based\non their functionalities and on the functionalities of their subcomponents. We\npresent a framework as the basis for specifying functional requirements via a\nmodeling language that produces a high-level diagrammatic representation. The\naim is to deliver an overall system description, facilitate communication and\nunderstanding, construct a holistic view of the system above the domains of\ndifferent expertise, and lay the foundation for the design phase. We analyze\nthe notion of function and its elementary types and apply examples of natural\nlanguage description and scenarios. The results reveal a new method that lays a\nfoundation for works on functionality and viable methodology for capturing its\nrequirements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.11667,review,pre_llm,2020,3,"{'ai_likelihood': 4.635916815863716e-07, 'text': ""Patch Quality and Diversity of Invariant-Guided Search-Based Program\n  Repair\n\n  Most automatic program repair techniques rely on test cases to specify\ncorrect program behavior. Due to test cases' frequently incomplete coverage of\ndesired behavior, however, patches often overfit and fail to generalize to\nbroader requirements. Moreover, in the absence of perfectly correct outputs,\nmethods to ensure higher patch quality, such as merging together several\npatches or a human evaluating patch recommendations, benefit from having access\nto a diverse set of patches, making patch diversity a potentially useful trait.\nWe evaluate the correctness and diversity of patches generated by GenProg and\nan invariant-based diversity-enhancing extension described in our prior work.\nWe find no evidence that promoting diversity changes the correctness of patches\nin a positive or negative direction. Using invariant- and test case\ngeneration-driven metrics for measuring semantic diversity, we find no observed\nsemantic differences between patches for most bugs, regardless of the repair\ntechnique used.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.07383,regular,pre_llm,2020,3,"{'ai_likelihood': 6.9207615322536894e-06, 'text': 'Lazy Product Discovery in Huge Configuration Spaces\n\n  Highly-configurable software systems can have thousands of interdependent\nconfiguration options across different subsystems. In the resulting\nconfiguration space, discovering a valid product configuration for some\nselected options can be complex and error prone. The configuration space can be\norganized using a feature model, fragmented into smaller interdependent feature\nmodels reflecting the configuration options of each subsystem.\n  We propose a method for lazy product discovery in large fragmented feature\nmodels with interdependent features. We formalize the method and prove its\nsoundness and completeness. The evaluation explores an industrial-size\nconfiguration space. The results show that lazy product discovery has\nsignificant performance benefits compared to standard product discovery, which\nin contrast to our method requires all fragments to be composed to analyze the\nfeature model. Furthermore, the method succeeds when more efficient,\nheuristics-based engines fail to find a valid configuration.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.04441,regular,pre_llm,2020,4,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Contract-based Hierarchical Resilience Management for Cyber-Physical\n  Systems\n\n  Orchestrated collaborative effort of physical and cyber components to satisfy\ngiven requirements is the central concept behind Cyber-Physical Systems (CPS).\nTo duly ensure the performance of components, a software-based resilience\nmanager is a flexible choice to detect and recover from faults quickly.\nHowever, a single resilience manager, placed at the centre of the system to\ndeal with every fault, suffers from decision-making overburden; and therefore,\nis out of the question for distributed large-scale CPS. On the other hand,\nprompt detection of failures and efficient recovery from them are challenging\nfor decentralised resilience managers. In this regard, we present a novel\nresilience management framework that utilises the concept of management\nhierarchy. System design contracts play a key role in this framework for prompt\nfault-detection and recovery. Besides the details of the framework, an Industry\n4.0 related test case is presented in this article to provide further insights.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.12647,review,pre_llm,2020,4,"{'ai_likelihood': 6.887647840711806e-06, 'text': 'Requirements engineering in global scaled agile software development\n  environment: a multi-vocal literature review protocol\n\n  Context: Requirements engineering in global scaled agile software development\nand the planning phase for a multi-vocal literature review. Objective: Develop\na protocol to specify the plan which will be followed to conduct a multi-vocal\nliterature review study on requirements engineering in global scaled agile\nsoftware development. Method: Kitchenham and Charters (2007), and Garousi et\nal. (2019) guidelines were followed to develop a protocol for multi-vocal\nliterature review. Result: A validated protocol to conduct a multi-vocal\nliterature review. Conclusion: The review protocol consist of five phases\nenumerated as follows: research questions, search strategies, validation of\nreview process, reporting the review, and making changes to the protocol.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.05332,regular,pre_llm,2020,4,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'A Procedure and Guidelines for Analyzing Groups of Software Engineering\n  Replications\n\n  Context: Researchers from different groups and institutions are collaborating\non building groups of experiments by means of replication (i.e., conducting\ngroups of replications). Disparate aggregation techniques are being applied to\nanalyze groups of replications. The application of unsuitable techniques to\naggregate replication results may undermine the potential of groups of\nreplications to provide in-depth insights from experiment results. Objectives:\nProvide an analysis procedure with a set of embedded guidelines to aggregate\nsoftware engineering (SE) replication results. Method: We compare the\ncharacteristics of groups of replications for SE and other mature experimental\ndisciplines such as medicine and pharmacology. In view of their differences,\nthe limitations with regard to the joint data analysis of groups of SE\nreplications and the guidelines provided in mature experimental disciplines to\nanalyze groups of replications, we build an analysis procedure with a set of\nembedded guidelines specifically tailored to the analysis of groups of SE\nreplications. We apply the proposed analysis procedure to a representative\ngroup of SE replications to illustrate its use. Results: All the information\ncontained within the raw data should be leveraged during the aggregation of\nreplication results. The analysis procedure that we propose encourages the use\nof stratified individual participant data and aggregated data in tandem to\nanalyze groups of SE replications. Conclusion: The aggregation techniques used\nto analyze groups of replications should be justified in research articles.\nThis will increase the reliability and transparency of joint results. The\nproposed guidelines should ease this endeavor.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.08518,regular,pre_llm,2020,4,"{'ai_likelihood': 1.655684577094184e-06, 'text': 'Improving The Effectiveness of Automatically Generated Test Suites Using\n  Metamorphic Testing\n\n  Automated test generation has helped to reduce the cost of software testing.\nHowever, developing effective test oracles for these automatically generated\ntest inputs is a challenging task. Therefore, most automated test generation\ntools use trivial oracles that reduce the fault detection effectiveness of\nthese automatically generated test cases. In this work, we provide results of\nan empirical study showing that utilizing metamorphic relations can increase\nthe fault detection effectiveness of automatically generated test cases.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.0938,regular,pre_llm,2020,4,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'A Procedure for Extracting Software Development Process Patterns\n\n  Process patterns represent well-structured and successful recurring\nactivities of Software Development Methodologies. They are able to form a\nlibrary of reusable building blocks that can be utilized in Situational Method\nEngineering for constructing a custom SDM or enhancing an existing one to fit\nspecific project situation. Recently, some researchers have subjectively\nextracted process patterns from existing SDMs based on cumulative experience in\nvarious domains; however, how to objectively extract process patterns from SDMs\nby adopting a systematic procedure has remained as question. In this regard,\nthis paper is concerned with a procedure aiming to take process patterns out of\nexisting SDMs. An example illustrates applicability of the proposed procedure\nfor extracting process patterns in a specific context.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.00289,regular,pre_llm,2020,4,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'A Case Study on Tool Support for Collaboration in Agile Development\n\n  We report on a longitudinal case study conducted at the Italian site of a\nlarge software company to further our understanding of how development and\ncommunication tools can be improved to better support agile practices and\ncollaboration. After observing inconsistencies in the way communication tools\n(i.e., email, Skype, and Slack) were used, we first reinforced the use of Slack\nas the central hub for internal communication, while setting clear rules\nregarding tools usage. As a second main change, we refactored the Jira Scrum\nboard into two separate boards, a detailed one for developers and a high-level\none for managers, while also introducing automation rules and the integration\nwith Slack. The first change revealed that the teams of developers used and\nappreciated Slack differently with the QA team being the most favorable and\nthat the use of channels is hindered by automatic notifications from\ndevelopment tools (e.g., Jenkins). The findings from the second change show\nthat 85\\% of the interviewees reported perceived improvements in their\nworkflow. Despite the limitations due to the single nature of the reported\ncase, we highlight the importance for companies to reflect on how to properly\nset up their agile work environment to improve communication and facilitate\ncollaboration.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.03727,regular,pre_llm,2020,4,"{'ai_likelihood': 2.4172994825575088e-06, 'text': 'Towards Highly Scalable Runtime Models with History\n\n  Advanced systems such as IoT comprise many heterogeneous, interconnected, and\nautonomous entities operating in often highly dynamic environments. Due to\ntheir large scale and complexity, large volumes of monitoring data are\ngenerated and need to be stored, retrieved, and mined in a time- and\nresource-efficient manner. Architectural self-adaptation automates the control,\norchestration, and operation of such systems. This can only be achieved via\nsophisticated decision-making schemes supported by monitoring data that fully\ncaptures the system behavior and its history.\n  Employing model-driven engineering techniques we propose a highly scalable,\nhistory-aware approach to store and retrieve monitoring data in form of\nenriched runtime models. We take advantage of rule-based adaptation where\nchange events in the system trigger adaptation rules. We first present a scheme\nto incrementally check model queries in the form of temporal logic formulas\nwhich represent the conditions of adaptation rules against a runtime model with\nhistory. Then we enhance the model to retain only information that is\ntemporally relevant to the queries, therefore reducing the accumulation of\ninformation to a required minimum. Finally, we demonstrate the feasibility and\nscalability of our approach via experiments on a simulated smart healthcare\nsystem employing a real-world medical guideline.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.07524,regular,pre_llm,2020,4,"{'ai_likelihood': 7.616149054633247e-07, 'text': ""Results from a replicated experiment on the affective reactions of\n  novice developers when applying test-driven development\n\n  Test-driven Development (TDD) is an incremental approach to software\ndevelopment. Despite it is claimed to improve both quality of software and\ndevelopers' productivity, the research on the claimed effects of TDD has so far\nshown inconclusive results. Some researchers have ascribed these inconclusive\nresults to the negative affective states that TDD would provoke. A previous\n(baseline) experiment has, therefore, studied the affective reactions of\n(novice) developers---i.e., 29 third-year undergraduates in Computer Science\n(CS)---when practicing TDD to implement software. To validate the results of\nthe baseline experiment, we conducted a replicated experiment that studies the\naffective reactions of novice developers when applying TDD to develop software.\nDevelopers in the treatment group carried out a development task using TDD,\nwhile those in the control group used a non-TDD approach. To measure the\naffective reactions of developers, we used the Self-Assessment Manikin\ninstrument complemented with a liking dimension. The most important differences\nbetween the baseline and replicated experiments are: (i) the kind of novice\ndevelopers involved in the experiments---third-year vs. second-year\nundergraduates in CS from two different universities; and (ii) their\nnumber---29 vs. 59. The results of the replicated experiment do not show any\ndifference in the affective reactions of novice developers. Instead, the\nresults of the baseline experiment suggest that developers seem to like TDD\nless as compared to a non-TDD approach and that developers following TDD seem\nto like implementing code less than the other developers, while testing code\nseems to make them less happy.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.06183,review,pre_llm,2020,4,"{'ai_likelihood': 3.2120280795627172e-06, 'text': 'When to Update Systematic Literature Reviews in Software Engineering\n\n  [Context] Systematic Literature Reviews (SLRs) have been adopted by the\nSoftware Engineering (SE) community for approximately 15 years to provide\nmeaningful summaries of evidence on several topics. Many of these SLRs are now\npotentially outdated, and there are no systematic proposals on when to update\nSLRs in SE. [Objective] The goal of this paper is to provide recommendations on\nwhen to update SLRs in SE. [Method] We evaluated, using a three-step approach,\na third-party decision framework (3PDF) employed in other fields, to decide\nwhether SLRs need updating. First, we conducted a literature review of SLR\nupdates in SE and contacted the authors to obtain their feedback relating to\nthe usefulness of the 3PDF within the context of SLR updates in SE. Second, we\nused these authors feedback to see whether the framework needed any adaptation;\nnone was suggested. Third, we applied the 3PDF to the SLR updates identified in\nour literature review. [Results] The 3PDF showed that 14 of the 20 SLRs did not\nneed updating. This supports the use of a decision support mechanism (such as\nthe 3PDF) to help the SE community decide when to update SLRs. [Conclusions] We\nput forward that the 3PDF should be adopted by the SE community to keep\nrelevant evidence up to date and to avoid wasting effort with unnecessary\nupdates.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.09786,regular,pre_llm,2020,4,"{'ai_likelihood': 9.934107462565105e-08, 'text': ""Chat activity is a better predictor than chat sentiment on software\n  developers productivity\n\n  Recent works have proposed that software developers' positive emotion has a\npositive impact on software developers' productivity. In this paper we\ninvestigate two data sources: developers chat messages (from Slack and Hipchat)\nand source code commits of a single co-located Agile team over 200 working\ndays. Our regression analysis shows that the number of chat messages is the\nbest predictor and predicts productivity measured both in the number of commits\nand lines of code with $R^2$ of 0.33 and 0.27 respectively. We then add\nsentiment analysis variables until AIC of our model no longer improves and gets\n$R^2$ values of 0.37 (commits) and 0.30 (lines of code). Thus, analyzing chat\nsentiment improves productivity prediction over chat activity alone but the\ndifference is not massive. This work supports the idea that emotional state and\nproductivity are linked in software development. We find that three positive\nsentiment metrics, but surprisingly also one negative sentiment metric is\nassociated with higher productivity.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.0312,review,pre_llm,2020,4,"{'ai_likelihood': 1.357661353217231e-06, 'text': ""Ranking Computer Vision Service Issues using Emotion\n\n  Software developers are increasingly using machine learning APIs to implement\n'intelligent' features. Studies show that incorporating machine learning into\nan application increases technical debt, creates data dependencies, and\nintroduces uncertainty due to non-deterministic behaviour. However, we know\nvery little about the emotional state of software developers who deal with such\nissues. In this paper, we do a landscape analysis of emotion found in 1,245\nStack Overflow posts about computer vision APIs. We investigate the application\nof an existing emotion classifier EmoTxt and manually verify our results. We\nfound that the emotion profile varies for different question categories.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.09381,regular,pre_llm,2020,4,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Process Patterns for Service Oriented Development\n\n  Software systems development nowadays has moved towards dynamic composition\nof services that run on distributed infrastructures aligned with continuous\nchanges in the system requirements. Consequently, software developers need to\ntailor project specific methodologies to fit their methodology requirements.\nProcess patterns present a suitable solution by providing reusable method\nchunks of software development methodologies for constructing methodologies to\nfit specific requirements. In this paper, we propose a set of high-level\nservice-oriented process patterns that can be used for constructing and\nenhancing situational service-oriented methodologies. We show how these\npatterns are used to construct a specific service-oriented methodology for the\ndevelopment of a sample system.\n  Keywords. Service-Oriented Software Development Methodologies, Process\nPatterns, Process Meta-Model, Situational Method Engineering\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.08016,review,pre_llm,2020,4,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'An Exploration of IoT Platform Development\n\n  Internet of Things platforms are key enablers for smart city initiatives,\ntargeting the improvement of citizens quality of life and economic growth. As\nIoT platforms are dynamic, proactive, and heterogeneous socio-technical\nartefacts, systematic approaches are required for their development. Limited\nsurveys have exclusively explored how IoT platforms are developed and\nmaintained from the perspective of information system development process\nlifecycle. In this paper, we present a detailed analysis of 63 approaches. This\nis accomplished by proposing an evaluation framework as a cornerstone to\nhighlight the characteristics, strengths, and weaknesses of these approaches.\nThe survey results not only provide insights of empirical findings,\nrecommendations, and mechanisms for the development of quality aware IoT\nplatforms, but also identify important issues and gaps that need to be\naddressed.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.14191,regular,pre_llm,2020,4,"{'ai_likelihood': 4.735257890489366e-06, 'text': ""Efficient Binary-Level Coverage Analysis\n\n  Code coverage analysis plays an important role in the software testing\nprocess. More recently, the remarkable effectiveness of coverage feedback has\ntriggered a broad interest in feedback-guided fuzzing. In this work, we\nintroduce bcov, a tool for binary-level coverage analysis. Our tool statically\ninstruments x86-64 binaries in the ELF format without compiler support. We\nimplement several techniques to improve efficiency and scale to large\nreal-world software. First, we bring Agrawal's probe pruning technique to\nbinary-level instrumentation and effectively leverage its superblocks to reduce\noverhead. Second, we introduce sliced microexecution, a robust technique for\njump table analysis which improves CFG precision and enables us to instrument\njump table entries. Additionally, smaller instructions in x86-64 pose a\nchallenge for inserting detours. To address this challenge, we aggressively\nexploit padding bytes and systematically host detours in neighboring basic\nblocks. We evaluate bcov on a corpus of 95 binaries compiled from eight popular\nand well-tested packages like FFmpeg and LLVM. Two instrumentation policies,\nwith different edge-level precision, are used to patch all functions in this\ncorpus - over 1.6 million functions. Our precise policy has average performance\nand memory overheads of 14% and 22% respectively. Instrumented binaries do not\nintroduce any test regressions. The reported coverage is highly accurate with\nan average F-score of 99.86%. Finally, our jump table analysis is comparable to\nthat of IDA Pro on gcc binaries and outperforms it on clang binaries.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.08671,review,pre_llm,2020,4,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'A Critical Review of Concepts, Benefits, and Pitfalls of Blockchain\n  Technology Using Concept Map\n\n  Blockchain is relatively a new area of research. However, a surge of research\nstudies on the blockchain has taken place in recent years. These research\nstudies have mostly focused on designing and developing conceptual frameworks\nto build more reliable, transparent and efficient digital systems. While\nblockchain brings a wide variety of benefits, it also imposes certain\nchallenges. Therefore, the objective of this research is to understand the\nproperties of blockchain, its current uses, observed benefits and pitfalls to\nprovide a balanced understanding of blockchain. A systematic literature review\napproach was adopted in this paper in order to attain the objective. A total of\n51 articles were selected and reviewed. As outcomes, this research provides a\nsummary of the state-of-the-art research studies conducted in the area of\nblockchain. Furthermore, we develop a set of concept maps aiming to provide\nin-depth knowledge on blockchain technology for its efficient and effective\nusage in the development of future technological solutions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.10777,review,pre_llm,2020,4,"{'ai_likelihood': 1.1821587880452475e-05, 'text': 'Code Smells and Refactoring: A Tertiary Systematic Review of Challenges\n  and Observations\n\n  In this paper, we present a tertiary systematic literature review of previous\nsurveys, secondary systematic literature reviews, and systematic mappings. We\nidentify the main observations (what we know) and challenges (what we do not\nknow) on code smells and refactoring. We show that code smells and refactoring\nhave a strong relationship with quality attributes, i.e., with\nunderstandability, maintainability, testability, complexity, functionality, and\nreusability. We argue that code smells and refactoring could be considered as\nthe two faces of a same coin. Besides, we identify how refactoring affects\nquality attributes, more than code smells. We also discuss the implications of\nthis work for practitioners, researchers, and instructors. We identify 13 open\nissues that could guide future research work. Thus, we want to highlight the\ngap between code smells and refactoring in the current state of\nsoftware-engineering research. We wish that this work could help the\nsoftware-engineering research community in collaborating on future work on code\nsmells and refactoring.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.08937,review,pre_llm,2020,4,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'The implications of digitalization on business model change\n\n  Context: Digitalization brings new opportunities and also challenges to\nsoftware companies.\n  Objective: Software companies have mostly focused on the technical aspects of\nhanding changes and mostly ignoring the business model changes and their\nimplications on software organization and the architecture. In this paper, we\nsynthesize implications of the digitalization based on an extensive literature\nsurvey and a longitudinal case study at Ericsson AB.\n  Method: Using thematic analysis, we present six propositions to be used to\nfacilitate the cross-disciplinary analysis of business model dynamics and the\neffectiveness and efficiency of the outcome of business modeling, by linking\nvalue, transaction, and organizational learning to business model change.\n  Conclusions: Business model alignment is highlighted as a new business model\nresearch area for understanding the relationships between the dynamic nature of\nbusiness models, organization design, and the value creation in the business\nmodel activities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.11044,regular,pre_llm,2020,4,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'BOLD: An Ontology-based Log Debugger for C Programs\n\n  The different activities related to debugging such as program\ninstrumentation, representation of execution trace and analysis of trace are\nnot typically performed in an unified framework. We propose \\textit{BOLD}, an\nOntology-based Log Debugger to unify and standardize the activities in\ndebugging. The syntactical information of programs can be represented in the\nfrom of Resource Description Framework (RDF) triples. Using the BOLD framework,\nthe programs can be automatically instrumented by using declarative\nspecifications over these triples. A salient feature of the framework is to\nstore the execution trace of the program also as RDF triples called\n\\textit{trace triples}. These triples can be queried to implement the common\ndebug operations. The novelty of the framework is to abstract these triples as\n\\textit{spans} for high-level reasoning. A span gives a way of examining the\nvalues of a particular variable over certain portion of the program execution.\nThe properties of the spans are defined formally as a Web Ontology Language\n(OWL) ontology called \\textit{Program Debug (PD) Ontology}. Using the span\nabstraction and PD ontology, end-users can debug a given buggy program in a\nstandard manner. A notable feature of using ontology is that users can\naccurately debug in some cases of missing information, which can be practically\nuseful. To demonstrate the feasibility of the proposed framework, we have\ndebugged the programs in a standard bug benchmark suite Software-artifact\nInfrastructure Repository (SIR). Experiments show that the querying time is\nalmost the same as in \\texttt{gdb}. The reasoning time depends on the\nsub-language of OWL. We find that the expressibility offered by OWL-DL language\nis sufficient for the bugs in SIR programs; but to achieve scalability in\nreasoning, a restricted OWL-RL language is required.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.10136,regular,pre_llm,2020,4,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Enhancing the OPEN Process Framework with Service-Oriented Method\n  Fragments\n\n  Service-orientation is a promising paradigm that enables the engineering of\nlarge-scale distributed software systems using rigorous software development\nprocesses. The existing problem is that every service-oriented software\ndevelopment project often requires a customized development process that\nprovides specific service-oriented software engineering tasks in support of\nrequirements unique to that project. To resolve this problem and allow\nsituational method engineering, we have defined a set of method fragments in\nsupport of the engineering of the project-specific service-oriented software\ndevelopment processes. We have derived the proposed method fragments from the\nrecurring features of eleven prominent service-oriented software development\nmethodologies using a systematic mining approach. We have added these new\nfragments to the repository of OPEN Process Framework to make them available to\nsoftware engineers as reusable fragments using this well-known method\nrepository.\n  Keyword. Service-Oriented Software Development, OPEN Process Framework, OPF\nRepository, Method Fragment, Situational Method Engineering\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.05851,regular,pre_llm,2020,4,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Detecting Latency Degradation Patterns in Service-based Systems\n\n  Performance in heterogeneous service-based systems shows non-determistic\ntrends. Even for the same request type, latency may vary from one request to\nanother. These variations can occur due to several reasons on different levels\nof the software stack: operating system, network, software libraries,\napplication code or others. Furthermore, a request may involve several Remote\nProcedure Calls (RPC), where each call can be subject to performance variation.\nPerformance analysts inspect distributed traces and seek for recurrent patterns\nin trace attributes, such as RPCs execution time, in order to cluster traces in\nwhich variations may be induced by the same cause. Clustering ""similar"" traces\nis a prerequisite for effective performance debugging. Given the scale of the\nproblem, such activity can be tedious and expensive. In this paper, we present\nan automated approach that detects relevant RPCs execution time patterns\nassociated to request latency degradation, i.e. latency degradation patterns.\nThe presented approach is based on a genetic search algorithm driven by an\ninformation retrieval relevance metric and an optimized fitness evaluation.\nEach latency degradation pattern identifies a cluster of requests subject to\nlatency degradation with similar patterns in RPCs execution time. We show on a\nmicroservice-based application case study that the proposed approach can\neffectively detect clusters identified by artificially injected latency\ndegradation patterns. Experimental results show that our approach outperforms\nin terms of F-score a state-of-art approach for latency profile analysis and\nwidely popular machine learning clustering algorithms. We also show how our\napproach can be easily extended to trace attributes other than RPC execution\ntime (e.g. HTTP headers, execution node, etc.).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.01261,regular,pre_llm,2020,5,"{'ai_likelihood': 5.496872795952691e-06, 'text': ""Formal Verification of Solidity contracts in Event-B\n\n  Smart contracts are the artifact of the blockchain that provide immutable and\nverifiable specifications of physical transactions. Solidity is a\ndomain-specific programming language with the purpose of defining smart\ncontracts. It aims at reducing the transaction costs occasioned by the\nexecution of contracts on the distributed ledgers such as the Ethereum.\nHowever, Solidity contracts need to adhere safety and security requirements\nthat require formal verification and certification. This paper proposes a\nmethod to meet such requirements by translating Solidity contracts to Event-B\nmodels, supporting certification. To that purpose, we define a restrained\nSolidity subset and a transfer function which translates Solidity contracts to\nEvent-B models. Then we take advantage of Event-B method capabilities to refine\nmodels at different levels of abstraction to verify Solidity contracts'\nproperties. And we can verify the generated proof obligations of the Event-B\nmodel with the help of the Rodin platform.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.02652,regular,pre_llm,2020,5,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'Enhancing Software Development Process (ESDP) using Data Mining\n  Integrated Environment\n\n  Nowadays, it has become a basic need to reuse existing Application\nProgramming Interface (API), Class Libraries, and frameworks for rapid software\ndevelopment. Software developers often reuse this by calling the respective\nAPIs or libraries. But in doing so, developers usually encounter different\nproblems in searching for appropriate code snippets. In most cases, API and\nLibraries are complex and not well structured or well documented. Online search\nengine consumes time in searching, yet match is not that relevant and\nrepresentation is not good. To get a suggestion according to the query we can\nfind that snippet online using search engines or code search engines. In some\ncases database dependent searching and remote web server based mined repository\nsearching bring a problem to the developers. Finding an API recommendation on\ncode search engine often deal with extra-large files that eventually slow down\nthe software development process. We have searched for a solution throughout\nour work and tried to bring a better outcome. As an alternative action, we have\nimplemented a system what we call Enhancing Software Development Process (ESDP)\ntool that is able to provide an efficient and working integrated environment to\nthe developers with a better abstraction and representation of the search\nresults and programmers need to be derived from the source codes. We also have\nbuilt and applied an XML based enriched repository to get a recommendation from\nthe mined repository on the client-side without interacting with the\ninternet-dependent server to save complications and times. We provide the most\nrelevant code skeletons or mapping to programmers or developers using\ngraph-based representation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.0719,review,pre_llm,2020,5,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Applying a Formal Method in Industry: a 25-Year Trajectory\n\n  Industrial applications involving formal methods are still exceptions to the\ngeneral rule. Lack of understanding, employees without proper education,\ndifficulty to integrate existing development cycles, no explicit requirement\nfrom the market, etc. are explanations often heard for not being more formal.\nHence the feedback provided by industry to academics is not as constructive as\nit might be. Summarizing a 25-year return of experience in the effective\napplication of a formal method - namely B and Event-B - in diverse application\ndomains (railways, smartcard, automotive), this article makes clear why and\nwhere formal methods have been applied, explains the added value obtained so\nfar, and tries to anticipate the future of these two formalisms for safety\ncritical systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.11218,regular,pre_llm,2020,5,"{'ai_likelihood': 4.635916815863716e-07, 'text': 'Updating Weight Values for Function Point Counting\n\n  While software development productivity has grown rapidly, the weight values\nassigned to count standard Function Point (FP) created at IBM twenty-five years\nago have never been updated. This obsolescence raises critical questions about\nthe validity of the weight values; it also creates other problems such as\nambiguous classification, crisp boundary, as well as subjective and locally\ndefined weight values. All of these challenges reveal the need to calibrate FP\nin order to reflect both the specific software application context and the\ntrend of todays software development techniques more accurately. We have\ncreated a FP calibration model that incorporates the learning ability of neural\nnetworks as well as the capability of capturing human knowledge using fuzzy\nlogic. The empirical validation using ISBSG Data Repository (release 8) shows\nan average improvement of 22% in the accuracy of software effort estimations\nwith the new calibration.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.00444,regular,pre_llm,2020,5,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Learning to Recognize Actionable Static Code Warnings (is Intrinsically\n  Easy)\n\n  Static code warning tools often generate warnings that programmers ignore.\nSuch tools can be made more useful via data mining algorithms that select the\n""actionable"" warnings; i.e. the warnings that are usually not ignored.\n  In this paper, we look for actionable warnings within a sample of 5,675\nactionable warnings seen in 31,058 static code warnings from FindBugs. We find\nthat data mining algorithms can find actionable warnings with remarkable ease.\nSpecifically, a range of data mining methods (deep learners, random forests,\ndecision tree learners, and support vector machines) all achieved very good\nresults (recalls and AUC (TRN, TPR) measures usually over 95% and false alarms\nusually under 5%).\n  Given that all these learners succeeded so easily, it is appropriate to ask\nif there is something about this task that is inherently easy. We report that\nwhile our data sets have up to 58 raw features, those features can be\napproximated by less than two underlying dimensions. For such intrinsically\nsimple data, many different kinds of learners can generate useful models with\nsimilar performance.\n  Based on the above, we conclude that learning to recognize actionable static\ncode warnings is easy, using a wide range of learning algorithms, since the\nunderlying data is intrinsically simple. If we had to pick one particular\nlearner for this task, we would suggest linear SVMs (since, at least in our\nsample, that learner ran relatively quickly and achieved the best median\nperformance) and we would not recommend deep learning (since this data is\nintrinsically very simple).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.09976,regular,pre_llm,2020,5,"{'ai_likelihood': 5.596213870578343e-06, 'text': 'Alternative Effort-optimal Model-based Strategy for State Machine\n  Testing of IoT Systems\n\n  To effectively test parts of the Internet of Things (IoT) systems with a\nstate machine character, Model-based Testing (MBT) approach can be taken. In\nMBT, a system model is created, and test cases are generated automatically from\nthe model, and a number of current strategies exist. In this paper, we propose\na novel alternative strategy that concurrently allows us to flexibly adjust the\npreferred length of the generated test cases, as well as to mark the states, in\nwhich the test case can start and end. Compared with an intuitive N-switch\ncoverage-based strategy that aims at the same goals, our proposal generates a\nlower number of shorter test cases with fewer test step duplications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.04137,regular,pre_llm,2020,5,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Corrigendum and Supplement to ""Improve Language Modelling for Code\n  Completion through Learning General Token Repetition of Source Code (with\n  Optimized Memory)""\n\n  This paper is written because I receive several inquiry emails saying it is\nhard to achieve good results when applying token repetition learning\ntechniques. If REP (proposed by me) or Pointer-Mixture (proposed by Jian Li) is\ndirectly applied to source code to decide all token repetitions, the model\nperformance will decrease sharply. As we use pre-order traversal to traverse\nthe Abstract Syntax Tree (AST) to generate token sequence, tokens corresponding\nto AST grammar are ignored when learning token repetition. For non-grammar\ntokens, there are many kinds: strings, chars, numbers and identifiers. For each\nkind of tokens, we try to learn its repetition pattern and find that only\nidentifiers have the property of token repetition. For identifiers, there are\nalso many kinds such as variables, package names, method names, simple types,\nqualified types or qualified names. Actually, some kinds of identifiers such as\npackage names, method names, qualified names or qualified types are unlikely to\nbe repeated. Thus, we ignore these kinds of identifiers that are unlikely to be\nrepeated when learning token repetition. This step is crucial and this\nimportant implementation trick is not clearly presented in the paper because we\nthink it is trivial and too many details may bother readers. We offer the\nGitHub address of our model in our conference paper and readers can check the\ndescription and implementation in that repository. Thus, in this paper, we\nsupplement the important implementation optimization details for the already\npublished papers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.06279,review,pre_llm,2020,5,"{'ai_likelihood': 9.768539004855686e-06, 'text': 'Failure Mode Reasoning in Model Based Safety Analysis\n\n  Failure Mode Reasoning (FMR) is a novel approach for analyzing failure in a\nSafety Instrumented System (SIS). The method uses an automatic analysis of an\nSIS program to calculate potential failures in parts of the SIS. In this paper\nwe use a case study from the power industry to demonstrate how FMR can be\nutilized in conjunction with other model-based safety analysis methods, such as\nHiP-HOPS and CFT, in order to achieve a comprehensive safety analysis of SIS.\nIn this case study, FMR covers the analysis of SIS inputs while HiP-HOPS/CFT\nmodels the faults of logic solver and final elements. The SIS program is\nanalyzed by FMR and the results are exported to HiP-HOPS/CFT via automated\ninterfaces. The final outcome is the collective list of SIS failure modes along\nwith their reliability measures. We present and review the results from both\nqualitative and quantitative perspectives.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.12574,regular,pre_llm,2020,5,"{'ai_likelihood': 3.311369154188368e-07, 'text': ""Selecting third-party libraries: The practitioners' perspective\n\n  The selection of third-party libraries is an essential element of virtually\nany software development project. However, deciding which libraries to choose\nis a challenging practical problem. Selecting the wrong library can severely\nimpact a software project in terms of cost, time, and development effort, with\nthe severity of the impact depending on the role of the library in the software\narchitecture, among others. Despite the importance of following a careful\nlibrary selection process, in practice, the selection of third-party libraries\nis still conducted in an ad-hoc manner, where dozens of factors play an\ninfluential role in the decision. In this paper, we study the factors that\ninfluence the selection process of libraries, as perceived by industry\ndevelopers. To that aim, we perform a cross-sectional interview study with 16\ndevelopers from 11 different businesses and survey 115 developers that are\ninvolved in the selection of libraries. We systematically devised a\ncomprehensive set of 26 technical, human, and economic factors that developers\ntake into consideration when selecting a software library. Eight of these\nfactors are new to the literature. We explain each of these factors and how\nthey play a role in the decision. Finally, we discuss the implications of our\nwork to library maintainers, potential library users, package manager\ndevelopers, and empirical software engineering researchers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.08309,regular,pre_llm,2020,5,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'The Bourgeois Gentleman, Engineering and Formal Methods\n\n  Industrial applications involving formal methods are still exceptions to the\ngeneral rule. Lack of understanding, employees without proper education,\ndifficulty to integrate existing development cycles, no explicit requirement\nfrom the market, etc. are explanations often heard for not being more formal.\nThis article reports some experience about a game changer that is going to\nseamlessly integrate formal methods into safety critical systems engineering.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.10295,regular,pre_llm,2020,5,"{'ai_likelihood': 1.447068320380317e-05, 'text': 'A refinement checking based strategy for component-based systems\n  evolution\n\n  We propose inheritance and refinement relations for a CSP-based component\nmodel (BRIC), which supports a constructive design based on composition rules\nthat preserve classical concurrency properties such as deadlock freedom. The\nproposed relations allow extension of functionality, whilst preserving\nbehavioural properties. A notion of extensibility is defined on top of a\nbehavioural relation called convergence, which distinguishes inputs from\noutputs and the context where they are communicated, allowing extensions to\nreuse existing events with different purposes. We mechanise the strategy for\nextensibility verification using the FDR4 tool, and illustrate our results with\nan autonomous healthcare robot case study.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.11315,regular,pre_llm,2020,5,"{'ai_likelihood': 2.185503641764323e-06, 'text': 'Java Decompiler Diversity and its Application to Meta-decompilation\n\n  During compilation from Java source code to bytecode, some information is\nirreversibly lost. In other words, compilation and decompilation of Java code\nis not symmetric. Consequently, decompilation, which aims at producing source\ncode from bytecode, relies on strategies to reconstruct the information that\nhas been lost. Different Java decompilers use distinct strategies to achieve\nproper decompilation. In this work, we hypothesize that the diverse ways in\nwhich bytecode can be decompiled has a direct impact on the quality of the\nsource code produced by decompilers. In this paper, we assess the strategies of\neight Java decompilers with respect to three quality indicators: syntactic\ncorrectness, syntactic distortion and semantic equivalence modulo inputs. Our\nresults show that no single modern decompiler is able to correctly handle the\nvariety of bytecode structures coming from real-world programs. The highest\nranking decompiler in this study produces syntactically correct, and\nsemantically equivalent code output for 84%, respectively 78%, of the classes\nin our dataset. Our results demonstrate that each decompiler correctly handles\na different set of bytecode classes. We propose a new decompiler called\nArlecchino that leverages the diversity of existing decompilers. To do so, we\nmerge partial decompilation into a new one based on compilation errors.\nArlecchino handles 37.6% of bytecode classes that were previously handled by no\ndecompiler. We publish the sources of this new bytecode decompiler.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.09317,regular,pre_llm,2020,5,"{'ai_likelihood': 2.615981631808811e-06, 'text': 'Combining Dynamic Symbolic Execution, Machine Learning and Search-Based\n  Testing to Automatically Generate Test Cases for Classes\n\n  This article discusses a new technique to automatically generate test cases\nfor object oriented programs. At the state of the art, the problem of\ngenerating adequate sets of complete test cases has not been satisfactorily\nsolved yet. There are various techniques to automatically generate test cases\n(random testing, search-based testing, etc.) but each one has its own\nweaknesses. This article proposes an approach that distinctively combines\ndynamic symbolic execution, search-based testing and machine learning, to\nefficiently generate thorough class-level test suites. The preliminary data\nobtained carrying out some experiments confirm that we are going in the right\ndirection.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.10762,regular,pre_llm,2020,5,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'Concurrency-related Flaky Test Detection in Android apps\n\n  Validation of Android apps via testing is difficult owing to the presence of\nflaky tests. Due to non-deterministic execution environments, a sequence of\nevents (a test) may lead to success or failure in unpredictable ways. In this\nwork, we present an approach and tool FlakeShovel for detecting flaky tests\nthrough systematic exploration of event orders. Our key observation is that for\na test in a mobile app, there is a testing framework thread which creates the\ntest events, a main User-Interface (UI) thread processing these events, and\nthere may be several other background threads running asynchronously. For any\nevent e whose execution involves potential non-determinism, we localize the\nearliest (latest) event after (before) which e must happen.We then efficiently\nexplore the schedules between the upper/lower bound events while grouping\nevents within a single statement, to find whether the test outcome is flaky. We\nalso create a suite of subject programs called DroidFlaker to study flaky tests\nin Android apps. Our experiments on subject-suite DroidFlaker demonstrate the\nefficacy of our flaky test detection. Our work is complementary to existing\nflaky test detection tools like Deflaker which check only failing tests.\nFlakeShovel can detect flaky tests among passing tests, as shown by our\napproach and experiments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.04008,regular,pre_llm,2020,5,"{'ai_likelihood': 2.2517310248480903e-06, 'text': 'Feature Location Benchmark for Decomposing and Reusing Android Apps\n\n  Software reuse enables developers to reuse architecture, programs and other\nsoftware artifacts. Realizing a systematical reuse in software brings a large\namount of benefits for stakeholders, including lower maintenance efforts, lower\ndevelopment costs, and time to market. Unfortunately, currently implementing a\nframework for large-scale software reuse in Android apps is still a huge\nproblem, regarding the complexity of the task and lacking of practical\ntechnical support from either tools or domain experts. Therefore, proposing a\nfeature location benchmark for apps will help developers either optimize their\nfeature location techniques or reuse the assets created in the benchmark for\nreusing. In this paper, we release a feature location benchmark, which can be\nused for those developers, who intend to compose software product lines (SPL)\nand release reuse in apps. The benchmark not only contributes to the research\ncommunity for reuse research, but also helps participants in industry for\noptimizing their architecture and enhancing modularity. In addition, we also\ndevelop an Android Studio plugin named caIDE for developers to view and operate\non the benchmark.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.11366,regular,pre_llm,2020,5,"{'ai_likelihood': 1.817941665649414e-05, 'text': ""Empowering Multilevel DSMLs with Integrated Runtime Verification\n\n  Within Model-Driven Software Engineering, Domain-Specific Modelling has\nproven to be a powerful technique to specify systems and systems' behaviour in\na formal, yet understandable way. Runtime verification (RV) has been\nsuccessfully used to verify the correctness of such behaviour. Specifying\nbehaviour requires managing various levels of abstractions, making multilevel\nmodelling (MLM) a suitable approach for this task. In this paper, we present an\napproach to combine MLM and RV with an example from the domain of distributed\nreal-time systems. The semantics of the specified behaviour as well as the\nevaluation of correctness properties are given by model transformation rules.\nThis facilitates simulation of the system and checking against real-time\ntemporal logic correctness properties.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.03084,review,pre_llm,2020,5,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Beware the Normative Fallacy\n\n  Behavioral research can provide important insights for SE practices. But in\nperforming it, many studies of SE are committing a normative fallacy - they\nmisappropriate normative and prescriptive theories for descriptive purposes.\nThe evidence from reviews of empirical studies of decision making in SE\nsuggests that the normative fallacy may is common. This article draws on\ncognitive psychology and behavioral economics to explains this fallacy. Because\ndata collection is framed by narrow and empirically invalid theories, flawed\nassumptions baked into those theories lead to misleading interpretations of\nobserved behaviors and ultimately, to invalid conclusions and flawed\nrecommendations. Researchers should be careful not to rely solely on\nengineering methods to explain what people do when they do engineering.\nInstead, insist that descriptive research be based on validated descriptive\ntheories, listen carefully to skilled practitioners, and only rely on validated\nfindings to prescribe what they should do.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.14306,regular,pre_llm,2020,5,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Large-Scale Microtask Programming\n\n  To make microtask programming more efficient and reduce the potential for\nconflicts between contributors, I developed a new behavior-driven approach to\nmicrotasking programming. In our approach, each microtask asks developers to\nidentify a behavior behavior from a high-level description of a function,\nimplement a unit test for it, implement the behavior, and debug it. It enables\ndevelopers to work on functions in isolation through high-level function\ndescriptions and stubs.\n  In addition, I developed the first approach for building microservices\nthrough microtasks. Building microservices through microtasks is a good match\nbecause our approach requires a client to first specify the functionality the\ncrowd will create through an API. This API can then take the form of a\nmicroservice description. A traditional project may ask a crowd to implement a\nnew microservice by simply describing the desired behavior in a API and\nrecruiting a crowd. We implemented our approach in a web-based IDE,\n\\textit{Crowd Microservices}. It includes an editor for clients to describe the\nsystem requirements through endpoint descriptions as well as a web-based\nprogramming environment where crowd workers can identify, test, implement, and\ndebug behaviors. The system automatically creates, manages, assigns microtasks.\nAfter the crowd finishes, the system automatically deploys the microservice to\na hosting site.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.00967,regular,pre_llm,2020,5,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'A Machine Learning Based Framework for Code Clone Validation\n\n  A code clone is a pair of code fragments, within or between software systems\nthat are similar. Since code clones often negatively impact the maintainability\nof a software system, several code clone detection techniques and tools have\nbeen proposed and studied over the last decade. To detect all possible similar\nsource code patterns in general, the clone detection tools work on the syntax\nlevel while lacking user-specific preferences. This often means the clones must\nbe manually inspected before analysis in order to remove those false positives\nfrom consideration. This manual clone validation effort is very time-consuming\nand often error-prone, in particular for large-scale clone detection. In this\npaper, we propose a machine learning approach for automating the validation\nprocess. Our machine learning-based approach is used to automatically validate\nclones without human inspection. Thus the proposed approach can be used to\nremove the false positive clones from the detection results, automatically\nevaluate the precision of any clone detectors for any given set of datasets,\nevaluate existing clone benchmark datasets, or even be used to build new clone\nbenchmarks and datasets with minimum effort. In an experiment with clones\ndetected by several clone detectors in several different software systems, we\nfound our approach has an accuracy of up to 87.4% when compared against the\nmanual validation by multiple expert judges. The proposed method also shows\nbetter results in several comparative studies with the existing related\napproaches for clone classification.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.0746,regular,pre_llm,2020,5,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Collective Risk Minimization via a Bayesian Model for Statistical\n  Software Testing\n\n  In the last four years, the number of distinct autonomous vehicles platforms\ndeployed in the streets of California increased 6-fold, while the reported\naccidents increased 12-fold. This can become a trend with no signs of subsiding\nas it is fueled by a constant stream of innovations in hardware sensors and\nmachine learning software. Meanwhile, if we expect the public and regulators to\ntrust the autonomous vehicle platforms, we need to find better ways to solve\nthe problem of adding technological complexity without increasing the risk of\naccidents. We studied this problem from the perspective of reliability\nengineering in which a given risk of an accident has severity and probability\nof occurring. Timely information on accidents is important for engineers to\nanticipate and reuse previous failures to approximate the risk of accidents in\na new city. However, this is challenging in the context of autonomous vehicles\nbecause of the sparse nature of data on the operational scenarios (driving\ntrajectories in a new city). Our approach was to mitigate data sparsity by\nreducing the state space through monitoring of multiple-vehicles operations. We\nthen minimized the risk of accidents by determining proper allocation of tests\nfor each equivalence class. Our contributions comprise (1) a set of strategies\nto monitor the operational data of multiple autonomous vehicles, (2) a Bayesian\nmodel that estimates changes in the risk of accidents, and (3) a feedback\ncontrol-loop that minimizes these risks by reallocating test effort. Our\nresults are promising in the sense that we were able to measure and control\nrisk for a diversity of changes in the operational scenarios. We evaluated our\nmodels with data from two real cities with distinct traffic patterns and made\nthe data available for the community.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.0512,review,pre_llm,2020,6,"{'ai_likelihood': 6.953875223795574e-07, 'text': 'Pattern Atlas\n\n  Pattern languages are well-established in the software architecture\ncommunity. Many different aspects of creating a software architecture are\naddressed by such languages. Thus, several pattern languages have to be\nconsidered when building a particular architecture. But these pattern languages\nare isolated, i.e. it is hard to determine the relevant patterns to be applied\nfrom the different pattern languages. Moreover, the sum of patterns from\ndifferent languages may be huge, i.e. restriction to relevant patterns is\ndesirable. In this contribution we envision an encompassing tool, the pattern\natlas, that supports building complex systems based on pattern languages. The\nanalogy to cartography motivates the name of the tool.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.11195,regular,pre_llm,2020,6,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'REBD:A Conceptual Framework for Big Data Requirements Engineering\n\n  Requirements engineering (RE), as a part of the project development life\ncycle, has increasingly been recognized as the key to ensuring on-time,\non-budget, and goal-based delivery of software projects;compromising this vital\nphase is nothing but project failures. RE of big data projects is even more\ncrucial because of the main characteristics of big data, including high volume,\nvelocity, and variety. As the traditional RE methods and tools are user-centric\nrather than data-centric, employing these methodologies is insufficient to\nfulfill the RE processes for big data projects. Because of the importance of RE\nand limitations of traditional RE methodologies in the context of big data\nsoftware projects, in this paper, a big data requirements engineering\nframework, named REBD, has been proposed. This conceptual framework describes\nthe systematic plan to carry out big data projects starting from requirements\nengineering to the development, assuring successful execution, and increased\nproductivity of the big data projects.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.10892,regular,pre_llm,2020,6,"{'ai_likelihood': 9.90099377102322e-06, 'text': 'Prioritizing documentation effort: Can we do better?\n\n  Code documentations are essential for software quality assurance, but due to\ntime or economic pressures, code developers are often unable to write documents\nfor all modules in a project. Recently, a supervised artificial neural network\n(ANN) approach is proposed to prioritize important modules for documentation\neffort. However, as a supervised approach, there is a need to use labeled\ntraining data to train the prediction model, which may not be easy to obtain in\npractice. Furthermore, it is unclear whether the ANN approach is generalizable,\nas it is only evaluated on several small data sets. In this paper, we propose\nan unsupervised approach based on PageRank to prioritize documentation effort.\nThis approach identifies ""important"" modules only based on the dependence\nrelationships between modules in a project. As a result, the PageRank approach\ndoes not need any training data to build the prediction model. In order to\nevaluate the effectiveness of the PageRank approach, we use six additional\nlarge data sets to conduct the experiments in addition to the same data sets\ncollected from open-source projects as used in prior studies. The experimental\nresults show that the PageRank approach is superior to the state-of-the-art ANN\napproach in prioritizing important modules for documentation effort. In\nparticular, due to the simplicity and effectiveness, we advocate that the\nPageRank approach should be used as an easy-to-implement baseline in future\nresearch on documentation effort prioritization, and any new approach should be\ncompared with it to demonstrate its effectiveness.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.10481,review,pre_llm,2020,6,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Identification of Practices and Capabilities in API Management: A\n  Systematic Literature Review\n\n  Traditional organizations are increasingly becoming software producing\norganizations. This software is enabling them to integrate business processes\nbetween different departments and with other organizations through Application\nProgramming Interfaces (APIs). The main task of managing APIs is to ensure that\nthe APIs are easy to use by third parties, such as providing helpful\ndocumentation, monitoring API performance, and even monetizing API usage. The\nknowledge on API management is scattered across academic literature. In this\ndocument, we describe a Systematic Literature Review (SLR) that has the goal of\ncollecting API Management practices and capabilities related to API Management,\nas well as proposing a comprehensive definition of the topic. In the scope of\nthis work, a practice is defined as any practice that has the express goal to\nimprove, encourage and manage the usage of APIs. Capabilities are defined as\nthe ability to achieve a certain goal related to API Management, through the\nexecution of two or more interrelated practices. We follow a standard method\nfor SLRs in software engineering. We managed to collect 24 unique definitions\nfor the topic, 114 practices and 39 capabilities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.01476,regular,pre_llm,2020,6,"{'ai_likelihood': 1.9371509552001953e-05, 'text': 'Kaya: A Testing Framework for Blockchain-based Decentralized\n  Applications\n\n  In recent years, many decentralized applications based on blockchain (DApp)\nhave been developed. However, due to inadequate testing, DApps are easily\nexposed to serious vulnerabilities. We find three main challenges for DApp\ntesting, i.e., the inherent complexity of DApp, inconvenient pre-state setting,\nand not-so-readable logs. In this paper, we propose a testing framework named\nKaya to bridge these gaps. Kaya has three main functions. Firstly, Kaya\nproposes DApp behavior description language (DBDL) to make writing test cases\neasier. Test cases written in DBDL can also be automatically executed by Kaya.\nSecondly, Kaya supports a flexible and convenient way for test engineers to set\nthe blockchain pre-states easily. Thirdly, Kaya transforms incomprehensible\naddresses into readable variables for easy comprehension. With these functions,\nKaya can help test engineers test DApps more easily. Besides, to fit the\nvarious application environments, we provide two ways for test engineers to use\nKaya, i.e., UI and command-line. Our experimental case demonstrates the\npotential of Kaya in helping test engineers to test DApps more easily.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.03844,regular,pre_llm,2020,6,"{'ai_likelihood': 4.337893591986763e-06, 'text': 'Replacements and Replaceables: Making the Case for Code Variants\n\n  There are often multiple ways to implement the same requirement in source\ncode. Different implementation choices can result in code snippets that are\nsimilar, and have been defined in multiple ways: code clones, examples, simions\nand variants. Currently, there is a lack of a consistent and unambiguous\ndefinition of such types of code snippets. Here we present a characterization\nstudy of code variants - a specific type of code snippets that differ from each\nother by at least one desired property, within a given code context. We\ndistinguish code variants from other types of redundancies in source code, and\ndemonstrate the significant role that they play: about 25% to 43% of developer\ndiscussions (in a set of nine open source projects) were about variants. We\ncharacterize different types of variants based on their code context and\ndesired properties. As a demonstration of the possible use of our\ncharacterization of code variants, we show how search results can be ranked\nbased on a desired property (e.g., speed of execution).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.04238,regular,pre_llm,2020,6,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Blurring Boundaries: Toward the Collective Empathic Understanding of\n  Product Requirements\n\n  Within the agile paradigm, many software product companies create\ncross-functional product development teams that own their product or a defined\nset of product features. In contrast to development teams operating within a\nheavily-disciplined software development process, these product teams often\nrequire a deeper and, importantly, a collective understanding of the product\ndomain to serve as a rich context within which to understand the product\nrequirements. Little is known about the factors that support or impede these\nteams in collectively achieving this deep understanding of the product domain.\nUsing Constructivist Grounded Theory method, we study individuals and teams\nacross seven software companies that create products for a diverse range of\nmarkets. The study found that certain organisational and planning process\nfactors play a significant role in whether product development teams have the\npotential to collectively develop deep domain understanding. These factors also\nimpact individual and development team dynamics. We identify two essential\nmetaphorical dynamics of broadening the lens and blurring boundaries that\ncross-functional product teams employ in order to fully embrace product\nownership, visioning, and planning towards achieving a deep collective domain\nunderstanding, creating a richer context in which to understand product\nrequirements. We also conclude that the highly specialised nature of many\norganisational models and development processes is contraindicated for\ncross-functional product development teams in achieving this deep collective\nunderstanding and we call for a rethinking of conventional organisational and\nproduct planning practices for software product development.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.14706,regular,pre_llm,2020,6,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Will Dynamic Arrays finally change the way Models are built?\n\n  Spreadsheets offer a supremely successful and intuitive means of processing\nand exchanging numerical content. Its intuitive ad-hoc nature makes it hugely\npopular for use in diverse areas including business and engineering, yet these\nvery same characteristics make it extraordinarily error-prone; many would\nquestion whether it is suitable for serious analysis or modelling tasks. A\nprevious EuSpRIG paper examined the role of Names in increasing solution\ntransparency and providing a readable notation to forge links with the problem\ndomain. Extensive use was made of CSE array formulas, but it is acknowledged\nthat their use makes spreadsheet development a distinctly cumbersome task.\nSince that time, the new dynamic arrays have been introduced and array\ncalculation is now the default mode of operation for Excel. This paper examines\nthe thesis that their adoption within a more professional development\nenvironment could replace traditional techniques where solution integrity is\nimportant. A major advantage of fully dynamic models is that they require less\nmanual intervention to keep them updated and so have the potential to reduce\nthe attendant errors and risk.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.04967,regular,pre_llm,2020,6,"{'ai_likelihood': 3.3775965372721357e-06, 'text': 'Summarising Big Data: Common GitHub Dataset for Software Engineering\n  Challenges\n\n  In open-source software development environments; textual, numerical and\nrelationship-based data generated are of interest to researchers. Various data\nsets are available for this data, which is frequently used in areas such as\nsoftware engineering and natural language processing. However, since these data\nsets contain all the data in the environment, the problem arises in the\nterabytes of data processing. For this reason, almost all of the studies using\nGitHub data use filtered data according to certain criteria. In this context,\nusing a different data set in each study makes a comparison of the accuracy of\nthe studies quite difficult. In order to solve this problem, a common dataset\nwas created and shared with the researchers, which would allow us to work on\nmany software engineering problems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.12086,review,pre_llm,2020,6,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Success and Failure in Software Engineering: a Followup Systematic\n  Literature Review\n\n  Success and failure in software engineering are still among the least\nunderstood phenomena in the discipline. In a recent special journal issue on\nthe topic, Mantyla et al. started discussing these topics from different\nangles; the authors focused their contributions on offering a general overview\nof both topics without deeper detail. Recognising the importance and impact of\nthe topic, we have executed a followup, more in-depth systematic literature\nreview with additional analyses beyond what was previously provided. These new\nanalyses offer: (a) a grounded-theory of success and failure factors,\nharvesting over 500+ factors from the literature; (b) 14 manually-validated\nclusters of factors that provide relevant areas for success- and\nfailure-specific measurement and risk-analysis; (c) a quality model composed of\npreviously unmeasured organizational structure quantities which are germane to\nsoftware product, process, and community quality. We show that the topics of\nsuccess and failure deserve further study as well as further automated tool\nsupport, e.g., monitoring tools and metrics able to track the factors and\npatterns emerging from our study. This paper provides managers with risks as\nwell as a more fine-grained analysis of the parameters that can be appraised to\nanticipate the risks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.1694,regular,pre_llm,2020,6,"{'ai_likelihood': 2.317958407931858e-06, 'text': 'Traceability Support for Multi-Lingual Software Projects\n\n  Software traceability establishes associations between diverse software\nartifacts such as requirements, design, code, and test cases. Due to the\nnon-trivial costs of manually creating and maintaining links, many researchers\nhave proposed automated approaches based on information retrieval techniques.\nHowever, many globally distributed software projects produce software artifacts\nwritten in two or more languages. The use of intermingled languages reduces the\nefficacy of automated tracing solutions. In this paper, we first analyze and\ndiscuss patterns of intermingled language use across multiple projects, and\nthen evaluate several different tracing algorithms including the Vector Space\nModel (VSM), Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA),\nand various models that combine mono- and cross-lingual word embeddings with\nthe Generative Vector Space Model (GVSM). Based on an analysis of 14\nChinese-English projects, our results show that best performance is achieved\nusing mono-lingual word embeddings integrated into GVSM with machine\ntranslation as a preprocessing step.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.12636,regular,pre_llm,2020,6,"{'ai_likelihood': 3.841188218858507e-06, 'text': ""Multitasking Across Industry Projects: A Replication Study\n\n  Background: Multitasking is usual in software development. It is the ability\nto stop working on a task, switch to another, and return eventually to the\nfirst one, as needed or as scheduled. Multitasking, however, comes at a\ncognitive cost: frequent context-switches can lead to distraction, sub-standard\nwork, and even greater stress. Aims: This paper reports a replication\nexperiment where we gathered data on a group of developers from a software\ndevelopment company from industry on a large collection of projects stored in\nGitLab repositories. Method: We reused the developed models and methods from\nthe original study for measuring the rate and breadth of a developers'\ncontext-switching behavior, and we study how context-switching affects their\nproductivity. We applied semi-structured interviews, replacing the original\nsurvey, to some of the developers to understand the reasons for and perceptions\nof multitasking. Results: We found out that industry developers multitask as\nmuch as OSS developers focusing more (on fewer projects), and working more\nrepetitively from one day to the next is associated with higher productivity,\nbut there is no effect for higher multitasking. Some commons reasons make them\nmultitask: dependencies, personal interests, and social relationships.\nConclusions: Short context change, less than three minutes, did not impact\nresults from industry developers; however, more than that, it brings a feeling\nof left the previous tasks behind. So, it is proportional to how much context\nis switched: as bigger the context and bigger the interruption, it is worst to\ncome back.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.16349,regular,pre_llm,2020,6,"{'ai_likelihood': 2.8477774726019966e-06, 'text': 'New developer metrics: Are comments as crucial as code contributions?\n\n  Open-source code development has become widespread in recent years. As a\nresult, open-source software platforms have also become popular, and millions\nof developers from diverse locations are able to contribute to the same\nprojects. On these platforms, various knowledge about them is obtained from\nuser activity. This information is used in the form of developer metrics to\nsolve a variety of challenges. In this study, we proposed new developer\nmetrics, including commenting and issue-related activity, that require less\ninformation. We concluded that commenting on any feature of a project can be\nequally as valuable as code contribution. In addition, besides the quantitative\nones, metrics based on only the existence of the activity have been shown to\noffer also considerable results. We saw that issues were crucial in identifying\nuser contributions. Even if a developer makes a contribution to only one issue\non a project, the relation between the developer and the project is tight. The\nhit scores are relatively lower because of the sparsity problem of our dataset;\neven so, we believe that we have presented improvable and remarkable new\ndeveloper metrics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.15682,review,pre_llm,2020,6,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'A Survey on the Evaluation of Clone Detection Performance and\n  Benchmarking\n\n  There are a great many clone detection tools proposed in the literature. In\nthis paper, we investigate the state of clone detection tool evaluation. We\nbegin by surveying the clone detection benchmarks, and performing a\nmulti-faceted evaluation and comparison of their features and capabilities. We\nthen survey the existing clone detection tool and technique publications, and\nevaluate how the authors of these works evaluate their own tools/techniques. We\nrank the individual works by how well they measure recall, precision, execution\ntime and scalability. We select the works the best evaluate all four metrics as\nexemplars that should be considered by future researchers publishing clone\ndetection tools/techniques when designing the empirical evaluation of their\ntool/technique. We measure statistics on tool evaluation by the authors, and\nfind that evaluation is poor amongst the authors. We finish our investigation\ninto clone detection evaluation by surveying the existing tool comparison\nstudies, including both the qualitative and quantitative studies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.11597,regular,pre_llm,2020,6,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Using Fault Injection to Assess Blockchain Systems in Presence of Faulty\n  Smart Contracts\n\n  Blockchain has become particularly popular due to its promise to support\nbusiness-critical services in very different domains (e.g., retail, supply\nchains, healthcare). Blockchain systems rely on complex middleware, like\nEthereum or Hyperledger Fabric, that allow running smart contracts, which\nspecify business logic in cooperative applications. The presence of software\ndefects or faults in these contracts has notably been the cause of failures,\nincluding severe security problems. In this paper, we use a software\nimplemented fault injection (SWIFI) technique to assess the behavior of\npermissioned blockchain systems in the presence of faulty smart contracts. We\nemulate the occurrence of general software faults (e.g., missing variable\ninitialization) and also blockchain-specific software faults (e.g., missing\nrequire statement on transaction sender) in smart contracts code to observe the\nimpact on the overall system dependability (i.e., reliability and integrity).\nWe also study the effectiveness of formal verification (i.e., done by\nsolc-verify) and runtime protections (e.g., using the assert statement)\nmechanisms in detection of injected faults. Results indicate that formal\nverification as well as additional runtime protections have to complement\nbuilt-in platform checks to guarantee the proper dependability of blockchain\nsystems and applications. The work presented in this paper allows smart\ncontract developers to become aware of possible faults in smart contracts and\nto understand the impact of their presence. It also provides valuable\ninformation for middleware developers to improve the behavior (e.g., overall\nfault tolerance) of their systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.04794,regular,pre_llm,2020,6,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Abstracting spreadsheet data flow through hypergraph redrawing\n\n  We believe the error prone nature of traditional spreadsheets is due to their\nlow level of abstraction. End user programmers are forced to construct their\ndata models from low level cells which we define as ""a data container or\nmanipulator linked by user-intent to model their world and positioned to\nreflect its structure"". Spreadsheet cells are limited in what they may contain\n(scalar values) and the links between them are inherently hidden. This paper\nproposes a method of raising the level of abstraction of spreadsheets by\n""redrawing the boundary"" of the cell. To expose the hidden linkage structure we\ntransform spreadsheets into fine-grained graphs with operators and values as\nnodes. ""cells"" are then represented as hypergraph edges by drawing a boundary\n""wall"" around a set of operator/data nodes. To extend what cells may contain\nand to create a higher level model of the spreadsheet we propose that\nresearchers should seek techniques to redraw these boundaries to create higher\nlevel ""cells"" which will more faithfully represent the end-user\'s real\nworld/mental model. We illustrate this approach via common sub-expression\nidentification and the application of sub-tree isomorphisms for the detection\nof vector (array) operations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.14505,regular,pre_llm,2020,6,"{'ai_likelihood': 4.569689432779948e-06, 'text': 'Source Code Comments: Overlooked in the Realm of Code Clone Detection\n\n  Reusing code can produce duplicate or near-duplicate code clones in code\nrepositories. Current code clone detection techniques, like Program Dependence\nGraphs, rely on code structure and their dependencies to detect clones. These\ntechniques are expensive, using large amounts of processing power, time, and\nmemory. In practice, programmers often utilize code comments to comprehend and\nreuse code, as comments carry important domain knowledge. But current code\ndetection techniques ignore code comments, mainly due to the ambiguity of the\nEnglish language. Recent advances in information retrieval techniques may have\nthe potential to utilize code comments for clone detection. We investigated\nthis by empirically comparing the accuracy of detecting clones with solely\ncomments versus solely source code (without comments) on the JHotDraw package,\nwhich contains 315 classes and 27K lines of code. To detect clones at the file\nlevel, we used a topic modeling technique, Latent Dirichlet Allocation, to\nanalyze code comments and GRAPLE -- utilizing Program Dependency Graph -- to\nanalyze code. Our results show 94.86 recall and 84.21 precision with Latent\nDirichlet Allocation and 28.7 recall and 55.39 precision using GRAPLE. We found\nLatent Dirichlet Allocation generated false positives in cases where programs\nlacked quality comments. But this limitation can be addressed by using a hybrid\napproach: utilizing code comments at the file level to reduce the clone set and\nthen using Program Dependency Graph-based techniques at the method level to\ndetect precise clones. Our further analysis across Java and Python packages,\nJava Swing and PyGUI, found a recall of 74.86\\% and a precision of 84.21\\%. Our\nfindings call for reexamining the assumptions regarding the use of code\ncomments in current clone detection techniques.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.12738,regular,pre_llm,2020,6,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Better User Recommendations using Enhancing Software Development Process\n  Repository\n\n  Reusing previously completed software repository to enhance the development\nprocess is a common phenomenon. If developers get suggestions from the existing\nprojects they might be benefited a lot what they eventually expect while\ncoding. The strategies available in this field have been rapidly changing day\nby day. There are a number of efforts that have been focusing on mining process\nand constructing repository. Some of them have emphasized on the web based code\nsearching while others have integrated web based code searching in their\ncustomized tool. But web based approaches have inefficiency especially in\nbuilding repository on which they apply mining technologies. To search the code\nsnippets in response to the user query we need an enriched repository with\nbetter representation and abstraction. To ensure that repository before mining\nprocess we have developed a concept based on Enhancing Software Development\nProcess (ESDP). In ESDP approach multiple sources of codes from both online and\noffline storages are considered to construct the central repository with XML\nrepresentation and applied mining techniques in the client side. The respective\nevaluation shows that ESDP approach works much better in response time and\nperformance than many other existing approaches available today.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.0724,regular,pre_llm,2020,6,"{'ai_likelihood': 1.5232298109266494e-06, 'text': 'Predicting Health Indicators for Open Source Projects (using\n  Hyperparameter Optimization)\n\n  Software developed on public platform is a source of data that can be used to\nmake predictions about those projects. While the individual developing activity\nmay be random and hard to predict, the developing behavior on project level can\nbe predicted with good accuracy when large groups of developers work together\non software projects.\n  To demonstrate this, we use 64,181 months of data from 1,159 GitHub projects\nto make various predictions about the recent status of those projects (as of\nApril 2020). We find that traditional estimation algorithms make many mistakes.\nAlgorithms like $k$-nearest neighbors (KNN), support vector regression (SVR),\nrandom forest (RFT), linear regression (LNR), and regression trees (CART) have\nhigh error rates. But that error rate can be greatly reduced using\nhyperparameter optimization.\n  To the best of our knowledge, this is the largest study yet conducted, using\nrecent data for predicting multiple health indicators of open-source projects.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.03863,regular,pre_llm,2020,6,"{'ai_likelihood': 2.3510720994737412e-06, 'text': 'Guarded Deep Learning using Scenario-Based Modeling\n\n  Deep neural networks (DNNs) are becoming prevalent, often outperforming\nmanually-created systems. Unfortunately, DNN models are opaque to humans, and\nmay behave in unexpected ways when deployed. One approach for allowing safer\ndeployment of DNN models calls for augmenting them with hand-crafted override\nrules, which serve to override decisions made by the DNN model when certain\ncriteria are met. Here, we propose to bring together DNNs and the well-studied\nscenario-based modeling paradigm, by expressing these override rules as simple\nand intuitive scenarios. This approach can lead to override rules that are\ncomprehensible to humans, but are also sufficiently expressive and powerful to\nincrease the overall safety of the model. We describe how to extend and apply\nscenario-based modeling to this new setting, and demonstrate our proposed\ntechnique on multiple DNN models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.03275,regular,pre_llm,2020,7,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'An Advanced Approach for Choosing Security Patterns and Checking their\n  Implementation\n\n  This paper tackles the problems of generating concrete test cases for testing\nwhether an application is vulnerable to attacks, and of checking whether\nsecurity solutions are correctly implemented. The approach proposed in the\npaper aims at guiding developers towards the implementation of secure\napplications, from the threat modelling stage up to the testing one. This\napproach relies on a knowledge base integrating varied security data, e.g.,\nattacks, attack steps, and security patterns that are generic and re-usable\nsolutions to design secure applications. The first stage of the approach\nconsists in assisting developers in the design of Attack Defense Trees\nexpressing the attacker possibilities to compromise an application and the\ndefenses that may be implemented. These defenses are given under the form of\nsecurity pattern combinations. In the second stage, these trees are used to\nguide developers in the test case generation. After the test case execution,\ntest verdicts show whether an application is vulnerable to the threats modelled\nby an ADTree. The last stage of the approach checks whether behavioural\nproperties of security patterns hold in the application traces collected while\nthe test case execution. These properties are formalised with LTL properties,\nwhich are generated from the knowledge base. Developers do not have to write\nLTL properties not to be expert in formal models. We experimented the approach\non 10 Web applications to evaluate its testing effectiveness and its\nperformance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.08927,regular,pre_llm,2020,7,"{'ai_likelihood': 5.629327562120226e-07, 'text': ""Towards a Model of Testers' Cognitive Processes: Software Testing as a\n  Problem Solving Approach\n\n  Software testing is a complex, intellectual activity based (at least) on\nanalysis, reasoning, decision making, abstraction and collaboration performed\nin a highly demanding environment. Naturally, it uses and allocates multiple\ncognitive resources in software testers. However, while a cognitive psychology\nperspective is increasingly used in the general software engineering\nliterature, it has yet to find its place in software testing. To the best of\nour knowledge, no theory of software testers' cognitive processes exists. Here,\nwe take the first step towards such a theory by presenting a cognitive model of\nsoftware testing based on how problem solving is conceptualized in cognitive\npsychology. Our approach is to instantiate a general problem solving process\nfor the specific problem of creating test cases. We then propose an experiment\nfor testing our cognitive test design model. The experiment makes use of verbal\nprotocol analysis to understand the mechanisms by which human testers choose,\ndesign, implement and evaluate test cases. An initial evaluation was then\nperformed with five software engineering master students as subjects. The\nresults support a problem solving-based model of test design for capturing\ntesters' cognitive processes.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.09863,review,pre_llm,2020,7,"{'ai_likelihood': 9.602970547146267e-07, 'text': 'Why Research on Test-Driven Development is Inconclusive?\n\n  [Background] Recent investigations into the effects of Test-Driven\nDevelopment (TDD) have been contradictory and inconclusive. This hinders\ndevelopment teams to use research results as the basis for deciding whether and\nhow to apply TDD. [Aim] To support researchers when designing a new study and\nto increase the applicability of TDD research in the decision-making process in\nthe industrial context, we aim at identifying the reasons behind the\ninconclusive research results in TDD. [Method] We studied the state of the art\nin TDD research published in top venues in the past decade, and analyzed the\nway these studies were set up. [Results] We identified five categories of\nfactors that directly impact the outcome of studies on TDD. [Conclusions] This\nwork can help researchers to conduct more reliable studies, and inform\npractitioners of risks they need to consider when consulting research on TDD.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.12356,regular,pre_llm,2020,7,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'A Case Study on Software Vulnerability Coordination\n\n  Context: Coordination is a fundamental tenet of software engineering.\nCoordination is required also for identifying discovered and disclosed software\nvulnerabilities with Common Vulnerabilities and Exposures (CVEs). Motivated by\nrecent practical challenges, this paper examines the coordination of CVEs for\nopen source projects through a public mailing list. Objective: The paper\nobserves the historical time delays between the assignment of CVEs on a mailing\nlist and the later appearance of these in the National Vulnerability Database\n(NVD). Drawing from research on software engineering coordination, software\nvulnerabilities, and bug tracking, the delays are modeled through three\ndimensions: social networks and communication practices, tracking\ninfrastructures, and the technical characteristics of the CVEs coordinated.\nMethod: Given a period between 2008 and 2016, a sample of over five thousand\nCVEs is used to model the delays with nearly fifty explanatory metrics.\nRegression analysis is used for the modeling. Results: The results show that\nthe CVE coordination delays are affected by different abstractions for noise\nand prerequisite constraints. These abstractions convey effects from the social\nnetwork and infrastructure dimensions. Particularly strong effect sizes are\nobserved for annual and monthly control metrics, a control metric for weekends,\nthe degrees of the nodes in the CVE coordination networks, and the number of\nreferences given in NVD for the CVEs archived. Smaller but visible effects are\npresent for metrics measuring the entropy of the emails exchanged, traces to\nbug tracking systems, and other related aspects. The empirical signals are\nweaker for the technical characteristics. Conclusion: [...]\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.02599,regular,pre_llm,2020,7,"{'ai_likelihood': 5.9604644775390625e-06, 'text': 'Sosed: a tool for finding similar software projects\n\n  In this paper, we present Sosed, a tool for discovering similar software\nprojects. We use fastText to compute the embeddings of subtokens into a dense\nspace for 120,000 GitHub repositories in 200 languages. Then, we cluster\nembeddings to identify groups of semantically similar sub-tokens that reflect\ntopics in source code. We use a dataset of 9 million GitHub projects as a\nreference search base. To identify similar projects, we compare the\ndistributions of clusters among their sub-tokens. The tool receives an\narbitrary project as input, extracts sub-tokens in 16 most popular programming\nlanguages, computes cluster distribution, and finds projects with the closest\ndistribution in the search base. We labeled subtoken clusters with short\ndescriptions to enable Sosed to produce interpretable output. Sosed is\navailable at https://github.com/JetBrains-Research/sosed/. The tool demo is\navailable at https://www.youtube.com/watch?v=LYLkztCGRt8. The multi-language\nextractor of sub-tokens is available separately at\nhttps://github.com/JetBrains-Research/buckwheat/.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.02194,review,pre_llm,2020,7,"{'ai_likelihood': 1.7219119601779514e-06, 'text': '30 Years of Software Refactoring Research:A Systematic Literature Review\n\n  Due to the growing complexity of software systems, there has been a dramatic\nincrease and industry demand for tools and techniques on software refactoring\nin the last ten years, defined traditionally as a set of program\ntransformations intended to improve the system design while preserving the\nbehavior. Refactoring studies are expanded beyond code-level restructuring to\nbe applied at different levels (architecture, model, requirements, etc.),\nadopted in many domains beyond the object-oriented paradigm (cloud computing,\nmobile, web, etc.), used in industrial settings and considered objectives\nbeyond improving the design to include other non-functional requirements (e.g.,\nimprove performance, security, etc.). Thus, challenges to be addressed by\nrefactoring work are, nowadays, beyond code transformation to include, but not\nlimited to, scheduling the opportune time to carry refactoring, recommendations\nof specific refactoring activities, detection of refactoring opportunities, and\ntesting the correctness of applied refactorings. Therefore, the refactoring\nresearch efforts are fragmented over several research communities, various\ndomains, and objectives. To structure the field and existing research results,\nthis paper provides a systematic literature review and analyzes the results of\n3183 research papers on refactoring covering the last three decades to offer\nthe most scalable and comprehensive literature review of existing refactoring\nresearch studies. Based on this survey, we created a taxonomy to classify the\nexisting research, identified research trends, and highlighted gaps in the\nliterature and avenues for further research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.01713,review,pre_llm,2020,7,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Towards the Adoption of OMG Standards in the Development of SOA-Based\n  IoT Systems\n\n  A common feature of the Internet of Things (IoT) is the high heterogeneity,\nregarding network protocols, data formats, hardware and software platforms.\nAiming to deal with such a degree of heterogeneity, several frameworks have\napplied the Model-Driven Development (MDD) to build IoT applications. On the\nsoftware architecture viewpoint, the literature has shown that the\nService-Oriented Architecture (SOA) is a promising style to address the\ninteroperability of entities composing these solutions. Some features of IoT\nmake it challenging to analyze the impact of design decisions on the SOA-based\nIoT applications behavior. Thus, it is a key requirement to simulate the model\nto verify whether the system performs as expected before its implementation.\nAlthough the literature has identified that the SOA style is suitable for\naddressing the interoperability, existing modelling languages do not consider\nSOA elements as first-class citizens when designing IoT applications.\nFurthermore, although existing MDD frameworks provide modeling languages\ncomprising well-defined syntax, they lack execution semantics, thus, are not\nsuitable for model execution and analysis. This work aims at addressing these\nissues by introducing IoTDraw. The framework provides a fully OMG-compliant\nexecutable modeling language for SOA-based IoT systems; thus, its\nspecifications can be implemented by any tool implementing OMG standards.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.01568,regular,pre_llm,2020,7,"{'ai_likelihood': 1.2252065870496963e-06, 'text': 'Identification and Remediation of Self-Admitted Technical Debt in Issue\n  Trackers\n\n  Technical debt refers to taking shortcuts to achieve short-term goals, which\nmight negatively influence software maintenance in the long-term. There is\nincreasing attention on technical debt that is admitted by developers in source\ncode comments (termed as self-admitted technical debt or SATD). But SATD in\nissue trackers is relatively unexplored. We performed a case study, where we\nmanually examined 500 issues from two open source projects (i.e. Hadoop and\nCamel), which contained 152 SATD items. We found that: 1) eight types of\ntechnical debt are identified in issues, namely architecture, build, code,\ndefect, design, documentation, requirement, and test debt; 2) developers\nidentify technical debt in issues in three different points in time, and a\nsmall part is identified by its creators; 3) the majority of technical debt is\npaid off, 4) mostly by those who identified it or created it; 5) the median\ntime and average time to repay technical debt are 872.3 and 25.0 hours\nrespectively.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.06122,regular,pre_llm,2020,7,"{'ai_likelihood': 5.231963263617622e-06, 'text': 'Industrial Experience of Finding Cryptographic Vulnerabilities in\n  Large-scale Codebases\n\n  Enterprise environment often screens large-scale (millions of lines of code)\ncodebases with static analysis tools to find bugs and vulnerabilities. Parfait\nis a static code analysis tool used in Oracle to find security vulnerabilities\nin industrial codebases. Recently, many studies show that there are complicated\ncryptographic vulnerabilities caused by misusing cryptographic APIs in Java. In\nthis paper, we describe how we realize a precise and scalable detection of\nthese complicated cryptographic vulnerabilities based on Parfait framework. The\nkey challenge in the detection of cryptographic vulnerabilities is the high\nfalse alarm rate caused by pseudo-influences. Pseudo-influences happen if\nsecurity-irrelevant constants are used in constructing security-critical\nvalues. Static analysis is usually unable to distinguish them from hard-coded\nconstants that expose sensitive information. We tackle this problem by\nspecializing the backward dataflow analysis used in Parfait with refinement\ninsights, an idea from the tool CryptoGuard. We evaluate our analyzer on a\ncomprehensive Java cryptographic vulnerability benchmark and eleven large\nreal-world applications. The results show that the Parfait-based cryptographic\nvulnerability detector can find real-world cryptographic vulnerabilities in\nlarge-scale codebases with high true-positive rates and low runtime cost.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.02135,regular,pre_llm,2020,7,"{'ai_likelihood': 2.284844716389974e-06, 'text': 'Towards Semantic Detection of Smells in Cloud Infrastructure Code\n\n  Automated deployment and management of Cloud applications relies on\ndescriptions of their deployment topologies, often referred to as\nInfrastructure Code. As the complexity of applications and their deployment\nmodels increases, developers inadvertently introduce software smells to such\ncode specifications, for instance, violations of good coding practices, modular\nstructure, and more. This paper presents a knowledge-driven approach enabling\ndevelopers to identify the aforementioned smells in deployment descriptions. We\ndetect smells with SPARQL-based rules over pattern-based OWL 2 knowledge graphs\ncapturing deployment models. We show the feasibility of our approach with a\nprototype and three case studies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.13592,review,pre_llm,2020,7,"{'ai_likelihood': 6.291601392957899e-07, 'text': 'Case Survey Studies in Software Engineering Research\n\n  Background: Given the social aspects of Software Engineering (SE), in the\nlast twenty years, researchers from the field started using research methods\ncommon in social sciences such as case study, ethnography, and grounded theory.\nMore recently, case survey, another imported research method, has seen its\nincreasing use in SE studies. It is based on existing case studies reported in\nthe literature and intends to harness the generalizability of survey and the\ndepth of case study. However, little is known on how case survey has been\napplied in SE research, let alone guidelines on how to employ it properly.\nAims: This article aims to provide a better understanding of how case survey\nhas been applied in Software Engineering research. Method: To address this\nknowledge gap, we performed a systematic mapping study and analyzed 12 Software\nEngineering studies that used the case survey method. Results: Our findings\nshow that these studies presented a heterogeneous understanding of the approach\nranging from secondary studies to primary inquiries focused on a large number\nof instances of a research phenomenon. They have not applied the case survey\nmethod consistently as defined in the seminal methodological papers.\nConclusions: We conclude that a set of clearly defined guidelines are needed on\nhow to use case survey in SE research, to ensure the quality of the studies\nemploying this approach and to provide a set of clearly defined criteria to\nevaluate such work.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.10744,review,pre_llm,2020,7,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Beyond Accuracy: Assessing Software Documentation Quality\n\n  Good software documentation encourages good software engineering, but the\nmeaning of ""good"" documentation is vaguely defined in the software engineering\nliterature. To clarify this ambiguity, we draw on work from the data and\ninformation quality community to propose a framework that decomposes\ndocumentation quality into ten dimensions of structure, content, and style. To\ndemonstrate its application, we recruited technical editors to apply the\nframework when evaluating examples from several genres of software\ndocumentation. We summarise their assessments -- for example, reference\ndocumentation and README files excel in quality whereas blog articles have more\nproblems -- and we describe our vision for reasoning about software\ndocumentation quality and for the expansion and potential of a unified quality\nframework.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.10611,review,pre_llm,2020,7,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Source Data for the Focus Area Maturity Model for API Management\n\n  We define API Management as an activity that enables organizations to design,\npublish and deploy their APIs for (external) developers to consume. API\nManagement capabilities such as controlling API lifecycles, access and\nauthentication to APIs, monitoring, throttling and analyzing API usage, as well\nas providing security and documentation. These capabilities are often\nimplemented through an integrated platform. This data set describes the API\nManagement Focus Area Maturity Model (API-m-FAMM). In a structured manner, this\nmodel aims to support organizations that expose their API(s) to third-party\ndevelopers in their API management activities. Through a thorough Systematic\nLiterature Review (SLR), 114 practices and 39 capabilities were collected.\nSubsequently, these practices and capabilities were categorized into 6 focus\nareas. Next, the practices and capabilities were analyzed and verified through\ninter-rater agreement and four validation sessions with all involved\nresearchers. Then, the collection of practices and capabilities was verified by\nusing information gathered from supplemental literature, online blog posts,\nwebsites, commercial API management platform documentation and third-party\ntooling. As a result, the initial body of practices and capabilities was\nnarrowed down to 87 practices and 23 capabilities. These practices are\ndescribed by a practice code, name, description, conditions for implementation,\nthe role responsible for the practice, and the associated literature in which\nthe practice was originally identified. Capabilities and focus areas are\ndescribed by a code, description and, optionally, the associated literature in\nwhich it was originally identified. Using the API-m-FAMM, organizations may\nevaluate, improve upon and assess the degree of maturity their business\nprocesses regarding the topic of API management have.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.10419,regular,pre_llm,2020,7,"{'ai_likelihood': 5.265076955159506e-06, 'text': ""Visual Testing of GUIs by Abstraction\n\n  Ensuring the correct visual appearance of graphical user interfaces (GUIs) is\nimportant because visual bugs can cause substantial losses for businesses. An\napplication might behave functionally correct in an automated test, but visual\nbugs can make the GUI effectively unusable for the user. Most of today's\napproaches for visual testing are pixel-based and tend to have flaws that are\ncharacteristic for image differencing. For instance, minor and unimportant\nvisual changes often cause false positives, which confuse the user with\nunnecessary error reports. Our idea is to introduce an abstract GUI state\n(AGS), where we define structural relations to identify relevant GUI changes\nand ignore those that are unimportant from the user's point of view. In\naddition, we explore several strategies to address the GUI element\nidentification problem in terms of AGS. This allows us to provide rich\ndiagnostic information that help the user to better interpret changes. Based on\nthe principles of golden master testing, we can support a fully-automated\napproach to visual testing by using the AGS. We have implemented our approach\nto visually test web pages and our experiments show that we are able to\nreliably detect GUI changes.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.06986,regular,pre_llm,2020,7,"{'ai_likelihood': 2.715322706434462e-06, 'text': ""Estimating the Potential of Program Repair Search Spaces with Commit\n  Analysis\n\n  The most natural method for evaluating program repair systems is to run them\non bug datasets, such as Defects4J. Yet, using this evaluation technique on\narbitrary real-world programs requires heavy configuration. In this paper, we\npropose a purely static method to evaluate the potential of the search space of\nrepair approaches. This new method enables researchers and practitioners to\nencode the search spaces of repair approaches and select potentially useful\nones without struggling with tool configuration and execution. We encode the\nsearch spaces by specifying the repair strategies they employ. Next, we use the\nspecifications to check whether past commits lie in repair search spaces. For a\nrepair approach, including many human-written past commits in its search space\nindicates its potential to generate useful patches. We implement our evaluation\nmethod in LighteR. LighteR gets a Git repository and outputs a list of commits\nwhose source code changes lie in repair search spaces. We run LighteR on 55,309\ncommits from the history of 72 Github repositories with and show that LighteR's\nprecision and recall are 77% and 92%, respectively. Overall, our experiments\nshow that our novel method is both lightweight and effective to study the\nsearch space of program repair approaches.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.16123,review,pre_llm,2020,7,"{'ai_likelihood': 4.278288947211372e-05, 'text': ""On Package Freshness in Linux Distributions\n\n  The open-source Linux operating system is available through a wide variety of\ndistributions, each containing a collection of installable software packages.\nIt can be important to keep these packages as fresh as possible to benefit from\nnew features, bug fixes and security patches. However, not all distributions\nplace the same emphasis on package freshness. We conducted a survey in the\nfirst half of 2020 with 170 Linux users to gauge their perception of package\nfreshness in the distributions they use, the value they place on package\nfreshness and the reasons why they do so, and the methods they use to update\npackages. The results of this survey reveal that, for the aforementioned\nreasons, keeping packages up to date is an important concern to Linux users and\nthat they install and update packages through their distribution's official\nrepositories whenever possible, but often resort to third-party repositories\nand package managers for proprietary software and programming language\nlibraries. Some distributions are perceived to be much quicker in deploying\npackage updates than others. These results are valuable to assess the\nrequirements and expectations of Linux users in terms of package freshness.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.12046,review,pre_llm,2020,7,"{'ai_likelihood': 1.0927518208821616e-06, 'text': 'Model Driven Engineering for Data Protection and Privacy: Application\n  and Experience with GDPR\n\n  In Europe and indeed worldwide, the General Data Protection Regulation (GDPR)\nprovides protection to individuals regarding their personal data in the face of\nnew technological developments. GDPR is widely viewed as the benchmark for data\nprotection and privacy regulations that harmonizes data privacy laws across\nEurope. Although the GDPR is highly beneficial to individuals, it presents\nsignificant challenges for organizations monitoring or storing personal\ninformation. Since there is currently no automated solution with broad\nindustrial applicability, organizations have no choice but to carry out\nexpensive manual audits to ensure GDPR compliance. In this paper, we present a\ncomplete GDPR UML model as a first step towards designing automated methods for\nchecking GDPR compliance. Given that the practical application of the GDPR is\ninfluenced by national laws of the EU Member States, we suggest a two-tiered\ndescription of the GDPR, generic and specialized. In this paper, we provide (1)\nthe GDPR conceptual model we developed with complete traceability from its\nclasses to the GDPR, (2) a glossary to help understand the model, (3) the\nplain-English description of 35 compliance rules derived from GDPR along with\ntheir encoding in OCL, and (4) the set of 20 variations points derived from\nGDPR to specialize the generic model. We further present the challenges we\nfaced in our modeling endeavor, the lessons we learned from it, and future\ndirections for research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.11449,regular,pre_llm,2020,7,"{'ai_likelihood': 2.053048875596788e-06, 'text': 'Fast and Precise On-the-fly Patch Validation for All\n\n  Generate-and-validate (G&V) automated program repair (APR) techniques have\nbeen extensively studied during the past decade. Meanwhile, such techniques can\nbe extremely time-consuming due to manipulation of the program code to\nfabricate a large number of patches and also repeated executions of tests on\npatches to identify potential fixes. PraPR, a recent G&V APR technique, reduces\nthese costs by modifying program code directly at the level of compiled\nbytecode, and further performing on-the-fly patching by allowing multiple\npatches to be tested within the same JVM session. However, PraPR is limited due\nto its pattern-based, bytecode-level nature and it is basically\nunsound/imprecise as it assumes that patch executions do not change global JVM\nstate and affect later patch executions on the same JVM session. Inspired by\nthe PraPR work, we propose a unified patch validation framework, named UniAPR,\nwhich aims to speed up the patch validation for both bytecode and source-code\nAPR via on-the-fly patching; furthermore, UniAPR addresses the imprecise patch\nvalidation issue by resetting the JVM global state via runtime bytecode\ntransformation. We have implemented UniAPR as a fully automated Maven Plugin.\nWe have also performed the first study of on-the-fly patch validation for\nstate-of-the-art source-code-level APR. Our experiments show the first\nempirical evidence that vanilla on-the-fly patch validation can be\nimprecise/unsound; in contrast, our UniAPR framework can speed up\nstate-of-the-art APR by over an order of magnitude without incurring any\nimprecision in patch validation, enabling all existing APR techniques to\nexplore a larger search space to fix more bugs in the near future. Furthermore,\nUniAPR directly enables hybrid source and bytecode APR to fix substantially\nmore bugs than all state-of-the-art APR techniques (under the same time limit)\nin the near future.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.05046,regular,pre_llm,2020,7,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'RulePad: Interactive Authoring of Checkable Design Rules\n\n  Good documentation offers the promise of enabling developers to easily\nunderstand design decisions. Unfortunately, in practice, design documents are\noften rarely updated, becoming inaccurate, incomplete, and untrustworthy. A\nbetter solution is to enable developers to write down design rules which are\nchecked against code for consistency. But existing rule checkers require\nlearning specialized query languages or program analysis frameworks, creating a\nbarrier to writing project-specific rules. We introduce two new techniques for\nauthoring design rules: snippet-based authoring and semi-natural-language\nauthoring. In snippet-based authoring, developers specify characteristics of\nelements to match by writing partial code snippets. In semi-natural language\nauthoring, a textual representation offers a representation for understanding\ndesign rules and resolving ambiguities. We implemented these approaches in\nRulePad. To evaluate RulePad, we conducted a between-subjects study with 14\nparticipants comparing RulePad to the PMD Designer, a utility for writing rules\nin a popular rule checker. We found that those with RulePad were able to\nsuccessfully author 13 times more query elements in significantly less time and\nreported being significantly more willing to use RulePad in their everyday\nwork.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.10405,review,pre_llm,2020,7,"{'ai_likelihood': 8.940696716308594e-07, 'text': 'Collecting Service-Based Maintainability Metrics from RESTful API\n  Descriptions: Static Analysis and Threshold Derivation\n\n  While many maintainability metrics have been explicitly designed for\nservice-based systems, tool-supported approaches to automatically collect these\nmetrics are lacking. Especially in the context of microservices,\ndecentralization and technological heterogeneity may pose challenges for static\nanalysis. We therefore propose the modular and extensible RAMA approach\n(RESTful API Metric Analyzer) to calculate such metrics from machine-readable\ninterface descriptions of RESTful services. We also provide prototypical tool\nsupport, the RAMA CLI, which currently parses the formats OpenAPI, RAML, and\nWADL and calculates 10 structural service-based metrics proposed in scientific\nliterature. To make RAMA measurement results more actionable, we additionally\ndesigned a repeatable benchmark for quartile-based threshold ranges (green,\nyellow, orange, red). In an exemplary run, we derived thresholds for all RAMA\nCLI metrics from the interface descriptions of 1,737 publicly available RESTful\nAPIs. Researchers and practitioners can use RAMA to evaluate the\nmaintainability of RESTful services or to support the empirical evaluation of\nnew service interface metrics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.01172,regular,pre_llm,2020,8,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Bet and Run for Test Case Generation\n\n  Anyone working in the technology sector is probably familiar with the\nquestion: ""Have you tried turning it off and on again?"", as this is usually the\ndefault question asked by tech support. Similarly, it is known in search based\ntesting that metaheuristics might get trapped in a plateau during a search. As\na human, one can look at the gradient of the fitness curve and decide to\nrestart the search, so as to hopefully improve the results of the optimization\nwith the next run. Trying to automate such a restart, it has to be\nprogrammatically decided whether the metaheuristic has encountered a plateau\nyet, which is an inherently difficult problem. To mitigate this problem in the\ncontext of theoretical search problems, the Bet and Run strategy was developed,\nwhere multiple algorithm instances are started concurrently, and after some\ntime all but the single most promising instance in terms of fitness values are\nkilled. In this paper, we adopt and evaluate the Bet and Run strategy for the\nproblem of test case generation. Our work indicates that use of this restart\nstrategy does not generally lead to gains in the quality metrics, when\ninstantiated with the best parameters found in the literature.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.07262,regular,pre_llm,2020,8,"{'ai_likelihood': 3.2782554626464844e-06, 'text': 'Temporal Conformance Checking at Runtime based on Time-infused Process\n  Models\n\n  Conformance checking quantifies the deviations between a set of traces in a\ngiven process log and a set of possible traces defined by a process model.\nCurrent approaches mostly focus on added or missing events. Lately,\nmulti-perspective mining has provided means to check for conformance with time\nand resource constraints encoded as data elements. This paper presents an\napproach for quantifying temporal deviations in conformance checking based on\ninfusing the input process model with a temporal profile. The temporal profile\nis calculated based on an associated process log considering task durations and\nthe temporal distance between events. Moreover, a simple semantic annotation on\ntasks in the process model signifies their importance with respect to time.\nDuring runtime, deviations between an event stream and the process model with\nthe temporal profile are quantified through a cost function for temporal\ndeviations. The evaluation of the approach shows that the results for two\nreal-world data sets from the financial and a manufacturing domain hold the\npromise to improve runtime process monitoring and control capabilities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.05015,review,pre_llm,2020,8,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Open Source Software Development Process: A Systematic Review\n\n  Open Source Software (OSS) has been recognized by the software development\ncommunity as an effective way to deliver software. Unlike traditional software\ndevelopment, OSS development is driven by collaboration among developers spread\ngeographically and motivated by common goals and interests. Besides this fact,\nit is recognized by OSS community the need of understand OSS development\nprocess and its activities. Our goal is to investigate the state-of-art about\nOSS process through conducting a systematic literature review providing an\noverview of how the OSS community has been investigating OSS process over past\nyears identifying and summarizing OSS process activities and their\ncharacteristics as well as translating OSS process in a macro process through\nBPMN notation. As a result, we systematically analysed 33 studies presenting an\noverview of the state-of-art of researches regarding OSS process, a generalized\nOSS development macro process represented by BPMN notation with a detailed\ndescription of each OSS process activity and roles in OSS environment. We\nconclude that OSS process can be in practice further investigated by\nresearchers. In addition, the presented OSS process can be used as a guide for\nOSS projects and being adapted according to each OSS project reality. It\nprovides insights to managers and developers who want to improve their\ndevelopment process even in OSS and traditional environments. Finally,\nrecommendations for OSS community regarding OSS process activities are\nprovided.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.03585,regular,pre_llm,2020,8,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Fully Automated Functional Fuzzing of Android Apps for Detecting\n  Non-crashing Logic Bugs\n\n  Android apps are GUI-based event-driven software and have become ubiquitous\nin recent years. Obviously, functional correctness is critical for an app\'s\nsuccess. However, in addition to crash bugs, non-crashing functional bugs (in\nshort as ""non-crashing bugs"" in this work) like inadvertent function failures,\nsilent user data lost and incorrect display information are prevalent, even in\npopular, well-tested apps. These non-crashing functional bugs are usually\ncaused by program logic errors and manifest themselves on the graphic user\ninterfaces (GUIs). In practice, such bugs pose significant challenges in\neffectively detecting them because (1) current practices heavily rely on\nexpensive, small-scale manual validation (the lack of automation); and (2)\nmodern fully automated testing has been limited to crash bugs (the lack of test\noracles). This paper fills this gap by introducing independent view fuzzing, a\nnovel, fully automated approach for detecting non-crashing functional bugs in\nAndroid apps. Inspired by metamorphic testing, our key insight is to leverage\nthe commonly-held independent view property of Android apps to manufacture\nproperty-preserving mutant tests from a set of seed tests that validate certain\napp properties. The mutated tests help exercise the tested apps under\nadditional, adverse conditions. Any property violations indicate likely\nfunctional bugs for further manual confirmation. We have realized our approach\nas an automated, end-to-end functional fuzzing tool, Genie. Given an app, (1)\nGenie automatically detects non-crashing bugs without requiring human-provided\ntests and oracles (thus fully automated); and (2) the detected non-crashing\nbugs are diverse (thus general and not limited to specific functional\nproperties), which set Genie apart from prior work.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.06214,regular,pre_llm,2020,8,"{'ai_likelihood': 8.675787183973525e-06, 'text': 'The Impact of Auto-Refactoring Code Smells on the Resource Utilization\n  of Cloud Software\n\n  Cloud-based software-as-a-service (SaaS) have gained popularity due to their\nlow cost and elasticity. However, like other software, SaaS applications suffer\nfrom code smells, which can drastically affect functionality and resource\nusage. Code smell is any design in the source code that indicates a deeper\nproblem. The software community deploys automated refactoring to eliminate\nsmells which can improve performance and also decrease the usage of critical\nresources. However, studies that analyze the impact of automatic refactoring\nsmells in SaaS on resources such as CPU and memory have been conducted to a\nlimited extent. Here, we aim to fill that gap and study the impact on resource\nusage of SaaS applications due to automatic refactoring of seven classic code\nsmells: god class, feature envy, type checking, cyclic dependency, shotgun\nsurgery, god method, and spaghetti code. We specified six real-life SaaS\napplications from Github called Zimbra, OneDataShare, GraphHopper, Hadoop,\nJENA, and JAMES which ran on Openstack cloud. Results show that refactoring\nsmells by tools like JDeodrant and JSparrow have widely varying impacts on the\nCPU and memory consumption of the tested applications based on the type of\nsmell refactored. We present the resource utilization impact of each smell and\nalso discuss the potential reasons leading to that effect.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.03042,regular,pre_llm,2020,8,"{'ai_likelihood': 8.344650268554688e-06, 'text': 'PSCS: A Path-based Neural Model for Semantic Code Search\n\n  To obtain code snippets for reuse, programmers prefer to search for related\ndocuments, e.g., blogs or Q&A, instead of code itself. The major reason is due\nto the semantic diversity and mismatch between queries and code snippets. Deep\nlearning models have been proposed to address this challenge. Compared with\napproaches using information retrieval techniques, deep learning models do not\nsuffer from the information loss caused by refining user intention into\nkeywords. However, the performance of previous works is not satisfactory\nbecause they ignore the importance of code structure. When the semantics of\ncode (e.g., identifier names, APIs) are ambiguous, code structure may be the\nonly feature for the model to utilize. In that case, previous works relearn the\nstructural information from lexical tokens of code, which is extremely\ndifficult for a model without any domain knowledge. In this work, we propose\nPSCS, a path-based neural model for semantic code search. Our model encodes\nboth the semantics and structures of code represented by AST paths. We train\nand evaluate our model over 330k-19k query-function pairs, respectively. The\nevaluation results demonstrate that PSCS achieves a SuccessRate of 47.6% and a\nMean Reciprocal Rank (MRR) of 30.4% when considering the top-10 results with a\nmatch. The proposed approach significantly outperforms both DeepCS, the first\napproach that applies deep learning to code search task, and CARLCS, a\nstate-of-the-art approach that introduces a co-attentive representation\nlearning model on the basis of DeepCS. The importance of code structure is\ndemonstrated with an ablation study on code features, which enlightens model\ndesign for further studies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.07729,review,pre_llm,2020,8,"{'ai_likelihood': 2.165635426839193e-05, 'text': 'A Systematic Mapping Study on Microservices Architecture in DevOps\n\n  Context: Applying Microservices Architecture (MSA) in DevOps has received\nsignificant attention in recent years. However, there exists no comprehensive\nreview of the state of research on this topic. Objective: This work aims to\nsystematically identify, analyze, and classify the literature on MSA in DevOps.\nMethod: A Systematic Mapping Study (SMS) has been conducted on the literature\npublished between January 2009 and July 2018. Results: Forty-seven studies were\nfinally selected and the key results are: (1) Three themes on the research on\nMSA in DevOps are ""microservices development and operations in DevOps"",\n""approaches and tool support for MSA based systems in DevOps"", and ""MSA\nmigration experiences in DevOps"". (2) 24 problems with their solutions\nregarding implementing MSA in DevOps are identified. (3) MSA is mainly\ndescribed by using boxes and lines. (4) Most of the quality attributes are\npositively affected when employing MSA in DevOps. (5) 50 tools that support\nbuilding MSA based systems in DevOps are collected. (6) The combination of MSA\nand DevOps has been applied in a wide range of application domains.\nConclusions: The results and findings will benefit researchers and\npractitioners to conduct further research and bring more dedicated solutions\nfor the issues of MSA in DevOps.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.12118,regular,pre_llm,2020,8,"{'ai_likelihood': 9.602970547146267e-07, 'text': 'M3: Semantic API Migrations\n\n  Library migration is a challenging problem, where most existing approaches\nrely on prior knowledge. This can be, for example, information derived from\nchangelogs or statistical models of API usage.\n  This paper addresses a different API migration scenario where there is no\nprior knowledge of the target library. We have no historical changelogs and no\naccess to its internal representation. To tackle this problem, this paper\nproposes a novel approach (M$^3$), where probabilistic program synthesis is\nused to semantically model the behavior of library functions. Then, we use an\nSMT-based code search engine to discover similar code in user applications.\nThese discovered instances provide potential locations for API migrations.\n  We evaluate our approach against 7 well-known libraries from varied\napplication domains, learning correct implementations for 94 functions. Our\napproach is integrated with standard compiler tooling, and we use this\nintegration to evaluate migration opportunities in 9 existing C/C++\napplications with over 1MLoC. We discover over 7,000 instances of these\nfunctions, of which more than 2,000 represent migration opportunities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.03046,regular,pre_llm,2020,8,"{'ai_likelihood': 5.3313043382432725e-06, 'text': 'Towards Using Probabilistic Models to Design Software Systems with\n  Inherent Uncertainty\n\n  The adoption of machine learning (ML) components in software systems raises\nnew engineering challenges. In particular, the inherent uncertainty regarding\nfunctional suitability and the operation environment makes architecture\nevaluation and trade-off analysis difficult. We propose a software architecture\nevaluation method called Modeling Uncertainty During Design (MUDD) that\nexplicitly models the uncertainty associated to ML components and evaluates how\nit propagates through a system. The method supports reasoning over how\narchitectural patterns can mitigate uncertainty and enables comparison of\ndifferent architectures focused on the interplay between ML and classical\nsoftware components. While our approach is domain-agnostic and suitable for any\nsystem where uncertainty plays a central role, we demonstrate our approach\nusing as example a perception system for autonomous driving.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.07947,regular,pre_llm,2020,8,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Differential coverage: automating coverage analysis\n\n  While it is easy to automate coverage data collection, it is a time\nconsuming/difficult/expensive manual process to analyze the data so that it can\nbe acted upon. Complexity arises from numerous sources, of which untested or\npoorly tested legacy code and third-party libraries are two of the most common.\n  Differential coverage and date binning are methods of combining coverage data\nand project/file history to determine if goals have been met and to identify\nareas of code which should be prioritized. These methods can be applied to any\ncoverage metric which can be associated with a location -- statement, function,\nexpression, toggle, etc. -- and to any language, including both software (C++,\nPython, etc.) and hardware description languages (SystemVerilog, VHDL). The\ngoal of these approaches is to reduce the cost and the barrier to entry of\nusing coverage data analysis in large-scale projects.\n  The approach is realized in gendiffcov, a recently released open-source tool.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.08859,regular,pre_llm,2020,8,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Model-based Automated Testing of Mobile Applications: An Industrial Case\n  Study\n\n  Automatic testing of mobile applications has been a well-researched area in\nrecent years. However, testing in industry is still a very manual practice, as\nresearch results have not been fully transferred and adopted. Considering\nmobile applications, manual testing has the additional burden of adequate\ntesting posed by a large number of available devices and different\nconfigurations, as well as the maintenance and setup of such devices.\n  In this paper, we propose and evaluate the use of a model-based test\ngeneration approach, where generated tests are executed on a set of\ncloud-hosted real mobile devices. By using a model-based approach we generate\ndynamic, less brittle, and implementation simple test cases. The test execution\non multiple real devices with different configurations increase the confidence\nin the implementation of the system under test. Our evaluation shows that the\nused approach produces a high coverage of the parts of the application related\nto user interactions. Nevertheless, the inclusion of external services in test\ngeneration is required in order to additionally increase the coverage of the\ncomplete application. Furthermore, we present the lessons learned while\ntransferring and implementing this approach in an industrial context and\napplying it to the real product.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.02502,regular,pre_llm,2020,8,"{'ai_likelihood': 6.291601392957899e-07, 'text': 'iMER: Iterative Process of Entity Relationship and Business Proces\n  Models Extraction from the Requirements\n\n  Extracting conceptual models, e.g., entity relationship model or Business\nProcess model, from software requirement document is an essential task in the\nsoftware development life cycle. Business process model presents a clear\npicture of required system functionality. Operations in business process model\ntogether with the data entity consumed, help the software developers to\nunderstand the database design and operations to be implemented. Researchers\nhave been aiming at automatic extraction of these artefacts from the\nrequirement document. In this paper, we present an automated approach to\nextract the entity relationship and business process models from requirements,\nwhich are possibly in different formats such as general requirements, use case\nspecification and user stories. Our approach is based on the efficient natural\nlanguage processing techniques.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.05159,regular,pre_llm,2020,8,"{'ai_likelihood': 3.6093923780653213e-06, 'text': 'Prevalence, Contents and Automatic Detection of KL-SATD\n\n  When developers use different keywords such as TODO and FIXME in source code\ncomments to describe self-admitted technical debt (SATD), we refer it as\nKeyword-Labeled SATD (KL-SATD). We study KL-SATD from 33 software repositories\nwith 13,588 KL-SATD comments. We find that the median percentage of KL-SATD\ncomments among all comments is only 1,52%. We find that KL-SATD comment\ncontents include words expressing code changes and uncertainty, such as remove,\nfix, maybe and probably. This makes them different compared to other comments.\nKL-SATD comment contents are similar to manually labeled SATD comments of prior\nwork. Our machine learning classifier using logistic Lasso regression has good\nperformance in detecting KL-SATD comments (AUC-ROC 0.88). Finally, we\ndemonstrate that using machine learning we can identify comments that are\ncurrently missing but which should have a SATD keyword in them. Automating SATD\nidentification of comments that lack SATD keywords can save time and effort by\nreplacing manual identification of comments. Using KL-SATD offers a potential\nto bootstrap a complete SATD detector.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.03731,regular,pre_llm,2020,8,"{'ai_likelihood': 8.54333241780599e-06, 'text': 'Combining Code Embedding with Static Analysis for Function-Call\n  Completion\n\n  Code completion is an important feature of integrated development\nenvironments (IDEs). It allows developers to produce code faster, especially\nnovice ones who are not fully familiar with APIs and others code. Previous\nworks on code completion have mainly exploited static type systems of\nprogramming languages or code history of the project under development or of\nother projects using common APIs. In this work, we present a novel approach for\nimproving current function-calls completion tools by learning from independent\ncode repositories, using well-known natural language processing models that can\nlearn vector representation of source code (code embeddings). Our models are\nnot trained on historical data of specific projects. Instead, our approach\nallows to learn high-level concepts and their relationships present among\nthousands of projects. As a consequence, the resulting system is able to\nprovide general suggestions that are not specific to particular projects or\nAPIs. Additionally, by taking into account the context of the call to complete,\nour approach suggests function calls relevant to that context. We evaluated our\napproach on a set of open-source projects unseen during the training. The\nresults show that the use of the trained model along with a code suggestion\nplug-in based on static type analysis improves significantly the correctness of\nthe completion suggestions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.12751,regular,pre_llm,2020,8,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'A Framework for Generating Diverse Haskell-IO Exercise Tasks\n\n  We present the design of a framework to automatically generate a large range\nof different exercise tasks on Haskell-I/O programming. Automatic task\ngeneration is useful in many different ways. Manual task creating is a time\nconsuming process, so automating it saves valuable time for the educator.\nTogether with an automated assessment system automatic task generation allows\nstudents to practice with as many exercise tasks as needed. Additionally, each\nstudent can be given a slightly different version of a task, reducing issues\nregarding plagiarism that arise naturally in an e-learning environment. Our\ntask generation is centered around a specification language for I/O behavior\nthat we developed in an earlier work. The task generation framework, an EDSL in\nHaskell, provides powerful primitives for the creation of various artifacts,\nincluding program code, from specifications. We will not go into detail on the\ntechnical realization of these primitives. This article instead showcases how\nsuch artifacts and the framework as a whole can be used to build exercise tasks\ntemplates that can then be (randomly) instantiated.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.13111,review,pre_llm,2020,8,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Role of Project Management in Virtual Teams Success\n\n  A virtual team is a group of geographically distant people who work together\nto achieve a shared goal for a common organization. From the past few years\nthis concept has been evolved and has emerged the idea of global project\nmanagement. Virtual teams have been beneficial in cost reduction; hiring\ncompetent work force and improving globalization. Although virtual teams are\nbeneficial for an organization; but they are hard to manage and control\nsuccessfully. There can be several challenges like cultural issues; different\ntime zones and communication gap. These challenges are not hard to manage; and\nwe can overcome these challenges using effective project management skills.\nThese skills will become the success factors for making virtual teams\nsuccessful and will be determined by comparison of the survey results of\ntraditional and virtual teams.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.07334,regular,pre_llm,2020,8,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Simpler Hyperparameter Optimization for Software Analytics: Why, How,\n  When?\n\n  How to make software analytics simpler and faster? One method is to match the\ncomplexity of analysis to the intrinsic complexity of the data being explored.\nFor example, hyperparameter optimizers find the control settings for data\nminers that improve for improving the predictions generated via software\nanalytics. Sometimes, very fast hyperparameter optimization can be achieved by\njust DODGE-ing away from things tried before. But when is it wise to use DODGE\nand when must we use more complex (and much slower) optimizers? To answer this,\nwe applied hyperparameter optimization to 120 SE data sets that explored bad\nsmell detection, predicting Github ssue close time, bug report analysis, defect\nprediction, and dozens of other non-SE problems. We find that DODGE works best\nfor data sets with low ""intrinsic dimensionality"" (D = 3) and very poorly for\nhigher-dimensional data (D over 8). Nearly all the SE data seen here was\nintrinsically low-dimensional, indicating that DODGE is applicable for many SE\nanalytics tasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.04093,regular,pre_llm,2020,8,"{'ai_likelihood': 8.27842288547092e-07, 'text': ""When Deep Learning Meets Smart Contracts\n\n  Ethereum has become a widely used platform to enable secure, Blockchain-based\nfinancial and business transactions. However, many identified bugs and\nvulnerabilities in smart contracts have led to serious financial losses, which\nraises serious concerns about smart contract security. Thus, there is a\nsignificant need to better maintain smart contract code and ensure its high\nreliability. In this research: (1) Firstly, we propose an automated deep\nlearning based approach to learn structural code embeddings of smart contracts\nin Solidity, which is useful for clone detection, bug detection and contract\nvalidation on smart contracts. We apply our approach to more than 22K solidity\ncontracts collected from the Ethereum blockchain, results show that the clone\nratio of solidity code is at around 90%, much higher than traditional software.\nWe collect a list of 52 known buggy smart contracts belonging to 10 kinds of\ncommon vulnerabilities as our bug database. Our approach can identify more than\n1000 clone related bugs based on our bug databases efficiently and accurately.\n(2) Secondly, according to developers' feedback, we have implemented the\napproach in a web-based tool, named SmartEmbed, to facilitate Solidity\ndevelopers for using our approach. Our tool can assist Solidity developers to\nefficiently identify repetitive smart contracts in the existing Ethereum\nblockchain, as well as checking their contract against a known set of bugs,\nwhich can help to improve the users' confidence in the reliability of the\ncontract. We optimize the implementations of SmartEmbed which is sufficient in\nsupporting developers in real-time for practical uses. The Ethereum ecosystem\nas well as the individual Solidity developer can both benefit from our\nresearch.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.02456,regular,pre_llm,2020,8,"{'ai_likelihood': 2.185503641764323e-06, 'text': 'Predicting Missing Information of Key Aspects in Vulnerability Reports\n\n  Software vulnerabilities have been continually disclosed and documented. An\nimportant practice in documenting vulnerabilities is to describe the key\nvulnerability aspects, such as vulnerability type, root cause, affected\nproduct, impact, attacker type and attack vector, for the effective search and\nmanagement of fast-growing vulnerabilities. We investigate 120,103\nvulnerability reports in the Common Vulnerabilities and Exposures (CVE) over\nthe past 20 years. We find that 56%, 85%, 38% and 28% of CVEs miss\nvulnerability type, root causes, attack vector and attacker type respectively.\nTo help to complete the missing information of these vulnerability aspects, we\npropose a neural-network based approach for predicting the missing information\nof a key aspect of a vulnerability based on the known aspects of the\nvulnerability. We explore the design space of the neural network models and\nempirically identify the most effective model design. Using a large-scale\nvulnerability datas\\-et from CVE, we show that we can effectively train a\nneural-network based classifier with less than 20% of historical CVEs. Our\nmodel achieves the prediction accuracy 94%, 79%, 89%and 70% for vulnerability\ntype, root cause, attacker type and attack vector, respectively. Our ablation\nstudy reveals the prominent correlations among vulnerability aspects and\nfurther confirms the practicality of our approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.05017,regular,pre_llm,2020,8,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Changes, States, and Events: The Thread from Staticity to Dynamism in\n  the Conceptual Modeling of Systems\n\n  This paper examines the concept of change in conceptual modeling. Change is\ninherent in the nature of things and has increasingly become a focus of much\ninterest and investigation. Change can be modeled as a transition between two\nstates of a finite state machine (FSM). This change represents an exploratory\nstarting point in this paper. Accordingly, a sample FSM that models a car s\ntransmission system is re-expressed in terms of a new modeling methodology\ncalled thinging machine (TM) modeling. Recasting the car-transmission model\ninvolves developing (1) an S model that captures the static aspects, (2) a D\nmodel that identifies states, and (3) a B model that specifies the behavior.\nThe analysis progresses as follows. - S represents an atemporal diagrammatic\ndescription that embeds underlying compositions (static changes) from which the\nroots of system behavior can be traced. - S is broken down into multiple\nsubsystems that correspond to static states (ordered constitutive components).\n- Introducing time into static states converts these states into events, and\nthe behavior (B) model is constructed based on the chronology of these events.\nThe analysis shows that FSM states are static (atemporal) changes that\nintroduce temporal events as carriers of behavior. This result enhances the\nsemantics of the concepts of change, states, and events in modeling and shows\nhow to specify a system s behavior through its static description.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.13747,regular,pre_llm,2020,9,"{'ai_likelihood': 7.847944895426433e-06, 'text': 'Dynamic Slicing for Deep Neural Networks\n\n  Program slicing has been widely applied in a variety of software engineering\ntasks. However, existing program slicing techniques only deal with traditional\nprograms that are constructed with instructions and variables, rather than\nneural networks that are composed of neurons and synapses. In this paper, we\npropose NNSlicer, the first approach for slicing deep neural networks based on\ndata flow analysis. Our method understands the reaction of each neuron to an\ninput based on the difference between its behavior activated by the input and\nthe average behavior over the whole dataset. Then we quantify the neuron\ncontributions to the slicing criterion by recursively backtracking from the\noutput neurons, and calculate the slice as the neurons and the synapses with\nlarger contributions. We demonstrate the usefulness and effectiveness of\nNNSlicer with three applications, including adversarial input detection, model\npruning, and selective model protection. In all applications, NNSlicer\nsignificantly outperforms other baselines that do not rely on data flow\nanalysis.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.07235,regular,pre_llm,2020,9,"{'ai_likelihood': 2.6490953233506944e-06, 'text': 'Deep Learning based Vulnerability Detection: Are We There Yet?\n\n  Automated detection of software vulnerabilities is a fundamental problem in\nsoftware security. Existing program analysis techniques either suffer from high\nfalse positives or false negatives. Recent progress in Deep Learning (DL) has\nresulted in a surge of interest in applying DL for automated vulnerability\ndetection. Several recent studies have demonstrated promising results achieving\nan accuracy of up to 95% at detecting vulnerabilities. In this paper, we ask,\n""how well do the state-of-the-art DL-based techniques perform in a real-world\nvulnerability prediction scenario?"". To our surprise, we find that their\nperformance drops by more than 50%. A systematic investigation of what causes\nsuch precipitous performance drop reveals that existing DL-based vulnerability\nprediction approaches suffer from challenges with the training data (e.g., data\nduplication, unrealistic distribution of vulnerable classes, etc.) and with the\nmodel choices (e.g., simple token-based models). As a result, these approaches\noften do not learn features related to the actual cause of the vulnerabilities.\nInstead, they learn unrelated artifacts from the dataset (e.g., specific\nvariable/function names, etc.). Leveraging these empirical findings, we\ndemonstrate how a more principled approach to data collection and model design,\nbased on realistic settings of vulnerability prediction, can lead to better\nsolutions. The resulting tools perform significantly better than the studied\nbaseline: up to 33.57% boost in precision and 128.38% boost in recall compared\nto the best performing model in the literature. Overall, this paper elucidates\nexisting DL-based vulnerability prediction systems\' potential issues and draws\na roadmap for future DL-based vulnerability prediction research. In that\nspirit, we make available all the artifacts supporting our results:\nhttps://git.io/Jf6IA.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.01702,review,pre_llm,2020,9,"{'ai_likelihood': 6.953875223795574e-07, 'text': ""Linking Stakeholders' Viewpoint Concerns and Microservices-based\n  Architecture\n\n  Widespread adoption of agile project management, independent delivery with\nmicroservices, and automated deployment with DevOps has tremendously speedup\nthe systems development. The real game-changer is continuous integration (CI),\ncontinuous delivery, and continuous deployment (CD). Organizations can do\nmultiple releases a day, shortening the test, release, and deployment cycles\nfrom weeks to minutes.\n  Maturity of container technologies like Docker and container orchestration\nplatforms like Kubernetes has promoted microservices architecture, especially\nin the cloud-native developments. Various tools are available for setting up\nCI/CD pipelines. Organizations are moving away from monolith applications and\nmoving towards microservices-based architectures. Organizations can quickly\naccumulate hundreds of such microservices accessible via application\nprogramming interfaces (APIs).\n  The primary purpose of these modern methodologies is agility, speed, and\nreusability. While DevOps offers speed and time to market, agility and\nreusability may not be guaranteed unless microservices and API's are linked to\nenterprise-wide stakeholders' needs. The link between business needs and\nmicroservices/APIs is not well captured nor adequately defined.\n  In this publication, we describe a structured method to create a logical link\namong APIs and microservices-based agile developments with enterprise\nstakeholders' needs and viewpoint concerns. This method enables capturing and\ndocumenting enterprise-wide stakeholders' needs, whether these are business\nowners, planners (product owners, architects), designers (developers, DevOps\nengineers), or the partners and subscribers of an enterprise.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.09331,regular,pre_llm,2020,9,"{'ai_likelihood': 6.622738308376736e-07, 'text': ""A Benchmark Study of the Contemporary Toxicity Detectors on Software\n  Engineering Interactions\n\n  Automated filtering of toxic conversations may help an Open-source software\n(OSS) community to maintain healthy interactions among the project\nparticipants. Although, several general purpose tools exist to identify toxic\ncontents, those may incorrectly flag some words commonly used in the Software\nEngineering (SE) context as toxic (e.g., 'junk', 'kill', and 'dump') and vice\nversa. To encounter this challenge, an SE specific tool has been proposed by\nthe CMU Strudel Lab (referred as the `STRUDEL' hereinafter) by combining the\noutput of the Perspective API with the output from a customized version of the\nStanford's Politeness detector tool. However, since STRUDEL's evaluation was\nvery limited with only 654 SE text, its practical applicability is unclear.\nTherefore, this study aims to empirically evaluate the Strudel tool as well as\nfour state-of-the-art general purpose toxicity detectors on a large scale SE\ndataset. On this goal, we empirically developed a rubric to manually label\ntoxic SE interactions. Using this rubric, we manually labeled a dataset of\n6,533 code review comments and 4,140 Gitter messages. The results of our\nanalyses suggest significant degradation of all tools' performances on our\ndatasets. Those degradations were significantly higher on our dataset of formal\nSE communication such as code review than on our dataset of informal\ncommunication such as Gitter messages. Two of the models from our study showed\nsignificant performance improvements during 10-fold cross validations after we\nretrained those on our SE datasets. Based on our manual investigations of the\nincorrectly classified text, we have identified several recommendations for\ndeveloping an SE specific toxicity detector.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.04115,regular,pre_llm,2020,9,"{'ai_likelihood': 5.596213870578343e-06, 'text': 'Search-based Testing for Scratch Programs\n\n  Block-based programming languages enable young learners to quickly implement\nfun programs and games. The Scratch programming environment is particularly\nsuccessful at this, with more than 50 million registered users at the time of\nthis writing. Although Scratch simplifies creating syntactically correct\nprograms, learners and educators nevertheless frequently require feedback and\nsupport. Dynamic program analysis could enable automation of this support, but\nthe test suites necessary for dynamic analysis do not usually exist for Scratch\nprograms. It is, however, possible to cast test generation for Scratch as a\nsearch problem. In this paper, we introduce an approach for automatically\ngenerating test suites for Scratch programs using grammatical evolution. The\nuse of grammatical evolution clearly separates the search encoding from\nframework-specific implementation details, and allows us to use advanced test\nacceleration techniques. We implemented our approach as an extension of the\nWhisker test framework. Evaluation on sample Scratch programs demonstrates the\npotential of the approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.1066,regular,pre_llm,2020,9,"{'ai_likelihood': 1.2252065870496963e-06, 'text': ""Measuring affective states from technical debt: A psychoempirical\n  software engineering experiment\n\n  Software engineering is a human activity. Despite this, human aspects are\nunder-represented in technical debt research, perhaps because they are\nchallenging to evaluate.\n  This study's objective was to investigate the relationship between technical\ndebt and affective states (feelings, emotions, and moods) from software\npractitioners. Forty participants (N = 40) from twelve companies took part in a\nmixed-methods approach, consisting of a repeated-measures (r = 5) experiment (n\n= 200), a survey, and semi-structured interviews.\n  The statistical analysis shows that different design smells (strong\nindicators of technical debt) negatively or positively impact affective states.\nFrom the qualitative data, it is clear that technical debt activates a\nsubstantial portion of the emotional spectrum and is psychologically taxing.\nFurther, the practitioners' reactions to technical debt appear to fall in\ndifferent levels of maturity.\n  We argue that human aspects in technical debt are important factors to\nconsider, as they may result in, e.g., procrastination, apprehension, and\nburnout.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.00981,review,pre_llm,2020,9,"{'ai_likelihood': 8.377763960096572e-06, 'text': 'Analysis of open source license selection for the GitHub programming\n  community\n\n  Developers usually select different open source licenses to restrain the\nconditions of using open source software, in order to protect intellectual\nproperty rights effectively and maintain the long-term development of the\nsoftware. However, the open source community has a wide variety of licenses\navailable, developers generally find it difficult to understand the differences\nbetween different open source license. And existing open source license\nselection tools require developers to understand the terms of the open source\nlicense and identify their business needs, which makes it hard for developers\nto make the right choice. Although academia has extensive research to the open\nsource license, but there is no systematic analysis on the actual difficulties\nof the developers to choose the open source license, thus lacking a clear\nunderstanding, for this reason, the purpose of this paper is to understand the\ndifficulties faced by open source developers in choosing open source licenses,\nanalyze the components of open source license and the affecting factors of open\nsource license selection, and to provide references for developers to choose\nopen source licenses.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.1386,regular,pre_llm,2020,9,"{'ai_likelihood': 9.834766387939453e-06, 'text': 'Automatically Tailoring Static Analysis to Custom Usage Scenarios\n\n  In recent years, there has been significant progress in the development and\nindustrial adoption of static analyzers. Such analyzers typically provide a\nlarge, if not huge, number of configurable options controlling the precision\nand performance of the analysis. A major hurdle in integrating static analyzers\nin the software-development life cycle is tuning their options to custom usage\nscenarios, such as a particular code base or certain resource constraints. In\nthis paper, we propose a technique that automatically tailors a static\nanalyzer, specifically an abstract interpreter, to the code under analysis and\nany given resource constraints. We implement this technique in a framework\ncalled TAILOR, which we use to perform an extensive evaluation on real-world\nbenchmarks. Our experiments show that the configurations generated by TAILOR\nare vastly better than the default analysis options, vary significantly\ndepending on the code under analysis, and most remain tailored to several\nsubsequent code versions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.0913,review,pre_llm,2020,9,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'How are Project-Specific Forums Utilized? A Study of Participation,\n  Content, and Sentiment in the Eclipse Ecosystem\n\n  Although many software development projects have moved their developer\ndiscussion forums to generic platforms such as Stack Overflow, Eclipse has been\nsteadfast in hosting their self-supported community forums. While recent\nstudies show forums share similarities to generic communication channels, it is\nunknown how project-specific forums are utilized. In this paper, we analyze\n832,058 forum threads and their linkages to four systems with 2,170 connected\ncontributors to understand the participation, content and sentiment. Results\nshow that Seniors are the most active participants to respond bug and\nnon-bug-related threads in the forums (i.e., 66.1% and 45.5%), and sentiment\namong developers are inconsistent while knowledge sharing within Eclipse. We\nrecommend the users to identify appropriate topics and ask in a positive\nprocedural way when joining forums. For developers, preparing project-specific\nforums could be an option to bridge the communication between members.\nIrrespective of the popularity of Stack Overflow, we argue the benefits of\nusing project-specific forum initiatives, such as GitHub Discussions, are\nneeded to cultivate a community and its ecosystem.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.1233,regular,pre_llm,2020,9,"{'ai_likelihood': 3.311369154188368e-06, 'text': 'Synthesis of Infinite-State Systems with Random Behavior\n\n  Diversity in the exhibited behavior of a given system is a desirable\ncharacteristic in a variety of application contexts. Synthesis of conformant\nimplementations often proceeds by discovering witnessing Skolem functions,\nwhich are traditionally deterministic. In this paper, we present a novel Skolem\nextraction algorithm to enable synthesis of witnesses with random behavior and\ndemonstrate its applicability in the context of reactive systems. The\nsynthesized solutions are guaranteed by design to meet the given\nspecification,while exhibiting a high degree of diversity in their responses to\nexternal stimuli. Case studies demonstrate how our proposed frame-work unveils\na novel application of synthesis in model-based fuzz testing to generate\nfuzzers of competitive performance to general-purpose alternatives, as well as\nthe practical utility of synthesized controllers in robot motion planning\nproblems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.14643,regular,pre_llm,2020,9,"{'ai_likelihood': 2.8808911641438804e-05, 'text': 'ESiWACE2 Services: RSE collaborations in Weather and Climate\n\n  We present the collaborative model of ESiWACE2 Services, where Research\nSoftware Engineers (RSEs) from the Netherlands eScience Center (NLeSC) and Atos\noffer their expertise to climate and earth system modeling groups across\nEurope. Within 6-month collaborative projects, the RSEs intend to provide\nguidance and advice regarding the performance, portability to new\narchitectures, and scalability of selected applications. We present the four\nawarded projects as examples of this funding structure.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.03678,review,pre_llm,2020,9,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'An Efficient Approach for Reviewing Security-Related Aspects in Agile\n  Requirements Specifications of Web Applications\n\n  Defects in requirements specifications can have severe consequences during\nthe software development lifecycle. Some of them may result in poor product\nquality and/or time and budget overruns due to incorrect or missing quality\ncharacteristics, such as security. This characteristic requires special\nattention in web applications because they have become a target for\nmanipulating sensible data. Several concerns make security difficult to deal\nwith. For instance, security requirements are often misunderstood and\nimproperly specified due to lack of security expertise and emphasis on security\nduring early stages of software development. This often leads to unspecified or\nill-defined security-related aspects. These concerns become even more\nchallenging in agile contexts, where lightweight documentation is typically\nproduced. To tackle this problem, we designed an approach for reviewing\nsecurity-related aspects in agile requirements specifications of web\napplications. Our proposal considers user stories and security specifications\nas inputs and relates those user stories to security properties via Natural\nLanguage Processing. Based on the related security properties, our approach\nidentifies high-level security requirements from the Open Web Application\nSecurity Project (OWASP) to be verified, and generates a reading technique to\nsupport reviewers in detecting defects. We evaluate our approach via three\nexperiment trials conducted with 56 novice software engineers, measuring\neffectiveness, efficiency, usefulness, and ease of use. We compare our approach\nagainst using: (1) the OWASP high-level security requirements, and (2) a\nperspective-based approach as proposed in contemporary state of the art. The\nresults strengthen our confidence that using our approach has a positive impact\n(with large effect size) on the performance of inspectors in terms of\neffectiveness and efficiency.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.01557,review,pre_llm,2020,9,"{'ai_likelihood': 1.2914339701334636e-06, 'text': ""Evaluation of Software Product Quality Metrics\n\n  Computing devices and associated software govern everyday life, and form the\nbackbone of safety critical systems in banking, healthcare, automotive and\nother fields. Increasing system complexity, quickly evolving technologies and\nparadigm shifts have kept software quality research at the forefront. Standards\nsuch as ISO's 25010 express it in terms of sub-characteristics such as\nmaintainability, reliability and security. A significant body of literature\nattempts to link these subcharacteristics with software metric values, with the\nend goal of creating a metric-based model of software product quality. However,\nresearch also identifies the most important existing barriers. Among them we\nmention the diversity of software application types, development platforms and\nlanguages. Additionally, unified definitions to make software metrics truly\nlanguage-agnostic do not exist, and would be difficult to implement given\nprogramming language levels of variety. This is compounded by the fact that\nmany existing studies do not detail their methodology and tooling, which\nprecludes researchers from creating surveys to enable data analysis on a larger\nscale. In our paper, we propose a comprehensive study of metric values in the\ncontext of three complex, open-source applications. We align our methodology\nand tooling with that of existing research, and present it in detail in order\nto facilitate comparative evaluation. We study metric values during the entire\n18-year development history of our target applications, in order to capture the\nlongitudinal view that we found lacking in existing literature. We identify\nmetric dependencies and check their consistency across applications and their\nversions. At each step, we carry out comparative evaluation with existing\nresearch and present our results.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.00331,regular,pre_llm,2020,9,"{'ai_likelihood': 4.172325134277344e-06, 'text': 'Fault Injection Analytics: A Novel Approach to Discover Failure Modes in\n  Cloud-Computing Systems\n\n  Cloud computing systems fail in complex and unexpected ways due to unexpected\ncombinations of events and interactions between hardware and software\ncomponents. Fault injection is an effective means to bring out these failures\nin a controlled environment. However, fault injection experiments produce\nmassive amounts of data, and manually analyzing these data is inefficient and\nerror-prone, as the analyst can miss severe failure modes that are yet unknown.\nThis paper introduces a new paradigm (fault injection analytics) that applies\nunsupervised machine learning on execution traces of the injected system, to\nease the discovery and interpretation of failure modes. We evaluated the\nproposed approach in the context of fault injection experiments on the\nOpenStack cloud computing platform, where we show that the approach can\naccurately identify failure modes with a low computational cost.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.06414,regular,pre_llm,2020,9,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Should Decorators Preserve the Component Interface?\n\n  Decorator design pattern is a well known pattern that allows dynamical\nattachment of additional functionality to an object. Decorators have been\nproposed as flexible alternative to subclassing for extending functionality.\nStill, the Decorator pattern has certain limitations, especially related to the\nfact that in its classical form it is constrained to a single interface, which\nis implicitly defined by the type of the concrete components that we intend to\ndecorate. Another problem associated to the Decorator pattern is related to the\nlinear composition of the decorations, which could lead to problems in\naccessing the newly added responsibilities. In this context, the paper presents\nvariants of the Decorator pattern: MixDecorator and D2Decorator, and a variant\nspecific only to C++ language based on templates - HybridDecorator.\nMixDecorator could be considered a new enhanced version of the Decorator\npattern that eliminates some constraints of the Decorator pattern, but also it\ncould be used as a base of a general extension mechanism. The main advantage of\nusing MixDecorator is that it allows direct access to all newly added\nresponsibilities, and so, we may combine different interface-responsibilities\n(newly added public methods) and operate with them directly and in any order,\nhiding the linear composition of the decorations. D2Decorator is a variant\nbased on a double-dispatch mechanism. The C++ metaprogramming mechanism based\non templates allows an interesting hybrid variant of the Decorator -\nHybridDecorator, which mixes on-demand defined inheritance with composition.\nUsing these variants of the Decorator pattern we are not longer limited to one\nsingle interface; the set of the messages that could be sent to an object could\nbe enlarged, and so, we may consider that using them, we can dynamically change\nthe type of objects.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.06848,regular,pre_llm,2020,9,"{'ai_likelihood': 3.344482845730252e-06, 'text': 'PRF: A Framework for Building Automatic Program Repair Prototypes for\n  JVM-Based Languages\n\n  PRF is a Java-based framework that allows researchers to build prototypes of\ntest-based generate-and-validate automatic program repair techniques for JVM\nlanguages by simply extending it with their patch generation plugins. The\nframework also provides other useful components for constructing automatic\nprogram repair tools, e.g., a fault localization component that provides\nspectrum-based fault localization information at different levels of\ngranularity, a configurable and safe patch validation component that is 11+X\nfaster than vanilla testing, and a customizable post-processing component to\ngenerate fix reports. A demo video of PRF is available at\nhttps://bit.ly/3ehduSS.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.02448,review,pre_llm,2020,9,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Are the Old Days Gone? A Survey on Actual Software Engineering Processes\n  in Video Game Industry\n\n  In the past 10 years, several researches studied video game development\nprocess who proposed approaches to improve the way how games are developed.\nThese approaches usually adopt agile methodologies because of claims that\ntraditional practices and the waterfall process are gone. However, are the ""old\ndays"" really gone in the game industry? In this paper, we present a survey of\nsoftware engineering processes in video game industry from postmortem project\nanalyses. We analyzed 20 postmortems from Gamasutra Portal. We extracted their\nprocesses and modelled them through using the Business Process Model and\nNotation (BPMN). This work presents three main contributions. First, a\npostmortem analysis methodology to identify and extract project processes.\nSecond, the study main result: \\textbf{the ""old days"" are gone, but not\ncompletely}. \\textbf{Iterative practices} are increasing and are applied to at\nleast \\textbf{65\\% of projects} in which \\textbf{45\\% of this projects}\nexplicitly adopted Agile practices. However, \\textbf{waterfall} process is\nstill applied at least \\textbf{30\\% of projects}. Finally, we discuss some\nimplications, directions and opportunities for video game development\ncommunity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.13113,regular,pre_llm,2020,9,"{'ai_likelihood': 3.046459621853299e-06, 'text': 'Automated Identification of On-hold Self-admitted Technical Debt\n\n  Modern software is developed under considerable time pressure, which implies\nthat developers more often than not have to resort to compromises when it comes\nto code that is well written and code that just does the job. This has led over\nthe past decades to the concept of ""technical debt"", a short-term hack that\npotentially generates long-term maintenance problems. Self-admitted technical\ndebt (SATD) is a particular form of technical debt: developers consciously\nperform the hack but also document it in the code by adding comments as a\nreminder (or as an admission of guilt). We focus on a specific type of SATD,\nnamely ""On-hold"" SATD, in which developers document in their comments the need\nto halt an implementation task due to conditions outside of their scope of work\n(e.g., an open issue must be closed before a function can be implemented). We\npresent an approach, based on regular expressions and machine learning, which\nis able to detect issues referenced in code comments, and to automatically\nclassify the detected instances as either ""On-hold"" (the issue is referenced to\nindicate the need to wait for its resolution before completing a task), or as\n""cross-reference"", (the issue is referenced to document the code, for example\nto explain the rationale behind an implementation choice). Our approach also\nmines the issue tracker of the projects to check if the On-hold SATD instances\nare ""superfluous"" and can be removed (i.e., the referenced issue has been\nclosed, but the SATD is still in the code). Our evaluation confirms that our\napproach can indeed identify relevant instances of On-hold SATD. We illustrate\nits usefulness by identifying superfluous On-hold SATD instances in open source\nprojects as confirmed by the original developers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.02065,regular,pre_llm,2020,9,"{'ai_likelihood': 1.8543667263454863e-06, 'text': 'Domain Priori Knowledge based Integrated Solution Design for Internet of\n  Services\n\n  Various types of services, such as web APIs, IoT services, O2O services, and\nmany others, have flooded on the Internet. Interconnections among these\nservices have resulted in a new phenomenon called ""Internet of Services"" (IoS).\nBy IoS,people don\'t need to request multiple services by themselves to fulfill\ntheir daily requirements, but it is an IoS platform that is responsible for\nconstructing integrated solutions for them. Since user requirements (URs) are\nusually coarse-grained and transboundary, IoS platforms have to integrate\nservices from multiple domains to fulfill the requirements. Considering there\nare too many available services in IoS, a big challenge is how to look for a\ntradeoff between the construction efficiency and the precision of final\nsolutions. For this challenge, we introduce a framework and a platform for\ntransboundary user requirement oriented solution design in IoS. The main idea\nis to make use of domain priori knowledge derived from the commonness and\nsimilarities among massive historical URs and among historical integrated\nservice solutions(ISSs). Priori knowledge is classified into three types:\nrequirement patterns (RPs), service patterns (SPs), and probabilistic matching\nmatrix (PMM) between RPs and SPs. A UR is modeled in the form of an intention\ntree (ITree) along with a set of constraints on intention nodes, and then\noptimal RPs are selected to cover the I-Tree as much as possible. By taking\nadvantage of the PMM, a set of SPs are filtered out and composed together to\nform the final ISS. Finally, the design of a platform supporting the above\nprocess is introduced.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.11229,regular,pre_llm,2020,9,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Introducing Aspect-Oriented Programming in Improving the Modularity of\n  Middleware for Internet of Things\n\n  Internet of Things (IoT) has become the buzzword for the development of Smart\nCity and its applications. In this context, development of supporting software\nforms the core part of the IoT infrastructure. A Middleware sits in between the\nIoT devices and interacts between them to exchange data among the components of\nthe automated architecture. The Middleware services include hand shaking, data\ntransfer and security among its core set of functionalities. It also includes\ncross-cutting functional services such as authentication, logging and caching.\nA software that can run these Middleware services requires a careful choice of\na good software modelling technique. Aspect-Oriented Programming (AOP) is a\nsoftware development methodology that can be used to independently encapsulate\nthe core and cross-cutting functionalities of the Middleware services of the\nIoT infrastructure. In this paper, an attempt has been made using a simulation\nenvironment to independently model the two orthogonal functionalities of the\nMiddleware with the focus to improve its modularity. Further, a quantitative\nmeasurement of the core design property of cohesion has been done to infer on\nthe improvement in the reusability of the modules encapsulated in the\nMiddleware of IoT. Based on the measurement, it was found that the modularity\nand reusability of functionalities in the Middleware software has improved in\nthe AspectJ version compared to its equivalent Java version.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.01924,regular,pre_llm,2020,10,"{'ai_likelihood': 1.0927518208821615e-05, 'text': 'On the Relevance of Cross-project Learning with Nearest Neighbours for\n  Commit Message Generation\n\n  Commit messages play an important role in software maintenance and evolution.\nNonetheless, developers often do not produce high-quality messages. A number of\ncommit message generation methods have been proposed in recent years to address\nthis problem. Some of these methods are based on neural machine translation\n(NMT) techniques. Studies show that the nearest neighbor algorithm (NNGen)\noutperforms existing NMT-based methods, although NNGen is simpler and faster\nthan NMT. In this paper, we show that NNGen does not take advantage of\ncross-project learning in the majority of the cases. We also show that there is\nan even simpler and faster variation of the existing NNGen method which\noutperforms it in terms of the BLEU_4 score without using cross-project\nlearning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.04476,regular,pre_llm,2020,10,"{'ai_likelihood': 2.086162567138672e-06, 'text': 'Modular Collaborative Program Analysis in OPAL\n\n  Current approaches combining multiple static analyses deriving different,\nindependent properties focus either on modularity or performance. Whereas\ndeclarative approaches facilitate modularity and automated,\nanalysis-independent optimizations, imperative approaches foster manual,\nanalysis-specific optimizations.\n  In this paper, we present a novel approach to static analyses that leverages\nthe modularity of blackboard systems and combines declarative and imperative\ntechniques. Our approach allows exchangeability, and pluggable extension of\nanalyses in order to improve sound(i)ness, precision, and scalability and\nexplicitly enables the combination of otherwise incompatible analyses. With our\napproach integrated in the OPAL framework, we were able to implement various\ndissimilar analyses, including a points-to analysis that outperforms an\nequivalent analysis from Doop, the state-of-the-art points-to analysis\nframework.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.03165,review,pre_llm,2020,10,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'Questions for Data Scientists in Software Engineering: A Replication\n\n  In 2014, a Microsoft study investigated the sort of questions that data\nscience applied to software engineering should answer. This resulted in 145\nquestions that developers considered relevant for data scientists to answer,\nthus providing a research agenda to the community. Fast forward to five years,\nno further studies investigated whether the questions from the software\nengineers at Microsoft hold for other software companies, including\nsoftware-intensive companies with different primary focus (to which we refer as\nsoftware-defined enterprises). Furthermore, it is not evident that the problems\nidentified five years ago are still applicable, given the technological\nadvances in software engineering.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.05584,regular,pre_llm,2020,10,"{'ai_likelihood': 1.4901161193847656e-06, 'text': 'Data Loss Detector: Automatically Revealing Data Loss Bugs in Android\n  Apps\n\n  Android apps must work correctly even if their execution is interrupted by\nexternal events. For instance, an app must work properly even if a phone call\nis received, or after its layout is redrawn because the smartphone has been\nrotated. Since these events may require destroying, when the execution is\ninterrupted, and recreating, when the execution is resumed, the foreground\nactivity of the app, the only way to prevent the loss of state information is\nsaving and restoring it. This behavior must be explicitly implemented by app\ndevelopers, who often miss to implement it properly, releasing apps affected by\ndata loss problems, that is, apps that may lose state information when their\nexecution is interrupted. Although several techniques can be used to\nautomatically generate test cases for Android apps, the obtained test cases\nseldom include the interactions and the checks necessary to exercise and reveal\ndata loss faults. To address this problem, this paper presents Data Loss\nDetector (DLD), a test case generation technique that integrates an exploration\nstrategy, data-loss-revealing actions, and two customized oracle strategies for\nthe detection of data loss failures. DLD has been able to reveal 75% of the\nfaults in a benchmark of 54 Android app releases affected by 110 known data\nloss faults. DLD also revealed unknown data loss problems, outperforming\ncompeting approaches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.09583,regular,pre_llm,2020,10,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Warrior1: A Performance Sanitizer for C++\n\n  This paper presents Warrior1, a tool that detects performance anti-patterns\nin C++ libraries. Many programs are slowed down by many small inefficiencies.\nLarge-scale C++ applications are large, complex, and developed by large groups\nof engineers over a long period of time, which makes the task of identifying\ninefficiencies difficult. Warrior1 was designed to detect the numerous small\nperformance issues that are the result of inefficient use of C++ libraries. The\ntool detects performance anti-patterns such as map double-lookup, vector\nreallocation, short lived objects, and lambda object capture by value. Warrior1\nis implemented as an instrumented C++ standard library and an off-line\ndiagnostics tool. The tool is very effective in detecting issues. We\ndemonstrate that the tool is able to find a wide range of performance\nanti-patterns in a number of popular performance sensitive open source\nprojects.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.13464,regular,pre_llm,2020,10,"{'ai_likelihood': 5.960464477539062e-07, 'text': ""What It Would Take to Use Mutation Testing in Industry--A Study at\n  Facebook\n\n  Traditionally, mutation testing generates an abundance of small deviations of\na program, called mutants. At industrial systems the scale and size of\nFacebook's, doing this is infeasible. We should not create mutants that the\ntest suite would likely fail on or that give no actionable signal to\ndevelopers. To tackle this problem, in this paper, we semi-automatically learn\nerror-inducing patterns from a corpus of common Java coding errors and from\nchanges that caused operational anomalies at Facebook specifically. We combine\nthe mutations with instrumentation that measures which tests exactly visited\nthe mutated piece of code. Results on more than 15,000 generated mutants show\nthat more than half of the generated mutants survive Facebook's rigorous test\nsuite of unit, integration, and system tests. Moreover, in a case study with 26\ndevelopers, all but two found information of automatically detected test holes\ninteresting in principle. As such, almost half of the 26 would actually act on\nthe mutant presented to them by adapting an existing or creating a new test.\nThe others did not for a variety of reasons often outside the scope of mutation\ntesting. It remains a practical challenge how we can include such external\ninformation to increase the true actionability rate on mutants.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.00964,regular,pre_llm,2020,10,"{'ai_likelihood': 7.119443681504991e-06, 'text': 'Augmenting Machine Learning with Information Retrieval to Recommend Real\n  Cloned Code Methods for Code Completion\n\n  Software developers frequently reuse source code from repositories as it\nsaves development time and effort. Code clones accumulated in these\nrepositories hence represent often repeated functionalities and are candidates\nfor reuse in an exploratory or rapid development. In previous work, we\nintroduced DeepClone, a deep neural network model trained by fine tuning GPT-2\nmodel over the BigCloneBench dataset to predict code clone methods. The\nprobabilistic nature of DeepClone output generation can lead to syntax and\nlogic errors that requires manual editing of the output for final reuse. In\nthis paper, we propose a novel approach of applying an information retrieval\n(IR) technique on top of DeepClone output to recommend real clone methods\nclosely matching the predicted output. We have quantitatively evaluated our\nstrategy, showing that the proposed approach significantly improves the quality\nof recommendation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.04759,regular,pre_llm,2020,10,"{'ai_likelihood': 7.5499216715494794e-06, 'text': 'A Generic Approach to Detect Design Patterns in Model Transformations\n  Using a String-Matching Algorithm\n\n  Maintaining software artifacts is among the hardest tasks an engineer faces.\nLike any other piece of code, model transformations developed by engineers are\nalso subject to maintenance. To facilitate the comprehension of programs,\nsoftware engineers rely on many techniques, such as design pattern detection.\nTherefore, detecting design patterns in model transformation implementations is\nof tremendous value for developers. In this paper, we propose a generic\ntechnique to detect design patterns and their variations in model\ntransformation implementations automatically. It takes as input a set of model\ntransformation rules and the participants of a model transformation design\npattern to find occurrences of the latter in the former. The technique also\ndetects certain kinds of degenerate forms of the pattern, thus indicating\npotential opportunities to improve the model transformation implementation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.01544,review,pre_llm,2020,10,"{'ai_likelihood': 6.02669186062283e-06, 'text': ""Review4Repair: Code Review Aided Automatic Program Repairing\n\n  Context: Learning-based automatic program repair techniques are showing\npromise to provide quality fix suggestions for detected bugs in the source code\nof the software. These tools mostly exploit historical data of buggy and fixed\ncode changes and are heavily dependent on bug localizers while applying to a\nnew piece of code. With the increasing popularity of code review, dependency on\nbug localizers can be reduced. Besides, the code review-based bug localization\nis more trustworthy since reviewers' expertise and experience are reflected in\nthese suggestions.\n  Objective: The natural language instructions scripted on the review comments\nare enormous sources of information about the bug's nature and expected\nsolutions. However, none of the learning-based tools has utilized the review\ncomments to fix programming bugs to the best of our knowledge. In this study,\nwe investigate the performance improvement of repair techniques using code\nreview comments.\n  Method: We train a sequence-to-sequence model on 55,060 code reviews and\nassociated code changes. We also introduce new tokenization and preprocessing\napproaches that help to achieve significant improvement over state-of-the-art\nlearning-based repair techniques.\n  Results: We boost the top-1 accuracy by 20.33% and top-10 accuracy by 34.82%.\nWe could provide a suggestion for stylistics and non-code errors unaddressed by\nprior techniques.\n  Conclusion: We believe that the automatic fix suggestions along with code\nreview generated by our approach would help developers address the review\ncomment quickly and correctly and thus save their time and effort.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.11611,regular,pre_llm,2020,10,"{'ai_likelihood': 5.298190646701389e-07, 'text': 'A Simple Methodology for Model-Driven Business Innovation and Low Code\n  Implementation\n\n  Low Code platforms, according to Gartner Group, represent one of the more\ndisruptive technologies in the development and maintenance of enterprise\napplications. The key factor is represented by the central involvement of\nbusiness people and domain expert, with a substantial disintermediation with\nrespect to technical people. In this paper we propose a methodology conceived\nto support non-technical people in addressing business process innovation and\ndeveloping enterprise software application. The proposed methodology, called\nEasInnova, is solidly rooted in Model-Driven Engineering and adopts a three\nstaged model of an innovation undertaking. The three stages are: AsIs that\nmodels the existing business scenario; Transformation that consists in the\nelaboration of the actual innovation; ToBe that concerns the modeling of new\nbusiness scenario. The core of EasInnova is represented by a matrix where\ncolumns are the three innovation stages and the rows are the three Model-Driven\nArchitecture layers: CIM, PIM, PSM. The cells indicate the steps to be followed\nin achieving the sought innovation. Finally, the produced models will be\ntransferred onto a BonitaSoft, the Low Code platform selected in our work. The\nmethodology is described by means of a simple example in the domain of home\nfood delivery.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.12939,review,pre_llm,2020,10,"{'ai_likelihood': 6.854534149169922e-06, 'text': 'Satisfying Increasing Performance Requirements with Caching at the\n  Application Level\n\n  Application-level caching is a form of caching that has been increasingly\nadopted to satisfy performance and throughput requirements. The key idea is to\nstore the results of a computation, to improve performance by reusing instead\nof recomputing those results. However, despite its provided gains, this form of\ncaching imposes new design, implementation and maintenance challenges. In this\narticle, we provide an overview of application-level caching, highlighting its\nbenefits as well as the challenges and the issues to adopt it. We introduce\nthree kinds of existing support that have been proposed, giving a broad view of\nresearch in the area. Finally, we present important open challenges that remain\nunaddressed, hoping to inspire future work on addressing them.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.06301,regular,pre_llm,2020,10,"{'ai_likelihood': 1.457002427842882e-06, 'text': 'Automating App Review Response Generation Based on Contextual Knowledge\n\n  User experience of mobile apps is an essential ingredient that can influence\nthe audience volumes and app revenue. To ensure good user experience and assist\napp development, several prior studies resort to analysis of app reviews, a\ntype of app repository that directly reflects user opinions about the apps.\nAccurately responding to the app reviews is one of the ways to relieve user\nconcerns and thus improve user experience. However, the response quality of the\nexisting method relies on the pre-extracted features from other tools,\nincluding manually-labelled keywords and predicted review sentiment, which may\nhinder the generalizability and flexibility of the method. In this paper, we\npropose a novel end-to-end neural network approach, named CoRe, with the\ncontextual knowledge naturally incorporated and without involving external\ntools. Specifically, CoRe integrates two types of contextual knowledge in the\ntraining corpus, including official app descriptions from app store and\nresponses of the retrieved semantically similar reviews, for enhancing the\nrelevance and accuracy of the generated review responses. Experiments on\npractical review data show that CoRe can outperform the state-of-the-art method\nby 11.53% in terms of BLEU-4, an accuracy metric that is widely used to\nevaluate text generation systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.15692,regular,pre_llm,2020,10,"{'ai_likelihood': 6.622738308376736e-08, 'text': ""Unveiling process insights from refactoring practices\n\n  Context : Software comprehension and maintenance activities, such as\nrefactoring, are said to be negatively impacted by software complexity. The\nmethods used to measure software product and processes complexity have been\nthoroughly debated in the literature. However, the discernment about the\npossible links between these two dimensions, particularly on the benefits of\nusing the process perspective, has a long journey ahead. Objective: To improve\nthe understanding of the liaison of developers' activities and software\ncomplexity within a refactoring task, namely by evaluating if process metrics\ngathered from the IDE, using process mining methods and tools, are suitable to\naccurately classify different refactoring practices and the resulting software\ncomplexity. Method: We mined source code metrics from a software product after\na quality improvement task was given in parallel to (117) software developers,\norganized in (71) teams. Simultaneously, we collected events from their IDE\nwork sessions (320) and used process mining to model their processes and\nextract the correspondent metrics. Results: Most teams using a plugin for\nrefactoring (JDeodorant) reduced software complexity more effectively and with\nsimpler processes than the ones that performed refactoring using only Eclipse\nnative features. We were able to find moderate correlations (43%) between\nsoftware cyclomatic complexity and process cyclomatic complexity. The best\nmodels found for the refactoring method and cyclomatic complexity level\npredictions, had an accuracy of 92.95% and 94.36%, respectively. Conclusions:\nOur approach agnostic to programming languages, geographic location, or\ndevelopment practices. Initial findings are encouraging, and lead us to suggest\npractitioners may use our method in other development tasks, such as, defect\nanalysis and unit or integration tests.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.00965,review,pre_llm,2020,10,"{'ai_likelihood': 1.7881393432617188e-06, 'text': 'Using Vision Videos in a Virtual Focus Group: Experiences and\n  Recommendations\n\n  Facilitated meetings are an established practice for the requirements\nengineering activities elicitation and validation. Focus groups are one\nwell-known technique to implement this practice. Several researchers already\nreported the successful use of vision videos to stimulate active discussions\namong the participants of on-site focus groups, e.g., for validating scenarios\nand eliciting feedback. These vision videos show scenarios of a system vision.\nIn this way, the videos serve all parties involved as a visual reference point\nto actively disclose, discuss, and align their mental models of the future\nsystem to achieve shared understanding. In the joint project TrUSD, we had\nplanned to conduct such an on-site focus group using a vision video to validate\na scenario of a future software tool, the so-called Privacy Dashboard. However,\nthe COVID-19 pandemic and its associated measures led to an increase in home\nand remote working, which also affected us. Therefore, we had to replan and\nconduct the focus group virtually. In this paper, we report about our\nexperiences and recommendations for the use of vision videos in virtual focus\ngroups.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.00242,regular,pre_llm,2020,10,"{'ai_likelihood': 1.4901161193847656e-06, 'text': ""A Qualitative Study of Application-level Caching\n\n  Latency and cost of Internet-based services are encouraging the use of\napplication-level caching to continue satisfying users' demands, and improve\nthe scalability and availability of origin servers. Despite its popularity,\nthis level of caching involves the manual implementation by developers and is\ntypically addressed in an ad-hoc way, given that it depends on specific details\nof the application. As a result, application-level caching is a time-consuming\nand error-prone task, becoming a common source of bugs. Furthermore, it forces\napplication developers to reason about a crosscutting concern, which is\nunrelated to the application business logic. In this paper, we present the\nresults of a qualitative study of how developers handle caching logic in their\nweb applications, which involved the investigation of ten software projects\nwith different characteristics. The study we designed is based on comparative\nand interactive principles of grounded theory, and the analysis of our data\nallowed us to extract and understand how developers address cache-related\nconcerns to improve performance and scalability of their web applications.\nBased on our analysis, we derived guidelines and patterns, which guide\ndevelopers while designing, implementing and maintaining application-level\ncaching, thus supporting developers in this challenging task that is crucial\nfor enterprise web applications.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.15545,review,pre_llm,2020,10,"{'ai_likelihood': 7.020102606879341e-06, 'text': 'Systematic literature review protocol Identification and classification\n  of feature modeling errors\n\n  Context: The importance of feature modeling languages for software product\nlines and the planning stage for a systematic literature review. Objective: A\nprotocol for carrying out a systematic literature review about the evidence for\nidentifying and classifying the errors in feature modeling languages. Method:\nThe definition of a protocol to conduct a systematic literature review\naccording to the guidelines of B. Kitchenham. Results: A validated protocol to\nconduct a systematic literature review. Conclusions: A proposal for the\nprotocol definition of a systematic literature review about the identification\nand classification of errors in feature modeling was built. Initial results\nshow that the effects and results for solving these errors should be carried\nout.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.15978,regular,pre_llm,2020,10,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Examining the Relationship of Code and Architectural Smells with\n  Software Vulnerabilities\n\n  Context: Security is vital to software developed for commercial or personal\nuse. Although more organizations are realizing the importance of applying\nsecure coding practices, in many of them, security concerns are not known or\naddressed until a security failure occurs. The root cause of security failures\nis vulnerable code. While metrics have been used to predict software\nvulnerabilities, we explore the relationship between code and architectural\nsmells with security weaknesses. As smells are surface indicators of a deeper\nproblem in software, determining the relationship between smells and software\nvulnerabilities can play a significant role in vulnerability prediction models.\nObjective: This study explores the relationship between smells and software\nvulnerabilities to identify the smells. Method: We extracted the class, method,\nfile, and package level smells for three systems: Apache Tomcat, Apache CXF,\nand Android. We then compared their occurrences in the vulnerable classes which\nwere reported to contain vulnerable code and in the neutral classes\n(non-vulnerable classes where no vulnerability had yet been reported). Results:\nWe found that a vulnerable class is more likely to have certain smells compared\nto a non-vulnerable class. God Class, Complex Class, Large Class, Data Class,\nFeature Envy, Brain Class have a statistically significant relationship with\nsoftware vulnerabilities. We found no significant relationship between\narchitectural smells and software vulnerabilities. Conclusion: We can conclude\nthat for all the systems examined, there is a statistically significant\ncorrelation between software vulnerabilities and some smells.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.02509,regular,pre_llm,2020,10,"{'ai_likelihood': 3.14580069647895e-05, 'text': 'Finding Ethereum Smart Contracts Security Issues by Comparing History\n  Versions\n\n  Smart contracts are Turing-complete programs running on the blockchain. They\ncannot be modified, even when bugs are detected. The Selfdestruct function is\nthe only way to destroy a contract on the blockchain system and transfer all\nthe Ethers on the contract balance. Thus, many developers use this function to\ndestroy a contract and redeploy a new one when bugs are detected. In this\npaper, we propose a deep learning-based method to find security issues of\nEthereum smart contracts by finding the updated version of a destructed\ncontract. After finding the updated versions, we use open card sorting to find\nsecurity issues.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.09974,regular,pre_llm,2020,10,"{'ai_likelihood': 1.3907750447591147e-06, 'text': ""Scalable Statistical Root Cause Analysis on App Telemetry\n\n  Despite engineering workflows that aim to prevent buggy code from being\ndeployed, bugs still make their way into the Facebook app. When symptoms of\nthese bugs, such as user submitted reports and automatically captured crashes,\nare reported, finding their root causes is an important step in resolving them.\nHowever, at Facebook's scale of billions of users, a single bug can manifest as\nseveral different symptoms according to the various user and execution\nenvironments in which the software is deployed. Root cause analysis (RCA)\ntherefore requires tedious manual investigation and domain expertise to extract\nout common patterns that are observed in groups of reports and use them for\ndebugging.\n  We propose Minesweeper, a technique for RCA that moves towards automatically\nidentifying the root cause of bugs from their symptoms. The method is based on\ntwo key aspects: (i) a scalable algorithm to efficiently mine patterns from\ntelemetric information that is collected along with the reports, and (ii)\nstatistical notions of precision and recall of patterns that help point towards\nroot causes. We evaluate Minesweeper's scalability and effectiveness in finding\nroot causes from symptoms on real world bug and crash reports from Facebook's\napps. Our evaluation demonstrates that Minesweeper can perform RCA for tens of\nthousands of reports in less than 3 minutes, and is more than 85% accurate in\nidentifying the root cause of regressions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.12218,regular,pre_llm,2020,10,"{'ai_likelihood': 3.6093923780653213e-06, 'text': 'When the Open Source Community Meets COVID-19: Characterizing COVID-19\n  themed GitHub Repositories\n\n  Ever since the beginning of the outbreak of the COVID-19 pandemic,\nresearchers from interdisciplinary domains have worked together to fight\nagainst the crisis. The open source community, plays a vital role in coping\nwith the pandemic which is inherently a collaborative process. Plenty of\nCOVID-19 related datasets, tools, software, deep learning models, are created\nand shared in research communities with great efforts. However, COVID-19 themed\nopen source projects have not been systematically studied, and we are still\nunaware how the open source community helps combat COVID-19 in practice. To\nfill this void, in this paper, we take the first step to study COVID-19 themed\nrepositories in GitHub, one of the most popular collaborative platforms. We\nhave collected over 67K COVID-19 themed GitHub repositories till July 2020. We\nthen characterize them from a number of aspects and classify them into six\ncategories. We further investigate the contribution patterns of the\ncontributors, and development and maintenance patterns of the repositories.\nThis study sheds light on the promising direction of adopting open source\ntechnologies and resources to rapidly tackle the worldwide public health\nemergency in practice, and reveals existing challenges for improvement.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.11942,regular,pre_llm,2020,11,"{'ai_likelihood': 1.3775295681423611e-05, 'text': 'A Family of Experiments on Test-Driven Development\n\n  Context: Test-driven development (TDD) is an agile software development\napproach that has been widely claimed to improve software quality. However, the\nextent to which TDD improves quality appears to be largely dependent upon the\ncharacteristics of the study in which it is evaluated (e.g., the research\nmethod, participant type, programming environment, etc.). The particularities\nof each study make the aggregation of results untenable. Objectives: The goal\nof this paper is to: increase the accuracy and generalizability of the results\nachieved in isolated experiments on TDD, provide joint conclusions on the\nperformance of TDD across different industrial and academic settings, and\nassess the extent to which the characteristics of the experiments affect the\nquality-related performance of TDD. Method: We conduct a family of 12\nexperiments on TDD in academia and industry. We aggregate their results by\nmeans of meta-analysis. We perform exploratory analyses to identify variables\nimpacting the quality-related performance of TDD. Results: TDD novices achieve\na slightly higher code quality with iterative test-last development (i.e., ITL,\nthe reverse approach of TDD) than with TDD. The task being developed largely\ndetermines quality. The programming environment, the order in which TDD and ITL\nare applied, or the learning effects from one development approach to another\ndo not appear to affect quality. The quality-related performance of\nprofessionals using TDD drops more than for students. We hypothesize that this\nmay be due to their being more resistant to change and potentially less\nmotivated than students. Conclusion: Previous studies seem to provide\nconflicting results on TDD performance (i.e., positive vs. negative,\nrespectively). We hypothesize that these conflicting results may be due to\ndifferent study durations, experiment participants being unfamiliar with the\nTDD process...\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.05382,regular,pre_llm,2020,11,"{'ai_likelihood': 3.907415601942274e-06, 'text': 'Wayback Machine: A tool to capture the evolutionary behaviour of the bug\n  reports and their triage process in open-source software systems\n\n  The issue tracking system (ITS) is a rich data source for data-driven\ndecision-making. Different characteristics of bugs, such as severity, priority,\nand time to fix, provide a clear picture of an ITS. Nevertheless, such\ninformation may be misleading. For example, the exact time and the effort spent\non a bug might be significantly different from the actual reporting time and\nthe fixing time. Similarly, these values may be subjective, e.g., severity and\npriority values are assigned based on the intuition of a user or a developer\nrather than a structured and well-defined procedure. Hence, we explore the\nevolution of the bug dependency graph together with priority and severity\nlevels to explore the actual triage process. Inspired by the idea of the\n""Wayback Machine"" for the World Wide Web, we aim to reconstruct the historical\ndecisions made in the ITS. Therefore, any bug prioritization or bug triage\nalgorithms/scenarios can be applied in the same environment using our proposed\nITS Wayback Machine. More importantly, we track the evolutionary metrics in the\nITS when a custom triage/prioritization strategy is employed. We test the\nefficiency of the proposed algorithm using data extracted from three\nopen-source projects. Our empirical study sheds light on the overlooked\nevolutionary metrics--e.g., overdue bugs and developers\' loads--which are\nfacilitated via our proposed past-event re-generator.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.05798,regular,pre_llm,2020,11,"{'ai_likelihood': 2.317958407931858e-06, 'text': 'Software Framework for Testing of Automated Driving Systems in a Dynamic\n  Traffic Environment\n\n  Virtual testing of automated driving systems (ADS) has become an essential\npart of testing procedures for all automation levels. As ADS from automation\nlevel 3 and up are very complex, virtual testing for such systems is\ninevitable. The complexity of these levels lies in the modelling and\ncalculation demand for the virtual environment which consists of roads,\ntraffic, static and dynamic objects as well as the modelling of the car itself.\nFor safety and performance analyses of ADS, the most important part is the\nmodelling and consideration of road traffic participants. There is multiple\ntraffic flow simulation software (TFSS) which are used to reproduce realistic\ntraffic behavior and are integrated directly or over interfaces with vehicle\nsimulation software (VSS). For these software environments, the possibility to\nmanipulate traffic participants in a defined manner e.g. in the vicinity of the\nvehicle under test or implementing defined driver models for traffic vehicles\nis beneficial. In this paper, we present a software framework based on the\nexternal driver model interface provided by Vissim. This framework makes it\npossible to easily manipulate traffic participants for testing purposes of ADS.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.10268,regular,pre_llm,2020,11,"{'ai_likelihood': 2.0199351840549047e-06, 'text': 'Hyperparameter Optimization for AST Differencing\n\n  Computing the differences between two versions of the same program is an\nessential task for software development and software evolution research. AST\ndifferencing is the most advanced way of doing so, and an active research area.\nYet, AST differencing algorithms rely on configuration parameters that may have\na strong impact on their effectiveness. In this paper, we present a novel\napproach named DAT (Diff Auto Tuning) for hyperparameter optimization of AST\ndifferencing. We thoroughly state the problem of hyper-configuration for AST\ndifferencing. We evaluate our data-driven approach DAT to optimize the\nedit-scripts generated by the state-of-the-art AST differencing algorithm named\nGumTree in different scenarios. DAT is able to find a new configuration for\nGumTree that improves the edit-scripts in 21.8% of the evaluated cases.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.05531,regular,pre_llm,2020,11,"{'ai_likelihood': 2.5497542487250434e-06, 'text': 'Leveraging the Defects Life Cycle to Label Affected Versions and\n  Defective Classes\n\n  Two recent studies explicitly recommend labeling defective classes in\nreleases using the affected versions (AV) available in issue trackers. The aim\nour study is threefold: 1) to measure the proportion of defects for which the\nrealistic method is usable, 2) to propose a method for retrieving the AVs of a\ndefect, thus making the realistic approach usable when AVs are unavailable, 3)\nto compare the accuracy of the proposed method versus three SZZ\nimplementations. The assumption of our proposed method is that defects have a\nstable life cycle in terms of the proportion of the number of versions affected\nby the defects before discovering and fixing these defects. Results related to\n212 open-source projects from the Apache ecosystem, featuring a total of about\n125,000 defects, reveal that the realistic method cannot be used in the\nmajority (51%) of defects. Therefore, it is important to develop automated\nmethods to retrieve AVs. Results related to 76 open-source projects from the\nApache ecosystem, featuring a total of about 6,250,000 classes, affected by\n60,000 defects, and spread over 4,000 versions and 760,000 commits, reveal that\nthe proportion of the number of versions between defect discovery and fix is\npretty stable (STDV < 2) across the defects of the same project. Moreover, the\nproposed method resulted significantly more accurate than all three SZZ\nimplementations in (i) retrieving AVs, (ii) labeling classes as defective, and\n(iii) in developing defects repositories to perform feature selection. Thus,\nwhen the realistic method is unusable, the proposed method is a valid automated\nalternative to SZZ for retrieving the origin of a defect. Finally, given the\nlow accuracy of SZZ, researchers should consider re-executing the studies that\nhave used SZZ as an oracle and, in general, should prefer selecting projects\nwith a high proportion of available and consistent AVs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.02861,regular,pre_llm,2020,11,"{'ai_likelihood': 1.1920928955078125e-06, 'text': 'Comparing the Results of Replications in Software Engineering\n\n  Context: It has been argued that software engineering replications are useful\nfor verifying the results of previous experiments. However, it has not yet been\nagreed how to check whether the results hold across replications. Besides, some\nauthors suggest that replications that do not verify the results of previous\nexperiments can be used to identify contextual variables causing the\ndiscrepancies. Objective: Study how to assess the (dis)similarity of the\nresults of SE replications when they are compared to verify the results of\nprevious experiments and understand how to identify whether contextual\nvariables are influencing results. Method: We run simulations to learn how\ndifferent ways of comparing replication results behave when verifying the\nresults of previous experiments. We illustrate how to deal with context-induced\nchanges. To do this, we analyze three groups of replications from our own\nresearch on test-driven development and testing techniques. Results: The direct\ncomparison of p-values and effect sizes does not appear to be suitable for\nverifying the results of previous experiments and examining the variables\npossibly affecting the results in software engineering. Analytical methods such\nas meta-analysis should be used to assess the similarity of software\nengineering replication results and identify discrepancies in results.\nConclusion: The results achieved in baseline experiments should no longer be\nregarded as a result that needs to be reproduced, but as a small piece of\nevidence within a larger picture that only emerges after assembling many small\npieces to complete the puzzle.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.0834,regular,pre_llm,2020,11,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Better Automatic Program Repair by Using Bug Reports and Tests Together\n\n  Automated program repair is already deployed in industry, but concerns remain\nabout repair quality. Recent research has shown that one of the main reasons\nrepair tools produce incorrect (but seemingly correct) patches is imperfect\nfault localization (FL). This paper demonstrates that combining information\nfrom natural-language bug reports and test executions when localizing faults\ncan have a significant positive impact on repair quality. For example, existing\nrepair tools with such FL are able to correctly repair 7 defects in the\nDefects4J benchmark that no prior tools have repaired correctly. We develop,\nBlues, the first information-retrieval-based, statement-level FL technique that\nrequires no training data. We further develop RAFL, the first unsupervised\nmethod for combining multiple FL techniques, which outperforms a supervised\nmethod. Using RAFL, we create SBIR by combining Blues with a spectrum-based\n(SBFL) technique. Evaluated on 815 real-world defects, SBIR consistently ranks\nbuggy statements higher than its underlying techniques. We then modify three\nstate-of-the-art repair tools, Arja, SequenceR, and SimFix, to use SBIR, SBFL,\nand Blues as their internal FL. We evaluate the quality of the produced patches\non 689 real-world defects. Arja and SequenceR significantly benefit from SBIR:\nArja using SBIR correctly repairs 28 defects, but only 21 using SBFL, and only\n15 using Blues; SequenceR using SBIR correctly repairs 12 defects, but only 10\nusing SBFL, and only 4 using Blues. SimFix, (which has internal mechanisms to\novercome poor FL), correctly repairs 30 defects using SBIR and SBFL, but only\n13 using Blues. Our work is the first investigation of simultaneously using\nmultiple software artifacts for automated program repair, and our promising\nfindings suggest future research in this directions is likely to be fruitful.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.01834,regular,pre_llm,2020,11,"{'ai_likelihood': 1.4901161193847656e-06, 'text': 'Reinforcement Learning for Test Case Prioritization\n\n  Continuous Integration (CI) significantly reduces integration problems,\nspeeds up development time, and shortens release time. However, it also\nintroduces new challenges for quality assurance activities, including\nregression testing, which is the focus of this work. Though various approaches\nfor test case prioritization have shown to be very promising in the context of\nregression testing, specific techniques must be designed to deal with the\ndynamic nature and timing constraints of CI.\n  Recently, Reinforcement Learning (RL) has shown great potential in various\nchallenging scenarios that require continuous adaptation, such as game playing,\nreal-time ads bidding, and recommender systems. Inspired by this line of work\nand building on initial efforts in supporting test case prioritization with RL\ntechniques, we perform here a comprehensive investigation of RL-based test case\nprioritization in a CI context. To this end, taking test case prioritization as\na ranking problem, we model the sequential interactions between the CI\nenvironment and a test case prioritization agent as an RL problem, using three\nalternative ranking models. We then rely on carefully selected and tailored\nstate-of-the-art RL techniques to automatically and continuously learn a test\ncase prioritization strategy, whose objective is to be as close as possible to\nthe optimal one. Our extensive experimental analysis shows that the best RL\nsolutions provide a significant accuracy improvement over previous RL-based\nwork, with prioritization strategies getting close to being optimal, thus\npaving the way for using RL to prioritize test cases in a CI context.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.03747,review,pre_llm,2020,11,"{'ai_likelihood': 2.3510720994737412e-06, 'text': ""Synthesising Privacy by Design Knowledge Towards Explainable Internet of\n  Things Application Designing in Healthcare\n\n  Privacy by Design (PbD) is the most common approach followed by software\ndevelopers who aim to reduce risks within their application designs, yet it\nremains commonplace for developers to retain little conceptual understanding of\nwhat is meant by privacy. A vision is to develop an intelligent privacy\nassistant to whom developers can easily ask questions in order to learn how to\nincorporate different privacy-preserving ideas into their IoT application\ndesigns. This paper lays the foundations toward developing such a privacy\nassistant by synthesising existing PbD knowledge so as to elicit requirements.\nIt is believed that such a privacy assistant should not just prescribe a list\nof privacy-preserving ideas that developers should incorporate into their\ndesign. Instead, it should explain how each prescribed idea helps to protect\nprivacy in a given application design context-this approach is defined as\n'Explainable Privacy'. A total of 74 privacy patterns were analysed and\nreviewed using ten different PbD schemes to understand how each privacy pattern\nis built and how each helps to ensure privacy. Due to page limitations, we have\npresented a detailed analysis in [3]. In addition, different real-world\nInternet of Things (IoT) use-cases, including a healthcare application, were\nused to demonstrate how each privacy pattern could be applied to a given\napplication design. By doing so, several knowledge engineering requirements\nwere identified that need to be considered when developing a privacy assistant.\nIt was also found that, when compared to other IoT application domains, privacy\npatterns can significantly benefit healthcare applications. In conclusion, this\npaper identifies the research challenges that must be addressed if one wishes\nto construct an intelligent privacy assistant that can truly augment software\ndevelopers' capabilities at the design phase.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.06244,regular,pre_llm,2020,11,"{'ai_likelihood': 6.953875223795574e-07, 'text': 'A Fine-grained Data Set and Analysis of Tangling in Bug Fixing Commits\n\n  Context: Tangled commits are changes to software that address multiple\nconcerns at once. For researchers interested in bugs, tangled commits mean that\nthey actually study not only bugs, but also other concerns irrelevant for the\nstudy of bugs.\n  Objective: We want to improve our understanding of the prevalence of tangling\nand the types of changes that are tangled within bug fixing commits.\n  Methods: We use a crowd sourcing approach for manual labeling to validate\nwhich changes contribute to bug fixes for each line in bug fixing commits. Each\nline is labeled by four participants. If at least three participants agree on\nthe same label, we have consensus.\n  Results: We estimate that between 17% and 32% of all changes in bug fixing\ncommits modify the source code to fix the underlying problem. However, when we\nonly consider changes to the production code files this ratio increases to 66%\nto 87%. We find that about 11% of lines are hard to label leading to active\ndisagreements between participants. Due to confirmed tangling and the\nuncertainty in our data, we estimate that 3% to 47% of data is noisy without\nmanual untangling, depending on the use case.\n  Conclusion: Tangled commits have a high prevalence in bug fixes and can lead\nto a large amount of noise in the data. Prior research indicates that this\nnoise may alter results. As researchers, we should be skeptics and assume that\nunvalidated data is likely very noisy, until proven otherwise.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.05106,review,pre_llm,2020,11,"{'ai_likelihood': 7.516807980007596e-06, 'text': ""How do Practitioners Perceive the Relevance of Requirements Engineering\n  Research?\n\n  The relevance of Requirements Engineering (RE) research to practitioners is\nvital for a long-term dissemination of research results to everyday practice.\nSome authors have speculated about a mismatch between research and practice in\nthe RE discipline. However, there is not much evidence to support or refute\nthis perception. This paper presents the results of a study aimed at gathering\nevidence from practitioners about their perception of the relevance of RE\nresearch and at understanding the factors that influence that perception. We\nconducted a questionnaire-based survey of industry practitioners with expertise\nin RE. The participants rated the perceived relevance of 435 scientific papers\npresented at five top RE-related conferences. The 153 participants provided a\ntotal of 2,164 ratings. The practitioners rated RE research as essential or\nworthwhile in a majority of cases. However, the percentage of non-positive\nratings is still higher than we would like. Among the factors that affect the\nperception of relevance are the research's links to industry, the research\nmethod used, and respondents' roles. The reasons for positive perceptions were\nprimarily related to the relevance of the problem and the soundness of the\nsolution, while the causes for negative perceptions were more varied. The\nrespondents also provided suggestions for future research, including topics\nresearchers have studied for decades, like elicitation or requirement quality\ncriteria.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.07821,review,pre_llm,2020,11,"{'ai_likelihood': 1.3245476616753473e-07, 'text': ""Forking Without Clicking: on How to Identify Software Repository Forks\n\n  The notion of software ''fork'' has been shifting over time from the\n(negative) phenomenon of community disagreements that result in the creation of\nseparate development lines and ultimately software products, to the (positive)\npractice of using distributed version control system (VCS) repositories to\ncollaboratively improve a single product without stepping on each others toes.\nIn both cases the VCS repositories participating in a fork share parts of a\ncommon development history. Studies of software forks generally rely on hosting\nplatform metadata, such as GitHub, as the source of truth for what constitutes\na fork. These ''forge forks'' however can only identify as forks repositories\nthat have been created on the platform, e.g., by clicking a ''fork'' button on\nthe platform user interface. The increased diversity in code hosting platforms\n(e.g., GitLab) and the habits of significant development communities (e.g., the\nLinux kernel, which is not primarily hosted on any single platform) call into\nquestion the reliability of trusting code hosting platforms to identify forks.\nDoing so might introduce selection and methodological biases in empirical\nstudies. In this article we explore various definitions of ''software forks'',\ntrying to capture forking workflows that exist in the real world. We quantify\nthe differences in how many repositories would be identified as forks on GitHub\naccording to the various definitions, confirming that a significant number\ncould be overlooked by only considering forge forks. We study the structure and\nsize of fork networks , observing how they are affected by the proposed\ndefinitions and discuss the potential impact on empirical research.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.07115,review,pre_llm,2020,11,"{'ai_likelihood': 3.741847144232856e-06, 'text': ""Initiatives and Challenges of Using Gamification in Software\n  Engineering: A Systematic Mapping\n\n  Context: Gamification is an emerging subject that has been applied in\ndifferent areas, bringing contributions to different types of activities.\nObjective: This paper aims to characterize how gamification has been adopted in\nnon-educational contexts of software engineering (SE) activities. Method: We\nperformed a Systematic Mapping of the literature obtained from relevant\ndatabases of the area. The searches retrieved 2640 studies (published up to\nJanuary 2020), of which 548 were duplicates, 82 were selected after applying\nthe inclusion and exclusion criteria, and 21 were included via the backward\nsnowballing technique, thus reaching a total of 103 studies to be analyzed.\nResults: Gamification provided benefits to activities like requirements\nspecification, development, testing, project management, and support process.\nThere is evidence of gamified support to some CMMI 2.0 Practice Areas. The most\ncommonly used gamification elements are points and leaderboards. The main\nbenefit achieved is the increased engagement and motivation to perform tasks.\nConclusion: The number of publications and new research initiatives have\nincreased over the years and, from the original authors' reports, many positive\nresults were achieved in SE activities. Despite this, gamification can still be\nexplored for many SE tasks; for the addressed ones, empirical evidence is very\nlimited.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.03016,review,pre_llm,2020,11,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Ethics in the Software Development Process: From Codes of Conduct to\n  Ethical Deliberation\n\n  Software systems play an ever more important role in our lives and software\nengineers and their companies find themselves in a position where they are held\nresponsible for ethical issues that may arise. In this paper, we try to\ndisentangle ethical considerations that can be performed at the level of the\nsoftware engineer from those that belong in the wider domain of business\nethics. The handling of ethical problems that fall into the responsibility of\nthe engineer have traditionally been addressed by the publication of Codes of\nEthics and Conduct. We argue that these Codes are barely able to provide\nnormative orientation in software development. The main contribution of this\npaper is, thus, to analyze the normative features of Codes of Ethics in\nsoftware engineering and to explicate how their value-based approach might\nprevent their usefulness from a normative perspective. Codes of Conduct cannot\nreplace ethical deliberation because they do not and cannot offer guidance\nbecause of their underdetermined nature. This lack of orientation, we argue,\ntriggers reactive behavior such as ""cherry-picking"", ""risk of indifference"",\n""ex-post orientation"" and the ""desire to rely on gut feeling"". In the light of\nthis, we propose to implement ethical deliberation within software development\nteams as a way out.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.13994,regular,pre_llm,2020,11,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'Who is Debugging the Debuggers? Exposing Debug Information Bugs in\n  Optimized Binaries\n\n  Despite the advancements in software testing, bugs still plague deployed\nsoftware and result in crashes in production. When debugging issues --\nsometimes caused by ""heisenbugs"" -- there is the need to interpret core dumps\nand reproduce the issue offline on the same binary deployed. This requires the\nentire toolchain (compiler, linker, debugger) to correctly generate and use\ndebug information. Little attention has been devoted to checking that such\ninformation is correctly preserved by modern toolchains\' optimization stages.\nThis is particularly important as managing debug information in optimized\nproduction binaries is non-trivial, often leading to toolchain bugs that may\nhinder post-deployment debugging efforts. In this paper, we present\nDebug$^{2}$, a framework to find debug information bugs in modern toolchains.\nOur framework feeds random source programs to the target toolchain and\nsurgically compares the debugging behavior of their optimized/unoptimized\nbinary variants. Such differential analysis allows Debug$^{2}$ to check\ninvariants at each debugging step and detect bugs from invariant violations.\nOur invariants are based on the (in)consistency of common debug entities, such\nas source lines, stack frames, and function arguments. We show that, while\nsimple, this strategy yields powerful cross-toolchain and cross-language\ninvariants, which can pinpoint several bugs in modern toolchains. We have used\nDebug$^{2}$ to find 23 bugs in the LLVM toolchain (clang/lldb), 8 bugs in the\nGNU toolchain (GCC/gdb), and 3 in the Rust toolchain (rustc/lldb) -- with 14\nbugs already fixed by the developers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.07914,regular,pre_llm,2020,11,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Determining the Intrinsic Structure of Public Software Development\n  History\n\n  Background. Collaborative software development has produced a wealth of\nversion control system (VCS) data that can now be analyzed in full. Little is\nknown about the intrinsic structure of the entire corpus of publicly available\nVCS as an interconnected graph. Understanding its structure is needed to\ndetermine the best approach to analyze it in full and to avoid methodological\npitfalls when doing so. Objective. We intend to determine the most salient\nnetwork topol-ogy properties of public software development history as captured\nby VCS. We will explore: degree distributions, determining whether they are\nscale-free or not; distribution of connect component sizes; distribution of\nshortest path lengths.Method. We will use Software Heritage-which is the\nlargest corpus of public VCS data-compress it using webgraph compression\ntechniques, and analyze it in-memory using classic graph algorithms. Analyses\nwill be performed both on the full graph and on relevant subgraphs.\nLimitations. The study is exploratory in nature; as such no hypotheses on the\nfindings is stated at this time. Chosen graph algorithms are expected to scale\nto the corpus size, but it will need to be confirmed experimentally. External\nvalidity will depend on how representative Software Heritage is of the software\ncommons.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.08489,regular,pre_llm,2020,11,"{'ai_likelihood': 5.1657358805338544e-06, 'text': 'Continuous Open Source License Compliance\n\n  In this article we consider the role of policy and process in open source\nusage and propose in-workflow automation as the best path to promoting\ncompliance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.14283,review,pre_llm,2020,11,"{'ai_likelihood': 2.6490953233506946e-07, 'text': ""Resolving code smells in software product line using refactoring and\n  reverse engineering\n\n  Software Product Lines SPL are recognized as a successful approach to reuse\nin software development.Its purpose is to reduce production costs. This\napproach allows products to be different with respect of particular\ncharacteristics and constraints in order to cover different markets. Software\nProduct Line engineering is the production process in product lines.It exploits\nthe commonalities between software products, but also to preserve the ability\nto vary the functionality between these products.Sometimes, an inappropriate\nimplementation of SPL during this process can conduct to code smells or code\nanomalies. Code smells are considered as problems in source code which can have\nan impact on the quality of the derived products of an SPL. The same problem\ncan be present in many derived products from an SPL due to reuse. A possible\nsolution to this problem can be the refactoring which can improve the internal\nstructure of source code without altering external behavior.This paper proposes\nan approach for building SPL from source code.Its purpose is to reduce code\nsmells in the obtained SPL using refactoring source code.Another part of the\napproach consists on obtained SPL's design based on reverse engineering.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.02297,review,pre_llm,2020,11,"{'ai_likelihood': 3.344482845730252e-06, 'text': 'Opportunities and Challenges in Code Search Tools\n\n  Code search is a core software engineering task. Effective code search tools\ncan help developers substantially improve their software development efficiency\nand effectiveness. In recent years, many code search studies have leveraged\ndifferent techniques, such as deep learning and information retrieval\napproaches, to retrieve expected code from a large-scale codebase. However,\nthere is a lack of a comprehensive comparative summary of existing code search\napproaches. To understand the research trends in existing code search studies,\nwe systematically reviewed 81 relevant studies. We investigated the publication\ntrends of code search studies, analyzed key components, such as codebase,\nquery, and modeling technique used to build code search tools, and classified\nexisting tools into focusing on supporting seven different search tasks. Based\non our findings, we identified a set of outstanding challenges in existing\nstudies and a research roadmap for future code search research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.14597,review,pre_llm,2020,11,"{'ai_likelihood': 2.914004855685764e-06, 'text': ""A Survey on Deep Learning for Software Engineering\n\n  In 2006, Geoffrey Hinton proposed the concept of training ''Deep Neural\nNetworks (DNNs)'' and an improved model training method to break the bottleneck\nof neural network development. More recently, the introduction of AlphaGo in\n2016 demonstrated the powerful learning ability of deep learning and its\nenormous potential. Deep learning has been increasingly used to develop\nstate-of-the-art software engineering (SE) research tools due to its ability to\nboost performance for various SE tasks. There are many factors, e.g., deep\nlearning model selection, internal structure differences, and model\noptimization techniques, that may have an impact on the performance of DNNs\napplied in SE. Few works to date focus on summarizing, classifying, and\nanalyzing the application of deep learning techniques in SE. To fill this gap,\nwe performed a survey to analyse the relevant studies published since 2006. We\nfirst provide an example to illustrate how deep learning techniques are used in\nSE. We then summarize and classify different deep learning techniques used in\nSE. We analyzed key optimization technologies used in these deep learning\nmodels, and finally describe a range of key research topics using DNNs in SE.\nBased on our findings, we present a set of current challenges remaining to be\ninvestigated and outline a proposed research road map highlighting key\nopportunities for future work.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.15627,regular,pre_llm,2020,12,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'FILO: FIx-LOcus Localization for Backward Incompatibilities Caused by\n  Android Framework Upgrades\n\n  Mobile operating systems evolve quickly, frequently updating the APIs that\napp developers use to build their apps. Unfortunately, API updates do not\nalways guarantee backward compatibility, causing apps to not longer work\nproperly or even crash when running with an updated system. This paper presents\nFILO, a tool that assists Android developers in resolving backward\ncompatibility issues introduced by API upgrades. FILO both suggests the method\nthat needs to be modified in the app in order to adapt the app to an upgraded\nAPI, and reports key symptoms observed in the failed execution to facilitate\nthe fixing activity. Results obtained with the analysis of 12 actual upgrade\nproblems and the feedback produced by early tool adopters show that FILO can\npractically support Android developers.FILO can be downloaded from\nhttps://gitlab.com/learnERC/filo, and its video demonstration is available at\nhttps://youtu.be/WDvkKj-wnlQ.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.15382,regular,pre_llm,2020,12,"{'ai_likelihood': 2.980232238769531e-07, 'text': ""Managed Information: A New Abstraction Mechanism for Handling\n  Information in Software-as-a-Service\n\n  Management of information is an important aspect of every application. This\nincludes, for example, protecting user data against breaches (like the one\nreported in the news about 50 million Facebook profiles being harvested for\nCambridge Analytica), complying with data protection laws and regulations (like\nEU's new General Data Protection Regulation), coping with large databases, and\nretaining user data across software versions. Today, every application needs to\ncope with such concerns by itself and on its own.\n  In this paper we introduce Managed Information (MI), an abstraction mechanism\nfor managing extra-functional data related concerns, similar to how managed\nmemory today abstracts away many memory related concerns. MI limits the access\napplications have to user data, which, in return, relieves them from\nresponsibility over it. This is achieved by hosting them on a Managed\nInformation Platform (MIP), and implementing their logic in a language that\nsupports MI. As evidence for the feasibility of MI we describe the design and\nimplementation of such a platform. For demonstration of MI, we describe a\nsimple social network application built with it. The implementation is open\nsource.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.15153,review,pre_llm,2020,12,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Importance of Secure Software Development Processes and Tools for\n  Developers\n\n  In this research paper of secure software systems, authors have discussed\nwhat the proper development process is when it comes to creating a secure\nsoftware, which will be suited for developers and relevent stakeholders alike.\nSecure Software Development Process for Developers is of crucial importance for\nsoftware engineers as more and more software-based devices are becoming\ncommonly available, and cloud services are evolving which require for the\nsoftware to be constantly connected to the internet. With this in mind, Secure\nSoftware Development needs to be transformed to something most developers can\nrely upon to make applied software safe and have the capability to mitigate\nagainst potential attacks by hackers. Furthermore, in this paper, existing\nSecure Software Development Process ideas and implementations are reviewed and\ninvestigated using the research paper pool available online. Thereafter, an\napproach is proposed to enhance the security aspect in software development\nprocess to resolve security issues. Lastly, the paper concludes with final\nremarks on practical implementation of security features in software\ndevelopment phases for production of secure and reliable software programs and\nsystems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.05085,regular,pre_llm,2020,12,"{'ai_likelihood': 3.2120280795627172e-06, 'text': ""TaskTracker-tool: a Toolkit for Tracking of Code Snapshots and Activity\n  Data During Solution of Programming Tasks\n\n  The process of writing code and use of features in an integrated development\nenvironment (IDE) is a fruitful source of data in computing education research.\nExisting studies use records of students' actions in the IDE, consecutive code\nsnapshots, compilation events, and others, to gain deep insight into the\nprocess of student programming. In this paper, we present a set of tools for\ncollecting and processing data of student activity during problem-solving. The\nfirst tool is a plugin for IntelliJ-based IDEs (PyCharm, IntelliJ IDEA, CLion).\nBy capturing snapshots of code and IDE interaction data, it allows to analyze\nthe process of writing code in different languages -- Python, Java, Kotlin, and\nC++. The second tool is designed for the post-processing of data collected by\nthe plugin and is capable of basic analysis and visualization. To validate and\nshowcase the toolkit, we present a dataset collected by our tools. It consists\nof records of activity and IDE interaction events during solution of\nprogramming tasks by 148 participants of different ages and levels of\nprogramming experience. We propose several directions for further exploration\nof the dataset.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.15533,regular,pre_llm,2020,12,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'Optimal Software Architecture From Initial Requirements: An End-to-End\n  Approach\n\n  A software architect turns system requirements into a suitable software\narchitecture through an architecture optimization process. However, how should\nthe architect decide which quality improvement to prioritize, e.g., security or\nreliability? In software product line, should a small improvement in multiple\nproducts be preferred over a large improvement in a single product? Existing\narchitecture optimization methods handle various steps in the process, but none\nof them systematically guides the architect in generating an optimal\narchitecture from the initial requirements. In this work we present an\nend-to-end approach for generating an optimal software architecture for a\nsingle software product and an optimal family of architectures for a family of\nproducts. We report on a case-study of applying our approach to optimize five\nindustry-grade products in a real-life product line architecture, where 359\npossible combinations of ten different quality efforts were prioritized.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.07274,review,pre_llm,2020,12,"{'ai_likelihood': 2.3510720994737412e-06, 'text': 'Determining Context Factors for Hybrid Development Methods with Trained\n  Models\n\n  Selecting a suitable development method for a specific project context is one\nof the most challenging activities in process design. Every project is unique\nand, thus, many context factors have to be considered. Recent research took\nsome initial steps towards statistically constructing hybrid development\nmethods, yet, paid little attention to the peculiarities of context factors\ninfluencing method and practice selection. In this paper, we utilize\nexploratory factor analysis and logistic regression analysis to learn such\ncontext factors and to identify methods that are correlated with these factors.\nOur analysis is based on 829 data points from the HELENA dataset. We provide\nfive base clusters of methods consisting of up to 10 methods that lay the\nfoundation for devising hybrid development methods. The analysis of the five\nclusters using trained models reveals only a few context factors, e.g.,\nproject/product size and target application domain, that seem to significantly\ninfluence the selection of methods. An extended descriptive analysis of these\npractices in the context of the identified method clusters also suggests a\nconsolidation of the relevant practice sets used in specific project contexts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.10206,regular,pre_llm,2020,12,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'An Empirical Investigation of Command-Line Customization\n\n  The interactive command line, also known as the shell, is a prominent\nmechanism used extensively by a wide range of software professionals\n(engineers, system administrators, data scientists, etc.). Shell customizations\ncan therefore provide insight into the tasks they repeatedly perform, how well\nthe standard environment supports those tasks, and ways in which the\nenvironment could be productively extended or modified. To characterize the\npatterns and complexities of command-line customization, we mined the\ncollective knowledge of command-line users by analyzing more than 2.2 million\nshell alias definitions found on GitHub. Shell aliases allow command-line users\nto customize their environment by defining arbitrarily complex command\nsubstitutions. Using inductive coding methods, we found three types of aliases\nthat each enable a number of customization practices: Shortcuts (for nicknaming\ncommands, abbreviating subcommands, and bookmarking locations), Modifications\n(for substituting commands, overriding defaults, colorizing output, and\nelevating privilege), and Scripts (for transforming data and chaining\nsubcommands). We conjecture that identifying common customization practices can\npoint to particular usability issues within command-line programs, and that a\ndeeper understanding of these practices can support researchers and tool\ndevelopers in designing better user experiences. In addition to our analysis,\nwe provide an extensive reproducibility package in the form of a curated\ndataset together with well-documented computational notebooks enabling further\nknowledge discovery and a basis for learning approaches to improve command-line\nworkflows.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.14538,review,pre_llm,2020,12,"{'ai_likelihood': 2.3510720994737412e-06, 'text': 'A Model for Software Contexts\n\n  It is widely acknowledged by researchers and practitioners that software\ndevelopment methodologies are generally adapted to suit specific project\ncontexts. Research into practices-as-implemented has been fragmented and has\ntended to focus either on the strength of adherence to a specific methodology\nor on how the efficacy of specific practices is affected by contextual factors.\nWe submit the need for a more holistic, integrated approach to investigating\ncontext-related best practice. We propose a six-dimensional model of the\nproblem-space, with dimensions organisational drivers (why), space and time\n(where), culture (who), product life-cycle stage (when), product constraints\n(what) and engagement constraints (how). We test our model by using it to\ndescribe and explain a reported implementation study. Our contributions are a\nnovel approach to understanding situated software practices and a preliminary\nmodel for software contexts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.01028,regular,pre_llm,2020,12,"{'ai_likelihood': 1.0530153910319012e-05, 'text': 'CRaDLe: Deep Code Retrieval Based on Semantic Dependency Learning\n\n  Code retrieval is a common practice for programmers to reuse existing code\nsnippets in open-source repositories. Given a user query (i.e., a natural\nlanguage description), code retrieval aims at searching for the most relevant\nones from a set of code snippets. The main challenge of effective code\nretrieval lies in mitigating the semantic gap between natural language\ndescriptions and code snippets. With the ever-increasing amount of available\nopen-source code, recent studies resort to neural networks to learn the\nsemantic matching relationships between the two sources. The statement-level\ndependency information, which highlights the dependency relations among the\nprogram statements during the execution, reflects the structural importance of\none statement in the code, which is favorable for accurately capturing the code\nsemantics but has never been explored for the code retrieval task. In this\npaper, we propose CRaDLe, a novel approach for Code Retrieval based on\nstatement-level semantic Dependency Learning. Specifically, CRaDLe distills\ncode representations through fusing both the dependency and semantic\ninformation at the statement level and then learns a unified vector\nrepresentation for each code and description pair for modeling the matching\nrelationship. Comprehensive experiments and analysis on real-world datasets\nshow that the proposed approach can accurately retrieve code snippets for a\ngiven query and significantly outperform the state-of-the-art approaches to the\ntask.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.14631,regular,pre_llm,2020,12,"{'ai_likelihood': 4.371007283528646e-06, 'text': 'Multi-task Learning based Pre-trained Language Model for Code Completion\n\n  Code completion is one of the most useful features in the Integrated\nDevelopment Environments (IDEs), which can accelerate software development by\nsuggesting the next probable token based on the contextual code in real-time.\nRecent studies have shown that statistical language modeling techniques can\nimprove the performance of code completion tools through learning from\nlarge-scale software repositories. However, these models suffer from two major\ndrawbacks: a) Existing research uses static embeddings, which map a word to the\nsame vector regardless of its context. The differences in the meaning of a\ntoken in varying contexts are lost when each token is associated with a single\nrepresentation; b) Existing language model based code completion models perform\npoor on completing identifiers, and the type information of the identifiers is\nignored in most of these models. To address these challenges, in this paper, we\ndevelop a multi-task learning based pre-trained language model for code\nunderstanding and code generation with a Transformer-based neural architecture.\nWe pre-train it with hybrid objective functions that incorporate both code\nunderstanding and code generation tasks. Then we fine-tune the pre-trained\nmodel on code completion. During the completion, our model does not directly\npredict the next token. Instead, we adopt multi-task learning to predict the\ntoken and its type jointly and utilize the predicted type to assist the token\nprediction. Experiments results on two real-world datasets demonstrate the\neffectiveness of our model when compared with state-of-the-art methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.09916,regular,pre_llm,2020,12,"{'ai_likelihood': 4.2054388258192275e-06, 'text': 'RAICC: Revealing Atypical Inter-Component Communication in Android Apps\n\n  Inter-Component Communication (ICC) is a key mechanism in Android. It enables\ndevelopers to compose rich functionalities and explore reuse within and across\napps. Unfortunately, as reported by a large body of literature, ICC is rather\n""complex and largely unconstrained"", leaving room to a lack of precision in\napps modeling. To address the challenge of tracking ICCs within apps, state of\nthe art static approaches such as Epicc, IccTA and Amandroid have focused on\nthe documented framework ICC methods (e.g., startActivity) to build their\napproaches. In this work we show that ICC models inferred in these state of the\nart tools may actually be incomplete: the framework provides other atypical\nways of performing ICCs. To address this limitation in the state of the art, we\npropose RAICC a static approach for modeling new ICC links and thus boosting\nprevious analysis tasks such as ICC vulnerability detection, privacy leaks\ndetection, malware detection, etc. We have evaluated RAICC on 20 benchmark\napps, demonstrating that it improves the precision and recall of uncovered\nleaks in state of the art tools. We have also performed a large empirical\ninvestigation showing that Atypical ICC methods are largely used in Android\napps, although not necessarily for data transfer. We also show that RAICC\nincreases the number of ICC links found by 61.6% on a dataset of real-world\nmalicious apps, and that RAICC enables the detection of new ICC\nvulnerabilities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.08387,review,pre_llm,2020,12,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'Run, Forest, Run? On Randomization and Reproducibility in Predictive\n  Software Engineering\n\n  Machine learning (ML) has been widely used in the literature to automate\nsoftware engineering tasks. However, ML outcomes may be sensitive to\nrandomization in data sampling mechanisms and learning procedures. To\nunderstand whether and how researchers in SE address these threats, we surveyed\n45 recent papers related to three predictive tasks: defect prediction (DP),\npredictive mutation testing (PMT), and code smell detection (CSD). We found\nthat less than 50% of the surveyed papers address the threats related to\nrandomized data sampling (via multiple repetitions); only 8% of the papers\naddress the random nature of ML; and parameter values are rarely reported (only\n18% of the papers). To assess the severity of these threats, we conducted an\nempirical study using 26 real-world datasets commonly considered for the three\npredictive tasks of interest, considering eight common supervised ML\nclassifiers. We show that different data resamplings for 10-fold\ncross-validation lead to extreme variability in observed performance results.\nFurthermore, randomized ML methods also show non-negligible variability for\ndifferent choices of random seeds. More worryingly, performance and variability\nare inconsistent for different implementations of the conceptually same ML\nmethod in different libraries, as also shown through multi-dataset pairwise\ncomparison. To cope with these critical threats, we provide practical\nguidelines on how to validate, assess, and report the results of predictive\nmethods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.08842,review,pre_llm,2020,12,"{'ai_likelihood': 2.1192762586805556e-06, 'text': 'Code smells detection and visualization: A systematic literature review\n\n  Context: Code smells (CS) tend to compromise software quality and also demand\nmore effort by developers to maintain and evolve the application throughout its\nlife-cycle. They have long been catalogued with corresponding mitigating\nsolutions called refactoring operations. Objective: This SLR has a twofold\ngoal: the first is to identify the main code smells detection techniques and\ntools discussed in the literature, and the second is to analyze to which extent\nvisual techniques have been applied to support the former. Method: Over 83\nprimary studies indexed in major scientific repositories were identified by our\nsearch string in this SLR. Then, following existing best practices for\nsecondary studies, we applied inclusion/exclusion criteria to select the most\nrelevant works, extract their features and classify them. Results: We found\nthat the most commonly used approaches to code smells detection are\nsearch-based (30.1%), and metric-based (24.1%). Most of the studies (83.1%) use\nopen-source software, with the Java language occupying the first position\n(77.1%). In terms of code smells, God Class (51.8%), Feature Envy (33.7%), and\nLong Method (26.5%) are the most covered ones. Machine learning techniques are\nused in 35% of the studies. Around 80% of the studies only detect code smells,\nwithout providing visualization techniques. In visualization-based approaches\nseveral methods are used, such as: city metaphors, 3D visualization techniques.\nConclusions: We confirm that the detection of CS is a non trivial task, and\nthere is still a lot of work to be done in terms of: reducing the subjectivity\nassociated with the definition and detection of CS; increasing the diversity of\ndetected CS and of supported programming languages; constructing and sharing\noracles and datasets to facilitate the replication of CS detection and\nvisualization techniques validation experiments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.03225,regular,pre_llm,2020,12,"{'ai_likelihood': 6.556510925292969e-06, 'text': ""NaturalCC: A Toolkit to Naturalize the Source Code Corpus\n\n  We present NaturalCC, an efficient and extensible toolkit to bridge the gap\nbetween natural language and programming language, and facilitate the research\non big code analysis. Using NaturalCC, researchers both from natural language\nor programming language communities can quickly and easily reproduce the\nstate-of-the-art baselines and implement their approach. NaturalCC is built\nupon Fairseq and PyTorch, providing (1) an efficient computation with multi-GPU\nand mixed-precision data processing for fast model training, (2) a modular and\nextensible framework that makes it easy to reproduce or implement an approach\nfor big code analysis, and (3) a command line interface and a graphical user\ninterface to demonstrate each model's performance. Currently, we have included\nseveral state-of-the-art baselines across different tasks (e.g., code\ncompletion, code comment generation, and code retrieval) for demonstration. The\nvideo of this demo is available at\nhttps://www.youtube.com/watch?v=q4W5VSI-u3E&t=25s.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.05231,regular,pre_llm,2020,12,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'The Fulib Solution to the TTC 2020 Migration Case\n\n  At Kassel University we are working on a solution for bidirectional\ntransformations based on event sourcing for about a year, now. It turned out,\nthat the TTC 2020 migration case is a special case of a bidirectional\ntransformation and that our approach provides a reasonable solution for it.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.14237,regular,pre_llm,2020,12,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'A Comprehensive Empirical Evaluation of Generating Test Suites for\n  Mobile Applications with Diversity\n\n  Context: In search-based software engineering we often use popular heuristics\nwith default configurations, which typically lead to suboptimal results, or we\nperform experiments to identify configurations on a trial-and-error basis,\nwhich may lead to better results for a specific problem. We consider the\nproblem of generating test suites for mobile applications (apps) and rely on\n\\Sapienz, a state-of-the-art approach to this problem that uses a popular\nheuristic (NSGA-II) with a default configuration. Objective: We want to achieve\nbetter results in generating test suites with \\Sapienz while avoiding\ntrial-and-error experiments to identify a more suitable configuration of\n\\Sapienz. Method: We conducted a fitness landscape analysis of \\Sapienz to\nanalytically understand the search problem, which allowed us to make informed\ndecisions about the heuristic and configuration of \\Sapienz when developing\n\\SapienzDiv. We comprehensively evaluated \\SapienzDiv in a head-to-head\ncomparison with \\Sapienz on 34 apps. Results: Analyzing the fitness landscape\nof \\Sapienz, we observed a lack of diversity of the evolved test suites and a\nstagnation of the search after 25 generations. \\SapienzDiv realizes mechanisms\nthat preserve the diversity of the test suites being evolved. The evaluation\nshowed that \\SapienzDiv achieves better or at least similar test results than\n\\Sapienz concerning coverage and the number of revealed faults. However,\n\\SapienzDiv typically produces longer test sequences and requires more\nexecution time than \\Sapienz. Conclusions: The understanding of the search\nproblem obtained by the fitness landscape analysis helped us to find a more\nsuitable configuration of \\Sapienz without trial-and-error experiments. By\npromoting diversity of test suites during the search, improved or at least\nsimilar test results in terms of faults and coverage can be achieved.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.15407,review,pre_llm,2020,12,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Consolidating a Model for Describing Situated Software Practices\n\n  Many prescriptive approaches to developing software intensive systems have\nbeen advocated but each is based on assumptions about context. It has been\nfound that practitioners do not follow prescribed methodologies, but rather\nselect and adapt specific practices according to local needs. As researchers,\nwe would like to be in a position to support such tailoring. However, at the\npresent time we simply do not have sufficient evidence relating practice and\ncontext for this to be possible. We have long understood that a deeper\nunderstanding of situated software practices is crucial for progress in this\narea, and have been exploring this problem from a number of perspectives. In\nthis position paper, we draw together the various aspects of our work into a\nholistic model and discuss the ways in which the model might be applied to\nsupport the long term goal of evidence-based decision support for\npractitioners. The contribution specific to this paper is a discussion on model\nevaluation, including a proof-of-concept demonstration of model utility. We map\nKernel elements from the Essence system to our model and discuss gaps and\nlimitations exposed in the Kernel. Finally, we overview our plans for further\nrefining and evaluating the model.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.08862,regular,pre_llm,2020,12,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'Practical use of Windows data collector process and testing analysis\n\n  The paper demonstrates the Windows data collectordevelopment process with the\nbuilt back-end from the require-ments gathering stage till the implementation\nand testing phase.Each phase throughout the development life cycle of the\nsystemis defined in details. The whole system idea and the objectivesbehind\ndeveloping this kind of framework is described in earlierpapers that creates\nthe importance of introducing the backgroundbefore reading this paper. The\ndetailed information about thedata collector features that are provided and\ntheir appropriatetesting types, documentation are demonstrated thoroughly.\nBe-sides, the process of development includes both design and overallsystem\narchitecture description.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.12466,regular,pre_llm,2020,12,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'A Framework for Conditional Statement Technical Debt Identification and\n  Description\n\n  Technical Debt occurs when development teams favour short-term operability\nover long-term stability. Since this places software maintainability at risk,\ntechnical debt requires early attention to avoid paying for accumulated\ninterest. Most of the existing work focuses on detecting technical debt using\ncode comments, known as Self-Admitted Technical Debt (SATD). However, there are\nmany cases where technical debt instances are not explicitly acknowledged but\ndeeply hidden in the code. In this paper, we propose a framework that caters\nfor the absence of SATD comments in code. Our Self-Admitted Technical Debt\nIdentification and Description (SATDID) framework determines if technical debt\nshould be self-admitted for an input code fragment. If that is the case, SATDID\nwill automatically generate the appropriate descriptive SATD comment that can\nbe attached with the code. While our approach is applicable in principle to any\ntype of code fragments, we focus in this study on technical debt hidden in\nconditional statements, one of the most TD-carrying parts of code. We explore\nand evaluate different implementations of SATDID. The evaluation results\ndemonstrate the applicability and effectiveness of our framework over multiple\nbenchmarks. Comparing with the results from the benchmarks, our approach\nprovides at least 21.35%, 59.36%, 31.78%, and 583.33% improvements in terms of\nPrecision, Recall, F-1, and Bleu-4 scores, respectively. In addition, we\nconduct human evaluation to the SATD comments generated by SATDID. In 1-5 and\n0-5 scales for Acceptability and Understandability, the total means achieved by\nour approach are 3.128 and 3.172, respectively.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.01057,review,pre_llm,2020,12,"{'ai_likelihood': 3.890858756171333e-05, 'text': ""Software Module Clustering: An In-Depth Literature Analysis\n\n  Software module clustering is an unsupervised learning method used to cluster\nsoftware entities (e.g., classes, modules, or files) with similar features. The\nobtained clusters may be used to study, analyze, and understand the software\nentities' structure and behavior. Implementing software module clustering with\noptimal results is challenging. Accordingly, researchers have addressed many\naspects of software module clustering in the past decade. Thus, it is essential\nto present the research evidence that has been published in this area. In this\nstudy, 143 research papers from well-known literature databases that examined\nsoftware module clustering were reviewed to extract useful data. The obtained\ndata were then used to answer several research questions regarding\nstate-of-the-art clustering approaches, applications of clustering in software\nengineering, clustering processes, clustering algorithms, and evaluation\nmethods. Several research gaps and challenges in software module clustering are\ndiscussed in this paper to provide a useful reference for researchers in this\nfield.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.021,review,pre_llm,2021,1,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'On the Requirements for Serious Games geared towards Software Developers\n  in the Industry\n\n  Teaching industry staff on cybersecurity issues is a fundamental activity\nthat must be undertaken in order to guarantee the delivery of successful and\nrobust products to market. Much research attention has been devoted to this\ntopic over the last years. However, the research which has been done has not\nfocused on developing secure code in industrial environments. In this paper we\ntake a look at the constraints and requirements for delivering a training, by\nmeans of cybersecurity challenges, that covers secure coding topics from an\nindustry perspective. Using requirements engineering, we aim at understanding\nthe design requirements for such challenges. Along the way, we give details on\nour experience of delivering cybersecurity challenges in an industrial setting\nand show the outcome and lessons learned. The proposed requirements for\ncybersecurity challenges geared towards software developers in an industrial\nenvironment are based on systematic literature review, interviews with security\nexperts from the industry and semi-structured evaluation of participant\nfeedback.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.11749,regular,pre_llm,2021,1,"{'ai_likelihood': 2.715322706434462e-06, 'text': 'An extensive empirical study of inconsistent labels in\n  multi-version-project defect data sets\n\n  The label quality of defect data sets has a direct influence on the\nreliability of defect prediction models. In this study, for\nmulti-version-project defect data sets, we propose an approach to automatically\ndetecting instances with inconsistent labels (i.e. the phenomena of instances\nhaving the same source code but different labels over multiple versions of a\nsoftware project) and understand their influence on the evaluation and\ninterpretation of defect prediction models. Based on five multi-version-project\ndefect data sets (either widely used or the most up-to-date in the literature)\ncollected by diverse approaches, we find that: (1) most versions in the\ninvestigated defect data sets contain inconsistent labels with varying degrees;\n(2) the existence of inconsistent labels in a training data set may\nconsiderably change the prediction performance of a defect prediction model as\nwell as can lead to the identification of substantially different true\ndefective modules; and (3) the importance ranking of independent variables in a\ndefect prediction model can be substantially shifted due to the existence of\ninconsistent labels. The above findings reveal that inconsistent labels in\ndefect data sets can profoundly change the prediction ability and\ninterpretation of a defect prediction model. Therefore, we strongly suggest\nthat practitioners should detect and exclude inconsistent labels in defect data\nsets to avoid their potential negative influence on defect prediction models.\nWhat is more, it is necessary for researchers to improve existing defect label\ncollection approaches to reduce inconsistent labels. Furthermore, there is a\nneed to re-examine the experimental conclusions of previous studies using\nmulti-version-project defect data sets with a high ratio of inconsistent\nlabels.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.01325,review,pre_llm,2021,1,"{'ai_likelihood': 1.158979203965929e-06, 'text': 'Advancing Behavior Engineering: Toward Integrated Events Modeling\n\n  The term behavior engineering (BE) encompasses a broad integration of\nbehavioral and compositional requirements needed to model large-scale systems.\nBE forms a connection between systems-engineering processes and\nsoftware-engineering processes. In software engineering, interpreting\nrequirements can be perceived as specifying behavior, which is viewed in terms\nof chronology of events in the modeled system. In this paper, we adopt BE in\nits general and integrating sense to search for a unifying notion of an event\nas a fundamental behavior-modeling concept. We examine several bodies of\nresearch with various definitions of an event and its basic units and\nstructure. We use the thinging machine (TM) model to analyze notions related to\nevents, including Dromey s behavior trees, fluents (change over time),\nrecurrent events, and Davidson s events. The results point to an underlying\nmeaning that can lead to a unifying event concept.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.10883,review,pre_llm,2021,1,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'A Fresh Look at FAIR for Research Software\n\n  This document captures the discussion and deliberation of the FAIR for\nResearch Software (FAIR4RS) subgroup that took a fresh look at the\napplicability of the FAIR Guiding Principles for scientific data management and\nstewardship for research software. We discuss the vision of research software\nas ideally reproducible, open, usable, recognized, sustained and robust, and\nthen review both the characteristic and practiced differences of research\nsoftware and data. This vision and understanding of initial conditions serves\nas a backdrop for an attempt at translating and interpreting the guiding\nprinciples to more fully align with research software. We have found that many\nof the principles remained relatively intact as written, as long as\nconsiderable interpretation was provided. This was particularly the case for\nthe ""Findable"" and ""Accessible"" foundational principles. We found that\n""Interoperability"" and ""Reusability"" are particularly prone to a broad and\nsometimes opposing set of interpretations as written. We propose two new\nprinciples modeled on existing ones, and provide modified guiding text for\nthese principles to help clarify our final interpretation. A series of gaps in\ntranslation were captured during this process, and these remain to be\naddressed. We finish with a consideration of where these translated principles\nfall short of the vision laid out in the opening.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.00293,regular,pre_llm,2021,1,"{'ai_likelihood': 1.5232298109266494e-06, 'text': 'Using Bayesian Modelling to Predict Software Incidents\n\n  Traditionally, fault- or event-tree analyses or FMEAs have been used to\nestimate the probability of a safety-critical device creating a dangerous\ncondition. However, these analysis techniques are less effective for systems\nprimarily reliant on software, and are perhaps least effective in Safety of the\nIntended Functionality (SOTIF) environments, where the failure or dangerous\nsituation occurs even though all components behaved as designed. This paper\ndescribes an approach we are considering at BlackBerry QNX: using Bayesian\nBelief Networks to predict defects in embedded software, and reports on early\nresults from our research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.02106,regular,pre_llm,2021,1,"{'ai_likelihood': 1.457002427842882e-06, 'text': 'When Interactive Graphic Storytelling Fails\n\n  Many people are unaware of the digital dangers that lie around each\ncyber-corner. Teaching people how to recognize dangerous situations is crucial,\nespecially for those who work on or with computers. We postulated that\ninteractive graphic vignettes could be a great way to expose professionals to\ndangerous situations and demonstrate the effects of their choices in these\nsituations. In that way, we aimed to inoculate employees against cybersecurity\nthreats.\n  We used the Comic-BEE platform to create interactive security awareness\nvignettes and evaluated for how employees of a major industrial company\nperceived them. For analysing the potential of these comics, we ran an\nevaluation study as part of a capture-the-flag (CTF) event, an interactive\nexercise for hacking vulnerable software. We evaluated whether the comics\nfulfilled our requirements based on the responses of the participants. We\nshowed the comics, on various cybersecurity concepts, to 20 volunteers. In the\ncontext of a CTF event, our requirements were not fulfilled. Most participants\nconsidered the images distracting, stating a preference for text-only material.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.05962,regular,pre_llm,2021,1,"{'ai_likelihood': 7.748603820800781e-06, 'text': ""A Data Flow Analysis Framework for Data Flow Subsumption\n\n  Data flow testing creates test requirements as definition-use (DU)\nassociations, where a definition is a program location that assigns a value to\na variable and a use is a location where that value is accessed. Data flow\ntesting is expensive, largely because of the number of test requirements.\nLuckily, many DU-associations are redundant in the sense that if one test\nrequirement (e.g., node, edge, DU-association) is covered, other\nDU-associations are guaranteed to also be covered. This relationship is called\nsubsumption. Thus, testers can save resources by only covering DU-associations\nthat are not subsumed by other testing requirements. In this work, we formally\ndescribe the Data Flow Subsumption Framework (DSF) conceived to tackle the data\nflow subsumption problem. We show that DFS is a distributive data flow analysis\nframework which allows efficient iterative algorithms to find the\nMeet-Over-All-Paths (MOP) solution for DSF transfer functions. The MOP solution\nimplies that the results at a point $p$ are valid for all paths that reach $p$.\nWe also present an algorithm, called Subsumption Algorithm (SA), that uses DSF\ntransfer functions and iterative algorithms to find the local\nDU-associations-node subsumption; that is, the set of DU-associations that are\ncovered whenever a node $n$ is toured by a test. A proof of SA's correctness is\npresented and its complexity is analyzed.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.01933,regular,pre_llm,2021,1,"{'ai_likelihood': 7.881058586968316e-06, 'text': 'Combining Genetic Programming and Model Checking to Generate Environment\n  Assumptions\n\n  Software verification may yield spurious failures when environment\nassumptions are not accounted for. Environment assumptions are the expectations\nthat a system or a component makes about its operational environment and are\noften specified in terms of conditions over the inputs of that system or\ncomponent. In this article, we propose an approach to automatically infer\nenvironment assumptions for Cyber-Physical Systems (CPS). Our approach improves\nthe state-of-the-art in three different ways: First, we learn assumptions for\ncomplex CPS models involving signal and numeric variables; second, the learned\nassumptions include arithmetic expressions defined over multiple variables;\nthird, we identify the trade-off between soundness and informativeness of\nenvironment assumptions and demonstrate the flexibility of our approach in\nprioritizing either of these criteria.\n  We evaluate our approach using a public domain benchmark of CPS models from\nLockheed Martin and a component of a satellite control system from LuxSpace, a\nsatellite system provider. The results show that our approach outperforms\nstate-of-the-art techniques on learning assumptions for CPS models, and\nfurther, when applied to our industrial CPS model, our approach is able to\nlearn assumptions that are sufficiently close to the assumptions manually\ndeveloped by engineers to be of practical value.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.00836,regular,pre_llm,2021,1,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'Lost in Zero Space -- An Empirical Comparison of 0.y.z Releases in\n  Software Package Distributions\n\n  Distributions of open source software packages dedicated to specific\nprogramming languages facilitate software development by allowing software\nprojects to depend on the functionality provided by such reusable packages. The\nhealth of a software project can be affected by the maturity of the packages on\nwhich it depends. The version numbers of the used package releases provide an\nindication of their maturity. Packages with a 0.y.z version number are commonly\nassumed to be under initial development, suggesting that they are likely to be\nless stable, and depending on them may be considered as less healthy. In this\npaper, we empirically study, for four open source package distributions (Cargo,\nnpm, Packagist and RubyGems) to which extent 0.y.z package releases and >=1.0.0\npackage releases behave differently. We quantify the prevalence of 0.y.z\nreleases, we explore how long packages remain in the initial development stage,\nwe compare the update frequency of 0.y.z and >=1.0.0 package releases, we study\nhow often 0.y.z releases are required by other packages, we assess whether\nsemantic versioning is respected for dependencies towards them, and we compare\nsome characteristics of 0.y.z and >=1.0.0 package repositories hosted on\nGitHub. Among others, we observe that package distributions are more permissive\nthan what semantic versioning dictates for 0.y.z releases, and that many of the\n0.y.z releases can actually be regarded as mature packages. As a consequence,\nthe version number does not provide a good indication of the maturity of a\npackage release.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.08432,review,pre_llm,2021,1,"{'ai_likelihood': 2.317958407931858e-06, 'text': 'Walking Through the Method Zoo: Does Higher Education really meet\n  Software Industry Demands?\n\n  Software engineering educators are continually challenged by rapidly evolving\nconcepts, technologies, and industry demands. Due to the omnipresence of\nsoftware in a digitalized society, higher education institutions (HEIs) have to\neducate the students such that they learn how to learn, and that they are\nequipped with a profound basic knowledge and with latest knowledge about modern\nsoftware and system development. Since industry demands change constantly, HEIs\nare challenged in meeting such current and future demands in a timely manner.\nThis paper analyzes the current state of practice in software engineering\neducation. Specifically, we want to compare contemporary education with\nindustrial practice to understand if frameworks, methods and practices for\nsoftware and system development taught at HEIs reflect industrial practice. For\nthis, we conducted an online survey and collected information about 67 software\nengineering courses. Our findings show that development approaches taught at\nHEIs quite closely reflect industrial practice. We also found that the choice\nof what process to teach is sometimes driven by the wish to make a course\nsuccessful. Especially when this happens for project courses, it could be\nbeneficial to put more emphasis on building learning sequences with other\ncourses.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.05862,regular,pre_llm,2021,1,"{'ai_likelihood': 4.967053731282552e-07, 'text': ""GloBug: Using Global Data in Fault Localization\n\n  Fault Localization (FL) is an important first step in software debugging and\nis mostly manual in the current practice. Many methods have been proposed over\nyears to automate the FL process, including information retrieval (IR)-based\ntechniques. These methods localize the fault based on the similarity of the\nreported bug report and the source code. Newer variations of IR-based FL (IRFL)\ntechniques also look into the history of bug reports and leverage them during\nthe localization. However, all existing IRFL techniques limit themselves to the\ncurrent project's data (local data). In this study, we introduce Globug, which\nis an IRFL framework consisting of methods that use models pre-trained on the\nglobal data (extracted from open-source benchmark projects). In Globug, we\ninvestigate two heuristics: a) the effect of global data on a state-of-the-art\nIR-FL technique, namely BugLocator, and b) the application of a Word Embedding\ntechnique (Doc2Vec) together with global data. Our large scale experiment on 51\nsoftware projects shows that using global data improves BugLocator on average\n6.6% and 4.8% in terms of MRR (Mean Reciprocal Rank) and MAP (Mean Average\nPrecision), with over 14% in a majority (64% and 54% in terms of MRR and MAP,\nrespectively) of the cases. This amount of improvement is significant compared\nto the improvement rates that five other state-of-the-art IRFL tools provide\nover BugLocator. In addition, training the models globally is a one-time\noffline task with no overhead on BugLocator's run-time fault localization. Our\nstudy, however, shows that a Word Embedding-based global solution did not\nfurther improve the results.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.12384,regular,pre_llm,2021,1,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'The significance of user-defined identifiers in Java source code\n  authorship identification\n\n  When writing source code, programmers have varying levels of freedom when it\ncomes to the creation and use of identifiers. Do they habitually use the same\nidentifiers, names that are different to those used by others? Is it then\npossible to tell who the author of a piece of code is by examining these\nidentifiers? If so, can we use the presence or absence of identifiers to assist\nin correctly classifying programs to authors? Is it possible to hide the\nprovenance of programs by identifier renaming? In this study, we assess the\nimportance of three types of identifiers in source code author classification\nfor two different Java program data sets. We do this through a sequence of\nexperiments in which we disguise one type of identifier at a time. These\nexperiments are performed using as a tool the Source Code Author Profiles\n(SCAP) method. The results show that, although identifiers when examined as a\nwhole do not seem to reflect program authorship for these data sets, when\nexamined separately there is evidence that class names do signal the author of\nthe program. In contrast, simple variables and method names used in Java\nprograms do not appear to reflect program authorship. On the contrary, our\nanalysis suggests that such identifiers are so common as to mask authorship. We\nbelieve that these results have applicability in relation to the robustness of\ncode plagiarism analysis and that the underlying methods could be valuable in\ncases of litigation arising from disputes over program authorship.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.02108,regular,pre_llm,2021,1,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Design of Secure Coding Challenges for Cybersecurity Education in the\n  Industry\n\n  According to a recent survey with more than 4000 software developers, less\nthan half of developers can spot security holes. As a result, software products\npresent a low-security quality expressed by vulnerabilities that can be\nexploited by cyber-criminals. This lack of quality and security is particularly\ndangerous if the software which contains the vulnerabilities is deployed in\ncritical infrastructures. Serious games, and in particular,\nCapture-the-Flag(CTF) events, have shown promising results in improving secure\ncoding awareness of software developers in the industry. The challenges in the\nCTF event, to be useful, must be adequately designed to address the target\ngroup. This paper presents novel contributions by investigating which challenge\ntypes are adequate to improve software developers\' ability to write secure code\nin an industrial context. We propose 1) six challenge types usable in the\nindustry context, and 2) a structure for the CTF challenges. Our investigation\nalso presents results on 3) how to include hints and penalties into the\ncyber-security challenges. We evaluated our work through a survey with security\nexperts. While our results show that ""traditional"" challenge types seem to be\nadequate, they also reveal a new class of challenges based on code entry and\ninteraction with an automated coach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.10658,review,pre_llm,2021,1,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Software Effort Estimation Accuracy Prediction of Machine Learning\n  Techniques: A Systematic Performance Evaluation\n\n  Software effort estimation accuracy is a key factor in effective planning,\ncontrolling and to deliver a successful software project within budget and\nschedule. The overestimation and underestimation both are the key challenges\nfor future software development, henceforth there is a continuous need for\naccuracy in software effort estimation (SEE). The researchers and practitioners\nare striving to identify which machine learning estimation technique gives more\naccurate results based on evaluation measures, datasets and the other relevant\nattributes. The authors of related research are generally not aware of\npreviously published results of machine learning effort estimation techniques.\nThe main aim of this study is to assist the researchers to know which machine\nlearning technique yields the promising effort estimation accuracy prediction\nin the software development. In this paper, the performance of the machine\nlearning ensemble technique is investigated with the solo technique based on\ntwo most commonly used accuracy evaluation metrics. We used the systematic\nliterature review methodology proposed by Kitchenham and Charters. This\nincludes searching for the most relevant papers, applying quality assessment\ncriteria, extracting data and drawing results. We have evaluated a\nstate-of-the-art accuracy performance of 28 selected studies (14 ensemble, 14\nsolo) using Mean Magnitude of Relative Error (MMRE) and PRED (25) as a set of\nreliable accuracy metrics for performance evaluation of accuracy among two\ntechniques to report the research questions stated in this study. We found that\nmachine learning techniques are the most frequently implemented in the\nconstruction of ensemble effort estimation (EEE) techniques. The results of\nthis study revealed that the EEE techniques usually yield a promising\nestimation accuracy than the solo techniques.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.0143,regular,pre_llm,2021,1,"{'ai_likelihood': 1.655684577094184e-06, 'text': 'Why Developers Refactor Source Code: A Mining-based Study\n\n  Refactoring aims at improving code non-functional attributes without\nmodifying its external behavior. Previous studies investigated the motivations\nbehind refactoring by surveying developers. With the aim of generalizing and\ncomplementing their findings, we present a large-scale study quantitatively and\nqualitatively investigating why developers perform refactoring in open source\nprojects. First, we mine 287,813 refactoring operations performed in the\nhistory of 150 systems. Using this dataset, we investigate the interplay\nbetween refactoring operations and process (e.g., previous changes/fixes) and\nproduct (e.g., quality metrics) metrics. Then, we manually analyze 551 merged\npull requests implementing refactoring operations and classify the motivations\nbehind the implemented refactorings (e.g., removal of code duplication). Our\nresults led to (i) quantitative evidence of the relationship existing between\ncertain process/product metrics and refactoring operations and (ii) a detailed\ntaxonomy, generalizing and complementing the ones existing in the literature,\nof motivations pushing developers to refactor source code.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.01487,regular,pre_llm,2021,1,"{'ai_likelihood': 2.7120113372802734e-05, 'text': ""The use of incentives to promote Technical Debt management\n\n  When developing software, it is vitally important to keep the level of\ntechnical debt down since it is well established from several studies that\ntechnical debt can, e.g., lower the development productivity, decrease the\ndevelopers' morale, and compromise the overall quality of the software.\nHowever, even if researchers and practitioners working in today's software\ndevelopment industry are quite familiar with the concept of technical debt and\nits related negative consequences, there has been no empirical research\nfocusing specifically on how software managers actively communicate and manage\nthe need to keep the level of technical debt as low as possible.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.01298,review,pre_llm,2021,1,"{'ai_likelihood': 1.8543667263454863e-06, 'text': 'A Taxonomy for Mining and Classifying Privacy Requirements in Issue\n  Reports\n\n  Context: Digital and physical trails of user activities are collected over\nthe use of software applications and systems. As software becomes ubiquitous,\nprotecting user privacy has become challenging. With the increase of user\nprivacy awareness and advent of privacy regulations and policies, there is an\nemerging need to implement software systems that enhance the protection of\npersonal data processing. However, existing data protection and privacy\nregulations provide key principles in high-level, making it difficult for\nsoftware engineers to design and implement privacy-aware systems. Objective: In\nthis paper, we develop a taxonomy that provides a comprehensive set of privacy\nrequirements based on four well-established personal data protection\nregulations and privacy frameworks, the General Data Protection Regulation\n(GDPR), ISO/IEC 29100, Thailand Personal Data Protection Act (Thailand PDPA)\nand Asia-Pacific Economic Cooperation (APEC) privacy framework. Methods: These\nrequirements are extracted, refined and classified (using the goal-based\nrequirements analysis method) into a level that can be used to map with issue\nreports. We have also performed a study on how two large open-source software\nprojects (Google Chrome and Moodle) address the privacy requirements in our\ntaxonomy through mining their issue reports. Results: The paper discusses how\nthe collected issues were classified, and presents the findings and insights\ngenerated from our study. Conclusion: Mining and classifying privacy\nrequirements in issue reports can help organisations be aware of their state of\ncompliance by identifying privacy requirements that have not been addressed in\ntheir software projects. The taxonomy can also trace back to regulations,\nstandards and frameworks that the software projects have not complied with\nbased on the identified privacy requirements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.02085,review,pre_llm,2021,1,"{'ai_likelihood': 7.28501213921441e-07, 'text': ""Awareness of Secure Coding Guidelines in the Industry -- A first data\n  analysis\n\n  Software needs to be secure, in particular, when deployed to critical\ninfrastructures. Secure coding guidelines capture practices in industrial\nsoftware engineering to ensure the security of code. This study aims to assess\nthe level of awareness of secure coding in industrial software engineering, the\nskills of software developers to spot weaknesses in software code, avoid them,\nand the organizational support to adhere to coding guidelines. The approach\ndraws on well-established theories of policy compliance, neutralization theory,\nand security-related stress and the authors' many years of experience in\nindustrial software engineering and on lessons identified from training secure\ncoding in the industry. The paper presents the questionnaire design for the\nonline survey and the first analysis of data from the pilot study.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.12393,review,pre_llm,2021,1,"{'ai_likelihood': 2.8808911641438803e-06, 'text': ""Causal Factors, Benefits and Challenges of Test-Driven Development:\n  Practitioner Perceptions\n\n  This report describes the experiences of one organization's adoption of Test\nDriven Development (TDD) practices as part of a medium-term software project\nemploying Extreme Programming as a methodology. Three years into this project\nthe team's TDD experiences are compared with their non-TDD experiences on other\nongoing projects. The perceptions of the benefits and challenges of using TDD\nin this context are gathered through five semi-structured interviews with key\nteam members. Their experiences indicate that use of TDD has generally been\npositive and the reasons for this are explored to deepen the understanding of\nTDD practice and its effects on code quality, application quality and\ndevelopment productivity. Lessons learned are identified to aid others with the\nadoption and implementation of TDD practices, and some potential further\nresearch areas are suggested.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.02817,regular,pre_llm,2021,1,"{'ai_likelihood': 3.6093923780653213e-06, 'text': 'Faster SAT Solving for Software with Repeated Structures (with Case\n  Studies on Software Test Suite Minimization)\n\n  Theorem provers has been used extensively in software engineering for\nsoftware testing or verification. However, software is now so large and complex\nthat additional architecture is needed to guide theorem provers as they try to\ngenerate test suites. The SNAP test suite generator (introduced in this paper)\ncombines the Z3 theorem prover with the following tactic: cluster some\ncandidate tests, then search for valid tests by proposing small mutations to\nthe cluster centroids. This technique effectively removes repeated structures\nin the tests since many repeated structures can be replaced with one centroid.\nIn practice, SNAP is remarkably effective. For 27 real-world programs with up\nto half a million variables, SNAP found test suites which were 10 to 750\nsmaller times than those found by the prior state-of-the-art. Also, SNAP ran\norders of magnitude faster and (unlike prior work) generated 100% valid tests.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.03116,regular,pre_llm,2021,2,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Worst-Case Execution Time Calculation for Query-Based Monitors by\n  Witness Generation\n\n  Runtime monitoring plays a key role in the assurance of modern intelligent\ncyber-physical systems, which are frequently data-intensive and\nsafety-critical. While graph queries can serve as an expressive yet formally\nprecise specification language to capture the safety properties of interest,\nthere are no timeliness guarantees for such auto-generated runtime monitoring\nprograms, which prevents their use in a real-time setting. While worst-case\nexecution time (WCET) bounds derived by existing static WCET estimation\ntechniques are safe, they may not be tight as they are unable to exploit\ndomain-specific (semantic) information about the input models. This paper\npresents a semantic-aware WCET analysis method for data-driven monitoring\nprograms derived from graph queries. The method incorporates results obtained\nfrom low-level timing analysis into the objective function of a modern graph\nsolver. This allows the systematic generation of input graph models up to a\nspecified size (referred to as witness models) for which the monitor is\nexpected to take the most time to complete. Hence the estimated execution time\nof the monitors on these graphs can be considered as safe and tight WCET.\nAdditionally, we perform a set of experiments with query-based programs running\non a real-time platform over a set of generated models to investigate the\nrelationship between execution times and their estimates, and compare WCET\nestimates produced by our approach with results from two well-known timing\nanalyzers, aiT and OTAWA.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.06666,review,pre_llm,2021,2,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'What helped, and what did not? An Evaluation of the Strategies to\n  Improve Continuous Integration\n\n  Continuous integration (CI) is a widely used practice in modern software\nengineering. Unfortunately, it is also an expensive practice - Google and\nMozilla estimate their CI systems in millions of dollars. There are a number of\ntechniques and tools designed to or having the potential to save the cost of CI\nor expand its benefit - reducing time to feedback. However, their benefits in\nsome dimensions may also result in drawbacks in others. They may also be\nbeneficial in other scenarios where they are not designed to help. In this\npaper, we perform the first exhaustive comparison of techniques to improve CI,\nevaluating 14 variants of 10 techniques using selection and prioritization\nstrategies on build and test granularity. We evaluate their strengths and\nweaknesses with 10 different cost and time-tofeedback saving metrics on 100\nreal-world projects. We analyze the results of all techniques to understand the\ndesign decisions that helped different dimensions of benefit. We also\nsynthesized those results to lay out a series of recommendations for the\ndevelopment of future research techniques to advance this area.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.05913,regular,pre_llm,2021,2,"{'ai_likelihood': 5.298190646701389e-07, 'text': 'RobOT: Robustness-Oriented Testing for Deep Learning Systems\n\n  Recently, there has been a significant growth of interest in applying\nsoftware engineering techniques for the quality assurance of deep learning (DL)\nsystems. One popular direction is deep learning testing, where adversarial\nexamples (a.k.a.~bugs) of DL systems are found either by fuzzing or guided\nsearch with the help of certain testing metrics. However, recent studies have\nrevealed that the commonly used neuron coverage metrics by existing DL testing\napproaches are not correlated to model robustness. It is also not an effective\nmeasurement on the confidence of the model robustness after testing. In this\nwork, we address this gap by proposing a novel testing framework called\nRobustness-Oriented Testing (RobOT). A key part of RobOT is a quantitative\nmeasurement on 1) the value of each test case in improving model robustness\n(often via retraining), and 2) the convergence quality of the model robustness\nimprovement. RobOT utilizes the proposed metric to automatically generate test\ncases valuable for improving model robustness. The proposed metric is also a\nstrong indicator on how well robustness improvement has converged through\ntesting. Experiments on multiple benchmark datasets confirm the effectiveness\nand efficiency of RobOT in improving DL model robustness, with 67.02% increase\non the adversarial robustness that is 50.65% higher than the state-of-the-art\nwork DeepGini.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.08502,regular,pre_llm,2021,2,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'Automatic API Usage Scenario Documentation from Technical Q&A Sites\n\n  The online technical Q&A site Stack Overflow (SO) is popular among developers\nto support their coding and diverse development needs. To address shortcomings\nin API official documentation resources, several research has thus focused on\naugmenting official API documentation with insights (e.g., code examples) from\nSO. The techniques propose to add code examples/insights about APIs into its\nofficial documentation. Reviews are opinionated sentences with\npositive/negative sentiments. However, we are aware of no previous research\nthat attempts to automatically produce API documentation from SO by considering\nboth API code examples and reviews. In this paper, we present two novel\nalgorithms that can be used to automatically produce API documentation from SO\nby combining code examples and reviews towards those examples. The first\nalgorithm is called statistical documentation, which shows the distribution of\npositivity and negativity around the code examples of an API using different\nmetrics (e.g., star ratings). The second algorithm is called concept-based\ndocumentation, which clusters similar and conceptually relevant usage\nscenarios. An API usage scenario contains a code example, a textual description\nof the underlying task addressed by the code example, and the reviews (i.e.,\nopinions with positive and negative sentiments) from other developers towards\nthe code example. We deployed the algorithms in Opiner, a web-based platform to\naggregate information about APIs from online forums. We evaluated the\nalgorithms by mining all Java JSON-based posts in SO and by conducting three\nuser studies based on produced documentation from the posts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.12267,regular,pre_llm,2021,2,"{'ai_likelihood': 2.6490953233506944e-06, 'text': 'OSS PESTO: An Open Source Software Project Evaluation and Selection TOol\n\n  Open source software (OSS), playing an increasingly critical role nowadays,\nhas been commonly adopted and integrated in various software products. For many\npractitioners, selecting and adopting suitable OSS can help them greatly.\nThough many studies have been conducted on proposing OSS evaluation and\nselection models, a limited number are followed and used in the industry.\nMeanwhile, many existing OSS evaluation tools, though providing valuable\ndetails, fall short on offering intuitive suggestions in terms of\nframework-supported evaluation factors. Towards filling the gap, we propose an\nOpen Source Software Project Evaluation and Selection TOol (OSS PESTO).\nTargeting OSS on Github, the largest OSS source code host, it facilitates the\nevaluation practice by enabling practitioners to compare candidates therein in\nterms of selected OSS evaluation models. It also allows in-time Github data\ncollection and customized evaluation that enriches its effectiveness and ease\nof use.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.02938,regular,pre_llm,2021,2,"{'ai_likelihood': 8.17908181084527e-06, 'text': 'The Impact of Sampling and Rule Set Size on Generated Fuzzy Inference\n  System Predictive Accuracy: Analysis of a Software Engineering Data Set\n\n  Software project management makes extensive use of predictive modeling to\nestimate product size, defect proneness and development effort. Although\nuncertainty is acknowledged in these tasks, fuzzy inference systems, designed\nto cope well with uncertainty, have received only limited attention in the\nsoftware engineering domain. In this study we empirically investigate the\nimpact of two choices on the predictive accuracy of generated fuzzy inference\nsystems when applied to a software engineering data set: sampling of\nobservations for training and testing; and the size of the rule set generated\nusing fuzzy c-means clustering. Over ten samples we found no consistent pattern\nof predictive performance given certain rule set size. We did find, however,\nthat a rule set compiled from multiple samples generally resulted in more\naccurate predictions than single sample rule sets. More generally, the results\nprovide further evidence of the sensitivity of empirical analysis outcomes to\nspecific model-building decisions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.09747,regular,pre_llm,2021,2,"{'ai_likelihood': 2.7848614586724176e-05, 'text': ""Prioritize Crowdsourced Test Reports via Deep Screenshot Understanding\n\n  Crowdsourced testing is increasingly dominant in mobile application (app)\ntesting, but it is a great burden for app developers to inspect the incredible\nnumber of test reports. Many researches have been proposed to deal with test\nreports based only on texts or additionally simple image features. However, in\nmobile app testing, texts contained in test reports are condensed and the\ninformation is inadequate. Many screenshots are included as complements that\ncontain much richer information beyond texts. This trend motivates us to\nprioritize crowdsourced test reports based on a deep screenshot understanding.\n  In this paper, we present a novel crowdsourced test report prioritization\napproach, namely DeepPrior. We first represent the crowdsourced test reports\nwith a novelly introduced feature, namely DeepFeature, that includes all the\nwidgets along with their texts, coordinates, types, and even intents based on\nthe deep analysis of the app screenshots, and the textual descriptions in the\ncrowdsourced test reports. DeepFeature includes the Bug Feature, which directly\ndescribes the bugs, and the Context Feature, which depicts the thorough context\nof the bug. The similarity of the DeepFeature is used to represent the test\nreports' similarity and prioritize the crowdsourced test reports. We formally\ndefine the similarity as DeepSimilarity. We also conduct an empirical\nexperiment to evaluate the effectiveness of the proposed technique with a large\ndataset group. The results show that DeepPrior is promising, and it outperforms\nthe state-of-the-art approach with less than half the overhead.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.08495,review,pre_llm,2021,2,"{'ai_likelihood': 1.6887982686360678e-06, 'text': ""Understanding How and Why Developers Seek and Analyze API-related\n  Opinions\n\n  With the advent and proliferation of online developer forums as informal\ndocumentation, developers often share their opinions about the APIs they use.\nThus, opinions of others often shape the developer's perception and decisions\nrelated to software development. For example, the choice of an API or how to\nreuse the functionality the API offers are, to a considerable degree,\nconditioned upon what other developers think about the API. While many\ndevelopers refer to and rely on such opinion-rich information about APIs, we\nfound little research that investigates the use and benefits of public\nopinions. To understand how developers seek and evaluate API opinions, we\nconducted two surveys involving a total of 178 software developers. We analyzed\nthe data in two dimensions, each corresponding to specific needs related to API\nreviews: (1) Needs for seeking API reviews, and (2) Needs for automated tool\nsupport to assess the reviews. We observed that developers seek API reviews and\noften have to summarize those for diverse development needs (e.g., API\nsuitability). Developers also make conscious efforts to judge the\ntrustworthiness of the provided opinions and believe that automated tool\nsupport for API reviews analysis can assist in diverse development scenarios,\nincluding, for example, saving time in API selection as well as making informed\ndecisions on a particular API features.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.00701,review,pre_llm,2021,2,"{'ai_likelihood': 5.2551428476969406e-05, 'text': ""Search-Based Software Re-Modularization: A Case Study at Adyen\n\n  Deciding what constitutes a single module, what classes belong to which\nmodule or the right set of modules for a specific software system has always\nbeen a challenging task. The problem is even harder in large-scale software\nsystems composed of thousands of classes and hundreds of modules. Over the\nyears, researchers have been proposing different techniques to support\ndevelopers in re-modularizing their software systems. In particular, the\nsearch-based software re-modularization is an active research topic within the\nsoftware engineering community for more than 20 years.\n  This paper describes our efforts in applying search-based software\nre-modularization approaches at Adyen, a large-scale payment company. Adyen's\ncode base has 5.5M+ lines of code, split into around hundreds of modules. We\nleveraged the existing body of knowledge in the field to devise our own search\nalgorithm and applied it to our code base. Our results show that search-based\napproaches scale to large code bases as ours. Our algorithm can find solutions\nthat improve the code base according to the metrics we optimize for, and\ndevelopers see value in the recommendations. Based on our experiences, we then\nlist a set of challenges and opportunities for future researchers, aiming at\nmaking search-based software re-modularization more efficient for large-scale\nsoftware companies.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.06355,regular,pre_llm,2021,2,"{'ai_likelihood': 4.006756676567926e-06, 'text': 'Same File, Different Changes: The Potential of Meta-Maintenance on\n  GitHub\n\n  Online collaboration platforms such as GitHub have provided software\ndevelopers with the ability to easily reuse and share code between\nrepositories. With clone-and-own and forking becoming prevalent, maintaining\nthese shared files is important, especially for keeping the most up-to-date\nversion of reused code. Different to related work, we propose the concept of\nmeta-maintenance -- i.e., tracking how the same files evolve in different\nrepositories with the aim to provide useful maintenance opportunities to those\nfiles. We conduct an exploratory study by analyzing repositories from seven\ndifferent programming languages to explore the potential of meta-maintenance.\nOur results indicate that a majority of active repositories on GitHub contains\nat least one file which is also present in another repository, and that a\nsignificant minority of these files are maintained differently in the different\nrepositories which contain them. We manually analyzed a representative sample\nof shared files and their variants to understand which changes might be useful\nfor meta-maintenance. Our findings support the potential of meta-maintenance\nand open up avenues for future work to capitalize on this potential.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.10222,regular,pre_llm,2021,2,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Merly.jl: Web Framework in Julia\n\n  Merly.jl is a package for creating web applications in Julia. It presents\nfeatures such as the creation of endpoints with function notation and with\nmacro notation, handling of static files, use of path parameters, processing of\ndata sent by a web client in the body in a personalized way, handling of CORS\nand compatibility with the use of middleware. It presents a familiar syntax\nwith the rest of the most popular web frameworks without neglecting the\nexecution performance. This manuscript mentions the operation and main features\nof Merly.jl\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.06909,regular,pre_llm,2021,2,"{'ai_likelihood': 5.629327562120226e-07, 'text': ""Why Security Defects Go Unnoticed during Code Reviews? A Case-Control\n  Study of the Chromium OS Project\n\n  Peer code review has been found to be effective in identifying security\nvulnerabilities. However, despite practicing mandatory code reviews, many Open\nSource Software (OSS) projects still encounter a large number of post-release\nsecurity vulnerabilities, as some security defects escape those. Therefore, a\nproject manager may wonder if there was any weakness or inconsistency during a\ncode review that missed a security vulnerability. Answers to this question may\nhelp a manager pinpointing areas of concern and taking measures to improve the\neffectiveness of his/her project's code reviews in identifying security\ndefects. Therefore, this study aims to identify the factors that differentiate\ncode reviews that successfully identified security defects from those that\nmissed such defects. With this goal, we conduct a case-control study of\nChromium OS project. Using multi-stage semi-automated approaches, we build a\ndataset of 516 code reviews that successfully identified security defects and\n374 code reviews where security defects escaped. The results of our empirical\nstudy suggest that the are significant differences between the categories of\nsecurity defects that are identified and that are missed during code reviews. A\nlogistic regression model fitted on our dataset achieved an AUC score of 0.91\nand has identified nine code review attributes that influence identifications\nof security defects. While time to complete a review, the number of mutual\nreviews between two developers, and if the review is for a bug fix have\npositive impacts on vulnerability identification, opposite effects are observed\nfrom the number of directories under review, the number of total reviews by a\ndeveloper, and the total number of prior commits for the file under review.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.06662,regular,pre_llm,2021,2,"{'ai_likelihood': 1.7219119601779514e-06, 'text': 'Qualifying Software Engineers Undergraduates in DevOps -- Challenges of\n  Introducing Technical and Non-technical Concepts in a Project-oriented Course\n\n  The constant changes in the software industry, practices, and methodologies\nimpose challenges to teaching and learning current software engineering\nconcepts and skills. DevOps is particularly challenging because it covers\ntechnical concepts, such as pipeline automation, and non-technical ones, such\nas team roles and project management. The present study investigates a course\nsetup to introduce these concepts to software engineering undergraduates. We\ndesigned the course by employing coding to associate DevOps concepts to Agile,\nLean, and Open source practices and tools. We present the main aspects of this\nproject-oriented DevOps course, with 240 students enrolled in it since its\nfirst offering in 2016. We conducted an empirical study, with both a\nquantitative and qualitative analysis, to evaluate this project-oriented course\nsetup. We collected the data from the projects repository and students\nperceptions from a questionnaire. We mined 148 repositories (corresponding to\n72 projects) and obtained 86 valid responses to the questionnaire. We also\nmapped the concepts which are more challenging to students learn from\nexperience. The results evidence that first-hand experience facilitates the\ncomprehension of DevOps concepts and enriches classes discussions. We present a\nset of lessons learned, which may help professors better design and conduct\nproject-oriented courses to cover DevOps concepts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.04287,regular,pre_llm,2021,2,"{'ai_likelihood': 4.967053731282552e-07, 'text': ""Operation is the hardest teacher: estimating DNN accuracy looking for\n  mispredictions\n\n  Deep Neural Networks (DNN) are typically tested for accuracy relying on a set\nof unlabelled real world data (operational dataset), from which a subset is\nselected, manually labelled and used as test suite. This subset is required to\nbe small (due to manual labelling cost) yet to faithfully represent the\noperational context, with the resulting test suite containing roughly the same\nproportion of examples causing misprediction (i.e., failing test cases) as the\noperational dataset. However, while testing to estimate accuracy, it is\ndesirable to also learn as much as possible from the failing tests in the\noperational dataset, since they inform about possible bugs of the DNN. A smart\nsampling strategy may allow to intentionally include in the test suite many\nexamples causing misprediction, thus providing this way more valuable inputs\nfor DNN improvement while preserving the ability to get trustworthy unbiased\nestimates. This paper presents a test selection technique (DeepEST) that\nactively looks for failing test cases in the operational dataset of a DNN, with\nthe goal of assessing the DNN expected accuracy by a small and ''informative''\ntest suite (namely with a high number of mispredictions) for subsequent DNN\nimprovement. Experiments with five subjects, combining four DNN models and\nthree datasets, are described. The results show that DeepEST provides DNN\naccuracy estimates with precision close to (and often better than) those of\nexisting sampling-based DNN testing techniques, while detecting from 5 to 30\ntimes more mispredictions, with the same test suite size.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.10432,regular,pre_llm,2021,2,"{'ai_likelihood': 5.629327562120226e-06, 'text': ""CyberSecurity Challenges: Serious Games for Awareness Training in\n  Industrial Environments\n\n  Awareness of cybersecurity topics, e.g., related to secure coding guidelines,\nenables software developers to write secure code. This awareness is vital in\nindustrial environments for the products and services in critical\ninfrastructures. In this work, we introduce and discuss a new serious game\ndesigned for software developers in the industry. This game addresses software\ndevelopers' needs and is shown to be well suited for raising secure coding\nawareness of software developers in the industry. Our work results from the\nexperience of the authors gained in conducting more than ten CyberSecurity\nChallenges in the industry. The presented game design, which is shown to be\nwell accepted by software developers, is a novel alternative to traditional\nclassroom training. We hope to make a positive impact in the industry by\nimproving the cybersecurity of products at their early production stages.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.12393,regular,pre_llm,2021,2,"{'ai_likelihood': 8.940696716308594e-07, 'text': 'Modelling a CubeSat-based Space Mission and its Operation\n\n  Since the early 2000\' years, the CubeSats have been growing and getting more\nand more ""space"" in the Space industry. Their short development schedule, low\ncost equipment and piggyback launches create a new way to access the space,\nprovide new services and enable the development of new technologies for\nprocesses and applications. That is the case of the Verification and Validation\nof these missions. As they are cheaper to launch than traditional space\nmissions, CubeSats win by numbers. With more than 1000 CubeSats launched they\nstill achieve less than 50% rate of successful missions and that is caused\nmainly by poor V&V processes. Model Based approaches are trying to help in\nthese problems as they help software developers along the last years. As\ncomplex systems, space products can be helped by the introduction of models in\ndifferent levels. Operational goals can be achieved by modeling behavioral\nscenarios and simulating operational procedures. Here, we present a possible\nmodeling solution using a tool that integrates the functionalities of FSM and\nStatechartes, the ATOM SysVAP (System for Validation of Finite Automatons and\nExecution Plans). With this tool we are able to model the behaviour of a space\nmission, from its top level (i.e. system and segments) to its low level\n(subsystems) and simulate their interactions (operation). With the help of Lua\nProgramming Language, it is possible to generate analysis files, specific\nscenarios and control internal variables.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.10431,review,pre_llm,2021,2,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Raising Secure Coding Awareness for Software Developers in the Industry\n\n  Many industrial IT security standards and policies mandate the usage of a\nsecure coding methodology in the software development process. This implies two\ndifferent aspects: first, secure coding must be based on a set of secure coding\nguidelines, and second software developers must be aware of these secure coding\npractices. On the one side, secure coding guidelines seems a bit like a\nblack-art: while there exist abstract guidelines that are widely accepted,\nlow-level secure coding guidelines for different programming languages are\nscarce.\n  On the other side, once a set of secure coding guidelines is chosen, a good\nmethodology is needed to make them known by the people which should be using\nthem, i.e. software developers.\n  Motivated both by the secure coding requirements from industry standards and\nalso by the mandate to train staff on IT security by the global industry\ninitiative ""Charter of Trust"", this paper presents an overview of important\nresearch questions on how to choose secure coding guidelines and on how to\nraise software developer awareness for secure coding using serious games.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.06588,review,pre_llm,2021,2,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'A taxonomy for quality in simulation-based development and testing of\n  automated driving systems\n\n  Ensuring the quality of automated driving systems is a major challenge the\nautomotive industry is facing. In this context, quality defines the degree to\nwhich an object meets expectations and requirements. Especially, automated\nvehicles at SAE level 4 and 5 will be expected to operate safely in various\ncontexts and complex situations without misconduct. Thus, a systematic approach\nis needed to show their safe operation. A way to address this challenge is\nsimulation-based testing as pure physical testing is not feasible. During\nsimulation-based testing, the data used to evaluate the actual quality of an\nautomated driving system are generated using a simulation. However, to rely on\nthese simulation data, the overall simulation, which also includes its\nsimulation models, must provide a certain quality level. This quality level\ndepends on the intended purpose for which the generated simulation data should\nbe used. Therefore, three categories of quality can be considered: quality of\nthe automated driving system and simulation quality, consisting of simulation\nmodel quality and scenario quality. Hence, quality must be determined and\nevaluated in various process steps in developing and testing automated driving\nsystems, the overall simulation, and the simulation models used for the\nsimulation. In this paper, we propose a taxonomy to serve a better\nunderstanding of the concept of quality in the development and testing process\nto have a clear separation and insight where further testing is needed -- both\nin terms of automated driving systems and simulation, including their\nsimulation models and scenarios used for testing.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.09154,regular,pre_llm,2021,2,"{'ai_likelihood': 3.973642985026042e-07, 'text': ""Categorising Software Contexts: Research-in-Progress\n\n  A growing number of researchers suggest that software process must be\ntailored to a project's context to achieve maximal performance. Researchers\nhave studied 'context' in an ad-hoc way, with focus on those contextual factors\nthat appear to be of significance. The result is that we have no useful basis\nupon which to contrast and compare studies. We are currently researching a\ntheoretical basis for software context for the purpose of tailoring and note\nthat a deeper consideration of the meaning of the term 'context' is required\nbefore we can proceed. In this paper, we examine the term and present a model\nbased on insights gained from our initial categorisation of contextual factors\nfrom the literature. We test our understanding by analysing a further six\ndocuments. Our contribution thus far is a model that we believe will support a\ntheoretical operationalisation of software context for the purpose of process\ntailoring.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.054,regular,pre_llm,2021,2,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Iterative and Scenario-based Requirements Specification in a System of\n  Systems Context\n\n  [Context&Motivation]Due to the managerial ,operational and evolutionary\nindependence of constituent systems (CSs) in a System of Systems (SoS) context,\ntop-down and linear requirements engineering (RE) approaches are insufficient.\nRE techniques for SoS must support iterating, changing, synchronizing, and\ncommunicating requirements across different abstraction and hierarchy levels as\nwell as scopes of responsibility. [Question/Problem] We address the challenge\nof SoS requirements specification, where requirements can describe the SoS\nbehavior, but also the behavior of CSs that are developed independently.\n[Principal Ideas] To support the requirements specification in an SoS\nenvironment, we propose a scenario-based and iterative specification technique.\nThis allows requirements engineers to continuously model and jointly execute\nand test the system behavior for the SoS and the CS in order to detect\ncontradictions in the requirement specifications at an early stage.\n[Contribution] In this paper, we describe an extension for the\nscenario-modeling language for Kotlin (SMLK) to continuously and formally model\nrequirements on SoS and CS level. To support the iterative requirements\nspecification and modeling we combine SMLK with agile development techniques.\nWe demonstrate the applicability of our approach with the help of an example\nfrom the field of e-mobility.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.03331,review,pre_llm,2021,3,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Secure Software Development in the Era of Fluid Multi-party Open\n  Software and Services\n\n  Pushed by market forces, software development has become fast-paced. As a\nconsequence, modern development projects are assembled from 3rd-party\ncomponents. Security & privacy assurance techniques once designed for large,\ncontrolled updates over months or years, must now cope with small, continuous\nchanges taking place within a week, and happening in sub-components that are\ncontrolled by third-party developers one might not even know they existed. In\nthis paper, we aim to provide an overview of the current software security\napproaches and evaluate their appropriateness in the face of the changed nature\nin software development. Software security assurance could benefit by switching\nfrom a process-based to an artefact-based approach. Further, security\nevaluation might need to be more incremental, automated and decentralized. We\nbelieve this can be achieved by supporting mechanisms for lightweight and\nscalable screenings that are applicable to the entire population of software\ncomponents albeit there might be a price to pay.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.06768,regular,pre_llm,2021,3,"{'ai_likelihood': 9.702311621771918e-06, 'text': 'CiRA: A Tool for the Automatic Detection of Causal Relationships in\n  Requirements Artifacts\n\n  Requirements often specify the expected system behavior by using causal\nrelations (e.g., If A, then B). Automatically extracting these relations\nsupports, among others, two prominent RE use cases: automatic test case\nderivation and dependency detection between requirements. However, existing\ntools fail to extract causality from natural language with reasonable\nperformance. In this paper, we present our tool CiRA (Causality detection in\nRequirements Artifacts), which represents a first step towards automatic\ncausality extraction from requirements. We evaluate CiRA on a publicly\navailable data set of 61 acceptance criteria (causal: 32; non-causal: 29)\ndescribing the functionality of the German Corona-Warn-App. We achieve a macro\nF_1 score of 83%, which corroborates the feasibility of our approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.09471,regular,pre_llm,2021,3,"{'ai_likelihood': 2.3146470387776695e-05, 'text': 'An Integration Test Order Strategy to Consider Control Coupling\n\n  Integration testing is a very important step in software testing. Existing\nmethods evaluate the stubbing cost for class integration test orders by\nconsidering only the interclass direct relationships such as inheritance,\naggregation, and association, but they omit the interclass indirect\nrelationship caused by control coupling, which can also affect the test orders\nand the stubbing cost. In this paper, we introduce an integration test order\nstrategy to consider control coupling. We advance the concept of transitive\nrelationship to describe this kind of interclass dependency and propose a new\nmeasurement method to estimate the complexity of control coupling, which is the\ncomplexity of stubs created for a transitive relationship. We evaluate our\nintegration test order strategy on 10 programs on various scales. The results\nshow that considering the transitive relationship when generating class\nintegration test orders can significantly reduce the stubbing cost for most\nprograms and that our integration test order strategy obtains satisfactory\nresults more quickly than other methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.08648,regular,pre_llm,2021,3,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Does the duration of rapid release cycles affect the bug handling\n  activity?\n\n  Software projects are regularly updated with new functionality and bug fixes\nthrough so-called releases. In recent years, many software projects have been\nshifting to shorter release cycles and this can affect the bug handling\nactivity. Past research has focused on the impact of switching from traditional\nto rapid release cycles with respect to bug handling activity, but the effect\nof the rapid release cycle duration has not yet been studied. We empirically\ninvestigate releases of 420 open source projects with rapid release cycles to\nunderstand the effect of variable and rapid release cycle durations on bug\nhandling activity. We group the releases of these projects into five categories\nof release cycle durations. For each project, we investigate how the sequence\nof releases is related to bug handling activity metrics and we study the effect\nof the variability of cycle durations on bug fixing. Our results did not reveal\nany statistically significant difference for the studied bug handling activity\nmetrics in the presence of variable rapid release cycle durations. This\nsuggests that the duration of fast release cycles does not seem to impact bug\nhandling activity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.10126,regular,pre_llm,2021,3,"{'ai_likelihood': 2.2186173333062066e-06, 'text': 'Interpretation-enabled Software Reuse Detection Based on a Multi-Level\n  Birthmark Model\n\n  Software reuse, especially partial reuse, poses legal and security threats to\nsoftware development. Since its source codes are usually unavailable, software\nreuse is hard to be detected with interpretation. On the other hand, current\napproaches suffer from poor detection accuracy and efficiency, far from\nsatisfying practical demands. To tackle these problems, in this paper, we\npropose \\textit{ISRD}, an interpretation-enabled software reuse detection\napproach based on a multi-level birthmark model that contains function level,\nbasic block level, and instruction level. To overcome obfuscation caused by\ncross-compilation, we represent function semantics with Minimum Branch Path\n(MBP) and perform normalization to extract core semantics of instructions. For\nefficiently detecting reused functions, a process for ""intent search based on\nanchor recognition"" is designed to speed up reuse detection. It uses strict\ninstruction match and identical library call invocation check to find anchor\nfunctions (in short anchors) and then traverses neighbors of the anchors to\nexplore potentially matched function pairs. Extensive experiments based on two\nreal-world binary datasets reveal that \\textit{ISRD} is interpretable,\neffective, and efficient, which achieves $97.2\\%$ precision and $94.8\\%$\nrecall. Moreover, it is resilient to cross-compilation, outperforming\nstate-of-the-art approaches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.0802,review,pre_llm,2021,3,"{'ai_likelihood': 4.172325134277344e-06, 'text': ""Exploring motivation and teamwork in a large software engineering\n  capstone course during the coronavirus pandemic\n\n  In the spring of 2020, the Department of Informatics covered a 20 ECTS\ncapstone course in Software Engineering, mainly focusing on developing a\ncomplex application. The course used active learning methods, and 240 students\nwere working in 42 cross-functional, agile teams. The pandemic caused by the\ncoronavirus had a significant impact on the teaching given by the University of\nOslo, as all physical education and collaboration among the teams had to be\ndigital from March 12. At the end of the semester, we conducted a survey that\nfocused on 1) aspects of teamwork (e.g., communication and coordination in the\nteams) and the relation to team performance (e.g., the application product) and\n2), the students' motivation and ability to cooperate through digital\nplatforms. A total of 151 respondents in 41 agile student teams answered the\nsurvey. This study aimed to investigate how the teamwork and motivation of the\nstudents were affected by having to work virtually. The results are compared to\nresults from the same course in 2019 and a similar survey on 71 professional\nteams published in 2016. Our results show that the teamwork was evaluated\nsimilarly to both the evaluation of survey conducted in 2019 and on the\nprofessional teams in 2016. The motivation among the students remained high,\neven though they had to collaborate virtually.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.09672,regular,pre_llm,2021,3,"{'ai_likelihood': 1.1987156338161893e-05, 'text': ""DUETS: A Dataset of Reproducible Pairs ofJava Library-Clients\n\n  Software engineering researchers look for software artifacts to study their\ncharacteristics or to evaluate new techniques. In this paper, we introduce\nDUETS, a new dataset of software libraries and their clients. This dataset can\nbe exploited to gain many different insights, such as API usage, usage inputs,\nor novel observations about the test suites of clients and libraries. DUETS is\nmeant to support both static and dynamic analysis. This means that the\nlibraries and the clients compile correctly, they are executable and their test\nsuites pass. The dataset is composed of open-source projects that have more\nthan five stars on GitHub. The final dataset contains 395 libraries and 2,874\nclients. Additionally, we provide the raw data that we use to create this\ndataset, such as 34,560 pom.xml files or the complete file list from 34,560\nprojects. This dataset can be used to study how libraries are used by their\nclients or as a list of software projects that successfully build. The client's\ntest suite can be used as an additional verification step for code\ntransformation techniques that modify the libraries.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.09314,regular,pre_llm,2021,3,"{'ai_likelihood': 1.7219119601779514e-06, 'text': ""iContractBot: A Chatbot for Smart Contracts' Specification and Code\n  Generation\n\n  Recently, Blockchain technology adoption has expanded to many application\nareas due to the evolution of smart contracts. However, developing smart\ncontracts is non-trivial and challenging due to the lack of tools and expertise\nin this field. A promising solution to overcome this issue is to use\nModel-Driven Engineering (MDE), however, using models still involves a learning\ncurve and might not be suitable for non-technical users. To tackle this\nchallenge, chatbot or conversational interfaces can be used to assess the\nnon-technical users to specify a smart contract in gradual and interactive\nmanner.\n  In this paper, we propose iContractBot, a chatbot for modeling and developing\nsmart contracts. Moreover, we investigate how to integrate iContractBot with\niContractML, a domain-specific modeling language for developing smart\ncontracts, and instantiate intention models from the chatbot. The iContractBot\nframework provides a domain-specific language (DSL) based on the user intention\nand performs model-to-text transformation to generate the smart contract code.\nA smart contract use case is presented to demonstrate how iContractBot can be\nutilized for creating models and generating the deployment artifacts for smart\ncontracts based on a simple conversation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.03473,regular,pre_llm,2021,3,"{'ai_likelihood': 1.6225708855523005e-06, 'text': 'Towards a standardised strategy to collect and distribute application\n  software artifacts\n\n  Reference sets contain known content that are used to identify relevant or\nfilter irrelevant content. Application profiles are a type of reference set\nthat contain digital artifacts associated with application software. An\napplication profile can be compared against a target data set to identify\nrelevant evidence of application usage in a variety of investigation scenarios.\nThe research objective is to design and implement a standardised strategy to\ncollect and distribute application software artifacts using application\nprofiles. An advanced technique for creating application profiles was designed\nusing a formalised differential analysis strategy. The design was implemented\nin a live differential forensic analysis tool, LiveDiff, to automate and\nsimplify data collection. A storage mechanism was designed based on a\npreviously standardised forensic data abstraction. The design was implemented\nin a new data abstraction, Application Profile XML (APXML), to provide storage,\ndistribution and automated processing of collected artifacts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.05118,regular,pre_llm,2021,3,"{'ai_likelihood': 1.8543667263454863e-06, 'text': 'Efficient Fuzz Testing for Apache Spark Using Framework Abstraction\n\n  The emerging data-intensive applications are increasingly dependent on\ndata-intensive scalable computing (DISC) systems, such as Apache Spark, to\nprocess large data. Despite their popularity, DISC applications are hard to\ntest. In recent years, fuzz testing has been remarkably successful; however, it\nis nontrivial to apply such traditional fuzzing to big data analytics directly\nbecause: (1) the long latency of DISC systems prohibits the applicability of\nfuzzing, and (2) conventional branch coverage is unlikely to identify\napplication logic from the DISC framework implementation. We devise a novel\nfuzz testing tool called BigFuzz that automatically generates concrete data for\nan input Apache Spark program. The key essence of our approach is that we\nabstract the dataflow behavior of the DISC framework with executable\nspecifications and we design schema-aware mutations based on common error types\nin DISC applications. Our experiments show that compared to random fuzzing,\nBigFuzz is able to speed up the fuzzing time by 1477X, improves application\ncode coverage by 271%, and achieves 157% improvement in detecting application\nerrors. The demonstration video of BigFuzz is available at\nhttps://www.youtube.com/watch?v=YvYQISILQHs&feature=youtu.be.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.10074,review,pre_llm,2021,3,"{'ai_likelihood': 3.344482845730252e-06, 'text': 'Blockchain Testing: Challenges, Techniques, and Research Directions\n\n  Specific testing solutions targeting blockchain-based software are gaining\nhuge attention as blockchain technologies are being increasingly incorporated\ninto enterprise systems. As blockchain-based software enters production\nsystems, it is paramount to follow proper engineering practices, ensure the\nrequired level of testing, and assess the readiness of the developed system.\nThe existing research aims at addressing the testing-related issues and\nchallenges of engineering blockchain-based software by providing suitable\ntechniques and tools. However, like any emerging discipline, the best practices\nand tools for testing blockchain-based systems are not yet sufficiently\ndeveloped. In this paper, we provide a comprehensive survey on the testing of\nBlockchain-based Applications (BC-Apps). First, we provide a discussion on\nidentified challenges that are associated with BCApp testing. Second, we use a\nlayered approach to discuss the state-of-the-art testing efforts in the area of\nBC technologies. In particular, we present an overview of the existing testing\ntools and techniques that provide testing solutions either for different\ncomponents at various layers of the BC-App stack or across the whole stack.\nThird, we provide a set of future research directions based on the identified\nBC testing challenges and gaps in the literature review of existing testing\nsolutions for BC-Apps. Moreover, we reflect on the specificity of BC-based\nsoftware development procedure, which makes some of the existing tools or\ntechniques inadequate, and call for the definition of standardised testing\nprocedures and techniques for BC-Apps. The aim of our study is to highlight the\nimportance of BC-based software testing and to pave the way for disciplined,\ntestable, and verifiable BC software development.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.0701,regular,pre_llm,2021,3,"{'ai_likelihood': 1.0596381293402778e-06, 'text': ""Combining Dynamic Analysis and Visualization to Explore the Distribution\n  of Unit Test Suites\n\n  As software systems have grown in scale and complexity the test suites built\nalongside those systems have also become increasingly complex. Understanding\nkey aspects of test suites, such as their coverage of production code, is\nimportant when maintaining or reengineering systems. This work investigates the\ndistribution of unit tests in Open Source Software (OSS) systems through the\nvisualization of data obtained from both dynamic and static analysis. Our\nlong-term aim is to support developers in their understanding of test\ndistribution and the relationship of tests to production code. We first obtain\ndynamic coupling information from five selected OSS systems and we then map the\ntest and production code results. The mapping is shown in graphs that depict\nboth the dependencies between classes and static test information. We analyze\nthese graphs using Centrality metrics derived from graph theory and SNA. Our\nfindings suggest that, for these five systems at least, unit test and dynamic\ncoupling information 'do not match', in that unit tests do not appear to be\ndistributed in line with the systems' dynamic coupling. We contend that, by\nmapping dynamic coupling data onto unit test information, and through the use\nof software metrics and visualization, we can locate central system classes and\nidentify to which classes unit testing effort has (or has not) been dedicated.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.16169,regular,pre_llm,2021,3,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'Modelling Quantum Circuits with UML\n\n  None of the quantum computing applications imagined will ever become a\nreality without quantum software. Quantum programmes have, to date, been coded\nwith ad hoc techniques. Researchers in the field of quantum software\nengineering are, therefore, now demanding more systematic techniques and\nmethods with which to produce software with sufficient quality. One of the\nchallenges and lessons learned from classic software engineering is the need\nfor high-level, abstract and technology-independent representations with which\nto design software before it is coded. This paper specifically addresses this\nchallenge for quantum software design. Since UML is a well-proven modelling\nlanguage that has been widely employed by industry for some time, we propose a\nUML extension for the representation of quantum algorithms. Our proposal\ncomprises the definition of a UML profile based on various stereotypes that can\nbe applied to the existing UML activity diagrams in order to represent quantum\ncircuits. The advantage of this representation is that UML quantum circuits can\nbe interrelated with other UML elements and diagrams, which will make it\npossible to represent various concerns and viewpoints of the so-called hybrid\ninformation systems. This will consequently enable classical and quantum\naspects to be modelled together in integrated designs in a\ntechnological-agnostic manner that is already supported by a considerable\nnumber of existing software design tools.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.07189,regular,pre_llm,2021,3,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'Does mutation testing improve testing practices?\n\n  Various proxy metrics for test quality have been defined in order to guide\ndevelopers when writing tests. Code coverage is particularly well established\nin practice, even though the question of how coverage relates to test quality\nis a matter of ongoing debate. Mutation testing offers a promising alternative:\nArtificial defects can identify holes in a test suite, and thus provide\nconcrete suggestions for additional tests. Despite the obvious advantages of\nmutation testing, it is not yet well established in practice. Until recently,\nmutation testing tools and techniques simply did not scale to complex systems.\nAlthough they now do scale, a remaining obstacle is lack of evidence that\nwriting tests for mutants actually improves test quality. In this paper we aim\nto fill this gap: By analyzing a large dataset of almost 15 million mutants, we\ninvestigate how these mutants influenced developers over time, and how these\nmutants relate to real faults. Our analyses suggest that developers using\nmutation testing write more tests, and actively improve their test suites with\nhigh quality tests such that fewer mutants remain. By analyzing a dataset of\npast fixes of real high-priority faults, our analyses further provide evidence\nthat mutants are indeed coupled with real faults. In other words, had mutation\ntesting been used for the changes introducing the faults, it would have\nreported a live mutant that could have prevented the bug.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.05055,regular,pre_llm,2021,3,"{'ai_likelihood': 3.311369154188368e-08, 'text': ""A Case Study of Onboarding in Software Teams: Tasks and Strategies\n\n  Developers frequently move into new teams or environments across software\ncompanies. Their onboarding experience is correlated with productivity, job\nsatisfaction, and other short-term and long-term outcomes. The majority of the\nonboarding process comprises engineering tasks such as fixing bugs or\nimplementing small features. Nevertheless, we do not have a systematic view of\nhow tasks influence onboarding. In this paper, we present a case study of\nMicrosoft, where we interviewed 32 developers moving into a new team and 15\nengineering managers onboarding a new developer into their team -- to\nunderstand and characterize developers' onboarding experience and expectations\nin relation to the tasks performed by them while onboarding. We present how\ntasks interact with new developers through three representative themes:\nlearning, confidence building, and socialization. We also discuss three\nonboarding strategies as inferred from the interviews that managers commonly\nuse unknowingly, and discuss their pros and cons and offer situational\nrecommendations. Furthermore, we triangulate our interview findings with a\ndeveloper survey ($N=189$) and a manager survey ($N=37$) and find that survey\nresults suggest that our findings are representative and our recommendations\nare actionable. Practitioners could use our findings to improve their\nonboarding processes, while researchers could find new research directions from\nthis study to advance the understanding of developer onboarding. Our research\ninstruments and anonymous data are available at\n\\url{https://zenodo.org/record/4455937#.YCOQCs_0lFd}\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.11339,review,pre_llm,2021,3,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'Escaping the Time Pit: Pitfalls and Guidelines for Using Time-Based Git\n  Data\n\n  Many software engineering research papers rely on time-based data (e.g.,\ncommit timestamps, issue report creation/update/close dates, release dates).\nLike most real-world data however, time-based data is often dirty. To date,\nthere are no studies that quantify how frequently such data is used by the\nsoftware engineering research community, or investigate sources of and quantify\nhow often such data is dirty. Depending on the research task and method used,\nincluding such dirty data could affect the research results. This paper\npresents the first survey of papers that utilize time-based data, published in\nthe Mining Software Repositories (MSR) conference series. Out of the 690\ntechnical track and data papers published in MSR 2004--2020, we saw at least\n35% of papers utilized time-based data. We then used the Boa and Software\nHeritage infrastructures to help identify and quantify several sources of dirty\ncommit timestamp data. Finally we provide guidelines/best practices for\nresearchers utilizing time-based data from Git repositories.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.11481,review,pre_llm,2021,3,"{'ai_likelihood': 6.291601392957899e-07, 'text': 'How do we Evaluate Self-adaptive Software Systems?\n\n  With the increase of research in self-adaptive systems, there is a need to\nbetter understand the way research contributions are evaluated. Such insights\nwill support researchers to better compare new findings when developing new\nknowledge for the community. However, so far there is no clear overview of how\nevaluations are performed in self-adaptive systems. To address this gap, we\nconduct a mapping study. The study focuses on experimental evaluations\npublished in the last decade at the prime venue of research in software\nengineering for self-adaptive systems -- the International Symposium on\nSoftware Engineering for Adaptive and Self-Managing Systems (SEAMS). Results\npoint out that specifics of self-adaptive systems require special attention in\nthe experimental process, including the distinction of the managing system\n(i.e., the target of evaluation) and the managed system, the presence of\nuncertainties that affect the system behavior and hence need to be taken into\naccount in data analysis, and the potential of managed systems to be reused\nacross experiments, beyond replications. To conclude, we offer a set of\nsuggestions derived from our study that can be used as input to enhance future\nexperiments in self-adaptive systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.16538,regular,pre_llm,2021,3,"{'ai_likelihood': 8.145968119303387e-06, 'text': ""Integra\\c{c}\\~ao e Entrega Cont\\'inua para aplica\\c{c}\\~oes m\\'oveis\n  desenvolvidas em React Native\n\n  Continuous integration and continuous delivery are not new for developers who\ncreate web applications, however in the development of mobile applications this\npractice is still not very common mainly because of the challenges during the\nprocess of distributing the application. In the face of the growing number of\napplications, a greater requirement for quality and ever-shorter delivery\ntimes, delivering a healthy code is often extremely important to keep up with\nthe competition. The purpose of this work is to implement an integration and\ncontinuous delivery pipeline for mobile applications developed in React Native.\nIt intends to automate the process of build and delivery of applications\ndeveloped with this technology.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.12218,regular,pre_llm,2021,3,"{'ai_likelihood': 2.6490953233506944e-06, 'text': 'Bug or not bug? That is the question\n\n  Nowadays, development teams often rely on tools such as Jira or Bugzilla to\nmanage backlogs of issues to be solved to develop or maintain software.\nAlthough they relate to many different concerns (e.g., bug fixing, new feature\ndevelopment, architecture refactoring), few means are proposed to identify and\nclassify these different kinds of issues, except for non mandatory labels that\ncan be manually associated to them. This may lead to a lack of issue\nclassification or to issue misclassification that may impact automatic issue\nmanagement (planning, assignment) or issue-derived metrics. Automatic issue\nclassification thus is a relevant topic for assisting backlog management. This\npaper proposes a binary classification solution for discriminating bug from non\nbug issues. This solution combines natural language processing (TF-IDF) and\nclassification (multi-layer perceptron) techniques, selected after comparing\ncommonly used solutions to classify issues. Moreover, hyper-parameters of the\nneural network are optimized using a genetic algorithm. The obtained results,\nas compared to existing works on a commonly used benchmark, show significant\nimprovements on the F1 measure for all datasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.02384,regular,pre_llm,2021,3,"{'ai_likelihood': 5.695554945203993e-06, 'text': 'How to Identify Boundary Conditions with Contrasty Metric?\n\n  The boundary conditions (BCs) have shown great potential in requirements\nengineering because a BC captures the particular combination of circumstances,\ni.e., divergence, in which the goals of the requirement cannot be satisfied as\na whole. Existing researches have attempted to automatically identify lots of\nBCs. Unfortunately, a large number of identified BCs make assessing and\nresolving divergences expensive. Existing methods adopt a coarse-grained\nmetric, generality, to filter out less general BCs. However, the results still\nretain a large number of redundant BCs since a general BC potentially captures\nredundant circumstances that do not lead to a divergence. Furthermore, the\nlikelihood of BC can be misled by redundant BCs resulting in costly repeatedly\nassessing and resolving divergences.\n  In this paper, we present a fine-grained metric to filter out the redundant\nBCs. We first introduce the concept of contrasty of BC. Intuitively, if two BCs\nare contrastive, they capture different divergences. We argue that a set of\ncontrastive BCs should be recommended to engineers, rather than a set of\ngeneral BCs that potentially only indicates the same divergence. Then we design\na post-processing framework (PPAc) to produce a set of contrastive BCs after\nidentifying BCs. Experimental results show that the contrasty metric\ndramatically reduces the number of BCs recommended to engineers. Results also\ndemonstrate that lots of BCs identified by the state-of-the-art method are\nredundant in most cases. Besides, to improve efficiency, we propose a joint\nframework (JAc) to interleave assessing based on the contrasty metric with\nidentifying BCs. The primary intuition behind JAc is that it considers the\nsearch bias toward contrastive BCs during identifying BCs, thereby pruning the\nBCs capturing the same divergence. Experiments confirm the improvements of JAc\nin identifying contrastive BCs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.07435,review,pre_llm,2021,4,"{'ai_likelihood': 8.079740736219619e-06, 'text': 'Student and Faculty Adviser Insights in an Agile Methodology Integrated\n  Filipino Company-Sponsored I.T. Capstone Program\n\n  To improve the Information Technology (I.T.) graduate skill set, students\nneed to be immersed in as realistic a software development environment as\npossible. In continuing our work on integrating Agile Methodology into the\nCapstone Program of our Bachelor of Science in I.T. (BSIT) degree program, this\npaper discusses the student challenges and difficulties during the software\ndevelopment project, and provides recommendations on improving the student\noverall learning process in such a program. We collected survey data from the\nwhole population of 90 BSITstudents across four academic years about their\nexperience with their client and the Capstone Program itself. Conceptual\ncontent analysis was then applied to discover and describe underlying themes.\nAlso, faculty advisers were tasked with writing about their interactions,\nthoughts, and observations on their respective student group advisees. These\nshowed issues with time management, communication, and competency. Also, groups\nthat excelled exhibited better team coordination and a complete grasp of the\nAgile methodology. For future implementations, clearer task definition and\nreducing the skill gaps are necessary for better execution.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.08993,regular,pre_llm,2021,4,"{'ai_likelihood': 2.5828679402669272e-06, 'text': 'Carrot and Stick approaches revisited when managing Technical Debt in an\n  educational context\n\n  Technical Debt management is an important aspect in the training of Software\nEngineering students. In this paper we study the effect of two assessment\nstrategies in an educational context: One based on penalisation, the other\nbased on rewards. Both are applied to assignments where the students develop a\nproject focusing on keeping a low technical debt level, and obtaining a high\nquality code. We describe the design, tools and context of the strategies\napplied. SonarQube, a tool commonly used in production environments, is used\nfor measuring the metrics. The penalisation strategy is based on a SonarQube\nquality gate. The reward strategy is based on a contest, where an automatic\njudge tool is devised to provide an online leaderboard with a classification\nbased on the SonarQube metrics. An empirical study is conducted to determine\nwhich of the strategies works better to help the students/trainees keep the\nTechnical Debt low. Statistically significant results are obtained in 5 of the\n8 analysed metrics, showing that the reward strategy works much better. The\neffect size of the executed statistical tests is analysed, resulting in medium\nand large effect size in the majority of the analysed metrics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.0933,review,pre_llm,2021,4,"{'ai_likelihood': 3.642506069607205e-07, 'text': ""What's behind tight deadlines? Business causes of technical debt\n\n  What are the business causes behind tight deadlines? What drives the\nprioritization of features that pushes quality matters to the back burner? We\nconducted a survey with 71 experienced practitioners and did a thematic\nanalysis of the open-ended answers to the question: ``Could you give examples\nof how business may contribute to technical debt?'' Business-related causes\nwere organized into two categories: pure-business and business/IT gap, and they\nwere related to `tight deadlines' and `features over quality', the most\nfrequently cited management reasons for technical debt. We contribute a\ncause-effect model which relates the various business causes of tight deadlines\nand the behavior of prioritizing features over quality aspects.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.06132,regular,pre_llm,2021,4,"{'ai_likelihood': 9.934107462565105e-08, 'text': ""An Agent-based Architecture for AI-Enhanced Automated Testing for XR\n  Systems, a Short Paper\n\n  This short paper presents an architectural overview of an agent-based\nframework called iv4XR for automated testing that is currently under\ndevelopment by an H2020 project with the same name. The framework's intended\nmain use case of is testing the family of Extended Reality (XR) based systems\n(e.g. 3D games, VR sytems, AR systems), though the approach can indeed be\nadapted to target other types of interactive systems. The framework is unique\nin that it is an agent-based system. Agents are inherently reactive, and\ntherefore are arguably a natural match to deal with interactive systems.\nMoreover, it is also a natural vessel for mounting and combining different AI\ncapabilities, e.g. reasoning, navigation, and learning.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.06033,regular,pre_llm,2021,4,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Not All Requirements Prioritization Criteria Are Equal at All Times: A\n  Quantitative Analysis\n\n  Requirement prioritization is recognized as an important decision-making\nactivity in requirements engineering and software development. Requirement\nprioritization is applied to determine which requirements should be implemented\nand released. In order to prioritize requirements, there are several\napproaches/techniques/tools that use different requirements prioritization\ncriteria, which are often identified by gut feeling instead of an in-depth\nanalysis of which criteria are most important to use. Therefore, in this study\nwe investigate which requirements prioritization criteria are most important to\nuse in industry when determining which requirements are implemented and\nreleased, and if the importance of the criteria change depending on how far a\nrequirement has reached in the development process. We conducted a quantitative\nstudy of one completed project from one software developing company by\nextracting 32,139 requirements prioritization decisions based on eight\nrequirements prioritization criteria for 11,110 requirements. The results show\nthat not all requirements prioritization criteria are equally important, and\nthis change depending on how far a requirement has reached in the development\nprocess.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.14323,regular,pre_llm,2021,4,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'The Behavioral Diversity of Java JSON Libraries\n\n  JSON is an essential file and data format in do-mains that span scientific\ncomputing, web APIs or configuration management. Its popularity has motivated\nsignificant software development effort to build multiple libraries to process\nJSON data. Previous studies focus on performance comparison among these\nlibraries and lack a software engineering perspective.We present the first\nsystematic analysis and comparison of the input / output behavior of 20 JSON\nlibraries, in a single software ecosystem: Java/Maven. We assess behavior\ndiversity by running each library against a curated set of 473 JSON files,\nincluding both well-formed and ill-formed files. The main design differences,\nwhich influence the behavior of the libraries, relate to the choice of data\nstructure to represent JSON objects and to the encoding of numbers. We observe\na remarkable behavioral diversity with ill-formed files, or corner cases such\nas large numbers or duplicate data. Our unique behavioral assessment of JSON\nlibraries paves the way for a robust processing of ill-formed files, through a\nmulti-version architecture.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.03476,regular,pre_llm,2021,4,"{'ai_likelihood': 1.3245476616753472e-06, 'text': ""Secure Software Engineering in the Financial Services: A Practitioners'\n  Perspective\n\n  Secure software engineering is a fundamental activity in modern software\ndevelopment. However, while the field of security research has been advancing\nquite fast, in practice, there is still a vast knowledge gap between the\nsecurity experts and the software development teams. After all, we cannot\nexpect developers and other software practitioners to be security experts.\nUnderstanding how software development teams incorporate security in their\nprocesses and the challenges they face is a step towards reducing this gap. In\nthis paper, we study how financial services companies ensure the security of\ntheir software systems. To that aim, we performed a qualitative study based on\nsemi-structured interviews with 16 software practitioners from 11 different\nfinancial companies in three continents. Our results shed light on the security\nconsiderations that practitioners take during the different phases of their\nsoftware development processes, the different security practices that software\nteams make use of to ensure the security of their software systems, the\nimprovements that practitioners perceive as important in existing\nstate-of-the-practice security tools, the different knowledge-sharing and\nlearning practices that developers use to learn more about software security,\nand the challenges that software practitioners currently face when it comes to\nsecure their systems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.08432,review,pre_llm,2021,4,"{'ai_likelihood': 3.5762786865234375e-06, 'text': 'Architectural Archipelagos: Technical Debt in Long-Lived Software\n  Research Platforms\n\n  This paper identifies a model of software evolution that is prevalent in\nlarge, long-lived academic research tool suites (3L-ARTS). This model results\nin an ""archipelago"" of related but haphazardly organized architectural\n""islands"", and inherently induces technical debt. We illustrate the archipelago\nmodel with examples from two 3L-ARTS archipelagos identified in literature.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.03824,review,pre_llm,2021,4,"{'ai_likelihood': 4.470348358154297e-06, 'text': ""On The Gap Between Software Maintenance Theory and Practitioners'\n  Approaches\n\n  The way practitioners perform maintenance tasks in practice is little known\nby researchers. In turn, practitioners are not always up to date with the\nproposals provided by the research community. This work investigates the gap\nbetween software maintenance techniques proposed by the research community and\nthe software maintenance practice. We carried out a survey with 112\npractitioners from 92 companies and 12 countries. We concentrate on analyzing\nif and how practitioners understand and apply the following subjects: bad\nsmells, refactoring, software metrics, and change impact analysis. This study\nshows that there is a large gap between research approaches and industry\npractice in those subjects, especially in change impact analysis and software\nmetrics.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.04385,review,pre_llm,2021,4,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Memory Error Detection in Security Testing\n\n  We study 10 C/C++ projects that have been using a static analysis security\ntesting tool. We analyze the historical scan reports generated by the tool and\nstudy how frequently memory-related alerts appeared. We also studied the\nsubsequent developer action on those alerts. We also look at the CVEs published\nfor these projects within the study timeline and investigate how many of them\nare memory related. Moreover, for one of this project, Linux, we investigate if\nthe involved flaws in the CVE were identified by the studied security tool when\nthey were first introduced in the code. We found memory related alerts to be\nfrequently detected during static analysis security testing. However, based on\nhow actively the project developers are monitoring the tool alerts, these\nerrors can take years to get fixed. For the ten studied projects, we found a\nmedian lifespan of 77 days before memory alerts get fixed. We also find that\naround 40% of the published CVEs for the studied C/C++ projects are related to\nmemory. These memory CVEs have higher CVSS severity ratings and likelihood of\nhaving an exploit script public than non-memory CVEs. We also found only 2.5%\nLinux CVEs were possibly detected during static analysis security testing.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.12349,regular,pre_llm,2021,4,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'Revisiting the size effect in software fault prediction models\n\n  BACKGROUND: In object oriented (OO) software systems, class size has been\nacknowledged as having an indirect effect on the relationship between certain\nartifact characteristics, captured via metrics, and faultproneness, and\ntherefore it is recommended to control for size when designing fault prediction\nmodels. AIM: To use robust statistical methods to assess whether there is\nevidence of any true effect of class size on fault prediction models. METHOD:\nWe examine the potential mediation and moderation effects of class size on the\nrelationships between OO metrics and number of faults. We employ regression\nanalysis and bootstrapping-based methods to investigate the mediation and\nmoderation effects in two widely-used datasets comprising seventeen systems.\nRESULTS: We find no strong evidence of a significant mediation or moderation\neffect of class size on the relationships between OO metrics and faults. In\nparticular, size appears to have a more significant mediation effect on CBO and\nFan-out than other metrics, although the evidence is not consistent in all\nexamined systems. On the other hand, size does appear to have a significant\nmoderation effect on WMC and CBO in most of the systems examined. Again, the\nevidence provided is not consistent across all examined systems CONCLUSION: We\nare unable to confirm if class size has a significant mediation or moderation\neffect on the relationships between OO metrics and the number of faults. We\ncontend that class size does not fully explain the relationships between OO\nmetrics and the number of faults, and it does not always affect the\nstrength/magnitude of these relationships. We recommend that researchers\nconsider the potential mediation and moderation effect of class size when\nbuilding their prediction models, but this should be examined independently for\neach system.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.02414,regular,pre_llm,2021,4,"{'ai_likelihood': 1.2252065870496963e-06, 'text': ""On Adaptive Fairness in Software Systems\n\n  Software systems are increasingly making decisions on behalf of humans,\nraising concerns about the fairness of such decisions. Such concerns are\nusually attributed to flaws in algorithmic design or biased data, but we argue\nthat they are often the result of a lack of explicit specification of fairness\nrequirements. However, such requirements are challenging to elicit, a problem\nexacerbated by increasingly dynamic environments in which software systems\noperate, as well as stakeholders' changing needs. Therefore, capturing all\nfairness requirements during the production of software is challenging, and is\ninsufficient for addressing software changes post deployment. In this paper, we\npropose adaptive fairness as a means for maintaining the satisfaction of\nchanging fairness requirements. We demonstrate how to combine\nrequirements-driven and resource-driven adaptation in order to address\nvariabilities in both fairness requirements and their associated resources.\nUsing models for fairness requirements, resources, and their relations, we show\nhow the approach can be used to provide systems owners and end-users with\ncapabilities that reflect adaptive fairness behaviours at runtime. We\ndemonstrate our approach using an example drawn from shopping experiences of\ncitizens. We conclude with a discussion of open research challenges in the\nengineering of adaptive fairness in human-facing software systems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.00196,regular,pre_llm,2021,4,"{'ai_likelihood': 9.967221154106988e-06, 'text': 'Modular Tree Network for Source Code Representation Learning\n\n  Learning representation for source code is a foundation of many program\nanalysis tasks. In recent years, neural networks have already shown success in\nthis area, but most existing models did not make full use of the unique\nstructural information of programs. Although abstract syntax tree-based neural\nmodels can handle the tree structure in the source code, they cannot capture\nthe richness of different types of substructure in programs. In this paper, we\npropose a modular tree network (MTN) which dynamically composes different\nneural network units into tree structures based on the input abstract syntax\ntree. Different from previous tree-structural neural network models, MTN can\ncapture the semantic differences between types of ASTsubstructures. We evaluate\nour model on two tasks: program classification and code clone detection.\nOurmodel achieves the best performance compared with state-of-the-art\napproaches in both tasks, showing the advantage of leveraging more elaborate\nstructure information of the source code.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.00949,review,pre_llm,2021,4,"{'ai_likelihood': 0.0, 'text': 'Feature-Driven Survey of Physical Protection Systems\n\n  Many systems nowadays require protection against security or safety threats.\nA physical protection system (PPS) integrates people, procedures, and equipment\nto protect assets or facilities. PPSs have targeted various systems, including\nairports, rail transport, highways, hospitals, bridges, the electricity grid,\ndams, power plants, seaports, oil refineries, and water systems. Hence, PPSs\nare characterized by a broad set of features, from which part is common, while\nother features are variant and depend on the particular system to be developed.\nThe notion of PPS has been broadly addressed in the literature, and even\ndomain-specific PPS development methods have been proposed. However, the common\nand variant features are fragmented across many studies. This situation\nseriously impedes the identification of the required features and likewise the\nguidance of the systems engineering process of PPSs. To enhance the\nunderstanding and support the guidance of the development of PPS, in this\npaper, we provide a feature-driven survey of PPSs. The approach applies a\nsystematic domain analysis process based on the state-of-the-art of PPSs. It\npresents a family feature model that defines the common and variant features\nand herewith the configuration space of PPSs\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.13435,review,pre_llm,2021,4,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Grey Literature in Software Engineering: A Critical Review\n\n  Context: Grey Literature (GL) recently has grown in Software Engineering (SE)\nresearch since the increased use of online communication channels by software\nengineers. However, there is still a limited understanding of how SE research\nis taking advantage of GL. Objective: This research aimed to understand how SE\nresearchers use GL in their secondary studies. Method: We conducted a tertiary\nstudy of studies published between 2011 and 2018 in high-quality software\nengineering conferences and journals. We then applied qualitative and\nquantitative analysis to investigate 446 potential studies. Results: From the\n446 selected studies, 126 studies cited GL but only 95 of those used GL to\nanswer a specific research question representing almost 21% of all the 446\nsecondary studies. Interestingly, we identified that few studies employed\nspecific search mechanisms and used additional criteria for assessing GL.\nMoreover, by the time we conducted this research, 49% of the GL URLs are not\nworking anymore. Based on our findings, we discuss some challenges in using GL\nand potential mitigation plans. Conclusion: In this paper, we summarized the\nlast 10 years of software engineering research that uses GL, showing that GL\nhas been essential for bringing practical new perspectives that are scarce in\ntraditional literature. By drawing the current landscape of use, we also raise\nsome awareness of related challenges (and strategies to deal with them).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.1052,regular,pre_llm,2021,4,"{'ai_likelihood': 2.615981631808811e-06, 'text': 'Which Event Happened First? Deferred Choice on Blockchain Using Oracles\n\n  First come, first served: Critical choices between alternative actions are\noften made based on events external to an organization, and reacting promptly\nto their occurrence can be a major advantage over the competition. In Business\nProcess Management (BPM), such deferred choices can be expressed in process\nmodels, and they are an important aspect of process engines. Blockchain-based\nprocess execution approaches are no exception to this, but are severely limited\nby the inherent properties of the platform: The closed-world environment\nprevents direct access to external entities and data, and the passive runtime\nbased entirely on atomic transactions impedes continual monitoring and\ndetection of events. In this paper we provide an in-depth examination of the\nsemantics of deferred choice, and transfer them to environments such as the\nblockchain. We introduce and compare several oracle architectures able to\nsatisfy certain requirements, and show that they can be implemented using\nstate-of-the-art blockchain technology.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.04675,review,pre_llm,2021,4,"{'ai_likelihood': 7.185671064588759e-06, 'text': '2nd Workshop on Hybrid Development Approaches in Software Systems\n  Development\n\n  Software and system development is complex and diverse, and a multitude of\ndevelopment approaches is used and combined with each other to address the\nmanifold challenges companies face today. To study the current state of the\npractice and to build a sound understanding about the utility of different\ndevelopment approaches and their application to modern software system\ndevelopment, in 2016, we launched the HELENA initiative. This paper introduces\nthe 2nd HELENA workshop and provides an overview of the current project state.\nIn the workshop, six teams present initial findings from their regions, impulse\ntalk are given, and further steps of the HELENA roadmap are discussed.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.10865,regular,pre_llm,2021,4,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Predictive Mutation Analysis via Natural Language Channel in Source Code\n\n  Mutation analysis can provide valuable insights into both System Under Test\n(SUT) and its test suite. However, it is not scalable due to the cost of\nbuilding and testing a large number of mutants. Predictive Mutation Testing\n(PMT) has been proposed to reduce the cost of mutation testing, but it can only\nprovide statistical inference about whether a mutant will be killed or not by\nthe entire test suite. We propose Seshat, a Predictive Mutation Analysis (PMA)\ntechnique that can accurately predict the entire kill matrix, not just the\nmutation score of the given test suite. Seshat exploits the natural language\nchannel in code, and learns the relationship between the syntactic and semantic\nconcepts of each test case and the mutants it can kill, from a given kill\nmatrix. The learnt model can later be used to predict the kill matrices for\nsubsequent versions of the program, even after both the source and test code\nhave changed significantly. Empirical evaluation using the programs in the\nDefects4J shows that Seshat can predict kill matrices with the average F-score\nof 0.83 for versions that are up to years apart. This is an improvement of\nF-score by 0.14 and 0.45 point over the state-of-the-art predictive mutation\ntesting technique, and a simple coverage based heuristic, respectively. Seshat\nalso performs as well as PMT for the prediction of mutation scores only. Once\nSeshat trains its model using a concrete mutation analysis, the subsequent\npredictions made by Seshat are on average 39 times faster than actual\ntest-based analysis.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.13713,regular,pre_llm,2021,4,"{'ai_likelihood': 2.4172994825575088e-06, 'text': ""Individual Differences Limit Predicting Well-being and Productivity\n  Using Software Repositories: A Longitudinal Industrial Study\n\n  Reports of poor work well-being and fluctuating productivity in software\nengineering have been reported in both academic and popular sources.\nUnderstanding and predicting these issues through repository analysis might\nhelp manage software developers' well-being. Our objective is to link data from\nsoftware repositories, that is commit activity, communication, expressed\nsentiments, and job events, with measures of well-being obtained with a daily\nexperience sampling questionnaire. To achieve our objective, we studied a\nsingle software project team for eight months in the software industry.\nAdditionally, we performed semi-structured interviews to explain our results.\nThe acquired quantitative data are analyzed with generalized linear\nmixed-effects models with autocorrelation structure. We find that individual\nvariance accounts for most of the $R^2$ values in models predicting developers'\nexperienced well-being and productivity. In other words, using software\nrepository variables to predict developers' well-being or productivity is\nchallenging due to individual differences. Prediction models developed for each\ndeveloper individually work better, with fixed effects $R^2$ value of up to\n0.24. The semi-structured interviews give insights into the well-being of\nsoftware developers and the benefits of chat interaction. Our study suggests\nthat individualized prediction models are needed for well-being and\nproductivity prediction in software development.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.01537,regular,pre_llm,2021,4,"{'ai_likelihood': 1.5894571940104167e-06, 'text': ""Code Reviews with Divergent Review Scores: An Empirical Study of the\n  OpenStack and Qt Communities\n\n  Code review is a broadly adopted software quality practice where developers\ncritique each others' patches. In addition to providing constructive feedback,\nreviewers may provide a score to indicate whether the patch should be\nintegrated. Since reviewer opinions may differ, patches can receive both\npositive and negative scores. If reviews with divergent scores are not\ncarefully resolved, they may contribute to a tense reviewing culture and may\nslow down integration. In this paper, we study patches with divergent review\nscores in the OPENSTACK and QT communities. Quantitative analysis indicates\nthat patches with divergent review scores: (1) account for 15%-37% of patches\nthat receive multiple review scores; (2) are integrated more often than they\nare abandoned; and (3) receive negative scores after positive ones in 70% of\ncases. Furthermore, a qualitative analysis indicates that patches with strongly\ndivergent scores that: (4) are abandoned more often suffer from external issues\n(e.g., integration planning, content duplication) than patches with weakly\ndivergent scores and patches without divergent scores; and (5) are integrated\noften address reviewer concerns indirectly (i.e., without changing patches).\nOur results suggest that review tooling should integrate with release schedules\nand detect concurrent development of similar patches to optimize review\ndiscussions with divergent scores. Moreover, patch authors should note that\neven the most divisive patches are often integrated through discussion,\nintegration timing, and careful revision.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.14881,regular,pre_llm,2021,5,"{'ai_likelihood': 8.940696716308594e-07, 'text': 'CrossASR++: A Modular Differential Testing Framework for Automatic\n  Speech Recognition\n\n  Developers need to perform adequate testing to ensure the quality of\nAutomatic Speech Recognition (ASR) systems. However, manually collecting\nrequired test cases is tedious and time-consuming. Our recent work proposes\nCrossASR, a differential testing method for ASR systems. This method first\nutilizes Text-to-Speech (TTS) to generate audios from texts automatically and\nthen feed these audios into different ASR systems for cross-referencing to\nuncover failed test cases. It also leverages a failure estimator to find\nfailing test cases more efficiently. Such a method is inherently\nself-improvable: the performance can increase by leveraging more advanced TTS\nand ASR systems. So in this accompanying tool demo paper, we devote more\nengineering and propose CrossASR++, an easy-to-use ASR testing tool that can be\nconveniently extended to incorporate different TTS and ASR systems, and failure\nestimators. We also make CrossASR++ chunk texts from a given corpus dynamically\nand enable the estimator to work in a more effective and flexible way. We\ndemonstrate that the new features can help CrossASR++ discover more failed test\ncases. Using the same TTS and ASR systems, CrossASR++ can uncover 26.2% more\nfailed test cases for 4 ASRs than the original tool. Moreover, by simply adding\none more ASR for cross-referencing, we can increase the number of failed test\ncases uncovered for each of the 4 ASR systems by 25.07%, 39.63%, 20.9\\% and\n8.17% respectively. We also extend CrossASR++ with 5 additional failure\nestimators. Compared to worst estimator, the best one can discover 10.41% more\nfailed test cases within the same amount of time.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.09595,regular,pre_llm,2021,5,"{'ai_likelihood': 4.437234666612413e-06, 'text': ""Training Software Engineers for Qualitative Evaluation of Software\n  Architecture\n\n  A software architect uses quality requirements to design the architecture of\na system. However, it is essential to ensure that the system's final\narchitectural design achieves the standard quality requirements. The existing\narchitectural evaluation frameworks require basic skills and experience for\npractical usage, which novice software architects lack.\n  We propose a framework that enables novice software architects to infer the\nsystem's quality requirements and tactics using the software architectural\nblock-line diagram. The framework takes an image as input, extracts various\ncomponents and connections, and maps them to viable architectural patterns,\nfollowed by identifying the system's corresponding quality attributes (QAs) and\ntactics. The framework includes a specifically trained machine learning model\nbased on image processing and semantic similarity methods to assist software\narchitects in evaluating a given design by a) evaluating an input architectural\ndesign based on the architectural patterns present in it, b) lists out the\nstrengths and weaknesses of the design in terms of QAs, c) recommends the\nnecessary architectural tactics that can be embedded in the design to achieve\nthe lacking QAs.\n  To train our framework, we developed a dataset of 2,035 architectural images\nfrom fourteen architectural patterns such as Client-Server, Microservices, and\nModel View Controller, available at\nhttps://www.doi.org/10.6084/m9.figshare.14156408. The framework achieves a\nCorrect Recognition Rate of 98.71% in identifying the architectural patterns.\nWe evaluated the proposed framework's effectiveness and usefulness by using\ncontrolled and experimental groups, in which the experimental group performed\napproximately 150% better than the controlled group. The experiments were\nperformed as a part of the Masters of Computer Science course in an Engineering\nInstitution.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.07157,regular,pre_llm,2021,5,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'A Feature Table approach to decomposing monolithic applications into\n  microservices\n\n  Microservice architecture refers to the use of numerous small-scale and\nindependently deployed services, instead of encapsulating all functions into\none monolith. It has been a challenge in software engineering to decompose a\nmonolithic system into smaller parts. In this paper, we propose the Feature\nTable approach, a structured approach to service decomposition based on the\ncorrelation between functional features and microservices: (1) we defined the\nconcept of {\\em Feature Cards} and 12 instances of such cards; (2) we\nformulated {\\em Decomposition Rules} to decompose monolithic applications; (3)\nwe designed the {\\em Feature Table Analysis Tool} to provide semi-automatic\nanalysis for identification of microservices; and (4) we formulated {\\em\nMapping Rules} to help developers implement microservice candidates. We\nperformed a case study on Cargo Tracking System to validate our\nmicroservice-oriented decomposition approach. Cargo Tracking System is a\ntypical case that has been decomposed by other related methods (dataflow-driven\napproach, Service Cutter, and API Analysis). Through comparison with the\nrelated methods in terms of specific coupling and cohesion metrics, the results\nshow that the proposed Feature Table approach can deliver more reasonable\nmicroservice candidates, which are feasible in implementation with\nsemi-automatic support.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.08198,regular,pre_llm,2021,5,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'In Search of Socio-Technical Congruence: A Large-Scale Longitudinal\n  Study\n\n  We report on a large-scale empirical study investigating the relevance of\nsocio-technical congruence over key basic software quality metrics, namely,\nbugs and churn. In particular, we explore whether alignment or misalignment of\nsocial communication structures and technical dependencies in large software\nprojects influences software quality. To this end, we have defined a\nquantitative and operational notion of socio-technical congruence, which we\ncall socio-technical motif congruence (STMC). STMC is a measure of the degree\nto which developers working on the same file or on two related files, need to\ncommunicate. As socio-technical congruence is a complex and multi-faceted\nphenomenon, the interpretability of the results is one of our main concerns, so\nwe have employed a careful mixed-methods statistical analysis. In particular,\nwe provide analyses with similar techniques as employed by seminal work in the\nfield to ensure comparability of our results with the existing body of work.\nThe major result of our study, based on an analysis of 25 large open-source\nprojects, is that STMC is not related to project quality measures -- software\nbugs and churn -- in any temporal scenario. That is, we find no statistical\nrelationship between the alignment of developer tasks and developer\ncommunications on the one hand, and project outcomes on the other hand. We\nconclude that, wherefore congruence does matter as literature shows, then its\nmeasurable effect lies elsewhere.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.14179,review,pre_llm,2021,5,"{'ai_likelihood': 1.8212530348036025e-06, 'text': 'Investigating the Significance of Bellwether Effect to Improve Software\n  Effort Estimation\n\n  Bellwether effect refers to the existence of exemplary projects (called the\nBellwether) within a historical dataset to be used for improved prediction\nperformance. Recent studies have shown an implicit assumption of using recently\ncompleted projects (referred to as moving window) for improved prediction\naccuracy. In this paper, we investigate the Bellwether effect on software\neffort estimation accuracy using moving windows. The existence of the\nBellwether was empirically proven based on six postulations. We apply\nstatistical stratification and Markov chain methodology to select the\nBellwether moving window. The resulting Bellwether moving window is used to\npredict the software effort of a new project. Empirical results show that\nBellwether effect exist in chronological datasets with a set of exemplary and\nrecently completed projects representing the Bellwether moving window. Result\nfrom this study has shown that the use of Bellwether moving window with the\nGaussian weighting function significantly improve the prediction accuracy.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.00928,regular,pre_llm,2021,5,"{'ai_likelihood': 2.086162567138672e-06, 'text': 'Development of a software complex for the diagnosis of dentoalveolar\n  anomalies using neural networks\n\n  This article describes the goals and objectives of developing a software\ncomplex for planning the treatment of dentoalveolar anomalies, the architecture\nof the software complex as interacting components for treatment planning, as\nwell as the principle of using algorithms using convolutional neural networks\nwithin the software complex for a component that solves the problem of decoding\na teleradiographic image.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.08887,review,pre_llm,2021,5,"{'ai_likelihood': 4.437234666612413e-06, 'text': 'Dialogue Disentanglement in Software Engineering: How Far are We?\n\n  Despite the valuable information contained in software chat messages,\ndisentangling them into distinct conversations is an essential prerequisite for\nany in-depth analyses that utilize this information. To provide a better\nunderstanding of the current state-of-the-art, we evaluate five popular dialog\ndisentanglement approaches on software-related chat. We find that existing\napproaches do not perform well on disentangling software-related dialogs that\ndiscuss technical and complex topics. Further investigation on how well the\nexisting disentanglement measures reflect human satisfaction shows that\nexisting measures cannot correctly indicate human satisfaction on\ndisentanglement results. Therefore, in this paper, we introduce and evaluate a\nnovel measure, named DLD. Using results of human satisfaction, we further\nsummarize four most frequently appeared bad disentanglement cases on\nsoftware-related chat to insight future improvements. These cases include (i)\nignoring interaction patterns; (ii) ignoring contextual information; (iii)\nmixing up topics; and (iv) ignoring user relationships. We believe that our\nfindings provide valuable insights on the effectiveness of existing dialog\ndisentanglement approaches and these findings would promote a better\napplication of dialog disentanglement in software engineering.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.06056,regular,pre_llm,2021,5,"{'ai_likelihood': 1.457002427842882e-06, 'text': 'VPP-ART: An Efficient Implementation of Fixed-Size-Candidate-Set\n  Adaptive Random Testing using Vantage Point Partitioning\n\n  Adaptive Random Testing (ART) is an enhancement of Random Testing (RT), and\naims to improve the RT failure-detection effectiveness by distributing test\ncases more evenly in the input domain. Many ART algorithms have been proposed,\nwith Fixed-Size-Candidate-Set ART (FSCS-ART) being one of the most effective\nand popular. FSCS-ART ensures high failure-detection effectiveness by selecting\nthe next test case as the candidate farthest from previously-executed test\ncases. Although FSCS-ART has good failure-detection effectiveness, it also\nfaces some challenges, including heavy computational overheads. In this paper,\nwe propose an enhanced version of FSCS-ART, Vantage Point Partitioning ART\n(VPP-ART). VPP-ART addresses the FSCS-ART computational overhead problem using\nvantage point partitioning, while maintaining the failure-detection\neffectiveness. VPP-ART partitions the input domain space using a modified\nVantage Point tree (VP-tree) and finds the approximate nearest executed test\ncases of a candidate test case in the partitioned sub-domains -- thereby\nsignificantly reducing the time overheads compared with the searches required\nfor FSCS-ART. To enable the FSCS-ART dynamic insertion process, we modify the\ntraditional VP-tree to support dynamic data. The simulation results show that\nVPP-ART has a much lower time overhead compared to FSCS-ART, but also delivers\nsimilar (or better) failure-detection effectiveness, especially in the higher\ndimensional input domains. According to statistical analyses, VPP-ART can\nimprove on the FSCS-ART failure-detection effectiveness by approximately 50% to\n58%. VPP-ART also compares favorably with the KDFC-ART algorithms (a series of\nenhanced ART algorithms based on the KD-tree). Our experiments also show that\nVPP-ART is more cost-effective than FSCS-ART and KDFC-ART.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.15152,regular,pre_llm,2021,5,"{'ai_likelihood': 1.3444158766004775e-05, 'text': 'UML Sequence Diagram: An Alternative Model\n\n  The general acceptance of sequence diagrams can be attributed to their\nrelatively intuitive nature and ability to describe partial behaviors (as\nopposed to such diagrams as state charts). However, studies have shown that\nover 80 percent of graduating students were unable to create a software design\nor even a partial design, and many students had no idea how sequence diagrams\nwere constrained by other models. Many students exhibited difficulties in\nidentifying valid interacting objects and constructing messages with\nappropriate arguments. Additionally, according to authorities, even though many\ndifferent semantics have been proposed for sequence diagrams (e.g.,\ntranslations to state machines), there exists no suitable semantic basis\nrefinement of required sequence diagram behavior because direct style semantics\ndo not precisely capture required sequence diagram behaviors; translations to\nother formalisms disregard essential features of sequence diagrams such as\nguard conditions and critical regions. This paper proposes an alternative to\nsequence diagrams, a generalized model that provides further understanding of\nsequence diagrams to assimilate them into a new modeling language called\nthinging machine (TM). The sequence diagram is extended horizontally by\nremoving the superficial vertical-only dimensional limitation of expansion to\npreserve the logical chronology of events. TM diagramming is spread nonlinearly\nin terms of actions. Events and their chronology are constructed on a second\nplane of description that is superimposed on the initial static description.\nThe result is a more refined representation that would simplify the modeling\nprocess. This is demonstrated through remodeling sequence diagram cases from\nthe literature.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.01451,regular,pre_llm,2021,5,"{'ai_likelihood': 2.615981631808811e-06, 'text': 'Automated Driver Testing for Small Footprint Embedded Systems\n\n  Embedded systems represent a billionaire market and are present in most of\nthe processors produced in the world.Embedded software interacts with external\nperipherals such as sensors and actuators through drivers. The strong\ninteraction between drivers and external peripherals often hamper embedded\nsoftware development, in special the testing task, giving that the physical\nenvironment may not be deterministic and difficult to be recreated during the\ntests, the hardware may not be available or its presence may not be desirable\nor costly. Aiming at addressing these problems, this paper introduces a\nsolution to test drivers on microcontrollers, based on a method that uses three\ncomponents: a Device Under Test (DUT), a Double device, and a computer. The\ncomputer runs a test orchestration code, while the DUT runs the test target\ncode, and the Double plays the role of the real external peripherals that\ninteract with the DUT. The proposed solution was successfully implemented and\nvalidated using different protocols.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.04421,regular,pre_llm,2021,5,"{'ai_likelihood': 1.6225708855523005e-06, 'text': 'Trials and Tribulations of Developing Hybrid Quantum-Classical\n  Microservices Systems\n\n  Quantum computing holds great promise to solve to problems where classical\ncomputers cannot reach. To the point where it already arouses the interest of\nboth scientific and industrial communities. Thus, it is expected that hybrid\nsystems will start to appear where quantum software interacts with classical\nsystems. Such coexistence can be fostered by service computing. Unfortunately,\nthe way in which quantum code can be offered as a service still misses out on\nmany of the potential benefits of service computing. This paper takes the\ntraveling salesman problem, and tackles the challenge of giving it an\nimplementation in the form of a quantum microservice. Then it is used to detect\nwhich of the benefits of service computing are lost in the process. The\nconclusions help to measure the distance between the current state of\ntechnology and the state that would be desirable in order to have a real\nquantum service engineering.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.07569,regular,pre_llm,2021,5,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'DeepMerge: Learning to Merge Programs\n\n  In collaborative software development, program merging is the mechanism to\nintegrate changes from multiple programmers. Merge algorithms in modern version\ncontrol systems report a conflict when changes interfere textually. Merge\nconflicts require manual intervention and frequently stall modern continuous\nintegration pipelines. Prior work found that, although costly, a large majority\nof resolutions involve re-arranging text without writing any new code. Inspired\nby this observation we propose the first data-driven approach to resolve merge\nconflicts with a machine learning model. We realize our approach in a tool\nDeepMerge that uses a novel combination of (i) an edit-aware embedding of merge\ninputs and (ii) a variation of pointer networks, to construct resolutions from\ninput segments. We also propose an algorithm to localize manual resolutions in\na resolved file and employ it to curate a ground-truth dataset comprising 8,719\nnon-trivial resolutions in JavaScript programs. Our evaluation shows that, on a\nheld out test set, DeepMerge can predict correct resolutions for 37% of\nnon-trivial merges, compared to only 4% by a state-of-the-art semistructured\nmerge technique. Furthermore, on the subset of merges with upto 3 lines\n(comprising 24% of the total dataset), DeepMerge can predict correct\nresolutions with 78% accuracy.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.14137,review,pre_llm,2021,5,"{'ai_likelihood': 4.867712656656901e-06, 'text': ""What Makes a Game High-rated? Towards Factors of Video Game Success\n\n  As the video game market grows larger, it becomes harder to stand out from\nthe crowd. Launching a successful game involves different aspects. But what are\nthey? In this paper, we investigate some aspects of the high-rated games from a\ndataset of 200 projects. The results show that the none of the aspects of this\nstudy have a strong relationship with the game's success. A further analysis on\nthe high-rated games shows that team, technical, and game-design aspects should\nbe the main focus of the game developers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.08789,review,pre_llm,2021,5,"{'ai_likelihood': 2.4172994825575088e-06, 'text': 'Pots of Gold at the End of the Rainbow: What is Success for Open Source\n  Contributors?\n\n  Success in Open Source Software (OSS) is often perceived as an exclusively\ncode-centric endeavor. This perception can exclude a variety of individuals\nwith a diverse set of skills and backgrounds, in turn helping create the\ncurrent diversity & inclusion imbalance in OSS. Because people\'s perspectives\nof success affect their personal, professional, and life choices, to be able to\nsupport a diverse class of individuals, we must first understand what OSS\ncontributors consider successful. Thus far, research has used a\nuni-dimensional, code-centric lens to define success. In this paper, we\nchallenge this status-quo and reveal the multi-faceted definition of success\namong OSS contributors. We do so through interviews with 27 OSS contributors\nwho are recognized as successful in their communities, and a follow-up open\nsurvey with 193 OSS contributors. Our study provides nuanced definitions of\nsuccess perceptions in OSS, which might help devise strategies to attract and\nretain a diverse set of contributors, helping them attain their ""pots of gold\nat the end of the rainbow"".\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.07366,review,pre_llm,2021,5,"{'ai_likelihood': 1.357661353217231e-06, 'text': ""Investigating the Significance of the Bellwether Effect to Improve\n  Software Effort Prediction: Further Empirical Study\n\n  Context: In addressing how best to estimate how much effort is required to\ndevelop software, a recent study found that using exemplary and recently\ncompleted projects [forming Bellwether moving windows (BMW)] in software effort\nprediction (SEP) models leads to relatively improved accuracy. More studies\nneed to be conducted to determine whether the BMW yields improved accuracy in\ngeneral, since different sizing and aging parameters of the BMW are known to\naffect accuracy. Objective: To investigate the existence of exemplary projects\n(Bellwethers) with defined window size and age parameters, and whether their\nuse in SEP improves prediction accuracy. Method: We empirically investigate the\nmoving window assumption based on the theory that the prediction outcome of a\nfuture event depends on the outcomes of prior events. Sampling of Bellwethers\nwas undertaken using three introduced Bellwether methods (SSPM, SysSam, and\nRandSam). The ergodic Markov chain was used to determine the stationarity of\nthe Bell-wethers. Results: Empirical results show that 1) Bellwethers exist in\nSEP and 2) the BMW has an approximate size of 50 to 80 exemplary projects that\nshould not be more than 2 years old relative to the new projects to be\nestimated. Conclusion: The study's results add further weight to the\nrecommended use of Bellwethers for improved prediction accuracy in SEP.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.08702,regular,pre_llm,2021,5,"{'ai_likelihood': 5.960464477539062e-07, 'text': ""Component Based Solutions Under Architecture\n\n  Many of today's applications have an, almost tangible, monolithic nature.\nThey are built as 'islands', purporting to be self contained, offering little\nor nothing in the way of integration with other applications. In the past,\nbeing large and self-contained may have eliminated the need to interact with\nother solutions to some extent. However, in the business environments of today\nthe interaction with other applications becomes paramount. As a result of this,\nmany ad-hoc point-to-point integration solutions have been built between\ndifferent applications. This has already led to an 'application spaghetti' at\nmany of our customer sites. Many of today's applications are poorly structured,\nwhich makes their responsiveness to business change sluggish. The application\nspaghetti with its plethora of point-to-point interfaces further inhibits the\nresponsiveness to change.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.08018,regular,pre_llm,2021,5,"{'ai_likelihood': 1.4901161193847656e-06, 'text': ""Buying time in software development: how estimates become commitments?\n\n  Despite years of research for improving accuracy, software practitioners\nstill face software estimation difficulties. Expert judgment has been the\nprevalent method used in industry, and researchers' focus on raising realism in\nestimates when using it seems not to be enough for the much-expected\nimprovements. Instead of focusing on the estimation process's technicalities,\nwe investigated the interaction of the establishment of commitments with\ncustomers and software estimation. By observing estimation sessions and\ninterviewing software professionals from companies in varying contexts, we\nfound that defensible estimates and padding of software estimates are crucial\nin converting estimates into commitments. Our findings show that software\nprofessionals use padding for three different reasons: contingency buffer,\ncompleting other tasks, or improving the overall quality of the product. The\nreasons to pad have a common theme: buying time to balance short- and long-term\nsoftware development commitments, including the repayment of technical debt.\nSuch a theme emerged from the human aspects of the interaction of estimation\nand the establishment of commitments: pressures and customers' conflicting\nshort and long-term needs play silent and unrevealed roles in-between the\ntechnical activities. Therefore, our study contributes to untangling the\nunderlying phenomena, showing how the practices used by software practitioners\nhelp to deal with the human and social context in which estimation is embedded.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.04974,regular,pre_llm,2021,5,"{'ai_likelihood': 7.198916541205512e-05, 'text': ""Towards the Use of Slice-based Cohesion Metrics with Learning Analytics\n  to Assess Programming Skills\n\n  In programming education, it makes a difference whether you are dealing with\nbeginners or advanced students. As our future students will become even more\ntech-savvy, it is necessary to assess programming skills appropriately and\nquickly to protect them from boredom and optimally support the learning\nprocess. In this work, we advocate for the use of slice-based cohesion metrics\nto assess the process of program construction in a learning analytics setting.\nWe argue that semantically related parts during program construction are an\nessential part of programming skills. Therefore, we propose using cohesion\nmetrics on the level of variables to identify programmers' trains of thought\nbased on the cohesion of semantically related parts during program\nconstruction.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.01038,regular,pre_llm,2021,5,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Formalizing the Four-layer Metamodeling Stack -- Potential and Benefits\n\n  Enterprise modeling deals with the increasing complexity of processes and\nsystems by operationalizing model content and by linking complementary models\nand languages, thus amplifying the model-value beyond mere comprehensible\npictures. To enable this amplification and turn models into\ncomputer-processable structures a comprehensive formalization is needed. This\npaper presents a generic formalism based on typed first-order logic and\nprovides a perspective on the potential and benefits arising for a variety of\nresearch issues in conceptual modeling. We define modeling languages as formal\nlanguages with a signature $\\Sigma$ - comprising object types, relation types,\nand attributes through types and function symbols - and a set of constraints.\nThree cases studies are included to show the effectiveness of the approach.\nApplying the formalism to the next level in the hierarchy of models we create\nM2FOL, a formal modeling language for metamodels. We show that M2FOL is\nself-describing and therefore complete the formalization of the full four-layer\nmetamodeling stack. On the basis of our generic formalism applicable to\narbitrary modeling languages we examine three current research topics -\nlanguage interleaving & consistency, operations on models, and automatic\ntranslation of formalizations to platform-specific code - and how to approach\nthem with the proposed formalism. This shows that the rich knowledge stack on\nformal languages in logic offers new tools for old problems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.03626,regular,pre_llm,2021,5,"{'ai_likelihood': 2.7484363979763457e-06, 'text': 'SuMo: A Mutation Testing Strategy for Solidity Smart Contracts\n\n  Smart Contracts are software programs that are deployed and executed within a\nblockchain infrastructure. Due to their immutable nature, directly resulting\nfrom the specific characteristics of the deploying infrastructure, smart\ncontracts must be thoroughly tested before their release. Testing is one of the\nmain activities that can help to improve the reliability of a smart contract,\nso as to possibly prevent considerable loss of valuable assets. It is therefore\nimportant to provide the testers with tools that permit them to assess the\nactivity they performed. Mutation testing is a powerful approach for assessing\nthe fault-detection capability of a test suite. In this paper, we propose SuMo,\na novel mutation testing tool for Ethereum Smart Contracts. SuMo implements a\nset of 44 mutation operators that were designed starting from the latest\nSolidity documentation, and from well-known mutation testing tools. These allow\nto simulate a wide variety of faults that can be made by smart contract\ndevelopers. The set of operators was designed to limit the generation of\nstillborn mutants, which slow down the mutation testing process and limit the\nusability of the tool. We report a first evaluation of SuMo on open-source\nprojects for which test suites were available. The results we got are\nencouraging, and they suggest that SuMo can effectively help developers to\ndeliver more reliable smart contracts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.12233,review,pre_llm,2021,6,"{'ai_likelihood': 6.258487701416016e-06, 'text': ""Testing of Autonomous Driving Systems: Where Are We and Where Should We\n  Go?\n\n  Autonomous driving has shown great potential to reform modern transportation.\nYet its reliability and safety have drawn a lot of attention and concerns.\nCompared with traditional software systems, autonomous driving systems (ADSs)\noften use deep neural networks in tandem with logic-based modules. This new\nparadigm poses unique challenges for software testing. Despite the recent\ndevelopment of new ADS testing techniques, it is not clear to what extent those\ntechniques have addressed the needs of ADS practitioners. To fill this gap, we\npresent the first comprehensive study to identify the current practices and\nneeds of ADS testing. We conducted semi-structured interviews with developers\nfrom 10 autonomous driving companies and surveyed 100 developers who have\nworked on autonomous driving systems. A systematic analysis of the interview\nand survey data revealed 7 common practices and 4 emerging needs of autonomous\ndriving testing. Through a comprehensive literature review, we developed a\ntaxonomy of existing ADS testing techniques and analyzed the gap between ADS\nresearch and practitioners' needs. Finally, we proposed several future\ndirections for SE researchers, such as developing test reduction techniques to\naccelerate simulation-based ADS testing.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.10789,regular,pre_llm,2021,6,"{'ai_likelihood': 9.503629472520617e-06, 'text': 'An empirical evaluation of the usefulness of Tree Kernels for\n  Commit-time Defect Detection in large software systems\n\n  Defect detection at commit check-in time prevents the introduction of defects\ninto software systems. Current defect detection approaches rely on metric-based\nmodels which are not very accurate and whose results are not directly useful\nfor developers. We propose a method to detect bug-inducing commits by comparing\nthe incoming changes with all past commits in the project, considering both\nthose that introduced defects and those that did not. Our method considers\nindividual changes in the commit separately, at the method-level granularity.\nDoing so helps developers as they are informed of specific methods that need\nfurther attention instead of being told that the entire commit is problematic.\nOur approach represents source code as abstract syntax trees and uses tree\nkernels to estimate the similarity of the code with previous commits. We\nexperiment with subtree kernels (STK), subset tree kernels (SSTK), or partial\ntree kernels (PTK). An incoming change is then classified using a K-NN\nclassifier on the past changes. We evaluate our approach on the BigCloneBench\nbenchmark and on the Technical Debt dataset, using the NiCad clone detector as\nthe baseline. Our experiments with the BigCloneBench benchmark show that the\ntree kernel approach can detect clones with a comparable MAP to that of NiCad.\nAlso, on defect detection with the Technical Debt dataset, tree kernels are\nleast as effective as NiCad with MRR, F-score, and Accuracy of 0.87, 0.80, and\n0.82 respectively.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.00073,regular,pre_llm,2021,6,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'SATDBailiff- Mining and Tracking Self-Admitted Technical Debt\n\n  Self-Admitted Technical Debt (SATD) is a metaphorical concept to describe the\nself-documented addition of technical debt to a software project in the form of\nsource code comments. SATD can linger in projects and degrade source-code\nquality, but it can also be more visible than unintentionally added or\nundocumented technical debt. Understanding the implications of adding SATD to a\nsoftware project is important because developers can benefit from a better\nunderstanding of the quality trade-offs they are making. However, empirical\nstudies, analyzing the survivability and removal of SATD comments, are\nchallenged by potential code changes or SATD comment updates that may interfere\nwith properly tracking their appearance, existence, and removal. In this paper,\nwe propose SATDBailiff, a tool that uses an existing state-of-the-art SATD\ndetection tool, to identify SATD in method comments, then properly track their\nlifespan. SATDBailiff is given as input links to open source projects, and its\noutput is a list of all identified SATDs, and for each detected SATD,\nSATDBailiff reports all its associated changes, including any updates to its\ntext, all the way to reporting its removal. The goal of SATDBailiff is to aid\nresearchers and practitioners in better tracking SATDs instances and providing\nthem with a reliable tool that can be easily extended. SATDBailiff was\nvalidated using a dataset of previously detected and manually validated SATD\ninstances.\n  SATDBailiff is publicly available as an open-source, along with the manual\nanalysis of SATD instances associated with its validation, on the project\nwebsite\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.03792,regular,pre_llm,2021,6,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Preference Discovery in Large Product Lines\n\n  When AI tools can generate many solutions, some human preference must be\napplied to determine which solution is relevant to the current project. One way\nto find those preferences is interactive search-based software engineering\n(iSBSE) where humans can influence the search process. Current iSBSE methods\ncan lead to cognitive fatigue (when they overwhelm humans with too many overly\nelaborate questions). WHUN is an iSBSE algorithm that avoids that problem. Due\nto its recursive clustering procedure, WHUN only pesters humans for\n$O(log_2{N})$ interactions. Further, each interaction is mediated via a feature\nselection procedure that reduces the number of asked questions. When compared\nto prior state-of-the-art iSBSE systems, WHUN runs faster, asks fewer\nquestions, and achieves better solutions that are within $0.1\\%$ of the best\nsolutions seen in our sample space. More importantly, WHUN scales to large\nproblems (in our experiments, models with 1000 variables can be explored with\nhalf a dozen interactions where, each time, we ask only four questions).\nAccordingly, we recommend WHUN as a baseline against which future iSBSE work\nshould be compared. To facilitate that, all our scripts are online at\nhttps://github.com/ai-se/whun.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.02209,regular,pre_llm,2021,6,"{'ai_likelihood': 1.158979203965929e-06, 'text': ""Influence of Roles in Decision-Making during OSS Development -- A Study\n  of Python\n\n  Governance has been highlighted as a key factor in the success of an Open\nSource Software (OSS) project. It is generally seen that in a mixed meritocracy\nand autocracy governance model, the decision-making (DM) responsibility\nregarding what features are included in the OSS is shared among members from\nselect roles; prominently the project leader. However, less examination has\nbeen made whether members from these roles are also prominent in DM discussions\nand how decisions are made, to show they play an integral role in the success\nof the project. We believe that to establish their influence, it is necessary\nto examine not only discussions of proposals in which the project leader makes\nthe decisions, but also those where others make the decisions. Therefore, in\nthis study, we examine the prominence of members performing different roles in:\n(i) making decisions, (ii) performing certain social roles in DM discussions\n(e.g., discussion starters), (iii) contributing to the OSS development social\nnetwork through DM discussions, and (iv) how decisions are made under both\nscenarios. We examine these aspects in the evolution of the well-known Python\nproject. We carried out a data-driven longitudinal study of their email\ncommunication spanning 20 years, comprising about 1.5 million emails. These\nemails contain decisions for 466 Python Enhancement Proposals (PEPs) that\ndocument the language's evolution. Our findings make the influence of different\nroles transparent to future (new) members, other stakeholders, and more\nbroadly, to the OSS research community.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.04687,regular,pre_llm,2021,6,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Does class size matter? An in-depth assessment of the effect of class\n  size in software defect prediction\n\n  In the past 20 years, defect prediction studies have generally acknowledged\nthe effect of class size on software prediction performance. To quantify the\nrelationship between object-oriented (OO) metrics and defects, modelling has to\ntake into account the direct, and potentially indirect, effects of class size\non defects. However, some studies have shown that size cannot be simply\ncontrolled or ignored, when building prediction models. As such, there remains\na question whether, and when, to control for class size. This study provides a\nnew in-depth examination of the impact of class size on the relationship\nbetween OO metrics and software defects or defect-proneness. We assess the\nimpact of class size on the number of defects and defect-proneness in software\nsystems by employing a regression-based mediation (with bootstrapping) and\nmoderation analysis to investigate the direct and indirect effect of class size\nin count and binary defect prediction. Our results show that the size effect is\nnot always significant for all metrics. Of the seven OO metrics we\ninvestigated, size consistently has significant mediation impact only on the\nrelationship between Coupling Between Objects (CBO) and\ndefects/defect-proneness, and a potential moderation impact on the relationship\nbetween Fan-out and defects/defect-proneness. Based on our results we make\nthree recommendations. One, we encourage researchers and practitioners to\nexamine the impact of class size for the specific data they have in hand and\nthrough the use of the proposed statistical mediation/moderation procedures.\nTwo, we encourage empirical studies to investigate the indirect effect of\npossible additional variables in their models when relevant. Three, the\nstatistical procedures adopted in this study could be used in other empirical\nsoftware engineering research to investigate the influence of potential\nmediators/moderators.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.16067,regular,pre_llm,2021,6,"{'ai_likelihood': 1.026524437798394e-06, 'text': ""Leveraging Team Dynamics to Predict Open-source Software Projects'\n  Susceptibility to Social Engineering Attacks\n\n  Open-source software (OSS) is a critical part of the software supply chain.\nRecent social engineering attacks against OSS development teams have enabled\nattackers to become code contributors and later inject malicious code or\nvulnerabilities into the project with the goal of compromising dependent\nsoftware. The attackers have exploited interactions among development team\nmembers and the social dynamics of team behavior to enable their attacks. We\nintroduce a security approach that leverages signatures and patterns of team\ndynamics to predict the susceptibility of a software development team to social\nengineering attacks that enable access to the OSS project code. The proposed\napproach is programming language-, platform-, and vulnerability-agnostic\nbecause it assesses the artifacts of OSS team interactions, rather than OSS\ncode.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.02306,review,pre_llm,2021,6,"{'ai_likelihood': 4.635916815863716e-07, 'text': 'Automatic Patch Linkage Detection in Code Review Using TextualContent\n  and File Location Features\n\n  Context: Contemporary code review tools are a popular choice for software\nquality assurance. Using these tools, reviewers are able to post a linkage\nbetween two patches during a review discussion. Large development teams that\nuse a review-then-commit model risk being unaware of these linkages. Objective:\nOur objective is to first explore how patch linkage impacts the review process.\nWe then propose and evaluate models that detect patch linkage based on\nrealistic time intervals. Method: First, we carry out an exploratory study on\nthree open source projects to conduct linkage impact analysis using 942\nmanually classified linkages. Second, we propose two techniques using textual\nand file location similarity to build detection models and evaluate their\nperformance. Results: The study provides evidence of latency in the linkage\nnotification. We show that a patch with the Alternative Solution linkage (i.e.,\npatches that implement similar functionality)undergoes a quicker review and\navoids additional revisions after the team has been notified, compared to other\nlinkage types. Our detection model experiments show promising recall rates for\nthe Alternative Solution linkage (from 32% to 95%), but precision has room for\nimprovement. Conclusion: Patch linkage detection is promising, with likely\nimprovements if the practice of posting linkages becomes more prevalent. From\nour implications, this paper lays the groundwork for future research on how to\nincrease patch linkage awareness to facilitate efficient reviews.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.15209,regular,pre_llm,2021,6,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Making the most of small Software Engineering datasets with modern\n  machine learning\n\n  This paper provides a starting point for Software Engineering (SE)\nresearchers and practitioners faced with the problem of training machine\nlearning models on small datasets. Due to the high costs associated with\nlabeling data, in Software Engineering,there exist many small (< 1 000 samples)\nand medium-sized (< 100 000 samples) datasets. While deep learning has set the\nstate of the art in many machine learning tasks, it is only recently that it\nhas proven effective on small-sized datasets, primarily thanks to pre-training,\na semi-supervised learning technique that leverages abundant unlabelled data\nalongside scarce labelled data.In this work, we evaluate pre-trained\nTransformer models on a selection of 13 smaller datasets from the SE\nliterature, covering both,source code and natural language. Our results suggest\nthat pre-trained Transformers are competitive and in some cases superior to\nprevious models, especially for tasks involving natural language; whereas for\nsource code tasks, in particular for very small datasets,traditional machine\nlearning methods often has the edge.In addition, we experiment with several\ntechniques that ought to aid training on small datasets, including active\nlearning, data augmentation, soft labels, self-training and intermediate-task\nfine-tuning, and issue recommendations on when they are effective. We also\nrelease all the data, scripts, and most importantly pre-trained models for the\ncommunity to reuse on their own datasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.13446,regular,pre_llm,2021,6,"{'ai_likelihood': 2.4172994825575088e-06, 'text': 'Discovering executable routine specifications from user interaction logs\n\n  Robotic Process Automation (RPA) is a technology to automate routine work\nsuch as copying data across applications or filling in document templates using\ndata from multiple applications. RPA tools allow organizations to automate a\nwide range of routines. However, identifying and scoping routines that can be\nautomated using RPA tools is time consuming. Manual identification of candidate\nroutines via interviews, walk-throughs, or job shadowing allow analysts to\nidentify the most visible routines, but these methods are not suitable when it\ncomes to identifying the long tail of routines in an organization. This article\nproposes an approach to discover automatable routines from logs of user\ninteractions with IT systems and to synthesize executable specifications for\nsuch routines. The approach starts by discovering frequent routines at a\ncontrol-flow level (candidate routines). It then determines which of these\ncandidate routines are automatable and it synthetizes an executable\nspecification for each such routine. Finally, it identifies semantically\nequivalent routines so as to produce a set of non-redundant automatable\nroutines. The article reports on an evaluation of the approach using a\ncombination of synthetic and real-life logs. The evaluation results show that\nthe approach can discover automatable routines that are known to be present in\na UI log, and that it identifies automatable routines that users recognize as\nsuch in real-life logs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.08948,regular,pre_llm,2021,6,"{'ai_likelihood': 3.675619761149089e-06, 'text': ""Muzeel: A Dynamic JavaScript Analyzer for Dead Code Elimination in\n  Today's Web\n\n  JavaScript contributes to the increasing complexity of today's web. To\nsupport user interactivity and accelerate the development cycle, web developers\nheavily rely on large general-purpose third-party JavaScript libraries. This\npractice increases the size and the processing complexity of a web page by\nbringing additional functions that are not used by the page but unnecessarily\ndownloaded and processed by the browser. In this paper, an analysis of around\n40,000 web pages shows that 70% of JavaScript functions on the median page are\nunused, and the elimination of these functions would contribute to the\nreduction of the page size by 60%. Motivated by these findings, we propose\nMuzeel (which means eliminator in Arabic); a solution for eliminating\nJavaScript functions that are not used in a given web page (commonly referred\nto as dead code). Muzeel extracts all of the page event listeners upon page\nload, and emulates user interactions using a bot that triggers each of these\nevents, in order to eliminate the dead code of functions that are not called by\nany of these events. Our evaluation results spanning several Android mobile\nphones and browsers show that Muzeel speeds up the page load by around 30% on\nlow-end phones, and by 25% on high-end phones under 3G network. It also reduces\nthe speed index (which is an important user experience metric) by 23% and 21%\nunder the same network on low-end, and high-end phones, respectively.\nAdditionally, Muzeel reduces the overall download size while maintaining the\nvisual content and interactive functionality of the pages.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.04916,regular,pre_llm,2021,6,"{'ai_likelihood': 1.4901161193847656e-06, 'text': 'Erratum: Leveraging Flexible Tree Matching to Repair Broken Locators in\n  Web Automation Scripts\n\n  Web applications are constantly evolving to integrate new features and fix\nreported bugs. Even an imperceptible change can sometimes entail significant\nmodifications of the Document Object Model (DOM), which is the underlying model\nused by browsers to render all the elements included in a web application.\nScripts that interact with web applications (e.g. web test scripts, crawlers,\nor robotic process automation) rely on this continuously evolving DOM which\nmeans they are often particularly fragile. More precisely, the major cause of\nbreakages observed in automation scripts are element locators, which are\nidentifiers used by automation scripts to navigate across the DOM. When the DOM\nevolves, these identifiers tend to break, thus causing the related scripts to\nno longer locate the intended target elements. For this reason, several\ncontributions explored the idea of automatically repairing broken locators on a\npage. These works attempt to repair a given broken locator by scanning all\nelements in the new DOM to find the most similar one. Unfortunately, this\napproach fails to scale when the complexity of web pages grows, leading either\nto long computation times or incorrect element repairs. This article,\ntherefore, adopts a different perspective on this problem by introducing a new\nlocator repair solution that leverages tree matching algorithms to relocate\nbroken locators. This solution, named Erratum, implements a holistic approach\nto reduce the element search space, which greatly eases the locator repair task\nand drastically improves repair accuracy. We compare the robustness of Erratum\non a large-scale benchmark composed of realistic and synthetic mutations\napplied to popular web applications currently deployed in production. Our\nempirical results demonstrate that Erratum outperforms the accuracy of WATER, a\nstate-of-the-art solution, by 67%.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.03458,review,pre_llm,2021,6,"{'ai_likelihood': 3.311369154188368e-07, 'text': ""A Grounded Theory of the Role of Coordination in Software Security Patch\n  Management\n\n  Several disastrous security attacks can be attributed to delays in patching\nsoftware vulnerabilities. While researchers and practitioners have paid\nsignificant attention to automate vulnerabilities identification and patch\ndevelopment activities of software security patch management, there has been\nrelatively little effort dedicated to gain an in-depth understanding of the\nsocio-technical aspects, e.g., coordination of interdependent activities of the\npatching process and patching decisions, that may cause delays in applying\nsecurity patches. We report on a Grounded Theory study of the role of\ncoordination in security patch management. The reported theory consists of four\ninter-related dimensions, i.e., causes, breakdowns, constraints, and\nmechanisms. The theory explains the causes that define the need for\ncoordination among interdependent software and hardware components and multiple\nstakeholders' decisions, the constraints that can negatively impact\ncoordination, the breakdowns in coordination, and the potential corrective\nmeasures. This study provides potentially useful insights for researchers and\npractitioners who can carefully consider the needs of and devise suitable\nsolutions for supporting the coordination of interdependencies involved in\nsecurity patch management.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.09482,regular,pre_llm,2021,6,"{'ai_likelihood': 2.053048875596788e-06, 'text': 'Elicitation of Adaptive Requirements Using Creativity Triggers: A\n  Controlled Experiment\n\n  Adaptive systems react to changes in their environment by changing their\nbehavior. Identifying these needed adaptations is very difficult, but central\nto requirements elicitation for adaptive systems. As the necessary or potential\nadaptations are typically not obvious to the stakeholders, the problem is how\nto effectively elicit adaptation-relevant information. One approach is to use\ncreativity techniques to support the systematic identification and elicitation\nof adaptation requirements. In particular, here, we analyze a set of creativity\ntriggers defined for systematic exploration of potential adaptation\nrequirements. We compare these triggers with brainstorming as a baseline in a\ncontrolled experiment with 85 master students. The results indicate that the\nproposed triggers are suitable for the efficient elicitation of adaptive\nrequirements and that the 15 trigger questions produce significantly more\nrequirements fragments than solo brainstorming.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.02245,regular,pre_llm,2021,6,"{'ai_likelihood': 1.079506344265408e-05, 'text': ""Towards offensive language detection and reduction in four Software\n  Engineering communities\n\n  Software Engineering (SE) communities such as Stack Overflow have become\nunwelcoming, particularly through members' use of offensive language. Research\nhas shown that offensive language drives users away from active engagement\nwithin these platforms. This work aims to explore this issue more broadly by\ninvestigating the nature of offensive language in comments posted by users in\nfour prominent SE platforms - GitHub, Gitter, Slack and Stack Overflow (SO). It\nproposes an approach to detect and classify offensive language in SE\ncommunities by adopting natural language processing and deep learning\ntechniques. Further, a Conflict Reduction System (CRS), which identifies\noffence and then suggests what changes could be made to minimize offence has\nbeen proposed. Beyond showing the prevalence of offensive language in over 1\nmillion comments from four different communities which ranges from 0.07% to\n0.43%, our results show promise in successful detection and classification of\nsuch language. The CRS system has the potential to drastically reduce manual\nmoderation efforts to detect and reduce offence in SE communities.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.00965,regular,pre_llm,2021,6,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'ALFRED: a methodology to enable component fault trees for layered\n  architectures\n\n  Identifying drawbacks or insufficiencies in terms of safety is important also\nin early development stages of safety critical systems. In industry,\ndevelopment artefacts such as components or units, are often reused from\nexisting artefacts to save time and costs. When development artefacts are\nreused, their existing safety analysis models are an important input for an\nearly safety assessment for the new system, since they already provide a valid\nmodel. Component fault trees support such reuse strategies by a compositional\nhorizontal approach. But current development strategies do not only divide\nsystems horizontally, e.g., By encapsulating different functionality into\nseparate components and hierarchies of components, but also vertically, e.g.\nInto software and hardware architecture layers. Current safety analysis\nmethodologies, such as component fault trees, do not support such vertical\nlayers. Therefore, we present here a methodology that is able to divide safety\nanalysis models into different layers of a systems architecture. We use so\ncalled Architecture Layer Failure Dependencies to enable component fault trees\non different layers of an architecture. These dependencies are then used to\ngenerate safety evidence for the entire system and over all different\narchitecture layers. A case study applies the approach to hardware and software\nlayers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.06422,regular,pre_llm,2021,6,"{'ai_likelihood': 5.496872795952691e-06, 'text': ""From Blackboard to the Office: A Look Into How Practitioners Perceive\n  Software Testing Education\n\n  The teaching-learning process may require specific pedagogical approaches to\nestablish a relationship with industry practices. Recently, some studies\ninvestigated the educators' perspectives and the undergraduate courses\ncurriculum to identify potential weaknesses and solutions for the software\ntesting teaching process. However, it is still unclear how the practitioners\nevaluate the acquisition of knowledge about software testing in undergraduate\ncourses. This study carried out an expert survey with 68 newly graduated\npractitioners to determine what the industry expects from them and what they\nlearned in academia. The yielded results indicated that those practitioners\nlearned at a similar rate as others with a long industry experience. Also, they\nstudied less than half of the 35 software testing topics collected in the\nsurvey and took industry-backed extracurricular courses to complement their\nlearning. Additionally, our findings point out a set of implications for future\nresearch, as the respondents' learning difficulties (e.g., lack of learning\nsources) and the gap between academic education and industry expectations\n(e.g., certifications).\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.01987,regular,pre_llm,2021,6,"{'ai_likelihood': 2.5828679402669272e-06, 'text': 'PRINS: Scalable Model Inference for Component-based System Logs\n\n  Behavioral software models play a key role in many software engineering\ntasks; unfortunately, these models either are not available during software\ndevelopment or, if available, quickly become outdated as implementations\nevolve. Model inference techniques have been proposed as a viable solution to\nextract finite state models from execution logs. However, existing techniques\ndo not scale well when processing very large logs that can be commonly found in\npractice.\n  In this paper, we address the scalability problem of inferring the model of a\ncomponent-based system from large system logs, without requiring any extra\ninformation. Our model inference technique, called PRINS, follows a\ndivide-and-conquer approach. The idea is to first infer a model of each system\ncomponent from the corresponding logs; then, the individual component models\nare merged together taking into account the flow of events across components,\nas reflected in the logs. We evaluated PRINS in terms of scalability and\naccuracy, using nine datasets composed of logs extracted from publicly\navailable benchmarks and a personal computer running desktop business\napplications. The results show that PRINS can process large logs much faster\nthan a publicly available and well-known state-of-the-art tool, without\nsignificantly compromising the accuracy of inferred models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.10524,regular,pre_llm,2021,6,"{'ai_likelihood': 5.132622188991971e-06, 'text': 'Test case prioritization using test case diversification and\n  fault-proneness estimations\n\n  Regression testing activities greatly reduce the risk of faulty software\nrelease. However, the size of the test suites grows throughout the development\nprocess, resulting in time-consuming execution of the test suite and delayed\nfeedback to the software development team. This has urged the need for\napproaches such as test case prioritization (TCP) and test-suite reduction to\nreach better results in case of limited resources. In this regard, proposing\napproaches that use auxiliary sources of data such as bug history can be\ninteresting. We aim to propose an approach for TCP that takes into account test\ncase coverage data, bug history, and test case diversification. To evaluate\nthis approach we study its performance on real-world open-source projects. The\nbug history is used to estimate the fault-proneness of source code areas. The\ndiversification of test cases is preserved by incorporating fault-proneness on\na clustering-based approach scheme. The proposed methods are evaluated on\ndatasets collected from the development history of five real-world projects\nincluding 357 versions in total. The experiments show that the proposed methods\nare superior to coverage-based TCP methods. The proposed approach shows that\nimprovement of coverage-based and fault-proneness-based methods is possible by\nusing a combination of diversification and fault-proneness incorporation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.14598,review,pre_llm,2021,6,"{'ai_likelihood': 2.1192762586805556e-06, 'text': ""Software quality: A Historical and Synthetic Content Analysis\n\n  Interconnected computers and software systems have become an indispensable\npart of people's lives, therefore software quality research is becoming more\nand more important. There have been multiple attempts to synthesize knowledge\ngained in software quality research, however, they were focused mainly on\nsingle aspects of software quality and not to structure the knowledge in a\nholistic way. The aim of our study was to close this gap. The software quality\npublications were harvested from the Scopus bibliographic database. The\nmetadata was exported first to CRexlporer, which was employed to identify\nhistorical roots, and next to VOSViewer, which was used as a part of the\nsynthetic content analysis. In our study we defined synthetic context analysis\nas a triangulation of bibliometrics and content analysis. Our search resulted\nin 14451 publications. The performance bibliometric study showed that the\nproduction of research publications relating to software quality is currently\nfollowing an exponential growth trend and that the software quality research\ncommunity is growing. The most productive country was the United States and the\nmost productive Institution The Florida Atlantic University. The synthetic\ncontent analysis revealed that the published knowledge can be structured into\n10 themes, the most important being the themes regarding software quality\nimprovement with enhancing software engineering, advanced software testing, and\nimproved defect and fault prediction with machine learning and data mining.\nAccording to the analysis of the hot topics, it seems that future research will\nbe directed into developing and using a full specter of new artificial\nintelligence tools (not just machine learning and data mining) and focusing on\nhow to assure software quality in agile development paradigms.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.09698,review,pre_llm,2021,7,"{'ai_likelihood': 2.6722749074300132e-05, 'text': ""Mono2Micro: A Practical and Effective Tool for Decomposing Monolithic\n  Java Applications to Microservices\n\n  In migrating production workloads to cloud, enterprises often face the\ndaunting task of evolving monolithic applications toward a microservice\narchitecture. At IBM, we developed a tool called Mono2Micro to assist with this\nchallenging task. Mono2Micro performs spatio-temporal decomposition, leveraging\nwell-defined business use cases and runtime call relations to create\nfunctionally cohesive partitioning of application classes. Our preliminary\nevaluation of Mono2Micro showed promising results. How well does Mono2Micro\nperform against other decomposition techniques, and how do practitioners\nperceive the tool? This paper describes the technical foundations of Mono2Micro\nand presents results to answer these two questions. To answer the first\nquestion, we evaluated Mono2Micro against four existing techniques on a set of\nopen-source and proprietary Java applications and using different metrics to\nassess the quality of decomposition and tool's efficiency. Our results show\nthat Mono2Micro significantly outperforms state-of-the-art baselines in\nspecific metrics well-defined for the problem domain. To answer the second\nquestion, we conducted a survey of twenty-one practitioners in various industry\nroles who have used Mono2Micro. This study highlights several benefits of the\ntool, interesting practitioner perceptions, and scope for further improvements.\nOverall, these results show that Mono2Micro can provide a valuable aid to\npractitioners in creating functionally cohesive and explainable microservice\ndecompositions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.09587,review,pre_llm,2021,7,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'A Survey-Based Qualitative Study to Characterize Expectations of\n  Software Developers from Five Stakeholders\n\n  Background: Studies on developer productivity and well-being find that the\nperceptions of productivity in a software team can be a socio-technical\nproblem. Intuitively, problems and challenges can be better handled by managing\nexpectations in software teams. Aim: Our goal is to understand whether the\nexpectations of software developers vary towards diverse stakeholders in\nsoftware teams. Method: We surveyed 181 professional software developers to\nunderstand their expectations from five different stakeholders: (1)\norganizations, (2) managers, (3) peers, (4) new hires, and (5) government and\neducational institutions. The five stakeholders are determined by conducting\nsemi-formal interviews of software developers. We ask open-ended survey\nquestions and analyze the responses using open coding. Results: We observed 18\nmulti-faceted expectations types. While some expectations are more specific to\na stakeholder, other expectations are cross-cutting. For example, developers\nexpect work-benefits from their organizations, but expect the adoption of\nstandard software engineering (SE) practices from their organizations, peers,\nand new hires. Conclusion: Out of the 18 categories, three categories are\nrelated to career growth. This observation supports previous research that\nhappiness cannot be assured by simply offering more money or a promotion. Among\nthe most number of responses, we find expectations from educational\ninstitutions to offer relevant teaching and from governments to improve job\nstability, which indicate the increasingly important roles of these\norganizations to help software developers. This observation can be especially\ntrue during the COVID-19 pandemic.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.13315,regular,pre_llm,2021,7,"{'ai_likelihood': 3.245141771104601e-06, 'text': 'Sorrel: an IDE Plugin for Managing Licenses and Detecting License\n  Incompatibilities\n\n  Software development is a complex process that includes many different tasks\nbesides just writing code. One of the aspects of software engineering is\nselecting and managing licenses for the given project. In this paper, we\npresent Sorrel - a plugin for managing licenses and detecting potential\nincompatibilities for IntelliJ IDEA, a popular Java IDE. The plugin scans the\nproject in search of information about the project license and the licenses of\nits libraries. If the project does not yet have a license, the plugin provides\nthe developer with recommendations for choosing the most suitable open license,\nand if there is a license, it informs the programmer about potential licensing\nviolations. The tool makes it easier for developers to choose a proper license\nfor a project and avoid most of the licensing errors - all inside the familiar\nIDE editor.\n  The plugin and its source code are available online on GitHub:\nhttps://github.com/JetBrains-Research/sorrel. A demonstration video can be\nfound at https://youtu.be/doUeAwPjcPE.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.07944,regular,pre_llm,2021,7,"{'ai_likelihood': 5.960464477539062e-07, 'text': ""Satisfaction and Performance of Software Developers during Enforced Work\n  from Home in the COVID-19 Pandemic\n\n  Following the onset of the COVID-19 pandemic and subsequent lockdowns, the\ndaily lives of software engineers were heavily disrupted as they were abruptly\nforced to work remotely from home. To better understand and contrast typical\nworking days in this new reality with work in pre-pandemic times, we conducted\none exploratory (N = 192) and one confirmatory study (N = 290) with software\nengineers recruited remotely. Specifically, we build on self-determination\ntheory to evaluate whether and how specific activities are associated with\nsoftware engineers' satisfaction and productivity. To explore the subject\ndomain, we first ran a two-wave longitudinal study. We found that the time\nsoftware engineers spent on specific activities (e.g., coding, bugfixing,\nhelping others) while working from home was similar to pre-pandemic times.\nAlso, the amount of time developers spent on each activity was unrelated to\ntheir general well-being, perceived productivity, and other variables such as\nbasic needs. Our confirmatory study found that activity-specific variables\n(e.g., how much autonomy software engineers had during coding) do predict\nactivity satisfaction and productivity but not by activity-independent\nvariables such as general resilience or a good work-life balance.\nInterestingly, we found that satisfaction and autonomy were significantly\nhigher when software engineers were helping others and lower when they were\nbugfixing. Finally, we discuss implications for software engineers, management,\nand researchers. In particular, active company policies to support developers'\nneed for autonomy, relatedness, and competence appear particularly effective in\na WFH context.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.07357,review,pre_llm,2021,7,"{'ai_likelihood': 8.940696716308594e-07, 'text': 'One Thousand and One Stories: A Large-Scale Survey of Software\n  Refactoring\n\n  Despite the availability of refactoring as a feature in popular IDEs, recent\nstudies revealed that developers are reluctant to use them, and still prefer\nthe manual refactoring of their code. At JetBrains, our goal is to fully\nsupport refactoring features in IntelliJ-based IDEs and improve their adoption\nin practice. Therefore, we start by raising the following main questions. How\nexactly do people refactor code? What refactorings are the most popular? Why do\nsome developers tend not to use convenient IDE refactoring tools?\n  In this paper, we investigate the raised questions through the design and\nimplementation of a survey targeting 1,183 users of IntelliJ-based IDEs. Our\nquantitative and qualitative analysis of the survey results shows that almost\ntwo-thirds of developers spend more than one hour in a single session\nrefactoring their code; that refactoring types vary greatly in popularity; and\nthat a lot of developers would like to know more about IDE refactoring features\nbut lack the means to do so. These results serve us internally to support the\nnext generation of refactoring features, as well as can help our research\ncommunity to establish new directions in the refactoring usability research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.00757,regular,pre_llm,2021,7,"{'ai_likelihood': 1.556343502468533e-06, 'text': 'TMUML: A Singular TM Model with UML Use Cases and Classes\n\n  In the systems and software modeling field, a conceptual model involves\nmodeling with concepts to support development and design. An example of a\nconceptual model is a description developed using the Unified Modeling Language\n(UML). UML uses a model multiplicity formulation approach, wherein a number of\nmodels are used to represent alternative views. By contrast, a model\nsingularity approach uses only a single integrated model. Each of these styles\nof modeling has its strengths and weaknesses. This paper introduces a partial\nsolution to the issue of multiplicity vs. singularity in modeling by adopting\nUML use cases and class models into the conceptual thinging machine (TM) model.\nTo apply use cases, we adopt the observation that a use-case diagram is a\ndescription that shows the internal structure of the part of the system\nrepresented by the use case in addition to being useful to people outside of\nthe system. Additionally, the UML class diagram is recast in TM representation.\nAccordingly, we develop a TMUML model that embraces the TM specification of the\nUML class diagram and the internal structure extracted from the UML use case.\nTMUML modeling introduces some of the advantages that have made UML a popular\nmodeling language to TM modeling. At the same time, this approach supplies UML\nwith partial model singularity. The paper details experimentation with TMUML\nusing examples from the literature. Our results indicate that mixing UML with\nother models could be a viable approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.13633,regular,pre_llm,2021,7,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Beyond SDLC: Process Modeling and Documentation Using Thinging Machines\n\n  The software development life cycle (SDLC) is a procedure used to develop a\nsoftware system that meets both the customer s needs and real-world\nrequirements. The first phase of the SDLC involves creating a conceptual model\nthat represents the involved domain in reality. In requirements engineering,\nbuilding such a model is considered a bridge to the design and construction\nphases. However, this type of model can also serve as a basic model for\nidentifying business processes and how these processes are interconnected to\nachieve the final result. This paper focuses on process modeling in\norganizations, per se, beyond its application in the SDLC when an organization\nneeds further documentation to meet its growth needs and address regular\nchanges over time. The resultant process documentation is created alongside the\ndaily operations of the business process. The model provides process\nvisualization and documentation to assist in defining work patterns, avoiding\nredundancy, or even designing new processes. In this paper, a proposed\ndiagrammatic representation models each process using one diagram comprising\nfive actions and two types of relationships to build three levels of depiction.\nThese levels consist of a static description, events, and the behavior of the\nmodeled process. The viability of a thinging machine is demonstrated by\nre-modeling some examples from the literature.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.01596,review,pre_llm,2021,7,"{'ai_likelihood': 6.953875223795574e-07, 'text': 'A Systematic Review of Mobile Apps for Child Sexual Abuse Education:\n  Limitations and Design Guidelines\n\n  The objectives of this study are understanding the requirements of a CSA\neducation app, identifying the limitations of existing apps, and providing a\nguideline for better app design. An electronic search across three major app\nstores(Google Play, Apple, and Microsoft) is conducted and the selected apps\nare rated by three independent raters. Total 191 apps are found and finally, 14\napps are selected for review based on defined inclusion and exclusion criteria.\nAn app rating scale for CSA education apps is devised by modifying existing\nscales and used to evaluate the selected 14 apps. Our rating scale evaluates\nessential features, criteria, and software quality characteristics that are\nnecessary for CSA education apps, and determined their effectiveness for\npotential use as CSA education programs for children. The internal consistency\nof the rating scale and the inter and intra-rater reliability among the raters\nare also calculated. User comments from the app stores are collected and\nanalyzed to understand their expectations and views. After analyzing the\nfeasibility of reviewed apps, CSA app design considerations are proposed that\nhighlight game-based teaching approaches. Evaluation results showed that most\nof the reviewed apps are not suitable for being used as CSA education programs.\nWhile a few may be able to teach children and parents individually, only the\napps ""Child Abuse Prevention"" (rate 3.89 out of 5) and ""Orbit Rescue"" (rate\n3.92 out of 5) could be deemed suitable for a school-based CSA education\nprogram. However, all those apps need to be improved both their software\nqualities and CSA-specific features for being considered as potential CSA\neducation programs. This study provides the necessary knowledge to developers\nand individuals regarding essential features and software quality\ncharacteristics for designing and developing CSA education apps.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.00033,review,pre_llm,2021,7,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Which RESTful API Design Rules Are Important and How Do They Improve\n  Software Quality? A Delphi Study with Industry Experts\n\n  Several studies analyzed existing Web APIs against the constraints of REST to\nestimate the degree of REST compliance among state-of-the-art APIs. These\nstudies revealed that only a small number of Web APIs are truly RESTful.\nMoreover, identified mismatches between theoretical REST concepts and practical\nimplementations lead us to believe that practitioners perceive many rules and\nbest practices aligned with these REST concepts differently in terms of their\nimportance and impact on software quality. We therefore conducted a Delphi\nstudy in which we confronted eight Web API experts from industry with a catalog\nof 82 REST API design rules. For each rule, we let them rate its importance and\nsoftware quality impact. As consensus, our experts rated 28 rules with high, 17\nwith medium, and 37 with low importance. Moreover, they perceived usability,\nmaintainability, and compatibility as the most impacted quality attributes. The\ndetailed analysis revealed that the experts saw rules for reaching Richardson\nmaturity level 2 as critical, while reaching level 3 was less important. As the\nacquired consensus data may serve as valuable input for designing a\ntool-supported approach for the automatic quality evaluation of RESTful APIs,\nwe briefly discuss requirements for such an approach and comment on the\napplicability of the most important rules.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.08185,review,pre_llm,2021,7,"{'ai_likelihood': 1.9106600019666883e-05, 'text': 'Assessing Support for Industry Standards in Reference Medical Software\n  Architectures\n\n  Industrial standards for developing medical device software provide\nrequirements that conforming devices must meet. A number of reference software\narchitectures have been proposed to develop such software. The ISO/IEC\n25010:2011 family of standards provides a comprehensive software product\nquality model, including characteristics that are highly desirable in medical\ndevices. Furthermore, frameworks like 4+1 Views provide a robust framework to\ndevelop the software architecture or high level design for any software,\nincluding for medical devices. However, the alignment between industrial\nstandards and reference architectures for medical device software, on one hand,\nand ISO/IEC 25010:2011 and 4+1 Views, on the other, is not well understood.\nThis paper aims to explore how ISO/IEC 25010:2011 and 4+1 Views are supported\nby current standards, namely ISO 13485:2016, ISO 14971:2012, IEC 62304:2006 and\nIEC 62366:2015, and current reference architectures for medical device\nsoftware. We classified requirements from medical devices standards into\nqualities from ISO/IEC 25010:2011 and architectural views from 4+1 Views. A\nsystematic literature review (SLR) method was followed to review current\nreferences software architectures and a mapping of their support for the\nidentified ISO/IEC 25010:2011 qualities in the previous step was carried out.\nOur results show that ISO/IEC 25010:2011 qualities like functional suitability,\nportability, maintainability, usability, security, reliability and\ncompatibility are highly emphasised in medical device standards. Furthermore,\nwe show that current reference architectures only partially support these\nqualities. This paper can help medical device developers identify focus areas\nfor developing standards-compliant software. A wider study involving\nunder-development medical devices can help improve the accuracy of our findings\nin the future.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.00213,regular,pre_llm,2021,7,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Adversarial Robustness of Deep Code Comment Generation\n\n  Deep neural networks (DNNs) have shown remarkable performance in a variety of\ndomains such as computer vision, speech recognition, or natural language\nprocessing. Recently they also have been applied to various software\nengineering tasks, typically involving processing source code. DNNs are\nwell-known to be vulnerable to adversarial examples, i.e., fabricated inputs\nthat could lead to various misbehaviors of the DNN model while being perceived\nas benign by humans. In this paper, we focus on the code comment generation\ntask in software engineering and study the robustness issue of the DNNs when\nthey are applied to this task. We propose ACCENT, an identifier substitution\napproach to craft adversarial code snippets, which are syntactically correct\nand semantically close to the original code snippet, but may mislead the DNNs\nto produce completely irrelevant code comments. In order to improve the\nrobustness, ACCENT also incorporates a novel training method, which can be\napplied to existing code comment generation models. We conduct comprehensive\nexperiments to evaluate our approach by attacking the mainstream\nencoder-decoder architectures on two large-scale publicly available datasets.\nThe results show that ACCENT efficiently produces stable attacks with\nfunctionality-preserving adversarial examples, and the generated examples have\nbetter transferability compared with baselines. We also confirm, via\nexperiments, the effectiveness in improving model robustness with our training\nmethod.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.05178,regular,pre_llm,2021,7,"{'ai_likelihood': 4.437234666612413e-06, 'text': 'Software Process Improvement Based on Defect Prevention Using Capability\n  and Testing Model Integration in Extreme Programming\n\n  Nowadays, Software Process Improvement popularly known as SPI has been able\nto receive an immense concern in the continuous process to purify software\nquality. Several Agile methodologies previously have worked with Extreme\nprogramming (XP). Before improving the process, defect prevention (DP) is\ninevitable. In addition, DP largely depends on defect detection either found\nearlier in the design and implementation stages or held in the testing phases.\nHowever, testing maturity model integration (TMMI) has a crucial aspect in DP\nas well as process improvement of the software. In particular, when software\ngets validated by being tested and fixed the defects up, it achieves the\nmaximum capability maturity model integration (CMMI) aiming the process\nimprovement. Here, the article has proposed an improved defect detection and\nprevention model to enhance the software process following the approach of XP.\nBesides, as a unique contribution, we have united the capability and testing\nmodel integration to ensure better SPI.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.05823,regular,pre_llm,2021,7,"{'ai_likelihood': 4.569689432779948e-06, 'text': ""A First Look at Developers' Live Chat on Gitter\n\n  Modern communication platforms such as Gitter and Slack play an increasingly\ncritical role in supporting software teamwork, especially in open source\ndevelopment.Conversations on such platforms often contain intensive, valuable\ninformation that may be used for better understanding OSS developer\ncommunication and collaboration. However, little work has been done in this\nregard. To bridge the gap, this paper reports a first comprehensive empirical\nstudy on developers' live chat, investigating when they interact, what\ncommunity structures look like, which topics are discussed, and how they\ninteract. We manually analyze 749 dialogs in the first phase, followed by an\nautomated analysis of over 173K dialogs in the second phase. We find that\ndevelopers tend to converse more often on weekdays, especially on Wednesdays\nand Thursdays (UTC), that there are three common community structures observed,\nthat developers tend to discuss topics such as API usages and errors, and that\nsix dialog interaction patterns are identified in the live chat communities.\nBased on the findings, we provide recommendations for individual developers and\nOSS communities, highlight desired features for platform vendors, and shed\nlight on future research directions. We believe that the findings and insights\nwill enable a better understanding of developers' live chat, pave the way for\nother researchers, as well as a better utilization and mining of knowledge\nembedded in the massive chat history.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.01093,regular,pre_llm,2021,7,"{'ai_likelihood': 1.5000502268473308e-05, 'text': 'Model Checking C++ Programs\n\n  In the last three decades, memory safety issues in system programming\nlanguages such as C or C++ have been one of the significant sources of security\nvulnerabilities. However, there exist only a few attempts with limited success\nto cope with the complexity of C++ program verification. Here we describe and\nevaluate a novel verification approach based on bounded model checking (BMC)\nand satisfiability modulo theories (SMT) to verify C++ programs formally. Our\nverification approach analyzes bounded C++ programs by encoding into SMT\nvarious sophisticated features that the C++ programming language offers, such\nas templates, inheritance, polymorphism, exception handling, and the Standard\nC++ Libraries. We formalize these features within our formal verification\nframework using a decidable fragment of first-order logic and then show how\nstate-of-the-art SMT solvers can efficiently handle that. We implemented our\nverification approach on top of ESBMC. We compare ESBMC to LLBMC and DIVINE,\nwhich are state-of-the-art verifiers to check C++ programs directly from the\nLLVM bitcode. Experimental results show that ESBMC can handle a wide range of\nC++ programs, presenting a higher number of correct verification results. At\nthe same time, it reduces the verification time if compared to LLBMC and DIVINE\ntools. Additionally, ESBMC has been applied to a commercial C++ application in\nthe telecommunication domain and successfully detected arithmetic overflow\nerrors, potentially leading to security vulnerabilities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.07212,regular,pre_llm,2021,7,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Duplicated Code Pattern Mining in Visual Programming Languages\n\n  Visual Programming Languages (VPLs), coupled with the high-level abstractions\nthat are commonplace in visual programming environments, enable users with less\ntechnical knowledge to become proficient programmers. However, the lower skill\nfloor required by VPLs also entails that programmers are more likely to not\nadhere to best practices of software development, producing systems with high\ntechnical debt, and thus poor maintainability. Duplicated code is one important\nexample of such technical debt. In fact, we observed that the amount of\nduplication in the OutSystems VPL code bases can reach as high as $39\\%$.\n  Duplicated code detection in text-based programming languages is still an\nactive area of research with important implications regarding software\nmaintainability and evolution. However, to the best of our knowledge, the\nliterature on duplicated code detection for VPLs is very limited. We propose a\nnovel and scalable duplicated code pattern mining algorithm that leverages the\nvisual structure of VPLs in order to not only detect duplicated code, but also\nhighlight duplicated code patterns that explain the reported duplication. The\nperformance of the proposed approach is evaluated on a wide range of real-world\nmobile and web applications developed using OutSystems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.05374,regular,pre_llm,2021,7,"{'ai_likelihood': 3.8080745273166235e-06, 'text': ""Integrated and Iterative Requirements Analysis and Test Specification: A\n  Case Study at Kostal\n\n  Currently, practitioners follow a top-down approach in automotive development\nprojects. However, recent studies have shown that this top-down approach is not\nsuitable for the implementation and testing of modern automotive systems.\nSpecifically, practitioners increasingly fail to specify requirements and tests\nfor systems with complex component interactions (e.g., e-mobility systems). In\nthis paper, we address this research gap and propose an integrated and\niterative scenario-based technique for the specification of requirements and\ntest scenarios. Our idea is to combine both a top-down and a bottom-up\nintegration strategy. For the top-down approach, we use a behavior-driven\ndevelopment (BDD) technique to drive the modeling of high-level system\ninteractions from the user's perspective. For the bottom-up approach, we\ndiscovered that natural language processing (NLP) techniques are suited to make\ntextual specifications of existing components accessible to our technique. To\nintegrate both directions, we support the joint execution and automated\nanalysis of system-level interactions and component-level behavior. We\ndemonstrate the feasibility of our approach by conducting a case study at\nKostal (Tier1 supplier). The case study corroborates, among other things, that\nour approach supports practitioners in improving requirements and test\nspecifications for integrated system behavior.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.02585,regular,pre_llm,2021,7,"{'ai_likelihood': 2.384185791015625e-06, 'text': 'Human resources management system for Higher Education institutions\n\n  In an environment where employees and their experience are of central value\nto the company, human resources management (HRM) represents a significant\naspect of business efficiency. In this paper we present a model of an HRM\ninformation system for universities. Special attention is paid to aspects of\nthe system that support processes specific to science and higher education. UML\nwas used for modelling the global and detailed system architecture and database\nmodel. FURPS+ methodology was used for classification of requirements and the\nMoSCoW method for analysis of requirement priority.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.03733,regular,pre_llm,2021,7,"{'ai_likelihood': 2.5497542487250434e-06, 'text': 'Daany -- DAta ANalYtics on .NET\n\n  Daany is .NET and cross platform data analytics and linear algebra library\nwritten in C# supposed to be a tool for data preparation, feature engineering\nand other kind of data transformations and feature engineering. The library is\nimplemented on top of .NET Standard 2.1 and supports .NET Core 3.0 and above\nseparated on several Visual Studio projects that can be installed as a NuGet\npackage. The library implements DataFrame as the core component with extensions\nof a set of data science and linear algebra features. The library contains\nseveral implementation of time series decomposition (SSA, STL ARIMA),\noptimization methods (SGD) as well as plotting support. The library also\nimplements set of features based on matrix, vectors and similar linear algebra\noperations. The main part of the library is the Daany.DataFrame with similar\nimplementation that can be found in python based Pandas library. The paper\npresents the main functionalities and the implementation behind the Daany\npackages in the form of developer guide and can be used as manual in using the\nDaany in every-day work. To end this the paper shows the list of papers used\nthe library.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.07485,regular,pre_llm,2021,7,"{'ai_likelihood': 3.2782554626464844e-06, 'text': ""A Hybrid Simulation Model for Open Software Development Processes\n\n  Open software development provides software organizations access to infinite\nonline resource supply. The resource supply is a pool of unknown workers who\nwork from different location and time zone and are interested in performing\nvarious type of tasks. Improper task execution in such dynamic and competitive\nenvironment leads to zero task registration, zero task submissions or low\nqualified submissions due to unforeseen reasons such as uncertainty in workers'\nbehavior and performance. Therefore, to ensure effectiveness of open software\ndevelopment, there is a need for improved understanding and visibility into\ncharacteristics associated with attracting reliable workers in making qualified\nsubmissions and reducing task failure.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.09753,review,pre_llm,2021,7,"{'ai_likelihood': 1.2252065870496963e-06, 'text': 'Empowering End-users with Object-aware Processes\n\n  Business process management systems from various vendors are used by\ncompanies around the globe. Most of these systems allow for the full or partial\nautomation of business processes by ensuring that tasks and data are presented\nto the right person at the right time during process execution. However, almost\nall established BPMS employ the activity-centric process support paradigm, in\nwhich the various forms, i.e., the main way for users to input data into the\nprocess, have to be created by hand. Furthermore, traditional activity-centric\nprocess management systems are limited in their flexibility as all possible\nexecution variants have to be taken into account by the process modeler.\nTherefore, large amounts of research have gone into developing alternative\nprocess support paradigms, with a large focus on enabling more flexibly\nexecutable processes. This article takes one of these paradigms, object-aware\nprocess management, and presents the concepts we developed while researching\nthe possibility of bringing the power and flexibility of non-activity-centric\nprocess support paradigms to the people that matter: the end-users working with\nthe processes. The contribution of this article are the concepts, ideas, and\nlessons learned during the development and evaluation of the PHILharmonicFlows\nruntime user interface, which allows for the generation of an entire user\ninterface, complete with navigation and forms, based on an object-aware process\nmodel. This novel approach allows for the generation of entire information\nsystems, complete with data storage, process logic, and now fully functional\nuser interfaces in a fully generic fashion from data-centric object-aware\nprocess models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.04981,regular,pre_llm,2021,8,"{'ai_likelihood': 1.0165903303358291e-05, 'text': ""What Drives and Sustains Self-Assignment in Agile Teams\n\n  Self-assignment, where software developers choose their own tasks, is a\ncommon practice in agile teams. However, it is not known why developers select\ncertain tasks. It is important for managers to be aware of these reasons to\nensure sustainable self-assignment practices. We investigated developers'\npreferences while they are choosing tasks for themselves. We collected data\nfrom 42 participants working in 25 different software companies. We applied\nGrounded Theory procedures to study and analyse factors for self-assigning\ntasks, which we grouped into three categories: task-based, developer-based, and\nopinion-based. We found that developers have individual preferences and not all\nfactors are important to every developer. Managers share some common and\nvarying perspectives around the identified factors. Most managers want\ndevelopers to give higher priority to certain factors. Developers often need to\nbalance between task priority and their own individual preferences, and\nmanagers facilitate this through a variety of strategies. More risk-averse\nmanagers encourage expertise-based self-assignment to ensure tasks are\ncompleted quickly. Managers who are risk-balancing encourage developers to\nchoose tasks that provide learning opportunities only when there is little risk\nof delays or reduced quality. Finally, growth-seeking managers regularly\nencourage team members to pick tasks outside their comfort zone to encourage\ngrowth opportunities. Our findings will help managers to understand what\ndevelopers consider when self-assigning tasks and help them empower their teams\nto practice self-assignment in a sustainable manner.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.12078,review,pre_llm,2021,8,"{'ai_likelihood': 1.655684577094184e-07, 'text': ""A Comparative Study of Vulnerability Reporting by Software Composition\n  Analysis Tools\n\n  Background: Modern software uses many third-party libraries and frameworks as\ndependencies. Known vulnerabilities in these dependencies are a potential\nsecurity risk. Software composition analysis (SCA) tools, therefore, are being\nincreasingly adopted by practitioners to keep track of vulnerable dependencies.\nAim: The goal of this study is to understand the difference in vulnerability\nreporting by various SCA tools. Understanding if and how existing SCA tools\ndiffer in their analysis may help security practitioners to choose the right\ntooling and identify future research needs. Method: We present an in-depth case\nstudy by comparing the analysis reports of 9 industry-leading SCA tools on a\nlarge web application, OpenMRS, composed of Maven (Java) and npm (JavaScript)\nprojects. Results: We find that the tools vary in their vulnerability\nreporting. The count of reported vulnerable dependencies ranges from 17 to 332\nfor Maven and from 32 to 239 for npm projects across the studied tools.\nSimilarly, the count of unique known vulnerabilities reported by the tools\nranges from 36 to 313 for Maven and from 45 to 234 for npm projects. Our manual\nanalysis of the tools' results suggest that accuracy of the vulnerability\ndatabase is a key differentiator for SCA tools. Conclusion: We recommend that\npractitioners should not rely on any single tool at the present, as that can\nresult in missing known vulnerabilities. We point out two research directions\nin the SCA space: i) establishing frameworks and metrics to identify false\npositives for dependency vulnerabilities; and ii) building automation\ntechnologies for continuous monitoring of vulnerability data from open source\npackage ecosystems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.06259,regular,pre_llm,2021,8,"{'ai_likelihood': 2.814663781060113e-06, 'text': 'VulnEx: Exploring Open-Source Software Vulnerabilities in Large\n  Development Organizations to Understand Risk Exposure\n\n  The prevalent usage of open-source software (OSS) has led to an increased\ninterest in resolving potential third-party security risks by fixing common\nvulnerabilities and exposures (CVEs). However, even with automated code\nanalysis tools in place, security analysts often lack the means to obtain an\noverview of vulnerable OSS reuse in large software organizations. In this\ndesign study, we propose VulnEx (Vulnerability Explorer), a tool to audit\nentire software development organizations. We introduce three complementary\ntable-based representations to identify and assess vulnerability exposures due\nto OSS, which we designed in collaboration with security analysts. The\npresented tool allows examining problematic projects and applications\n(repositories), third-party libraries, and vulnerabilities across a software\norganization. We show the applicability of our tool through a use case and\npreliminary expert feedback.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.12511,regular,pre_llm,2021,8,"{'ai_likelihood': 1.0927518208821616e-06, 'text': ""An Experimental Analysis of Graph-Distance Algorithms for Comparing API\n  Usages\n\n  Modern software development heavily relies on the reuse of functionalities\nthrough Application Programming Interfaces (APIs). However, client developers\ncan have issues identifying the correct usage of a certain API, causing misuses\naccompanied by software crashes or usability bugs. Therefore, researchers have\naimed at identifying API misuses automatically by comparing client code usages\nto correct API usages. Some techniques rely on certain API-specific graph-based\ndata structures to improve the abstract representation of API usages. Such\ntechniques need to compare graphs, for instance, by computing distance metrics\nbased on the minimal graph edit distance or the largest common subgraphs, whose\ncomputations are known to be NP-hard problems. Fortunately, there exist many\nabstractions for simplifying graph distance computation. However, their\napplicability for comparing graph representations of API usages has not been\nanalyzed. In this paper, we provide a comparison of different distance\nalgorithms of API-usage graphs regarding correctness and runtime. Particularly,\ncorrectness relates to the algorithms' ability to identify similar correct API\nusages, but also to discriminate similar correct and false usages as well as\nnon-similar usages. For this purpose, we systematically identified a set of\neight graph-based distance algorithms and applied them on two datasets of\nreal-world API usages and misuses. Interestingly, our results suggest that\nexisting distance algorithms are not reliable for comparing API usage graphs.\nTo improve on this situation, we identified and discuss the algorithms' issues,\nbased on which we formulate hypotheses to initiate research on overcoming them.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.05316,regular,pre_llm,2021,8,"{'ai_likelihood': 1.7219119601779514e-06, 'text': 'Why are Some Bugs Non-Reproducible? An Empirical Investigation using\n  Data Fusion\n\n  Software developers attempt to reproduce software bugs to understand their\nerroneous behaviours and to fix them. Unfortunately, they often fail to\nreproduce (or fix) them, which leads to faulty, unreliable software systems.\nHowever, to date, only a little research has been done to better understand\nwhat makes the software bugs non-reproducible. In this paper, we conduct a\nmultimodal study to better understand the non-reproducibility of software bugs.\nFirst, we perform an empirical study using 576 non-reproducible bug reports\nfrom two popular software systems (Firefox, Eclipse) and identify 11 key\nfactors that might lead a reported bug to non-reproducibility. Second, we\nconduct a user study involving 13 professional developers where we investigate\nhow the developers cope with non-reproducible bugs. We found that they either\nclose these bugs or solicit for further information, which involves long\ndeliberations and counter-productive manual searches. Third, we offer several\nactionable insights on how to avoid non-reproducibility (e.g., false-positive\nbug report detector) and improve reproducibility of the reported bugs (e.g.,\nsandbox for bug reproduction) by combining our analyses from multiple studies\n(e.g., empirical study, developer study).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.08589,regular,pre_llm,2021,8,"{'ai_likelihood': 1.986821492513021e-06, 'text': 'Software Development Processes in Ocean System Modeling\n\n  Scientific modeling provides mathematical abstractions of real-world systems\nand builds software as implementations of these mathematical abstractions.\nOcean science is a multidisciplinary discipline developing scientific models\nand simulations as ocean system models that are an essential research asset.\n  In software engineering and information systems research, modeling is also an\nessential activity. In particular, business process modeling for business\nprocess management and systems engineering is the activity of representing\nprocesses of an enterprise, so that the current process may be analyzed,\nimproved, and automated.\n  In this paper, we employ process modeling for analyzing scientific software\ndevelopment in ocean science to advance the state in engineering of ocean\nsystem models and to better understand how ocean system models are developed\nand maintained in ocean science. We interviewed domain experts in\nsemi-structured interviews, analyzed the results via thematic analysis, and\nmodeled the results via the business process modeling notation BPMN.\n  The processes modeled as a result describe an aspired state of software\ndevelopment in the domain, which are often not (yet) implemented. This enables\nexisting processes in simulation-based system engineering to be improved with\nthe help of these process models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.07774,regular,pre_llm,2021,8,"{'ai_likelihood': 9.934107462565105e-08, 'text': ""Are Code Review Processes Influenced by the Genders of the Participants?\n\n  Background: Contemporary software development organizations lack diversity\nand the ratios of women in Free and open-source software (FOSS) communities are\neven lower than the industry average. Although the results of recent studies\nhint the existence of biases against women, it is unclear to what extent such\nbiases influence the outcomes of software development tasks.\n  Objective: This study aims to conceptually replicate two recent studies\ninvestigating gender biases in FOSS communities \\textit{ to identify whether\nthe outcomes of or participation in code reviews (or pull requests) are\ninfluenced by the gender of a developer.} In particular, this study focuses on\ntwo outcome aspects (i.e., code acceptance, and review interval) and one\nparticipation aspect (i.e., code review participation) of code review\nprocesses.\n  Method: We will augment the dataset used in the original studies with code\nreviews /pull requests created during recent years. Using this dataset, we will\ntrain multivariate regression models to accurately model the influences of\ndevelopers' genders on code acceptance, review intervals, and code review\nparticipation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.02044,regular,pre_llm,2021,8,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'A Comparison of Different Source Code Representation Methods for\n  Vulnerability Prediction in Python\n\n  In the age of big data and machine learning, at a time when the techniques\nand methods of software development are evolving rapidly, a problem has arisen:\nprogrammers can no longer detect all the security flaws and vulnerabilities in\ntheir code manually. To overcome this problem, developers can now rely on\nautomatic techniques, like machine learning based prediction models, to detect\nsuch issues. An inherent property of such approaches is that they work with\nnumeric vectors (i.e., feature vectors) as inputs. Therefore, one needs to\ntransform the source code into such feature vectors, often referred to as code\nembedding. A popular approach for code embedding is to adapt natural language\nprocessing techniques, like text representation, to automatically derive the\nnecessary features from the source code. However, the suitability and\ncomparison of different text representation techniques for solving Software\nEngineering (SE) problems is rarely studied systematically. In this paper, we\npresent a comparative study on three popular text representation methods,\nword2vec, fastText, and BERT applied to the SE task of detecting\nvulnerabilities in Python code. Using a data mining approach, we collected a\nlarge volume of Python source code in both vulnerable and fixed forms that we\nembedded with word2vec, fastText, and BERT to vectors and used a Long\nShort-Term Memory network to train on them. Using the same LSTM architecture,\nwe could compare the efficiency of the different embeddings in deriving\nmeaningful feature vectors. Our findings show that all the text representation\nmethods are suitable for code representation in this particular task, but the\nBERT model is the most promising as it is the least time consuming and the LSTM\nmodel based on it achieved the best overall accuracy(93.8%) in predicting\nPython source code vulnerabilities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.07618,review,pre_llm,2021,8,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Using Guilds to Foster Internal Startups in Large Organizations: A case\n  study\n\n  Software product innovation in large organizations is fundamentally\nchal-lenging because of restrained freedom and flexibility to conduct\nexperi-ments. As a response, large agile companies form internal startups to\ninitiate employ-driven innovation, inspired by Lean startup. This case study\ninvesti-gates how communities of practice support five internal startups in\ndevelop-ing new software products within a large organization. We observed six\ncommunities of practice meetings, two workshops and conducted ten\nsemi-structured interviews over the course of a year. Our findings show that a\ncommunity of practice, called the Innovation guild, allowed internal startups\nto help each other by collectively solving problems, creating shared\npractic-es, and sharing knowledge. This study confirms that benefits documented\nin earlier research into CoPs also hold true in the context of software product\ninnovation in large organizations. Henceforth, we suggest that similar\ninnova-tion guilds, as described in this paper, can support large companies in\nthe in-novation race for new software products.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.08139,regular,pre_llm,2021,8,"{'ai_likelihood': 5.298190646701389e-07, 'text': 'Towards Mapping Control Theory and Software Engineering Properties using\n  Specification Patterns\n\n  A traditional approach to realize self-adaptation in software engineering\n(SE) is by means of feedback loops. The goals of the system can be specified as\nformal properties that are verified against models of the system. On the other\nhand, control theory (CT) provides a well-established foundation for designing\nfeedback loop systems and providing guarantees for essential properties, such\nas stability, settling time, and steady state error. Currently, it is an open\nquestion whether and how traditional SE approaches to self-adaptation consider\nproperties from CT. Answering this question is challenging given the principle\ndifferences in representing properties in both fields. In this paper, we take a\nfirst step to answer this question. We follow a bottom up approach where we\nspecify a control design (in Simulink) for a case inspired by Scuderia Ferrari\n(F1) and provide evidence for stability and safety. The design is then\ntransferred into code (in C) that is further optimized. Next, we define\nproperties that enable verifying whether the control properties still hold at\ncode level. Then, we consolidate the solution by mapping the properties in both\nworlds using specification patterns as common language and we verify the\ncorrectness of this mapping. The mapping offers a reusable artifact to solve\nsimilar problems. Finally, we outline opportunities for future work,\nparticularly to refine and extend the mapping and investigate how it can\nimprove the engineering of self-adaptive systems for both SE and CT engineers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.07117,regular,pre_llm,2021,8,"{'ai_likelihood': 3.543164994981554e-06, 'text': 'A Program Synthesis Approach for Adding Architectural Tactics to An\n  Existing Code Base\n\n  Automatically constructing a program based on given specifications has been\nstudied for decades. Despite the advances in the field of Program Synthesis,\nthe current approaches still synthesize a block of code snippet and leave the\ntask of reusing it in an existing code base to program developers. Due to its\nprogram-wide effects, synthesizing an architectural tactic and reusing it in a\nprogram is even more challenging. Architectural tactics need to be synthesized\nbased on the context of different locations of the program, broken down to\nsmaller pieces, and added to corresponding locations in the code. Moreover,\neach piece needs to establish correct data- and control-dependencies to its\nsurrounding environment as well as to the other synthesized pieces. This is an\nerror-prone and challenging task, especially for novice program developers. In\nthis paper, we introduce a novel program synthesis approach that synthesizes\narchitectural tactics and adds them to an existing code base.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.09991,regular,pre_llm,2021,8,"{'ai_likelihood': 3.642506069607205e-07, 'text': ""Q&A MAESTRO: Q&A Post Recommendation for Fixing Java Runtime Exceptions\n\n  Programmers often use Q&A sites (e.g., Stack Overflow) to understand a root\ncause of program bugs. Runtime exceptions is one of such important class of\nbugs that is actively discussed on Stack Overflow. However, it may be difficult\nfor beginner programmers to come up with appropriate keywords for search.\nMoreover, they need to switch their attentions between IDE and browser, and it\nis time-consuming. To overcome these difficulties, we proposed a method, ``Q&A\nMAESTRO'', to find suitable Q&A posts automatically for Java runtime exception\nby utilizing structure information of codes described in programming Q&A\nwebsite. In this paper, we describe a usage scenario of IDE-plugin, the\narchitecture and user interface of the implementation, and results of user\nstudies. A video is available at https://youtu.be/4X24jJrMUVw. A demo software\nis available at https://github.com/FujitsuLaboratories/Q-A-MAESTRO.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.13861,review,pre_llm,2021,8,"{'ai_likelihood': 9.934107462565105e-08, 'text': ""An Artificial Intelligence Life Cycle: From Conception to Production\n\n  Drawing on our experience of more than a decade of AI in academic research,\ntechnology development, industry engagement, postgraduate teaching, doctoral\nsupervision and organisational consultancy, we present the 'CDAC AI Life\nCycle', a comprehensive life cycle for the design, development and deployment\nof Artificial Intelligence (AI) systems and solutions. It consists of three\nphases, Design, Develop and Deploy, and 17 constituent stages across the three\nphases from conception to production of any AI initiative. The 'Design' phase\nhighlights the importance of contextualising a problem description by reviewing\npublic domain and service-based literature on state-of-the-art AI applications,\nalgorithms, pre-trained models and equally importantly ethics guidelines and\nframeworks, which then informs the data, or Big Data, acquisition and\npreparation. The 'Develop' phase is technique-oriented, as it transforms data\nand algorithms into AI models that are benchmarked, evaluated and explained.\nThe 'Deploy' phase evaluates computational performance, which then apprises\npipelines for model operationalisation, culminating in the hyperautomation of a\nprocess or system as a complete AI solution, that is continuously monitored and\nevaluated to inform the next iteration of the life cycle. An ontological\nmapping of AI algorithms to applications, followed by an organisational context\nfor the AI life cycle are further contributions of this article.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.04311,regular,pre_llm,2021,8,"{'ai_likelihood': 3.907415601942274e-06, 'text': 'Recommender Systems for Software Project Managers\n\n  The design of recommendation systems is based on complex information\nprocessing and big data interaction. This personalized view has evolved into a\nhot area in the past decade, where applications might have been proved to help\nfor solving problem in the software development field. Therefore, with the\nevolvement of Recommendation System in Software Engineering (RSSE), the\ncoordination of software projects with their stakeholders is improving. This\nexperiment examines four open source recommender systems and implemented a\ncustomized recommender engine with two industrial-oriented packages: Lenskit\nand Mahout. Each of the main functions was examined and issues were identified\nduring the experiment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.07509,regular,pre_llm,2021,8,"{'ai_likelihood': 2.0199351840549047e-05, 'text': 'Robustifying Controller Specifications of Cyber-Physical Systems Against\n  Perceptual Uncertainty\n\n  Formal reasoning on the safety of controller systems interacting with plants\nis complex because developers need to specify behavior while taking into\naccount perceptual uncertainty. To address this, we propose an automated\nworkflow that takes an Event-B model of an uncertainty-unaware controller and a\nspecification of uncertainty as input. First, our workflow automatically\ninjects the uncertainty into the original model to obtain an uncertainty-aware\nbut potentially unsafe controller. Then, it automatically robustifies the\ncontroller so that it satisfies safety even under the uncertainty. The case\nstudy shows how our workflow helps developers to explore multiple levels of\nperceptual uncertainty. We conclude that our workflow makes design and analysis\nof uncertainty-aware controller systems easier and more systematic.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.01734,regular,pre_llm,2021,8,"{'ai_likelihood': 7.947285970052084e-07, 'text': ""Tutorials on Testing Neural Networks\n\n  Deep learning achieves remarkable performance on pattern recognition, but can\nbe vulnerable to defects of some important properties such as robustness and\nsecurity. This tutorial is based on a stream of research conducted since the\nsummer of 2018 at a few UK universities, including the University of Liverpool,\nUniversity of Oxford, Queen's University Belfast, University of Lancaster,\nUniversity of Loughborough, and University of Exeter.\n  The research aims to adapt software engineering methods, in particular\nsoftware testing methods, to work with machine learning models. Software\ntesting techniques have been successful in identifying software bugs, and\nhelping software developers in validating the software they design and\nimplement. It is for this reason that a few software testing techniques -- such\nas the MC/DC coverage metric -- have been mandated in industrial standards for\nsafety critical systems, including the ISO26262 for automotive systems and the\nRTCA DO-178B/C for avionics systems. However, these techniques cannot be\ndirectly applied to machine learning models, because the latter are drastically\ndifferent from traditional software, and their design follows a completely\ndifferent development life-cycle.\n  As the outcome of this thread of research, the team has developed a series of\nmethods that adapt the software testing techniques to work with a few classes\nof machine learning models. The latter notably include convolutional neural\nnetworks, recurrent neural networks, and random forest. The tools developed\nfrom this research are now collected, and publicly released, in a GitHub\nrepository: \\url{https://github.com/TrustAI/DeepConcolic}, with the BSD\n3-Clause licence.\n  This tutorial is to go through the major functionalities of the tools with a\nfew running examples, to exhibit how the developed techniques work, what the\nresults are, and how to interpret them.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.02966,regular,pre_llm,2021,8,"{'ai_likelihood': 1.1192427741156685e-05, 'text': 'Verifying Time Complexity of Binary Search using Dafny\n\n  Formal software verification techniques are widely used to specify and prove\nthe functional correctness of programs. However, nonfunctional properties such\nas time complexity are usually carried out with pen and paper. Inefficient code\nin terms of time complexity may cause massive performance problems in\nlarge-scale complex systems. We present a proof of concept for using the Dafny\nverification tool to specify and verify the worst-case time complexity of\nbinary search. This approach can also be used for academic purposes as a new\nway to teach algorithms and complexity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.08543,regular,pre_llm,2021,8,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'Unsupervised Topic Discovery in User Comments\n\n  On social media platforms like Twitter, users regularly share their opinions\nand comments with software vendors and service providers. Popular software\nproducts might get thousands of user comments per day. Research has shown that\nsuch comments contain valuable information for stakeholders, such as feature\nideas, problem reports, or support inquiries. However, it is hard to manually\nmanage and grasp a large amount of user comments, which can be redundant and of\na different quality. Consequently, researchers suggested automated approaches\nto extract valuable comments, e.g., through problem report classifiers.\nHowever, these approaches do not aggregate semantically similar comments into\nspecific aspects to provide insights like how often users reported a certain\nproblem.\n  We introduce an approach for automatically discovering topics composed of\nsemantically similar user comments based on deep bidirectional natural language\nprocessing algorithms. Stakeholders can use our approach without the need to\nconfigure critical parameters like the number of clusters. We present our\napproach and report on a rigorous multiple-step empirical evaluation to assess\nhow cohesive and meaningful the resulting clusters are. Each evaluation step\nwas peer-coded and resulted in inter-coder agreements of up to 98%, giving us\nhigh confidence in the approach. We also report a thematic analysis on the\ntopics discovered from tweets in the telecommunication domain.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.05846,regular,pre_llm,2021,8,"{'ai_likelihood': 1.8874804178873699e-06, 'text': ""Automating the Removal of Obsolete TODO Comments\n\n  TODO comments are very widely used by software developers to describe their\npending tasks during software development. However, after performing the task\ndevelopers sometimes neglect or simply forget to remove the TODO comment,\nresulting in obsolete TODO comments. These obsolete TODO comments can confuse\ndevelopment teams and may cause the introduction of bugs in the future,\ndecreasing the software's quality and maintainability. In this work, we propose\na novel model, named TDCleaner (TODO comment Cleaner), to identify obsolete\nTODO comments in software projects. TDCleaner can assist developers in\njust-in-time checking of TODO comments status and avoid leaving obsolete TODO\ncomments. Our approach has two main stages: offline learning and online\nprediction. During offline learning, we first automatically establish\n<code_change, todo_comment, commit_msg> training samples and leverage three\nneural encoders to capture the semantic features of TODO comment, code change\nand commit message respectively. TDCleaner then automatically learns the\ncorrelations and interactions between different encoders to estimate the final\nstatus of the TODO comment. For online prediction, we check a TODO comment's\nstatus by leveraging the offline trained model to judge the TODO comment's\nlikelihood of being obsolete. We built our dataset by collecting TODO comments\nfrom the top-10,000 Python and Java Github repositories and evaluated TDCleaner\non them. Extensive experimental results show the promising performance of our\nmodel over a set of benchmarks. We also performed an in-the-wild evaluation\nwith real-world software projects, we reported 18 obsolete TODO comments\nidentified by TDCleaner to Github developers and 9 of them have already been\nconfirmed and removed by the developers, demonstrating the practical usage of\nour approach.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.06289,regular,pre_llm,2021,8,"{'ai_likelihood': 2.4504131740993925e-06, 'text': ""Code Perfumes: Reporting Good Code to Encourage Learners\n\n  Block-based programming languages like Scratch enable children to be creative\nwhile learning to program. Even though the block-based approach simplifies the\ncreation of programs, learning to program can nevertheless be challenging.\nAutomated tools such as linters therefore support learners by providing\nfeedback about potential bugs or code smells in their programs. Even when this\nfeedback is elaborate and constructive, it still represents purely negative\ncriticism and by construction ignores what learners have done correctly in\ntheir programs. In this paper we introduce an orthogonal approach to linting:\nWe complement the criticism produced by a linter with positive feedback. We\nintroduce the concept of code perfumes as the counterpart to code smells,\nindicating the correct application of programming practices considered to be\ngood. By analysing not only what learners did wrong but also what they did\nright we hope to encourage learners, to provide teachers and students a better\nunderstanding of learners' progress, and to support the adoption of automated\nfeedback tools. Using a catalogue of 25 code perfumes for Scratch, we\nempirically demonstrate that these represent frequent practices in Scratch, and\nwe find that better programs indeed contain more code perfumes.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.13768,regular,pre_llm,2021,9,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'To VR or not to VR: Is virtual reality suitable to understand software\n  development metrics?\n\n  Background/Context: Currently, the usual interface for visualizing data is\nbased on 2-D screens. Recently, devices capable of visualizing data while\nimmersed in VR scenes are becoming common. However, it has not been studied in\ndetail to which extent these devices are suitable for interacting with data\nvisualizations in the specific case of data about software development.\nObjective/Aim: In this registered report, we propose to answer the following\nquestion: ""Is comprehension of software development processes, via the\nvisualization of their metrics, better when presented in VR scenes than in 2D\nscreens?"" In particular, we will study if answers obtained after interacting\nwith visualizations presented as VR scenes are more or less correct than those\nobtained from traditional screens, and if it takes more or less time to produce\nthose answers. Method: We will run an experiment with volunteer subjects from\nseveral backgrounds. We will have two setups: an on-screen application, and a\nVR scene. Both will be designed to be as much equivalent as possible in terms\nof the information they provide. For the former, we use a commercial-grade set\nof \\kibana-based interactive dashboards that stakeholders currently use to get\ninsights. For the latter, we use a set of visualizations similar to those in\nthe on-screen case, prepared to provide the same set of data using the museum\nmetaphor in a VR room. The field of analysis will be related to modern code\nreview, in particular pull request activity. The subjects will try to answer\nsome questions in both setups (some will work first in VR, some on-screen),\nwhich will be presented to them in random order. To draw results, we will\ncompare and statistically analyze both the correctness of their answers, and\nthe time spent until they are produced.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.14217,regular,pre_llm,2021,9,"{'ai_likelihood': 6.622738308376736e-06, 'text': ""Live Visualization of Dynamic Software Cities with Heat Map Overlays\n\n  The 3D city metaphor in software visualization is a well-explored rendering\nmethod. Numerous tools use their custom variation to visualize offline-analyzed\ndata. Heat map overlays are one of these variants. They introduce a separate\ninformation layer in addition to the software city's own semantics. Results\nshow that their usage facilitates program comprehension.\n  In this paper, we present our heat map approach for the city metaphor\nvisualization based on live trace analysis. In comparison to previous\napproaches, our implementation uses live dynamic analysis of a software\nsystem's runtime behavior. At any time, users can toggle the heat map feature\nand choose which runtime-dependent metric the heat map should visualize. Our\napproach continuously and automatically renders both software cities and heat\nmaps. It does not require a manual or semi-automatic generation of heat maps\nand seamlessly blends into the overall software visualization. We implemented\nthis approach in our web-based tool ExplorViz, such that the heat map overlay\nis also available in our augmented reality environment. ExplorViz is developed\nas open source software and is continuously published via Docker images. A live\ndemo of ExplorViz is publicly available.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.09601,review,pre_llm,2021,9,"{'ai_likelihood': 4.6690305074055995e-06, 'text': 'DevOps Adoption: Eight Emergent Perspectives\n\n  DevOps is an approach based on lean and agile principles in which business,\ndevelopment, operations, and quality teams cooperate to deliver software\ncontinuously aiming at reducing time to market, and receiving constant feedback\nfrom customers. However, implementing DevOps can be a complex and challenging\nmission due it requires significant paradigm shift. Consequently, many failures\nand misconceptions can occur about DevOps adoption by organizations, despite\nits numerous benefits. This work identifies, describes, and compares different\nperspectives related to DevOps adoption in academy and industry. The\nperspectives can be understood as factors or variables that influence or help\nto understand the DevOps journey. We employed a sequential multi-method\nresearch approach, including Systematic Literature Review (SLR) and Case Study.\nAs a result, eight perspectives were found: concepts, models, principles,\npractices, difficulties, challenges, benefits, and strategies. More\nspecifically, the SLR produced 390 items, which can be understood as\noccurrences of a perspective. The conducted case study confirmed 75 items,\ncorroborating the SLR findings, while another 29 items emerged. This global\nview on DevOps adoption may guide beginners, both theorists, and practitioners,\nto make the necessary organizational transformation less painful.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.00872,review,pre_llm,2021,9,"{'ai_likelihood': 1.556343502468533e-06, 'text': 'Non-functional Requirements for Machine Learning: Understanding Current\n  Use and Challenges in Industry\n\n  Machine Learning (ML) is an application of Artificial Intelligence (AI) that\nuses big data to produce complex predictions and decision-making systems, which\nwould be challenging to obtain otherwise. To ensure the success of ML-enabled\nsystems, it is essential to be aware of certain qualities of ML solutions\n(performance, transparency, fairness), known from a Requirement Engineering\n(RE) perspective as non-functional requirements (NFRs). However, when systems\ninvolve ML, NFRs for traditional software may not apply in the same ways; some\nNFRs may become more prominent or less important; NFRs may be defined over the\nML model, data, or the entire system; and NFRs for ML may be measured\ndifferently. In this work, we aim to understand the state-of-the-art and\nchallenges of dealing with NFRs for ML in industry. We interviewed ten\nengineering practitioners working with NFRs and ML. We find examples of (1) the\nidentification and measurement of NFRs for ML, (2) identification of more and\nless important NFRs for ML, and (3) the challenges associated with NFRs and ML\nin the industry. This knowledge paints a picture of how ML-related NFRs are\ntreated in practice and helps to guide future RE for ML efforts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.11902,regular,pre_llm,2021,9,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Broccoli: Bug localization with the help of text search engines\n\n  Bug localization is a tedious activity in the bug fixing process in which a\nsoftware developer tries to locate bugs in the source code described in a bug\nreport. Since this process is time-consuming and requires additional knowledge\nabout the software project, information retrieval techniques can aid the bug\nlocalization process. In this paper, we investigate if normal text search\nengines can improve existing bug localization approaches. In a case study, we\nevaluate the performance of our search engine approach Broccoli against seven\nstate-of-the-art bug localization algorithms on 82 open source projects in two\ndata sets. Our results show that including a search engine can increase the\nperformance of the bug localization and that it is a useful extension to\nexisting approaches. As part of our analysis we also exposed a flaw in a\ncommonly used benchmark strategy, i.e., that files of a single release are\nconsidered. To increase the number of detectable files, we mitigate this flaw\nby considering the state of the software repository at the time of the bug\nreport. Our results show that using single releases may lead to an\nunderestimation of the the prediction performance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.07868,review,pre_llm,2021,9,"{'ai_likelihood': 9.602970547146267e-07, 'text': 'The Influence of Human Aspects on Requirements Engineering-related\n  Activities: Software Practitioners Perspective\n\n  Requirements Engineering (RE)-related activities require high collaboration\nbetween various roles in software engineering (SE), such as requirements\nengineers, stakeholders, developers, etc. Their demographics, views,\nunderstanding of technologies, working styles, communication and collaboration\ncapabilities make RE highly human dependent. Identifying how ""human aspects""\nsuch as motivation, domain knowledge, communication skills, personality,\nemotions, culture, etc. might impact RE-related activities would help us\nimprove the RE and SE in general. This study aims to better understand current\nindustry perspectives on the influence of human aspects on RE-related\nactivities, specifically focusing on motivation and personality by targeting\nsoftware practitioners involved in RE-related activities. Our findings indicate\nthat software practitioners consider motivation, domain knowledge, attitude,\ncommunication skills and personality as highly important human aspects when\ninvolved in RE-related activities. A set of factors were identified as software\npractitioners motivational factors when involved in RE-related activities and\nidentified important personality characteristics to have when involved in RE.\nWe also identified factors that made individuals less effective when involved\nin RE-related activities and obtained an initial idea on measuring individuals\nperformance when involved in RE. The findings from our study suggest various\nareas needing more investigation, and we summarise a set of key recommendations\nfor further research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.03146,regular,pre_llm,2021,9,"{'ai_likelihood': 1.3245476616753472e-06, 'text': ""Toward Generating Sufficiently Valid Test Case Results: A Method for\n  Systematically Assigning Test Cases to Test Bench Configurations in a\n  Scenario-Based Test Approach for Automated Vehicles\n\n  To successfully launch automated vehicles into the consumer market, there\nmust be credible proof that the vehicles will operate safely. However, finding\na method to validate the vehicles' safe operation is a challenging problem.\nWhile scenario-based test approaches seem to be possible solutions, they\nrequire execution of a large number of test cases. Several test benches,\nranging from actual test vehicles to partly or fully simulated environments,\nare available to execute these test cases. Each test bench provides different\nelements, which in turn, have different parameters and parameter ranges. The\ncomposition of elements with their specific parameter values at a specific test\nbench that is used to execute a test case is referred to as a test bench\nconfiguration. However, selecting the most suitable test bench configuration is\ndifficult. The selected test bench configuration determines whether the\nexecution of a specific test case provides sufficiently valid test case results\nwith respect to the intended purpose, for example, validating a vehicle's safe\noperation. The effective and efficient execution of a large number of test\ncases requires a method for systematically assigning test cases to the most\nsuitable test bench configuration. Based on a proposed method for classifying\ntest bench configurations, we propose and illustrate a method for\nsystematically assigning test cases to test bench configurations in a\nscenario-based test approach for automated vehicles. This assignment method\nallows for the effective and efficient execution of a large number of test\ncases while generating sufficiently valid test case results.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.02063,regular,pre_llm,2021,9,"{'ai_likelihood': 6.291601392957899e-07, 'text': 'How Do Practitioners Interpret Conditionals in Requirements?\n\n  Context: Conditional statements like ""If A and B then C"" are core elements\nfor describing software requirements. However, there are many ways to express\nsuch conditionals in natural language and also many ways how they can be\ninterpreted. We hypothesize that conditional statements in requirements are a\nsource of ambiguity, potentially affecting downstream activities such as test\ncase generation negatively. Objective: Our goal is to understand how specific\nconditionals are interpreted by readers who work with requirements. Method: We\nconduct a descriptive survey with 104 RE practitioners and ask how they\ninterpret 12 different conditional clauses. We map their interpretations to\nlogical formulas written in Propositional (Temporal) Logic and discuss the\nimplications. Results: The conditionals in our tested requirements were\ninterpreted ambiguously. We found that practitioners disagree on whether an\nantecedent is only sufficient or also necessary for the consequent.\nInterestingly, the disagreement persists even when the system behavior is known\nto the practitioners. We also found that certain cue phrases are associated\nwith specific interpretations. Conclusion: Conditionals in requirements are a\nsource of ambiguity and there is not just one way to interpret them formally.\nThis affects any analysis that builds upon formalized requirements (e.g.,\ninconsistency checking, test-case generation). Our results may also influence\nguidelines for writing requirements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.01192,review,pre_llm,2021,9,"{'ai_likelihood': 9.139378865559896e-06, 'text': ""Log severity levels matter: A multivocal mapping\n\n  The choice of log severity level can be challenging and cause problems in\nproducing reliable logging data. However, there is a lack of specifications and\npractical guidelines to support this challenge. In this study, we present a\nmultivocal systematic mapping of log severity levels from peer-reviewed\nliterature, logging libraries, and practitioners' views. We analyzed 19\nseverity levels, 27 studies, and 40 logging libraries. Our results show\nredundancy and semantic similarity between the levels and a tendency to\nconverge the levels for a total of six levels. Our contributions help leverage\nthe reliability of log entries: (i) mapping the literature about log severity\nlevels, (ii) mapping the severity levels in logging libraries, (iii) a set of\nsynthesized six definitions and four general purposes for severity levels. We\nrecommend that developers use a standard nomenclature, and for logging library\ncreators, we suggest providing accurate and unambiguous definitions of log\nseverity levels.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.0574,review,pre_llm,2021,9,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Data Preparation for Software Vulnerability Prediction: A Systematic\n  Literature Review\n\n  Software Vulnerability Prediction (SVP) is a data-driven technique for\nsoftware quality assurance that has recently gained considerable attention in\nthe Software Engineering research community. However, the difficulties of\npreparing Software Vulnerability (SV) related data is considered as the main\nbarrier to industrial adoption of SVP approaches. Given the increasing, but\ndispersed, literature on this topic, it is needed and timely to systematically\nselect, review, and synthesize the relevant peer-reviewed papers reporting the\nexisting SV data preparation techniques and challenges. We have carried out a\nSystematic Literature Review (SLR) of SVP research in order to develop a\nsystematized body of knowledge of the data preparation challenges, solutions,\nand the needed research. Our review of the 61 relevant papers has enabled us to\ndevelop a taxonomy of data preparation for SVP related challenges. We have\nanalyzed the identified challenges and available solutions using the proposed\ntaxonomy. Our analysis of the state of the art has enabled us identify the\nopportunities for future research. This review also provides a set of\nrecommendations for researchers and practitioners of SVP approaches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.10134,review,pre_llm,2021,9,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'A Quality Assessment Instrument for Systematic Literature Reviews in\n  Software Engineering\n\n  Context: Systematic literature reviews (SLRs) have become standard practise\nas part of software engineering research, although their quality varies. To\nbuild on the reviews, both for future research and industry practice, they need\nto be of high quality. Objective: To assess the quality of SLRs in software\nengineering, we put forward an appraisal instrument for SLRs. The instrument is\nintended for use by appraisers of reviews, but authors may also use it as a\nchecklist when designing and documenting their reviews. Method: A\nwell-established appraisal instrument from research in healthcare was used as a\nstarting point to develop a quality assessment instrument. It is adapted to\nsoftware engineering using guidelines, checklists, and experiences from\nsoftware engineering. As a validation step, the first version was reviewed by\nfour external experts on SLRs in software engineering and updated based on\ntheir feedback. Results: The outcome of the research is an appraisal instrument\nfor the quality assessment of SLRs in software engineering. The instrument\nintends to support the appraiser in assessing the quality of an SLR. The\ninstrument includes 16 items with different options to capture the quality. The\nitem is assessed on a two or three-grade scale, depending on the item. The\ninstrument also supports consolidating the items into groups, which are then\nused to assess the overall quality of a systematic literature review.\nConclusion: It is concluded that the presented instrument may be helpful\nsupport for an appraiser in assessing the quality of SLRs in software\nengineering.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.01138,regular,pre_llm,2021,9,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'APIzation: Generating Reusable APIs from StackOverflow Code Snippets\n\n  Developer forums like StackOverflow have become essential resources to modern\nsoftware development practices. However, many code snippets lack a well-defined\nmethod declaration, and thus they are often incomplete for immediate reuse.\nDevelopers must adapt the retrieved code snippets by parameterizing the\nvariables involved and identifying the return value. This activity, which we\ncall APIzation of a code snippet, can be tedious and time-consuming. In this\npaper, we present APIzator to perform APIzations of Java code snippets\nautomatically. APIzator is grounded by four common patterns that we extracted\nby studying real APIzations in GitHub. APIzator presents a static analysis\nalgorithm that automatically extracts the method parameters and return\nstatements. We evaluated APIzator with a ground-truth of 200 APIzations\ncollected from 20 developers. For 113 (56.50 %) and 115 (57.50 %) APIzations,\nAPIzator and the developers extracted identical parameters and return\nstatements, respectively. For 163 (81.50 %) APIzations, either the parameters\nor the return statements were identical.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.08848,regular,pre_llm,2021,9,"{'ai_likelihood': 3.3775965372721357e-06, 'text': 'An Exploration of npm Package Co-Usage Examples from Stack Overflow: A\n  Case Study\n\n  Third-party package usage has become a common practice in contemporary\nsoftware development. Developers often face different challenges, including\nchoosing the right libraries, installing errors, discrepancies, setting up the\nenvironment, and building failures during software development. The risks of\nmaintaining a third-party package are well known, but it is unclear how\ninformation from Stack Overflow (SO) can be useful. This paper performed an\nempirical study to explore npm co-usage in SO. From over 30,000 SO posts, we\nextracted 2,100 SO posts related to npm and matched them to 217,934 npm library\npackages. We find that, popular and highly used libraries are not discussed as\noften in SO. However, we can see that the accepted answers may prove useful, as\nwe believe that the usage examples and executable commands could be reused for\ntool support.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.12645,regular,pre_llm,2021,9,"{'ai_likelihood': 1.457002427842882e-06, 'text': 'Defect Prediction Guided Search-Based Software Testing\n\n  Today, most automated test generators, such as search-based software testing\n(SBST) techniques focus on achieving high code coverage. However, high code\ncoverage is not sufficient to maximise the number of bugs found, especially\nwhen given a limited testing budget. In this paper, we propose an automated\ntest generation technique that is also guided by the estimated degree of\ndefectiveness of the source code. Parts of the code that are likely to be more\ndefective receive more testing budget than the less defective parts. To measure\nthe degree of defectiveness, we leverage Schwa, a notable defect prediction\ntechnique.\n  We implement our approach into EvoSuite, a state of the art SBST tool for\nJava. Our experiments on the Defects4J benchmark demonstrate the improved\nefficiency of defect prediction guided test generation and confirm our\nhypothesis that spending more time budget on likely defective parts increases\nthe number of bugs found in the same time budget.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.06578,review,pre_llm,2021,9,"{'ai_likelihood': 7.28501213921441e-07, 'text': ""Joining Forces: Applying Design Thinking Techniques in Scrum Meetings\n\n  The most prominent Agile framework Scrum, is often criticized for its amount\nof meetings. These regular events are essential to the empirical\ninspect-and-adapt cycle proposed by Agile methods. Scrum meetings face several\nchallenges, such as being perceived as boring, repetitive, or irrelevant,\nleading to decreased cooperation in teams and less successful projects. In an\nattempt to address these challenges, Agile practitioners have adopted teamwork,\ninnovation, and design techniques geared towards improving collaboration.\nAdditionally, they have developed their own activities to be used in Scrum\nmeetings, most notably for conducting retrospective and planning events. Design\nthinking incorporates non-designers and designers in design and\nconceptualization activities, including user research, ideation, or testing.\nAccordingly, the design thinking approach provides a process with different\nphases and accompanying techniques for each step. These design thinking\ntechniques can support shared understanding in teams and can improve\ncollaboration, creativity, and product understanding. For these reasons, design\nthinking techniques represent a worthwhile addition to the Scrum meeting\ntoolkit and can support Agile meetings in preventing or countering common\nmeeting challenges and achieving meeting goals. This chapter explores how\ntechniques from the design thinking toolkit can support Scrum meetings from a\ntheoretical and practical viewpoint. We analyze Scrum meetings' requirements,\ngoals, and challenges and link them to groups of techniques from the design\nthinking toolkit. In addition, we review interview and observational data from\ntwo previous studies with software development practitioners and derive\nconcrete examples. As a result, we present initial guidelines on integrating\ndesign thinking techniques into Scrum meetings to make them more engaging,\ncollaborative, and interactive.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.13168,regular,pre_llm,2021,9,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Scalable and Accurate Test Case Prioritization in Continuous Integration\n  Contexts\n\n  Continuous Integration (CI) requires efficient regression testing to ensure\nsoftware quality without significantly delaying its CI builds. This warrants\nthe need for techniques to reduce regression testing time, such as Test Case\nPrioritization (TCP) techniques that prioritize the execution of test cases to\ndetect faults as early as possible. Many recent TCP studies employ various\nMachine Learning (ML) techniques to deal with the dynamic and complex nature of\nCI. However, most of them use a limited number of features for training ML\nmodels and evaluate the models on subjects for which the application of TCP\nmakes little practical sense, due to their small regression testing time and\nlow number of failed builds.\n  In this work, we first define, at a conceptual level, a data model that\ncaptures data sources and their relations in a typical CI environment. Second,\nbased on this data model, we define a comprehensive set of features that covers\nall features previously used by related studies. Third, we develop methods and\ntools to collect the defined features for 25 open-source software systems with\nenough failed builds and whose regression testing takes at least five minutes.\nFourth, relying on the collected dataset containing a comprehensive feature\nset, we answer four research questions concerning data collection time, the\neffectiveness of ML-based TCP, the impact of the features on effectiveness, the\ndecay of ML-based TCP models over time, and the trade-off between data\ncollection time and the effectiveness of ML-based TCP techniques.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.05843,regular,pre_llm,2021,9,"{'ai_likelihood': 3.973642985026042e-07, 'text': ""OSS effort estimation using software features similarity and developer\n  activity-based metrics\n\n  Software development effort estimation (SDEE) generally involves leveraging\nthe information about the effort spent in developing similar software in the\npast. Most organizations do not have access to sufficient and reliable forms of\nsuch data from past projects. As such, the existing SDEE methods suffer from\nlow usage and accuracy.\n  We propose an efficient SDEE method for open source software, which provides\naccurate and fast effort estimates. The significant contributions of our paper\nare i) Novel SDEE software metrics derived from developer activity information\nof various software repositories, ii) SDEE dataset comprising the SDEE metrics'\nvalues derived from $\\approx13,000$ GitHub repositories from 150 different\nsoftware categories, iii) an effort estimation tool based on SDEE metrics and a\nsoftware description similarity model. Our software description similarity\nmodel is basically a machine learning model trained using the Paragraph Vectors\nalgorithm on the software product descriptions of GitHub repositories. Given\nthe software description of a newly-envisioned software, our tool yields an\neffort estimate for developing it.\n  Our method achieves the highest Standard Accuracy score of 87.26% (with\ncliff's $\\delta$=0.88 at 99.999% confidence level) and 42.7% with the Automatic\nTransformed Linear Baseline model. Our software artifacts are available at\nhttps://doi.org/10.5281/zenodo.5095723.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.00659,review,pre_llm,2021,9,"{'ai_likelihood': 6.291601392957899e-07, 'text': 'Semantic Slicing of Architectural Change Commits: Towards Semantic\n  Design Review\n\n  Software architectural changes involve more than one module or component and\nare complex to analyze compared to local code changes. Development teams aiming\nto review architectural aspects (design) of a change commit consider many\nessential scenarios such as access rules and restrictions on usage of program\nentities across modules. Moreover, design review is essential when proper\narchitectural formulations are paramount for developing and deploying a system.\nUntangling architectural changes, recovering semantic design, and producing\ndesign notes are the crucial tasks of the design review process. To support\nthese tasks, we construct a lightweight tool [4] that can detect and decompose\nsemantic slices of a commit containing architectural instances. A semantic\nslice consists of a description of relational information of involved modules,\ntheir classes, methods and connected modules in a change instance, which is\neasy to understand to a reviewer. We extract various directory and naming\nstructures (DANS) properties from the source code for developing our tool.\nUtilizing the DANS properties, our tool first detects architectural change\ninstances based on our defined metric and then decomposes the slices (based on\nstring processing). Our preliminary investigation with ten open-source projects\n(developed in Java and Kotlin) reveals that the DANS properties produce highly\nreliable precision and recall (93-100%) for detecting and generating\narchitectural slices. Our proposed tool will serve as the preliminary approach\nfor the semantic design recovery and design summary generation for the project\nreleases.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.13546,regular,pre_llm,2021,9,"{'ai_likelihood': 2.6490953233506946e-07, 'text': ""Code Comprehension Confounders: A Study of Intelligence and Personal\n\n  Literature and intuition suggest that a developer's intelligence and\npersonality have an impact on their performance in comprehending source code.\nResearchers made this suggestion in the past when discussing threats to\nvalidity of their study results. However, the lack of studies investigating the\nrelationship of intelligence and personality to performance in code\ncomprehension makes scientifically sound reasoning about their influence\ndifficult. We conduct the first empirical evaluation, a correlational study\nwith undergraduates, to investigate the correlation of intelligence and\npersonality with performance in code comprehension, that is with correctness in\nanswering comprehension questions on code snippets. We found that personality\ntraits are unlikely to impact code comprehension performance, at least not\nconsidered in isolation. Conscientiousness, in combination with other factors,\nhowever, explains some of the variance in code comprehension performance. For\nintelligence, significant small to moderate positive effects on code\ncomprehension performance were found for three of four factors measured, i.e.,\nfluid intelligence, visual perception, and cognitive speed. Crystallized\nintelligence has a positive but statistically insignificant effect on code\ncomprehension performance. According to our results, several intelligence\nfacets as well as the personality trait conscientiousness are potential\nconfounders that should not be neglected in code comprehension studies of\nindividual performance and should be controlled for via an appropriate study\ndesign. We call for the conduct of further studies on the relationship between\nintelligence and personality with code comprehension, in part because code\ncomprehension involves more facets than we can measure in a single study and\nbecause our regression model explains only a small portion of the variance in\ncode comprehension performance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.01378,review,pre_llm,2021,9,"{'ai_likelihood': 1.5232298109266494e-06, 'text': ""Open Data Ecosystems -- an empirical investigation into an emerging\n  industry collaboration concept\n\n  Software systems are increasingly depending on data, particularly with the\nrising use of machine learning, and developers are looking for new sources of\ndata. Open Data Ecosystems (ODE) is an emerging concept for data sharing under\npublic licenses in software ecosystems, similar to Open Source Software (OSS).\nIt has certain similarities to Open Government Data (OGD), where public\nagencies share data for innovation and transparency. We aimed to explore open\ndata ecosystems involving commercial actors. Thus, we organized five focus\ngroups with 27 practitioners from 22 companies, public organizations, and\nresearch institutes. Based on the outcomes, we surveyed three cases of emerging\nODE practice to further understand the concepts and to validate the initial\nfindings. The main outcome is an initial conceptual model of ODEs' value,\nintrinsics, governance, and evolution, and propositions for practice and\nfurther research. We found that ODE must be value driven. Regarding the\nintrinsics of data, we found their type, meta-data, and legal frameworks\ninfluential for their openness. We also found the characteristics of ecosystem\ninitiation, organization, data acquisition and openness be differentiating,\nwhich we advise research and practice to take into consideration.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.0382,review,pre_llm,2021,10,"{'ai_likelihood': 2.980232238769531e-07, 'text': ""How Tertiary Studies perform Quality Assessment of Secondary Studies in\n  Software Engineering\n\n  Context: Tertiary studies are becoming increasingly popular in software\nengineering as an instrument to synthesise evidence on a research topic in a\nsystematic way. In order to understand and contextualize their findings, it is\nimportant to assess the quality of the selected secondary studies. Objective:\nThis paper aims to provide a state of the art on the assessment of secondary\nstudies' quality as conducted in tertiary studies in the area of software\nengineering, reporting the frameworks used as instruments, the facets examined\nin these frameworks, and the purposes of the quality assessment. Method: We\ndesigned this study as a systematic mapping responding to four research\nquestions derived from the objective above. We applied a rigorous search\nprotocol over the Scopus digital library, resulting in 47 papers after\napplication of inclusion and exclusion criteria. The extracted data was\nsynthesised using content analysis. Results: A majority of tertiary studies\nperform quality assessment. It is not often used for excluding studies, but to\nsupport some kind of investigation. The DARE quality assessment framework is\nthe most frequently used, with customizations in some cases to cover missing\nfacets. We outline the first steps towards building a new framework to address\nthe shortcomings identified. Conclusion: This paper is a step forward\nestablishing a foundation for researchers in two different ways. As authors of\ntertiary studies, understanding the different possibilities in which they can\nperform quality assessment of secondary studies. As readers, having an\ninstrument to understand the methodological rigor upon which tertiary studies\nmay claim their findings.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.01962,regular,pre_llm,2021,10,"{'ai_likelihood': 2.6490953233506946e-07, 'text': ""Gender Bias in Remote Pair Programming among Software Engineering\n  Students: The twincode Exploratory Study\n\n  Context. Pair programming (PP) has been found to increase student interest in\nComputer Science, particularly so for women, and would therefore appear to be a\nway to help remedy their under-representation, which could be partially\nmotivated by gender stereotypes applied to software engineers, assuming that\nmen perform better than their women peers. If this same bias is present in pair\nprogramming, it could work against the goal of improving gender balance.\n  Objective. In a remote setting in which students cannot directly observe\ntheir peers, we aim to explore whether they behave differently when the\nperceived gender of their remote PP partners changes, searching for differences\nin (i) the perceived productivity compared to solo programming; (ii) the\npartner's perceived technical competency compared to their own; (iii) the\npartner's perceived skill level; (iv) the interaction behavior, such as the\nfrequency of source code additions, deletions, etc.; and (v) the type and\nrelative frequencies of dialog messages in a chat window.\n  Method. Using the twincode platform, several behaviors are automatically\nmeasured during the remote PP process, together with two questionnaires and a\nsemantic tagging of the pairs' chats. A series of experiments to identify the\neffect, if any, of possible gender bias shall be performed. The control group\nwill have no information about their partner's gender, whereas the treatment\ngroup will receive such information but will be selectively deceived about\ntheir partner's gender. For each response variable we will (i) compare control\nand experimental groups for the score distance between two in-pair tasks; then,\nusing the data from the experimental group only, we will (ii) compare scores\nusing the partner's perceived gender as a within-subjects variable; and (iii)\nanalyze the interaction between the partner's perceived gender and the\nsubject's gender.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.01023,review,pre_llm,2021,10,"{'ai_likelihood': 8.27842288547092e-06, 'text': 'Feedback Loops in Open Data Ecosystems\n\n  Public agencies are increasingly publishing open data to increase\ntransparency and fuel data-driven innovation. For these organizations,\nmaintaining sufficient data quality is key to continuous re-use but also\nheavily dependent on feedback loops being initiated between data publishers and\nusers. This paper reports from a longitudinal engagement with Scandinavian\ntransportation agencies, where such feedback loops have been successfully\nestablished. Based on these experiences, we propose four distinct types of data\nfeedback loops in which both data publishers and re-users play critical roles.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.14162,regular,pre_llm,2021,10,"{'ai_likelihood': 2.814663781060113e-06, 'text': ""Stubbifier: Debloating Dynamic Server-Side JavaScript Applications\n\n  JavaScript is an increasingly popular language for server-side development,\nthanks in part to the Node.js runtime environment and its vast ecosystem of\nmodules. With the Node.js package manager npm, users are able to easily include\nexternal modules as dependencies in their projects. However, npm installs\nmodules with all of their functionality, even if only a fraction is needed,\nwhich causes an undue increase in code size. Eliminating this unused\nfunctionality from distributions is desirable, but the sound analysis required\nto find unused code is difficult due to JavaScript's extreme dynamicity.\n  We present a fully automatic technique that identifies unused code by\nconstructing static or dynamic call graphs from the application's tests, and\nreplacing code deemed unreachable with either file- or function-level stubs. If\na stub is called, it will fetch and execute the original code on-demand, thus\nrelaxing the requirement that the call graph be sound. The technique also\nprovides an optional guarded execution mode to guard application against\ninjection vulnerabilities in untested code that resulted from stub expansion.\n  This technique is implemented in an open source tool called Stubbifier, which\nsupports the ECMAScript 2019 standard. In an empirical evaluation on 15 Node.js\napplications and 75 clients of these applications, Stubbifier reduced\napplication size by 56% on average while incurring only minor performance\noverhead. The evaluation also shows that Stubbifier's guarded execution mode is\ncapable of preventing several known injection vulnerabilities that are\nmanifested in stubbed-out code. Finally, Stubbifier can work alongside\nbundlers, popular JavaScript tools for bundling an application with its\ndependencies. For the considered subject applications, we measured an average\nsize reduction of 37% in bundled distributions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.08403,regular,pre_llm,2021,10,"{'ai_likelihood': 1.0331471761067709e-05, 'text': 'Nalanda: A Socio-Technical Graph for Building Software Analytics Tools\n  at Enterprise Scale\n\n  Software development is information-dense knowledge work that requires\ncollaboration with other developers and awareness of artifacts such as work\nitems, pull requests, and files. With the speed of development increasing,\ninformation overload is a challenge for people developing and maintaining these\nsystems. Finding information and people is difficult for software engineers,\nespecially when they work in large software systems or have just recently\njoined a project. In this paper, we build a large scale data platform named\nNalanda platform, which contains two subsystems: 1. A large scale\nsocio-technical graph system, named Nalanda graph system 2. A large scale\nrecommendation system, named Nalanda index system that aims at satisfying the\ninformation needs of software developers. The Nalanda graph is an enterprise\nscale graph with data from 6,500 repositories, with 37,410,706 nodes and\n128,745,590 edges. On top of the Nalanda graph system, we built software\nanalytics applications including a newsfeed named MyNalanda, and based on\norganic growth alone, it has Daily Active Users (DAU) of 290 and Monthly Active\nUsers (MAU) of 590. A preliminary user study shows that 74% of developers and\nengineering managers surveyed are favorable toward continued use of the\nplatform for information discovery. The Nalanda index system constitutes two\nindices: artifact index and expert index. It uses the socio-technical graph\n(Nalanda graph system) to rank the results and provide better recommendations\nto software developers. A large scale quantitative evaluation shows that the\nNalanda index system provides recommendations with an accuracy of 78% for the\ntop three recommendations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.15103,regular,pre_llm,2021,10,"{'ai_likelihood': 8.27842288547092e-07, 'text': ""Be Lean -- How to Fit a Model-Based System Architecture Development\n  Process Based on ARP4754 Into an Agile Environment\n\n  An emerging service is moving the known aviation sector in terms of\ntechnology, paradigms, and key players - the Urban Air Mobility. The reason:\nnew developments in non-aviation industries are driving technological progress\nin aviation. For instance electrical motors, modern sensor technologies and\nbetter energy storage expand the possibilities and enable novel vehicle\nconcepts which require also novel system architectures for flight control\nsystems. Their development is governed by aviation authority and industry\nrecognized standards, guidelines and recommended practices. Comprehensive\nmethods for Model-Based Systems Engineering exist which address these guidance\nmaterials but their setup and their application can be quite\nresource-demanding. Especially the new and rather small key players - start-ups\nand development teams in an educational environment - can be overwhelmed to\nsetup such development processes. For these clients, the authors propose a\ncustom workflow for the development of system architectures. It shall ensure\ndevelopment rigor, quality and consistency. The authors show how the custom\nworkflow has been established based on the ARP4754A and its level of compliance\nto the standard's process objectives. Based on automation of life cycle\nactivities, manual effort can be reduced to allow the application even in small\nteams. The custom workflow's activities are explained and demonstrated within a\ncase study of an Experimental Autopilot system architecture.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.03728,regular,pre_llm,2021,10,"{'ai_likelihood': 4.371007283528646e-06, 'text': 'HABCSm: A Hamming Based t-way Strategy Based on Hybrid Artificial Bee\n  Colony for Variable Strength Test Sets Generation\n\n  Search-based software engineering that involves the deployment of\nmeta-heuristics in applicable software processes has been gaining wide\nattention. Recently, researchers have been advocating the adoption of\nmeta-heuristic algorithms for t-way testing strategies (where t points the\ninteraction strength among parameters). Although helpful, no single\nmeta-heuristic based t-way strategy can claim dominance over its counterparts.\nFor this reason, the hybridization of meta-heuristic algorithms can help to\nascertain the search capabilities of each by compensating for the limitations\nof one algorithm with the strength of others. Consequently, a new\nmeta-heuristic based t-way strategy called Hybrid Artificial Bee Colony\n(HABCSm) strategy, based on merging the advantages of the Artificial Bee Colony\n(ABC) algorithm with the advantages of a Particle Swarm Optimization (PSO)\nalgorithm is proposed in this paper. HABCSm is the first t-way strategy to\nadopt Hybrid Artificial Bee Colony (HABC) algorithm with Hamming distance as\nits core method for generating a final test set and the first to adopt the\nHamming distance as the final selection criterion for enhancing the exploration\nof new solutions. The experimental results demonstrate that HABCSm provides\nsuperior competitive performance over its counterparts. Therefore, this finding\ncontributes to the field of software testing by minimizing the number of test\ncases required for test execution.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.02835,regular,pre_llm,2021,10,"{'ai_likelihood': 3.973642985026042e-07, 'text': ""Characterizing the Experience of Subjects in Software Engineering\n  Studies\n\n  Context: Empirical studies in software engineering are typically centered on\nhuman subjects, ranging from novice to experienced developers. The experience\nof these individuals is a key context factor that should be properly\ncharacterized for supporting the design of empirical studies and interpreting\ntheir results. However, the criteria adopted for characterizing the experience\nof subjects do not follow a standard and are frequently limited. Goal: Our\nresearch aims at establishing an optimized and comprehensive scheme to\ncharacterize the subjects' experience for studies in software engineering.\nMethod: Based on previous work, we defined the first version of this scheme,\ncomposed of three experience attributes, including time, number of projects,\nand self-perception. In the last years, we applied the characterization scheme\nover four empirical studies, reaching the characterization of 79 subjects in\nthree different skills. Results: We found that the attributes from our scheme\nare positively but moderately correlated. This finding suggests these\nattributes play a complementary role in characterizing the subjects'\nexperience. Besides, we found that study subjects tend to enumerate the\ntechnical diversity of their background when summarizing their professional\nexperience. Conclusion: The scheme proposed represents a feasible alternative\nfor characterizing subjects of empirical studies in the field. However, we\nintend to conduct additional investigations with developers to evolve it.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.14081,regular,pre_llm,2021,10,"{'ai_likelihood': 1.3245476616753472e-06, 'text': ""A Controlled Experiment of Different Code Representations for\n  Learning-Based Bug Repair\n\n  Training a deep learning model on source code has gained significant traction\nrecently. Since such models reason about vectors of numbers, source code needs\nto be converted to a code representation before vectorization. Numerous\napproaches have been proposed to represent source code, from sequences of\ntokens to abstract syntax trees. However, there is no systematic study to\nunderstand the effect of code representation on learning performance. Through a\ncontrolled experiment, we examine the impact of various code representations on\nmodel accuracy and usefulness in deep learning-based program repair. We train\n21 different generative models that suggest fixes for name-based bugs,\nincluding 14 different homogeneous code representations, four mixed\nrepresentations for the buggy and fixed code, and three different embeddings.\nWe assess if fix suggestions produced by the model in various code\nrepresentations are automatically patchable, meaning they can be transformed to\na valid code that is ready to be applied to the buggy code to fix it. We also\nconduct a developer study to qualitatively evaluate the usefulness of inferred\nfixes in different code representations. Our results highlight the importance\nof code representation and its impact on learning and usefulness. Our findings\nindicate that (1) while code abstractions help the learning process, they can\nadversely impact the usefulness of inferred fixes from a developer's point of\nview; this emphasizes the need to look at the patches generated from the\npractitioner's perspective, which is often neglected in the literature, (2)\nmixed representations can outperform homogeneous code representations, (3) bug\ntype can affect the effectiveness of different code representations; although\ncurrent techniques use a single code representation for all bug types, there is\nno single best code representation applicable to all bug types.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.12241,regular,pre_llm,2021,10,"{'ai_likelihood': 7.28501213921441e-07, 'text': ""Changing Software Engineers' Self-Efficacy with Bootcamps:A Research\n  Proposal\n\n  In several areas of knowledge, self-efficacy is related to the perfomance of\nindividuals, including in Software Engineering. However,it is not clear how\nself-efficacy can be modified in training conducted by the industry.\nFurthermore, we still do not understand how self-efficacy can impact an\nindividual's team and career in the industry. This lack of understanding can\nnegatively impact how companies and individuals perceive the importance of\nself-efficacy in the field. Therefore, We present a research proposal that aims\nto understand the relationship between self-efficacy and training in Software\nEngineering. Moreover, we look to understand the role of self-efficacy at\nSoftware Development industry. We propose a longitudinal case study with\nsoftware engineers at Zup Innovation that participating of our bootcamp\ntraining. We expect to collect data to support our assumptions that\nself-efficacy can be related to training in Software Engineering. The other\nassumption is that self-efficacy at the beginning of training is higher than\nthe middle, and that self-efficacy at the end of training is higher than the\nmiddle. We expect that the study proposed in this article will motivate a\ndiscussion about self-efficacy and the importance of training employers in the\nindustry of software development.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.00361,review,pre_llm,2021,10,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'An analysis of open source software licensing questions in Stack\n  Exchange sites\n\n  Free and open source software is widely used in the creation of software\nsystems, whereas many organisations choose to provide their systems as open\nsource. Open source software carries licenses that determine the conditions\nunder which the original software can be used. Appropriate use of licenses\nrequires relevant expertise by the practitioners, and has an important legal\nangle. Educators and employers need to ensure that developers have the\nnecessary training to understand licensing risks and how they can be addressed.\nAt the same time, it is important to understand which issues practitioners face\nwhen they are using a specific open source license, when they are developing\nnew open source software products or when they are reusing open source\nsoftware. In this work, we examine questions posed about open source software\nlicensing using data from the following Stack Exchange sites: Stack Overflow,\nSoftware Engineering, Open Source and Law. We analyse the indication of\nspecific licenses and topics in the questions, investigate the attention the\nposts receive and trends over time, whether appropriate answers are provided\nand which type of questions are asked. Our results indicate that practitioners\nneed, among other, clarifications about licensing specific software when other\nlicenses are used, and for understanding license content. The results of the\nstudy can be useful for educators and employers, organisations that are\nauthoring open source software licenses and developers for understanding the\nissues faced when using licenses, whereas they are relevant to other software\nengineering research areas, such as software reusability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.09758,regular,pre_llm,2021,10,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'KernelHaven -- An Open Infrastructure for Product Line Analysis\n\n  KernelHaven is an open infrastructure for Software Product Line (SPL)\nanalysis. It is intended both as a production-quality analysis tool set as well\nas a research support tool, e.g., to support researchers in systematically\nexploring research hypothesis. For flexibility and ease of experimentation\nKernelHaven components are plug-ins for extracting certain information from SPL\nartifacts and processing this information, e.g., to check the correctness and\nconsistency of variability information or to apply metrics. A\nconfiguration-based setup along with automatic documentation functionality\nallows different experiments and supports their easy reproduction. Here, we\ndescribe KernelHaven as a product line analysis research tool and highlight its\nbasic approach as well as its fundamental capabilities. In particular, we\ndescribe available information extraction and processing plug-ins and how to\ncombine them. On this basis, researchers and interested professional users can\nrapidly conduct a first set of experiments. Further, we describe the concepts\nfor extending KernelHaven by new plug-ins, which reduces development effort\nwhen realizing new experiments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.14301,review,pre_llm,2021,10,"{'ai_likelihood': 1.953707800971137e-06, 'text': 'From Complexity Measurement to Holistic Quality Evaluation for\n  Automotive Software Development\n\n  In recent years, the role and the importance of software in the automotive\ndomain have changed dramatically. Being able to systematically evaluate and\nmanage software quality is becoming even more crucial. In practice, however, we\nstill find a largely static approach for measuring software quality based on a\npredefined list of complexity metrics with static thresholds to fulfill. We\npropose using a more flexible framework instead, which systematically derives\nmeasures and evaluation rules based on the goals and context of a development\nproject.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.0171,regular,pre_llm,2021,10,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'PyTorrent: A Python Library Corpus for Large-scale Language Models\n\n  A large scale collection of both semantic and natural language resources is\nessential to leverage active Software Engineering research areas such as code\nreuse and code comprehensibility. Existing machine learning models ingest data\nfrom Open Source repositories (like GitHub projects) and forum discussions\n(like Stackoverflow.com), whereas, in this showcase, we took a step backward to\norchestrate a corpus titled PyTorrent that contains 218,814 Python package\nlibraries from PyPI and Anaconda environment. This is because earlier studies\nhave shown that much of the code is redundant and Python packages from these\nenvironments are better in quality and are well-documented. PyTorrent enables\nusers (such as data scientists, students, etc.) to build off the shelf machine\nlearning models directly without spending months of effort on large\ninfrastructure. The dataset, schema and a pretrained language model is\navailable at: https://github.com/fla-sil/PyTorrent\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.15246,review,pre_llm,2021,10,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'On the Importance and Shortcomings of Code Readability Metrics: A Case\n  Study on Reactive Programming\n\n  Well structured and readable source code is a pre-requisite for maintainable\nsoftware and successful collaboration among developers. Static analysis enables\nthe automated extraction of code complexity and readability metrics which can\nbe leveraged to highlight potential improvements in code to both attain\nsoftware of high quality and reinforce good practices for developers as an\neducational tool. This assumes reliable readability metrics which are not\ntrivial to obtain since code readability is somewhat subjective. Recent\nresearch has resulted in increasingly sophisticated models for predicting\nreadability as perceived by humans primarily with a procedural and object\noriented focus, while functional and declarative languages and language\nextensions advance as they often are said to lead to more concise and readable\ncode. In this paper, we investigate whether the existing complexity and\nreadability metrics reflect that wisdom or whether the notion of readability\nand its constituents requires overhaul in the light of programming language\nchanges. We therefore compare traditional object oriented and reactive\nprogramming in terms of code complexity and readability in a case study.\nReactive programming is claimed to increase code quality but few studies have\nsubstantiated these claims empirically. We refactored an object oriented open\nsource project into a reactive candidate and compare readability with the\noriginal using cyclomatic complexity and two state-of-the-art readability\nmetrics. More elaborate investigations are required, but our findings suggest\nthat both cyclomatic complexity and readability decrease significantly at the\nsame time in the reactive candidate, which seems counter-intuitive. We\nexemplify and substantiate why readability metrics may require adjustment to\nbetter suit popular programming styles other than imperative and\nobject-oriented to better match human expectations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.07889,review,pre_llm,2021,10,"{'ai_likelihood': 2.6490953233506944e-06, 'text': 'Breaking Bad? Semantic Versioning and Impact of Breaking Changes in\n  Maven Central\n\n  Just like any software, libraries evolve to incorporate new features, bug\nfixes, security patches, and refactorings. However, when a library evolves, it\nmay break the contract previously established with its clients by introducing\nBreaking Changes (BCs) in its API. These changes might trigger compile-time,\nlink-time, or run-time errors in client code. As a result, clients may hesitate\nto upgrade their dependencies, raising security concerns and making future\nupgrades even more difficult.Understanding how libraries evolve helps client\ndevelopers to know which changes to expect and where to expect them, and\nlibrary developers to understand how they might impact their clients. In the\nmost extensive study to date, Raemaekers et al. investigate to what extent\ndevelopers of Java libraries hosted on the Maven Central Repository (MCR)\nfollow semantic versioning conventions to signal the introduction of BCs and\nhow these changes impact client projects. Their results suggest that BCs are\nwidespread without regard for semantic versioning, with a significant impact on\nclients.In this paper, we conduct an external and differentiated replication\nstudy of their work. We identify and address some limitations of the original\nprotocol and expand the analysis to a new corpus spanning seven more years of\nthe MCR. We also present a novel static analysis tool for Java bytecode,\nMaracas, which provides us with: (i) the set of all BCs between two versions of\na library; and (ii) the set of locations in client code impacted by individual\nBCs. Our key findings, derived from the analysis of 119, 879 library upgrades\nand 293, 817 clients, contrast with the original study and show that 83.4% of\nthese upgrades do comply with semantic versioning. Furthermore, we observe that\nthe tendency to comply with semantic versioning has significantly increased\nover time. Finally, we find that most BCs affect code that is not used by any\nclient, and that only 7.9% of all clients are affected by BCs. These findings\nshould help (i) library developers to understand and anticipate the impact of\ntheir changes; (ii) library users to estimate library upgrading effort and to\npick libraries that are less likely to break; and (iii) researchers to better\nunderstand the dynamics of library-client co-evolution in Java.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.0515,regular,pre_llm,2021,10,"{'ai_likelihood': 2.4172994825575088e-06, 'text': 'Human Values in Mobile App Development: An Empirical Study on\n  Bangladeshi Agriculture Mobile Apps\n\n  Given the ubiquity of mobile applications (apps) in daily lives,\nunderstanding and reflecting end-users\' human values (e.g., transparency,\nprivacy, social recognition etc.) in apps has become increasingly important.\nViolations of end users\' values by software applications have been reported in\nthe media and have resulted in a wide range of difficulties for end users.\nValue violations may bring more and lasting problems for marginalized and\nvulnerable groups of end-users. This research aims to understand the extent to\nwhich the values of Bangladeshi female farmers, marginalized and vulnerable\nend-users, who are less studied by the software engineering community, are\nreflected in agriculture apps in Bangladesh. Further to this, we aim to\nidentify possible strategies to embed their values in those apps. To this end,\nwe conducted a mixed-methods empirical study consisting of 13 interviews with\napp practitioners and four focus groups with 20 Bangladeshi female farmers. The\naccumulated results from the interviews and focus groups identified 22 values\nof Bangladeshi female farmers, which the participants expect to be reflected in\nthe agriculture apps. Among these 22 values, 15 values (e.g., accuracy,\nindependence) are already reflected and 7 values (e.g., accessibility,\npleasure) are ignored/violated in the existing agriculture apps. We also\nidentified 14 strategies (e.g., ""applying human-centered approaches to elicit\nvalues"", ""establishing a dedicated team/person for values concerns"") to address\nBangladeshi female farmers\' values in agriculture apps.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.01092,regular,pre_llm,2021,10,"{'ai_likelihood': 1.7219119601779514e-06, 'text': 'Towards Informative Tagging of Code Fragments to Support the\n  Investigation of Code Clones\n\n  Investigating the code fragments of code clones detected by code clone\ndetection tools is a time-consuming task, especially when a large number of\nreference source files are available. This paper proposes (i) a method for\nclustering a clone class, which is detected by code clone detection tools using\nsyntactic similarity, based on topic similarity by considering its code\nfragments as sequences of words and (ii) a method for assigning short tags to\nclusters of the clustering result. We also report an experiment of applying the\nproposed method to packages of an open source operating system.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.09755,regular,pre_llm,2021,10,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'MetricHaven -- More Than 23,000 Metrics for Measuring Quality Attributes\n  of Software Product Lines\n\n  Variability-aware metrics are designed to measure qualitative aspects of\nsoftware product lines. As we identified in a prior SLR\n\\cite{El-SharkawyYamagishi-EichlerSchmid19}, there exist already many metrics\nthat address code or variability separately, while the combination of both has\nbeen less researched. MetricHaven fills this gap, as it extensively supports\ncombining information from code files and variability models. Further, we also\nenable the combination of well established single system metrics with novel\nvariability-aware metrics, going beyond existing variability-aware metrics. Our\ntool supports most prominent single system and variability-aware code metrics.\nWe provide configuration support for already implemented metrics, resulting in\n23,342 metric variations. Further, we present an abstract syntax tree developed\nfor MetricHaven, that allows the realization of additional code metrics.\n  Tool: https://github.com/KernelHaven/MetricHaven\n  Video: https://youtu.be/vPEmD5Sr6gM\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.03296,regular,pre_llm,2021,10,"{'ai_likelihood': 2.317958407931858e-06, 'text': 'Ranking Warnings of Static Analysis Tools Using Representation Learning\n\n  Static analysis tools are frequently used to detect potential vulnerabilities\nin software systems. However, an inevitable problem of these tools is their\nlarge number of warnings with a high false positive rate, which consumes time\nand effort for investigating. In this paper, we present DeFP, a novel method\nfor ranking static analysis warnings. Based on the intuition that warnings\nwhich have similar contexts tend to have similar labels (true positive or false\npositive), DeFP is built with two BiLSTM models to capture the patterns\nassociated with the contexts of labeled warnings. After that, for a set of new\nwarnings, DeFP can calculate and rank them according to their likelihoods to be\ntrue positives (i.e., actual vulnerabilities). Our experimental results on a\ndataset of 10 real-world projects show that using DeFP, by investigating only\n60% of the warnings, developers can find +90% of actual vulnerabilities.\nMoreover, DeFP improves the state-of-the-art approach 30% in both Precision and\nRecall.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.05287,regular,pre_llm,2021,11,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Test cases as a measurement instrument in experimentation\n\n  Background: Test suites are frequently used to quantify relevant software\nattributes, such as quality or productivity. Problem: We have detected that the\nsame response variable, measured using different test suites, yields different\nexperiment results. Aims: Assess to which extent differences in test case\nconstruction influence measurement accuracy and experimental outcomes. Method:\nTwo industry experiments have been measured using two different test suites,\none generated using an ad-hoc method and another using equivalence\npartitioning. The accuracy of the measures has been studied using standard\nprocedures, such as ISO 5725, Bland-Altman and Interclass Correlation\nCoefficients. Results: There are differences in the values of the response\nvariables up to +-60%, depending on the test suite (ad-hoc vs. equivalence\npartitioning) used. Conclusions: The disclosure of datasets and analysis code\nis insufficient to ensure the reproducibility of SE experiments. Experimenters\nshould disclose all experimental materials needed to perform independent\nmeasurement and re-analysis.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.07538,review,pre_llm,2021,11,"{'ai_likelihood': 1.0199016994900174e-05, 'text': 'Proceedings First Workshop on Applicable Formal Methods\n\n  This volume contains the proceedings of the 1st International Workshop on\nApplicable Formal Methods (AppFM 2021), 23 November 2021, held online as part\nof the 24th International Symposium on Formal Methods (FM). The aim of the\nAppFM workshop is to bring together researchers who improve and evaluate\nexisting formal approaches and new variants in practical contexts and support\nthe transfer of these approaches to software engineering practice.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.12513,regular,pre_llm,2021,11,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'FLACOCO: Fault Localization for Java based on Industry-grade Coverage\n\n  Fault localization is an essential step in the debugging process.\nSpectrum-Based Fault Localization (SBFL) is a popular fault localization family\nof techniques, utilizing code-coverage to predict suspicious lines of code. In\nthis paper, we present FLACOCO, a new fault localization tool for Java. The key\nnovelty of FLACOCO is that it is built on top of one of the most used and most\nreliable coverage libraries for Java, JaCoCo. FLACOCO is made available through\na well-designed command-line interface and Java API and supports all Java\nversions. We validate FLACOCO on two use-cases from the automatic program\nrepair domain by reproducing previous scientific experiments. We find it is\ncapable of effectively replacing the state-of-the-art FL library. Overall, we\nhope that FLACOCO will help research in fault localization as well as industry\nadoption thanks to being founded on industry-grade code coverage. An\nintroductory video is available at https://youtu.be/RFRyvQuwRYA\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.04362,regular,pre_llm,2021,11,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Is knowledge the key? An experiment on debiasing architectural\n  decision-making -- a pilot study\n\n  The impact of cognitive biases on architectural decision-making has been\nproven by previous research. In this work, we endeavour to create a debiasing\ntreatment that would minimise the impact of cognitive biases on architectural\ndecision-making. We conducted a pilot study on two groups of students, to\ninvestigate whether a simple debiasing presentation reporting on the influences\nof cognitive biases, can provide a debiasing effect. The preliminary results\nshow that this kind of treatment is ineffective. Through analysing our results,\nwe propose a set of modifications that could result in a better effect.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.05132,regular,pre_llm,2021,11,"{'ai_likelihood': 1.457002427842882e-06, 'text': 'BreakBot: Analyzing the Impact of Breaking Changes to Assist Library\n  Evolution\n\n  ""If we make this change to our code, how will it impact our clients?"" It is\ndifficult for library maintainers to answer this simple-yet essential!-question\nwhen evolving their libraries. Library maintainers are constantly balancing\nbetween two opposing positions: make changes at the risk of breaking some of\ntheir clients, or avoid changes and maintain compatibility at the cost of\nimmobility and growing technical debt. We argue that the lack of objective\nusage data and tool support leaves maintainers with their own subjective\nperception of their community to make these decisions. We introduce BreakBot, a\nbot that analyses the pull requests of Java libraries on GitHub to identify the\nbreaking changes they introduce and their impact on client projects. Through\nstatic analysis of libraries and clients, it extracts and summarizes objective\ndata that enrich the code review process by providing maintainers with the\nappropriate information to decide whether-and how-changes should be accepted,\ndirectly in the pull requests.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.0921,review,pre_llm,2021,11,"{'ai_likelihood': 4.867712656656901e-06, 'text': 'The Integrated List of Agile Practices -- A Tertiary Study\n\n  Context: Companies adapt agile methods, practices or artifacts for their use\nin practice since more than two decades. This adaptions result in a wide\nvariety of described agile practices. For instance, the Agile Alliance lists 75\ndifferent practices in its Agile Glossary. This situation may lead to\nmisunderstandings, as agile practices with similar names can be interpreted and\nused differently. Objective: This paper synthesize an integrated list of agile\npractices, both from primary and secondary sources. Method: We performed a\ntertiary study to identify existing overviews and lists of agile practices in\nthe literature. We identified 876 studies, of which 37 were included. Results:\nThe results of our paper show that certain agile practices are listed and used\nmore often in existing studies. Our integrated list of agile practices\ncomprises 38 entries structured in five categories. Contribution: The high\nnumber of agile practices and thus, the wide variety increased steadily over\nthe past decades due to the adaption of agile methods. Based on our findings,\nwe present a comprehensive overview of agile practices. The research community\nbenefits from our integrated list of agile practices as a potential basis for\nfuture research. Also, practitioners benefit from our findings, as the\nstructured overview of agile practices provides the opportunity to select or\nadapt practices for their specific needs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.10132,regular,pre_llm,2021,11,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Formal verification of space systems designed with TASTE\n\n  Model-Based Systems Engineering (MBSE) is a development approach aiming to\nbuild correct-by-construction systems, provided the use of clear, unambiguous\nand complete models to describe them along the design process. The approach is\nsupported by several engineering tools that automate the development steps, for\nexample the production of code, documentation, test cases and more. TASTE [1]\nis pragmatic MBSE toolset supported by ESA that encapsulates several\ntechnologies to design a system (data modelling, architecture modelling,\nbehaviour modelling/implementation), to automatically generate the binary\napplication(s), and to validate it. One topic left open in TASTE is the formal\nverification of a system design with respect to specified properties. In this\npaper we describe our approach based on the IF model-checker [4] to enable the\nformal verification of properties on TASTE designs. The approach is currently\nunder development in the ESA MoC4Space project.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.11607,regular,pre_llm,2021,11,"{'ai_likelihood': 1.7219119601779514e-06, 'text': 'Using DevOps Toolchains in Agile Model-Driven Engineering\n\n  For Model-Driven Engineering (MDE) to become Agile, it is has to be usable\nwith and for DevOps as the technical basis of Agility. We describe our\nexperiences in implementing and applying the BB8 architecture that provides a\nmeans to reuse Eclipse-based MDE toolkits within an integrating workflow\nsystem. We describe the use of the architecture with in the Gitlab DevOps\nsystem and discuss future research fields that emerge as the result of our\nimplementation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.08968,regular,pre_llm,2021,11,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'How a 4-day Work Week affects Agile Software Development Teams\n\n  Context: Agile software development (ASD) sets social aspects like\ncommunication and collaboration in focus. Thus, one may assume that the\nspecific work organization of companies impacts the work of ASD teams. A major\nchange in work organization is the switch to a 4-day work week, which some\ncompanies investigated in experiments. Also, recent studies show that ASD teams\nare affected by the switch to remote work since the Covid 19 pandemic outbreak\nin 2020. Objective: Our study presents empirical findings on the effects on ASD\nteams operating remote in a 4-day work week organization. Method: We performed\na qualitative single case study and conducted seven semi-structured interviews,\nobserved 14 agile practices and screened eight project documents and protocols\nof agile practices. Results: We found, that the teams adapted the agile method\nin use due to the change to a 4-day work week environment and the switch to\nremote work. The productivity of the two ASD teams did not decrease. Although\nthe stress level of the ASD team member increased due to the 4-day work week,\nwe found that the job satisfaction of the individual ASD team members is\naffected positively. Finally, we point to affects on social facets of the ASD\nteams. Conclusions: The research community benefits from our results as the\ncurrent state of research dealing with the effects of a 4-day work week on ASD\nteams is limited. Also, our findings provide several practical implications for\nASD teams working remote in a 4-day work week.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.01667,review,pre_llm,2021,11,"{'ai_likelihood': 8.145968119303387e-06, 'text': 'International Comparative Studies on the Software Testing Profession\n\n  This work attempts to fill a gap by exploring the human dimension in\nparticular, by trying to understand the motivation of software professionals\nfor taking up and sustaining their careers as software testers. Towards that\ngoal, four surveys were conducted in four countries - India, Canada, Cuba, and\nChina - to try to understand how professional software engineers perceive and\nvalue work-related factors that could influence their motivation to start or\nmove into software testing careers. From our sample of 220 software\nprofessionals, we observed that very few were keen to take up testing careers.\nSome aspects of software testing, such as the potential for learning\nopportunities and the importance of the job, appear to be common motivators\nacross the four countries, whereas the treatment of testers as second-class\ncitizens and the complexity of the job appeared to be the most prominent\nde-motivators.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.01631,regular,pre_llm,2021,11,"{'ai_likelihood': 3.0795733133951826e-06, 'text': ""SO{U}RCERER: Developer-Driven Security Testing Framework for Android\n  Apps\n\n  Frequently advised secure development recommendations often fall short in\npractice for app developers. Tool-driven (e.g., using static analysis tools)\napproaches lack context and domain-specific requirements of an app being\ntested. App developers struggle to find an actionable and prioritized list of\nvulnerabilities from a laundry list of security warnings reported by static\nanalysis tools. Process-driven (e.g., applying threat modeling methods)\napproaches require substantial resources (e.g., security testing team, budget)\nand security expertise, which small to medium-scale app dev teams could barely\nafford. To help app developers securing their apps, we propose SO{U}RCERER, a\nguiding framework for Android app developers for security testing. SO{U}RCERER\nguides developers to identify domain-specific assets of an app, detect and\nprioritize vulnerabilities, and mitigate those vulnerabilities based on secure\ndevelopment guidelines. We evaluated SO{U}RCERER with a case study on analyzing\nand testing 36 Android mobile money apps. We found that by following activities\nguided by SO{U}RCERER, an app developer could get a concise and actionable list\nof vulnerabilities (24-61% fewer security warnings produced by SO{U}RCERER than\na standalone static analyzer), directly affecting a mobile money app's critical\nassets, and devise a mitigation plan. Our findings from this preliminary study\nindicate a viable approach to Android app security testing without being\noverwhelmingly complex for app developers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.15338,regular,pre_llm,2021,11,"{'ai_likelihood': 2.4835268656412763e-06, 'text': 'A Semi-automated Method for Domain-Specific Ontology Creation from\n  Medical Guidelines\n\n  The automated capturing and summarization of medical consultations has the\npotential to reduce the administrative burden in healthcare. Consultations are\nstructured conversations that broadly follow a guideline with a systematic\nexamination of predefined observations and symptoms to diagnose and treat\nwell-defined medical conditions. A key component in automated conversation\nsummarization is the matching of the knowledge graph of the consultation\ntranscript with a medical domain ontology for the interpretation of the\nconsultation conversation. Existing general medical ontologies such as SNOMED\nCT provide a taxonomic view on the terminology, but they do not capture the\nessence of the guidelines that define consultations. As part of our research on\nmedical conversation summarization, this paper puts forward a semi-automated\nmethod for generating an ontological representation of a medical guideline. The\nmethod, which takes as input the well-known SNOMED CT nomenclature and a\nmedical guideline, maps the guidelines to a so-called Medical Guideline\nOntology (MGO), a machine-processable version of the guideline that can be used\nfor interpreting the conversation during a consultation. We illustrate our\napproach by discussing the creation of an MGO of the medical condition of ear\ncanal inflammation (Otitis Externa) given the corresponding guideline from a\nDutch medical authority.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.01501,regular,pre_llm,2021,11,"{'ai_likelihood': 1.3609727223714193e-05, 'text': ""Constructing a software requirements specification and design for\n  electronic IT news magazine system\n\n  Requirements engineering process intends to obtain software services and\nconstraints. This process is essential to meet the customer's needs and\nexpectations. This process includes three main activities in general. These are\ndetecting requirements by interacting with software stakeholders, transferring\nthese requirements into a standard document, and examining that the\nrequirements really define the software that the client needs. Functional\nrequirements are services that the software should deliver to the end-user. In\naddition, functional requirements describe how the software should respond to\nspecific inputs, and how the software should behave in certain circumstances.\nThis paper aims to develop a software requirements specification document of\nthe electronic IT news magazine system. The electronic magazine provides users\nto post and view up-to-date IT news. Still, there is a lack in the literature\nof comprehensive studies about the construction of the electronic magazine\nsoftware specification and design in conformance with the contemporary software\ndevelopment processes. Moreover, there is a need for a suitable research\nframework to support the requirements engineering process. The novelty of this\npaper is the construction of software specification and design of the\nelectronic magazine by following the Al-Msie'deen research framework. All the\ndocuments of software requirements specification and design have been\nconstructed to conform to the agile usage-centered design technique and the\nproposed research framework. A requirements specification and design are\nsuggested and followed for the construction of the electronic magazine\nsoftware. This study proved that involving users extensively in the process of\nsoftware requirements specification and design will lead to the creation of\ndependable and acceptable software systems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.0491,regular,pre_llm,2021,11,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'Structure-Behavior Coalescence Process Algebra -- Toward a Unified View\n  of the System in Model-Based Systems Engineering\n\n  In Model-Based Systems Engineering (MBSE), the Systems Modeling Language\n(SysML) specification includes a metamodel that defines the language concepts\nand a user model that defines how the language concepts are represented. In\nSysML, an important use of metamodel is to provide an integrated semantic\nframework that every diagram in the user model can be projected as a view of\nthe metamodel. However, most existing SysML metamodels lack such capability of\nbeing a basis for unification of different views of a system. To overcome the\nshortcomings of the current SysML metamodel approaches, we developed\nChannel-Based Multi-Queue Structure-Behavior Coalescence Process Algebra\n(C-M-SBC-PA), which provides an integrated semantic framework that is able to\nintegrate structural constructs with behavioral constructs. Using C-M-SBC-PA as\nthe metamodel of SysML, each diagram in the user model can be projected as a\nview of the C-M-SBC-PA metamodel.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.0684,regular,pre_llm,2021,11,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Reliability Models for Smartphone Applications\n\n  Smartphones have become the most used electronic devices. They carry out most\nof the functionalities of desktops, offering various useful applications that\nsuit the users needs. Therefore, instead of the operator, the user has been the\nmain controller of the device and its applications, therefore its reliability\nhas become an emergent requirement. As a first step, based on collected\nsmartphone applications failure data, we investigated and evaluated the\nefficacy of Software Reliability Growth Models (SRGMs) when applied to these\nsmartphone data in order to check whether they achieve the same accuracy as in\nthe desktop/laptop area. None of the selected models were able to account for\nthe smartphone data satisfactorily. Their failure is traced back to: (i) the\nhardware and software differences between desktops and smartphones, (ii) the\nspecific features of mobile applications compared to desktop applications, and\n(iii) the different operational conditions and usage profiles. Thus, a\nreliability model suited to smartphone applications is still needed. In the\nsecond step, we applied the Weibull and Gamma distributions, and their two\nparticular cases, Rayleigh and S-Shaped, to model the smartphone failure data\nsorted by application version number and grouped into different time periods.\nAn estimation of the expected number of defects in each application version was\nobtained. The performances of the distributions were then compared amongst each\nother. We found that both Weibull and Gamma distributions can fit the failure\ndata of mobile applications, although the Gamma distribution is frequently more\nsuited.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.08426,regular,pre_llm,2021,11,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Formal Quantum Software Engineering: Introducing the Formal Methods of\n  Software Engineering to Quantum Computing\n\n  Quantum computing (QC) represents the future of computing systems, but the\ntools for reasoning about the quantum model of computation, in which the laws\nobeyed are those on the quantum mechanical scale, are still a mix of linear\nalgebra and Dirac notation; two subjects more suitable for physicists, rather\nthan computer scientists and software engineers. On this ground, we believe it\nis possible to provide a more intuitive approach to thinking and writing about\nquantum computing systems, in order to simplify the design of quantum\nalgorithms and the development of quantum software. In this paper, we move the\nfirst step in such direction, introducing a specification language as the tool\nto represent the operations of a quantum computer via axiomatic definitions, by\nadopting the same symbolisms and reasoning principles used by formal methods in\nsoftware engineering. We name this approach formal quantum software engineering\n(F-QSE). This work assumes familiarity with the basic principles of quantum\nmechanics (QM), with the use of Zed (Z) which is a formal language of software\nengineering (SE), and with the notation and techniques of first-order logic\n(FOL) and functional programming (FP).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.10426,regular,pre_llm,2021,11,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'Modeling and Analysis of the Landing Gear System with the Generalized Contracts\n\nNowadays, there are several complex systems in different sectors such as aviation, air traffic control ...etc. These systems do not have a precise perimeter, they are open and made of various specific components built with different languages and environments. The modeling, assembly and analysis of such open and complex heterogeneous systems are challenges in software engineering. This paper describes how the Minarets method decreases the difficulty of modeling, composition and analysis of the well known case study of the landing gear system. The method consists in: equipping individual components with generalized contracts that integrate various facets related to different concerns, composing these components according to their facets and verifying the resulting system with respect to the involved facets as well. The proposed method may be used or extended to cover more facets, and by strengthening assistance tool through proactive aspects in modeling, composing multi-facets contracts and finally the verification of the heterogeneous systems.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.07739,regular,pre_llm,2021,11,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code\n  Elements\n\n  Software Fault Localization refers to the activity of finding code elements\n(e.g., statements) that are related to a software failure. The state-of-the-art\nfault localization techniques, however, produce coarse-grained results that can\ndeter manual debugging or mislead automated repair tools. In this work, we\nfocus specifically on the fine-grained identification of code elements (i.e.,\ntokens) that must be changed to fix a buggy program: we refer to it as fix\nlocalization. This paper introduces a neural network architecture (named Beep)\nthat builds on AST paths to predict the buggy code element as well as the\nchange action that must be applied to repair a program. Leveraging massive data\nof bugs and patches within the CoCoNut dataset, we trained a model that was (1)\neffective in localizing the buggy tokens with the Mean First Rank significantly\nhigher than a statistics based baseline and a machine learning-based baseline,\nand (2) effective in predicting the repair operators (with the associated buggy\ncode elements) with a Recall@1= 30-45% and the Mean First Rank=7-12 (evaluated\nby CoCoNut, ManySStuBs4J, and Defects4J datasets). To showcase how fine-grained\nfix localization can help program repair, we employ it in two repair pipelines\nwhere we use either a code completion engine to predict the correct token or a\nset of heuristics to search for the suitable donor code. A key strength of\naccurate fix localization for program repair is that it reduces the chance of\npatch overfitting, a challenge in generate-and-validate automated program\nrepair: both two repair pipelines achieve a correctness ratio of 100%, i.e.,\nall generated patches are found to be correct. Moreover, accurate fix\nlocalization helps enhance the efficiency of program repair.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.05713,regular,pre_llm,2021,11,"{'ai_likelihood': 2.947118547227648e-06, 'text': 'Towards More Reliable Automated Program Repair by Integrating Static\n  Analysis Techniques\n\n  A long-standing open challenge for automated program repair is the\noverfitting problem, which is caused by having insufficient or incomplete\nspecifications to validate whether a generated patch is correct or not. Most\navailable repair systems rely on weak specifications (i.e., specifications that\nare synthesized from test cases) which limits the quality of generated repairs.\nTo strengthen specifications and improve the quality of repairs, we propose to\ncloser integrate static bug detection techniques with automated program repair.\nThe integration combines automated program repair with static analysis\ntechniques in such a way that bug detection patterns can be synthesized into\nspecifications that the repair system can use. We explore the feasibility of\nsuch integration using two types of bugs: arithmetic bugs, such as integer\noverflow, and logical bugs, such as termination bugs. As part of our analysis,\nwe make several observations that help to improve patch generation for these\nclasses of bugs. Moreover, these observations assist with narrowing down the\ncandidate patch search space, and inferring an effective search order.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.05327,regular,pre_llm,2021,11,"{'ai_likelihood': 3.311369154188368e-07, 'text': ""An adaptive 3D virtual learning environment for training software\n  developers in scrum\n\n  Scrum is one of the most used frameworks for agile software development\nbecause of its potential improvements in productivity, quality, and client\nsatisfaction. Academia has also focussed on teaching Scrum practices to prepare\nstudents to face common software engineering challenges and facilitate their\ninsertion in professional contexts. Furthermore, advances in learning\ntechnologies currently offer many virtual learning environments to enhance\nlearning in many ways. Their capability to consider the individual learner\npreferences has led a shift to more personalised training approaches, requiring\nthat the environments adapt themselves to the learner. We propose an adaptive\napproach for training developers in Scrum, including an adaptive virtual\nlearning environment based on Felder's learning style theory. Although still\npreliminary, our findings show that students who used the environment and\nreceived instruction matching their preferences obtained sightly higher\nlearning gains than students who received a different instruction than the one\nthey preferred. We also noticed less variability in the learning gains of\nstudents who received instruction matching their preferences. The relevance of\nthis work goes beyond the impact on learning gains since it describes how\nadaptive virtual learning environments can be used in the domain of Software\nEngineering.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.10056,regular,pre_llm,2021,12,"{'ai_likelihood': 3.311369154188368e-08, 'text': ""Reproducibility Challenges and Their Impacts on Technical Q&A Websites:\n  The Practitioners' Perspectives\n\n  Software developers often look for solutions to their code-level problems by\nsubmitting questions to technical Q&A websites like Stack Overflow (SO). They\nusually include example code segments with questions to describe the\nprogramming issues. SO users prefer to reproduce the reported issues using the\ngiven code segments when they attempt to answer the questions. Unfortunately,\nsuch code segments could not always reproduce the issues due to several unmet\nchallenges (e.g., external library not found) that might prevent questions from\nreceiving prompt and appropriate solutions. A previous study produced a catalog\nof potential challenges that hinder the reproducibility of issues reported at\nSO questions. However, it is unknown how the practitioners (i.e., developers)\nperceive the challenge catalog. Understanding the developers' perspective is\ninevitable to introduce interactive tool support that promotes reproducibility.\nWe thus attempt to understand developers' perspectives by surveying 53 users of\nSO. In particular, we attempt to -- (1) see developers' viewpoints on the\nagreement to those challenges, (2) find the potential impact of those\nchallenges, (3) see how developers address them, and (4) determine and\nprioritize tool support needs. Survey results show that about 90% of\nparticipants agree to the already exposed challenges. However, they report some\nadditional challenges (e.g., error log missing) that might prevent\nreproducibility. According to the participants, too short code segment and\nabsence of required Class/Interface/Method from code segments severely prevent\nreproducibility, followed by missing important part of code. To promote\nreproducibility, participants strongly recommend introducing tool support that\ninteracts with question submitters with suggestions for improving the code\nsegments if the given code segments fail to reproduce the issues.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.01359,review,pre_llm,2021,12,"{'ai_likelihood': 3.046459621853299e-06, 'text': 'Software and Security Engineering in Digital Transformation\n\n  Digital transformation is a hot topic in the current global environment as a\nlarge number of organizations have been working to adopt digital solutions.\nSoftware engineering has also emerged to be a more important role as a large\nnumber of systems, either traditional or smart, are dependent on the software\nthat collects, store, and process data. The role of software engineers has also\nbecome crucial in digital transformation. In this regard, this paper aims to\nexamine the trends of software engineering and the role of software engineers\nin digital transformation. In addition to this, this paper also examines the\nimportance of secure software development in digital transformation. It can be\nconcluded that software engineering is an integral part of digital\ntransformation as all digital systems make use of software to perform their\nfunctions efficiently. Software act as a bridge between digital systems and\nhumans to use the systems interactively and efficiently.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.05549,review,pre_llm,2021,12,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Combining Design Thinking and Software Requirements Engineering to\n  create Human-centered Software-intensive Systems\n\n  Effective Requirements Engineering is a crucial activity in softwareintensive\ndevelopment projects. The human-centric working mode of Design Thinking is\nconsidered a powerful way to complement such activities when designing\ninnovative systems. Research has already made great strides to illustrate the\nbenefits of using Design Thinking for Requirements Engineering. However, it has\nremained mostly unclear how to actually realize a combination of both. In this\nchapter, we contribute an artifact-based model that integrates Design Thinking\nand Requirements Engineering for innovative software-intensive systems. Drawing\nfrom our research and project experiences, we suggest three strategies for\ntailoring and integrating Design Thinking and Requirements Engineering with\ncomplementary synergies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.12653,review,pre_llm,2021,12,"{'ai_likelihood': 8.079740736219619e-06, 'text': 'Revisiting, Benchmarking and Exploring API Recommendation: How Far Are\n  We?\n\n  Application Programming Interfaces (APIs), which encapsulate the\nimplementation of specific functions as interfaces, greatly improve the\nefficiency of modern software development. As numbers of APIs spring up\nnowadays, developers can hardly be familiar with all the APIs, and usually need\nto search for appropriate APIs for usage. So lots of efforts have been devoted\nto improving the API recommendation task. However, it has been increasingly\ndifficult to gauge the performance of new models due to the lack of a uniform\ndefinition of the task and a standardized benchmark. For example, some studies\nregard the task as a code completion problem; while others recommend relative\nAPIs given natural language queries. To reduce the challenges and better\nfacilitate future research, in this paper, we revisit the API recommendation\ntask and aim at benchmarking the approaches. Specifically, the paper groups the\napproaches into two categories according to the task definition, i.e.,\nquery-based API recommendation and code-based API recommendation. We study 11\nrecently-proposed approaches along with 4 widely-used IDEs. One benchmark named\nas APIBench is then built for the two respective categories of approaches.\nBased on APIBench, we distill some actionable insights and challenges for API\nrecommendation. We also achieve some implications and directions for improving\nthe performance of recommending APIs, including data source selection,\nappropriate query reformulation, low resource setting, and cross-domain\nadaptation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.09725,regular,pre_llm,2021,12,"{'ai_likelihood': 1.158979203965929e-06, 'text': 'scenoRITA: Generating Less-Redundant, Safety-Critical and Motion\n  Sickness-Inducing Scenarios for Autonomous Vehicles\n\n  There is tremendous global enthusiasm for research, development, and\ndeployment of autonomous vehicles (AVs), e.g., self-driving taxis and trucks\nfrom Waymo and Baidu. The current practice for testing AVs uses virtual\ntests-where AVs are tested in software simulations-since they offer a more\nefficient and safer alternative compared to field operational tests.\nSpecifically, search-based approaches are used to find particularly critical\nsituations. These approaches provide an opportunity to automatically generate\ntests; however, systematically creating valid and effective tests for AV\nsoftware remains a major challenge. To address this challenge, we introduce\nscenoRITA, a test generation approach for AVs that uses evolutionary algorithms\nwith (1) a novel gene representation that allows obstacles to be fully mutable,\nhence, resulting in more reported violations, (2) 5 test oracles to determine\nboth safety and motion sickness-inducing violations, and (3) a novel technique\nto identify and eliminate duplicate tests. Our extensive evaluation shows that\nscenoRITA can produce effective driving scenarios that expose an ego car to\nsafety critical situations. scenoRITA generated tests that resulted in a total\nof 1,026 unique violations, increasing the number of reported violations by\n23.47% and 24.21% compared to random test generation and state-of-the-art\npartially-mutable test generation, respectively.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.12331,regular,pre_llm,2021,12,"{'ai_likelihood': 3.1458006964789497e-06, 'text': 'Flakify: A Black-Box, Language Model-based Predictor for Flaky Tests\n\n  Software testing assures that code changes do not adversely affect existing\nfunctionality. However, a test case can be flaky, i.e., passing and failing\nacross executions, even for the same version of the source code. Flaky test\ncases introduce overhead to software development as they can lead to\nunnecessary attempts to debug production or testing code. The state-of-the-art\nML-based flaky test case predictors rely on pre-defined sets of features that\nare either project-specific, require access to production code, which is not\nalways available to software test engineers. Therefore, in this paper, we\npropose Flakify, a black-box, language model-based predictor for flaky test\ncases. Flakify relies exclusively on the source code of test cases, thus not\nrequiring to (a) access to production code (black-box), (b) rerun test cases,\n(c) pre-define features. To this end, we employed CodeBERT, a pre-trained\nlanguage model, and fine-tuned it to predict flaky test cases using the source\ncode of test cases. We evaluated Flakify on two publicly available datasets\n(FlakeFlagger and IDoFT) for flaky test cases and compared our technique with\nthe FlakeFlagger approach using two different evaluation procedures:\ncross-validation and per-project validation. Flakify achieved high F1-scores on\nboth datasets using cross-validation and per-project validation, and surpassed\nFlakeFlagger by 10 and 18 percentage points in terms of precision and recall,\nrespectively, when evaluated on the FlakeFlagger dataset, thus reducing the\ncost bound to be wasted on unnecessarily debugging test cases and production\ncode by the same percentages. Flakify also achieved significantly higher\nprediction results when used to predict test cases on new projects, suggesting\nbetter generalizability over FlakeFlagger. Our results further show that a\nblack-box version of FlakeFlagger is not a viable option for predicting flaky\ntest cases.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.05528,regular,pre_llm,2021,12,"{'ai_likelihood': 2.9802322387695312e-06, 'text': ""Improving Productivity through Corporate Hackathons: A Multiple Case\n  Study of Two Large-scale Agile Organizations\n\n  Software development companies organize hackathons to encourage innovation.\nDespite many benefits of hackathons, in large-scale agile organizations where\nmany teams work together, stopping the ongoing work results in a significant\ndecrease in the immediate output. Motivated by the need to understand whether\nand how to run hackathons, we investigated how the practice affects\nproductivity on the individual and organizational levels. By mapping the\nbenefits and challenges to an established productivity framework, we found that\nhackathons improve developers' satisfaction and well-being, strengthen the\ncompany culture, improve performance (as many ideas are tested), increase\nactivity (as the ideas are developed quickly), and improve communication and\ncollaboration (because the social network is strengthened). Addressing\nmanagerial concerns, we found that hackathons also increase efficiency and flow\nbecause people learn to complete work and make progress quickly, and they build\nnew competence. Finally, with respect to virtual hackathons we found that\ndevelopers work more in isolation because tasks are split between team members\nresulting in less collaboration. This means that some important, expected\nhackathon values in virtual contexts require extra effort and cannot be taken\nfor granted.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.11155,regular,pre_llm,2021,12,"{'ai_likelihood': 2.0199351840549047e-06, 'text': 'AmPyfier: Test Amplification in Python\n\n  Test Amplification is a method to extend handwritten tests into a more\nrigorous test suite covering corner cases in the system under test.\nUnfortunately, the current state-of-the-art for test amplification heavily\nrelies on program analysis techniques which benefit a lot from explicit type\ndeclarations present in statically typed languages like Java and C++. In\ndynamically typed languages, such type declarations are not available and as a\nconsequence test amplification has yet to find its way to programming languages\nlike Python, Ruby and Javascript. In this paper, we present AmPyfier, a\nproof-of-concept tool, which brings test amplification to the dynamically\ntyped, interpreted language Python. We evaluated this approach on 7 open-source\nprojects, and found that AmPyfier could successfully strengthen 7 out of 10\ntest classes (70%). As such we demonstrate that test amplification is feasible\nfor one of the most popular programming languages in use today.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.01644,review,pre_llm,2021,12,"{'ai_likelihood': 4.5365757412380644e-06, 'text': ""Systematically reviewing the layered architectural pattern principles\n  and their use to reconstruct software architectures\n\n  Architectural reconstruction is a reverse engineering activity aiming at\nrecovering the missing decisions on a system. It can help identify the\ncomponents, within a legacy software application, according to the\napplication's architectural pattern. It is useful to identify architectural\ntechnical debt. We are interested in identifying layers within a layered\napplication since the layered pattern is one of the most used patterns to\nstructure large systems. Earlier component reconstruction work focusing on that\npattern relied on generic component identification criteria, such as cohesion\nand coupling. Recent work has identified architectural-pattern specific\ncriteria to identify components within that pattern. However, the\narchitectural-pattern specific criteria that the layered pattern embodies are\nloosely defined. In this paper, we present a first systematic literature review\n(SLR) of the literature aiming at inventorying such criteria for layers within\nlegacy applications and grouping them under four principles that embody the\nfundamental design principles under-lying the architectural pattern. We\nidentify six such criteria in the form of design rules. We also perform a\nsecond systematic literature review to synthesize the literature on software\narchitecture reconstruction in the light of these criteria. We report those\nprinciples, the rules they encompass, their representation, and their usage in\nsoftware architecture reconstruction.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.00858,regular,pre_llm,2021,12,"{'ai_likelihood': 0.0, 'text': 'Common Bugs in Scratch Programs\n\n  Bugs in Scratch programs can spoil the fun and inhibit learning success. Many\ncommon bugs are the result of recurring patterns of bad code. In this paper we\npresent a collection of common code patterns that typically hint at bugs in\nScratch programs, and the LitterBox tool which can automatically detect them.\nWe empirically evaluate how frequently these patterns occur, and how severe\ntheir consequences usually are. While fixing bugs inevitably is part of\nlearning, the possibility to identify the bugs automatically provides the\npotential to support learners\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.08267,regular,pre_llm,2021,12,"{'ai_likelihood': 3.102752897474501e-05, 'text': 'Harvesting Production GraphQL Queries to Detect Schema Faults\n\n  GraphQL is a new paradigm to design web APIs. Despite its growing popularity,\nthere are few techniques to verify the implementation of a GraphQL API. We\npresent a new testing approach based on GraphQL queries that are logged while\nusers interact with an application in production. Our core motivation is that\nproduction queries capture real usages of the application, and are known to\ntrigger behavior that may not be tested by developers. For each logged query, a\ntest is generated to assert the validity of the GraphQL response with respect\nto the schema. We implement our approach in a tool called AutoGraphQL, and\nevaluate it on two real-world case studies that are diverse in their domain and\ntechnology stack: an open-source e-commerce application implemented in Python\ncalled Saleor, and an industrial case study which is a PHP-based finance\nwebsite called Frontapp. AutoGraphQL successfully generates test cases for the\ntwo applications. The generated tests cover 26.9% of the Saleor schema,\nincluding parts of the API not exercised by the original test suite, as well as\n48.7% of the Frontapp schema, detecting 8 schema faults, thanks to production\nqueries.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.06874,regular,pre_llm,2021,12,"{'ai_likelihood': 4.635916815863716e-07, 'text': ""Software Micro-Rejuvenation for Android Mobile Systems\n\n  Software aging -- the phenomenon affecting many long-running systems, causing\nperformance degradation or an increasing failure rate over mission time, and\neventually leading to failure - is known to affect mobile devices and their\noperating systems, too. Software rejuvenation -- the technique typically used\nto counteract aging -- may compromise the user's perception of availability and\nreliability of the personal device, if applied at a coarse grain, e.g., by\nrestarting applications or, worse, rebooting the entire device. This article\nproposes a configurable micro-rejuvenation technique to counteract software\naging in Android-based mobile devices, acting at a fine-grained level, namely\non in-memory system data structures. The technique is engineered in two phases.\nBefore releasing the (customized) Android version, a heap profiling facility is\nused by the manufacturer's developers to identify potentially bloating data\nstructures in Android services and to instrument their code. After release, an\naging detection and rejuvenation service will safely clean up the bloating data\nstructures, with a negligible impact on user perception and device\navailability, as neither the device nor operating system's processes are\nrestarted. The results of experiments show the ability of the technique to\nprovide significant gains in aging mobile operating system responsiveness and\ntime to failure.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.1092,review,pre_llm,2021,12,"{'ai_likelihood': 1.556343502468533e-06, 'text': 'How Do Developers Search for Architectural Information? An Industrial\n  Survey\n\n  Building software systems often requires knowledge and skills beyond what\ndevelopers already possess. In such cases, developers have to leverage\ndifferent sources of information to seek help. A growing number of researchers\nand practitioners have started investigating what programming-related\ninformation developers seek during software development. However, being a high\nlevel and a type of the most important development-related information,\narchitectural information search activity is seldom explored. To fill this gap,\nwe conducted an industrial survey completed by 103 participants to understand\nhow developers search for architectural information to solve their\narchitectural problems in development. Our main findings are: (1) searching for\narchitectural information to learn about the pros and cons of certain\narchitectural solutions (e.g., patterns, tactics) and to make an architecture\ndecision among multiple choices are the most frequent purposes or tasks; (2)\ndevelopers find difficulties mostly in getting relevant architectural\ninformation for addressing quality concerns and making design decisions among\nmultiple choices when seeking architectural information; (3) taking too much\ntime to go through architectural information retrieved from various sources and\nfeeling overwhelmed due to the dispersion and abundance of architectural\ninformation in various sources are the top two major challenges developers face\nwhen searching for architectural information. Our findings (1) provide\nresearchers with future directions, such as the design and development of\napproaches and tools for searching architectural information from multiple\nsources, and (2) can be used to provide guidelines for practitioners to refer\nto when seeking architectural information and providing architectural\ninformation that could be considered useful.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.14464,regular,pre_llm,2021,12,"{'ai_likelihood': 3.0795733133951826e-06, 'text': 'Forking Around: Correlation of forking practices with the success of a\n  project\n\n  Forking-based development has made it easier and straightforward for\ndevelopers to contribute to open-source software (OSS). Developers can fork an\nexisting project and add changes in their local version without interrupting\nthe development process in the main project. Despite the efficiency of OSS,\nmore than 80% of the projects are not sustainable. Identifying the elements\nrelated to OSS success can enlighten developers regarding the sustainability of\na project. In our study, we explore whether or not the inefficiencies which\narise due to forking-based development like redundant development, fragmented\ncommunities, lack of modularity, etc. have any relation to the outcome of a\nproject in terms of sustainability. We formulate eight metrics to quantify\nattributes for projects in the ASFI dataset. To find the correlation between\nthe metrics and the success of a project, we built a logistic regression model\nto metrics with significant p-values and performed backward stepwise regression\nanalysis, using the stepAIC function in R to cross-check our findings. The\nfindings show that modularity, centralized management index, and hard forks are\nconsequential for the success of a project. Developers can use the outcomes of\nour research to plan and structure their projects to increase the probability\nof their success.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.08315,regular,pre_llm,2021,12,"{'ai_likelihood': 8.086363474527995e-05, 'text': 'Nirikshak: A Clustering Based Autonomous API Testing Framework\n\n  Quality Assurance (QA) is a critical component in product development,\nparticularly in software testing. Despite the evolution of automated methods,\ntesting for REST APIs often involves repetitive tasks. A significant portion of\nresources is dedicated more to scripting tests than to detecting and resolving\nactual software bugs. Additionally, conventional testing methods frequently\nstruggle to adapt to software updates. However, with advancements in data\nscience, a new paradigm is emerging: a self-reliant testing framework. This\ninnovative approach minimizes the need for user intervention, achieving level 2\nof autonomy in executing REST API testing procedures. It does so by employing a\nclustering method and analysis on logs categorizing test cases efficiently and\nthereby streamlining the testing process as well as ensuring more dynamic\nadaptability to software changes. Nirikshak is publicly available as an\nopen-source software for the community at\nhttps://github.com/yashmahalwal/nirikshak.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.08265,regular,pre_llm,2021,12,"{'ai_likelihood': 2.4835268656412763e-06, 'text': 'Causality in Requirements Artifacts: Prevalence, Detection, and Impact\n\n  Background: Causal relations in natural language (NL) requirements convey\nstrong, semantic information. Automatically extracting such causal information\nenables multiple use cases, such as test case generation, but it also requires\nto reliably detect causal relations in the first place. Currently, this is\nstill a cumbersome task as causality in NL requirements is still barely\nunderstood and, thus, barely detectable. Objective: In our empirically informed\nresearch, we aim at better understanding the notion of causality and supporting\nthe automatic extraction of causal relations in NL requirements. Method: In a\nfirst case study, we investigate 14.983 sentences from 53 requirements\ndocuments to understand the extent and form in which causality occurs. Second,\nwe present and evaluate a tool-supported approach, called CiRA, for causality\ndetection. We conclude with a second case study where we demonstrate the\napplicability of our tool and investigate the impact of causality on NL\nrequirements. Results: The first case study shows that causality constitutes\naround 28% of all NL requirements sentences. We then demonstrate that our\ndetection tool achieves a macro-F1 score of 82% on real-world data and that it\noutperforms related approaches with an average gain of 11.06% in macro-Recall\nand 11.43% in macro-Precision. Finally, our second case study corroborates the\npositive correlations of causality with features of NL requirements.\nConclusion: The results strengthen our confidence in the eligibility of causal\nrelations for downstream reuse, while our tool and publicly available data\nconstitute a first step in the ongoing endeavors of utilizing causality in RE\nand beyond.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.08207,regular,pre_llm,2021,12,"{'ai_likelihood': 6.953875223795574e-07, 'text': 'Using the DELPHI Method for Model for Role Assignment in the Software\n  Industry\n\n  Over the past two decades, there has been a growing interest in modeling the\nelements that need to be considered when assigning people to roles in software\nprojects, as evidenced by the number of available publications related to the\ntopic. However, for the most part, these studies, have taken only a partial\napproach to the issue. Some have focused on the target roles competency\nprofile, while others have tried to understand the preferences of software\ndevelopers for activities linked to certain roles and the relationship between\nthese preferences and the candidates personal traits, to mention only two\nexamples. Our research aims to find elements that can be integrated into an\nallocation model that complements current approaches by including competencies,\npersonal traits, and project team building theories. To do so we performed an\nexperts consultation exercise using the DELPHI method; which allowed us to\nvalidate a set of patterns related to different candidates personal traits, and\nthe link between the teams motivational motors and their roles within the\nsoftware development.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.1037,regular,pre_llm,2021,12,"{'ai_likelihood': 1.079506344265408e-05, 'text': 'Operation-based Refactoring-aware Merging: An Empirical Evaluation\n\n  Dealing with merge conflicts in version control systems is a challenging task\nfor software developers. Resolving merge conflicts is a time-consuming and\nerror-prone process, which distracts developers from important tasks. Recent\nwork shows that refactorings are often involved in merge conflicts and that\nrefactoring-related conflicts tend to be larger, making them harder to resolve.\nIn the literature, there are two refactoring-aware merging techniques that\nclaim to automatically resolve refactoring-related conflicts; however, these\ntwo techniques have never been empirically compared. In this paper, we present\nRefMerge, a rejuvenated Java-based design and implementation of the first\ntechnique, which is an operation-based refactoring-aware merging algorithm. We\ncompare RefMerge to Git and the state-of-the-art graph-based refactoring-aware\nmerging tool, IntelliMerge, on 2,001 merge scenarios with refactoring-related\nconflicts from 20 open-source projects. We find that RefMerge resolves or\nreduces conflicts in 497 (25%) merge scenarios while increasing conflicting LOC\nin only 214 (11%) scenarios. On the other hand, we find that IntelliMerge\nresolves or reduces conflicts in 478 (24%) merge scenarios but increases\nconflicting LOC in 597 (30%) merge scenarios. We additionally conduct a\nqualitative analysis of the differences between the three merging algorithms\nand provide insights of the strengths and weaknesses of each tool. We find that\nwhile IntelliMerge does well with ordering and formatting conflicts, it\nstruggles with class-level refactorings and scenarios with several\nrefactorings. On the other hand, RefMerge is resilient to the number of\nrefactorings in a merge scenario, but we find that RefMerge introduces\nconflicts when inverting move-related refactorings.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.00699,regular,pre_llm,2021,12,"{'ai_likelihood': 3.013345930311415e-06, 'text': 'BERT_SE: A Pre-trained Language Representation Model for Software\n  Engineering\n\n  The application of Natural Language Processing (NLP) has achieved a high\nlevel of relevance in several areas. In the field of software engineering (SE),\nNLP applications are based on the classification of similar texts (e.g.\nsoftware requirements), applied in tasks of estimating software effort,\nselection of human resources, etc. Classifying software requirements has been a\ncomplex task, considering the informality and complexity inherent in the texts\nproduced during the software development process. The pre-trained embedding\nmodels are shown as a viable alternative when considering the low volume of\ntextual data labeled in the area of software engineering, as well as the lack\nof quality of these data. Although there is much research around the\napplication of word embedding in several areas, to date, there is no knowledge\nof studies that have explored its application in the creation of a specific\nmodel for the domain of the SE area. Thus, this article presents the proposal\nfor a contextualized embedding model, called BERT_SE, which allows the\nrecognition of specific and relevant terms in the context of SE. The assessment\nof BERT_SE was performed using the software requirements classification task,\ndemonstrating that this model has an average improvement rate of 13% concerning\nthe BERT_base model, made available by the authors of BERT. The code and\npre-trained models are available at https://github.com/elianedb.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.03619,regular,pre_llm,2021,12,"{'ai_likelihood': 2.4504131740993925e-06, 'text': 'IntelliTC: Automating Type Changes in IntelliJ IDEA\n\n  Developers often change types of program elements. Such refactoring often\ninvolves updating not only the type of the element itself, but also the API of\nall type-dependent references in the code, thus it is tedious and\ntime-consuming. Despite type changes being more frequent than renamings, just a\nfew current IDE tools provide partially-automated support only for a small set\nof hard-coded types. Researchers have recently proposed a data-driven approach\nto inferring API rewrite rules for type change patterns in Java using code\ncommits history. In this paper, we build upon these recent advances and\nintroduce IntelliTC - a tool to perform Java type change refactoring. We\nimplemented it as a plugin for IntelliJ IDEA, a popular Java IDE developed by\nJetBrains. We present 3 different ways of providing support for such a\nrefactoring from the standpoint of the user experience: Classic mode, Suggested\nRefactoring, and Inspection mode. To evaluate these modalities of using\nIntelliTC, we surveyed 22 experienced software developers. They positively\nrated the usefulness of the tool.\n  The source code and distribution of the plugin are available on GitHub:\nhttps://github.com/JetBrains-Research/data-driven-type-migration. A\ndemonstration video is on YouTube: https://youtu.be/pdcfvADA1PY.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.02215,review,pre_llm,2022,1,"{'ai_likelihood': 9.338061014811199e-06, 'text': 'On the Prevalence, Impact, and Evolution of SQL Code Smells in\n  Data-Intensive Systems\n\n  Code smells indicate software design problems that harm software quality.\nData-intensive systems that frequently access databases often suffer from SQL\ncode smells besides the traditional smells. While there have been extensive\nstudies on traditional code smells, recently, there has been a growing interest\nin SQL code smells. In this paper, we conduct an empirical study to investigate\nthe prevalence and evolution of SQL code smells in open-source, data-intensive\nsystems. We collected 150 projects and examined both traditional and SQL code\nsmells in these projects. Our investigation delivers several important\nfindings. First, SQL code smells are indeed prevalent in data-intensive\nsoftware systems. Second, SQL code smells have a weak co-occurrence with\ntraditional code smells. Third, SQL code smells have a weaker association with\nbugs than that of traditional code smells. Fourth, SQL code smells are more\nlikely to be introduced at the beginning of the project lifetime and likely to\nbe left in the code without a fix, compared to traditional code smells.\nOverall, our results show that SQL code smells are indeed prevalent and\npersistent in the studied data-intensive software systems. Developers should be\naware of these smells and consider detecting and refactoring SQL code smells\nand traditional code smells separately, using dedicated tools.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.01988,regular,pre_llm,2022,1,"{'ai_likelihood': 1.953707800971137e-06, 'text': ""Can Identifier Splitting Improve Open-Vocabulary Language Model of Code?\n\n  Statistical language models on source code have successfully assisted\nsoftware engineering tasks. However, developers can create or pick arbitrary\nidentifiers when writing source code. Freely chosen identifiers lead to the\nnotorious out-of-vocabulary (OOV) problem that negatively affects model\nperformance. Recently, Karampatsis et al. showed that using the Byte Pair\nEncoding (BPE) algorithm to address the OOV problem can improve the language\nmodels' predictive performance on source code. However, a drawback of BPE is\nthat it cannot split the identifiers in a way that preserves the meaningful\nsemantics. Prior researchers also show that splitting compound identifiers into\nsub-words that reflect the semantics can benefit software development tools.\nThese two facts motivate us to explore whether identifier splitting techniques\ncan be utilized to augment the BPE algorithm and boost the performance of\nopen-vocabulary language models considered in Karampatsis et al.'s work.\n  This paper proposes to split identifiers in both constructing vocabulary and\nprocessing model inputs procedures, thus exploiting three different settings of\napplying identifier splitting to language models for the code completion task.\nWe contrast models' performance under these settings and find that simply\ninserting identifier splitting into the pipeline hurts the model performance,\nwhile a hybrid strategy combining identifier splitting and the BPE algorithm\ncan outperform the original open-vocabulary models on predicting identifiers by\n3.68% of recall and 6.32% of Mean Reciprocal Rank. The results also show that\nthe hybrid strategy can improve the entropy of language models by 2.02%.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.07029,regular,pre_llm,2022,1,"{'ai_likelihood': 1.5762117173936633e-05, 'text': 'Conflict-aware Inference of Python Compatible Runtime Environments with\n  Domain Knowledge Graph\n\n  Code sharing and reuse is a widespread use practice in software engineering.\nAlthough a vast amount of open-source Python code is accessible on many online\nplatforms, programmers often find it difficult to restore a successful runtime\nenvironment. Previous studies validated automatic inference of Python\ndependencies using pre-built knowledge bases. However, these studies do not\ncover sufficient knowledge to accurately match the Python code and also ignore\nthe potential conflicts between their inferred dependencies, thus resulting in\na low success rate of inference. In this paper, we propose PyCRE, a new\napproach to automatically inferring Python compatible runtime environments with\ndomain knowledge graph (KG). Specifically, we design a domain-specific ontology\nfor Python third-party packages and construct KGs for over 10,000 popular\npackages in Python 2 and Python 3. PyCRE discovers candidate libraries by\nmeasuring the matching degree between the known libraries and the third-party\nresources used in target code. For the NP-complete problem of dependency\nsolving, we propose a heuristic graph traversal algorithm to efficiently\nguarantee the compatibility between packages. PyCRE achieves superior\nperformance on a real-world dataset and efficiently resolves nearly half more\nimport errors than previous methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.11327,regular,pre_llm,2022,1,"{'ai_likelihood': 3.2120280795627172e-06, 'text': 'Aspect-Based API Review Classification: How Far Can Pre-Trained\n  Transformer Model Go?\n\n  APIs (Application Programming Interfaces) are reusable software libraries and\nare building blocks for modern rapid software development. Previous research\nshows that programmers frequently share and search for reviews of APIs on the\nmainstream software question and answer (Q&A) platforms like Stack Overflow,\nwhich motivates researchers to design tasks and approaches related to process\nAPI reviews automatically. Among these tasks, classifying API reviews into\ndifferent aspects (e.g., performance or security), which is called the\naspect-based API review classification, is of great importance. The current\nstate-of-the-art (SOTA) solution to this task is based on the traditional\nmachine learning algorithm. Inspired by the great success achieved by\npre-trained models on many software engineering tasks, this study fine-tunes\nsix pre-trained models for the aspect-based API review classification task and\ncompares them with the current SOTA solution on an API review benchmark\ncollected by Uddin et al. The investigated models include four models (BERT,\nRoBERTa, ALBERT and XLNet) that are pre-trained on natural languages,\nBERTOverflow that is pre-trained on text corpus extracted from posts on Stack\nOverflow, and CosSensBERT that is designed for handling imbalanced data. The\nresults show that all the six fine-tuned models outperform the traditional\nmachine learning-based tool. More specifically, the improvement on the F1-score\nranges from 21.0% to 30.2%. We also find that BERTOverflow, a model pre-trained\non the corpus from Stack Overflow, does not show better performance than BERT.\nThe result also suggests that CosSensBERT also does not exhibit better\nperformance than BERT in terms of F1, but it is still worthy of being\nconsidered as it achieves better performance on MCC and AUC.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.08368,regular,pre_llm,2022,1,"{'ai_likelihood': 8.54333241780599e-06, 'text': ""An Alternative Issue Tracking Dataset of Public Jira Repositories\n\n  Organisations use issue tracking systems (ITSs) to track and document their\nprojects' work in units called issues. This style of documentation encourages\nevolutionary refinement, as each issue can be independently improved, commented\non, linked to other issues, and progressed through the organisational workflow.\nCommonly studied ITSs so far include GitHub, GitLab, and Bugzilla, while Jira,\none of the most popular ITS in practice with a wealth of additional\ninformation, has yet to receive similar attention. Unfortunately, diverse\npublic Jira datasets are rare, likely due to the difficulty in finding and\naccessing these repositories. With this paper, we release a dataset of 16\npublic Jiras with 1822 projects, spanning 2.7 million issues with a combined\ntotal of 32 million changes, 9 million comments, and 1 million issue links. We\nbelieve this Jira dataset will lead to many fruitful research projects\ninvestigating issue evolution, issue linking, cross-project analysis, as well\nas cross-tool analysis when combined with existing well-studied ITS datasets.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.04837,regular,pre_llm,2022,1,"{'ai_likelihood': 6.953875223795574e-07, 'text': 'Using Deep Learning to Generate Complete Log Statements\n\n  Logging is a practice widely adopted in several phases of the software\nlifecycle. For example, during software development log statements allow\nengineers to verify and debug the system by exposing fine-grained information\nof the running software. While the benefits of logging are undisputed, taking\nproper decisions about where to inject log statements, what information to log,\nand at which log level (e.g., error, warning) is crucial for the logging\neffectiveness. In this paper, we present LANCE (Log stAtemeNt reCommEnder), the\nfirst approach supporting developers in all these decisions. LANCE features a\nText-To-Text-Transfer-Transformer (T5) model that has been trained on 6,894,456\nJava methods. LANCE takes as input a Java method and injects in it a full log\nstatement, including a human-comprehensible logging message and properly\nchoosing the needed log level and the statement location. Our results show that\nLANCE is able to (i) properly identify the location in the code where to inject\nthe statement in 65.9% of Java methods requiring it; (ii) selecting the proper\nlog level in 66.2% of cases; and (iii) generate a completely correct log\nstatement including a meaningful logging message in 15.2% of cases.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.07351,review,pre_llm,2022,1,"{'ai_likelihood': 3.741847144232856e-06, 'text': 'A Taxonomy of Testable HTML5 Canvas Issues\n\n  The HTML5 <canvas> is widely used to display high quality graphics in web\napplications. However, the combination of web, GUI, and visual techniques that\nare required to build <canvas> applications, together with the lack of testing\nand debugging tools, makes developing such applications very challenging. To\nhelp direct future research on testing <canvas> applications, in this paper we\npresent a taxonomy of testable <canvas> issues. First, we extracted 2,403\n<canvas>-related issue reports from 123 open-source GitHub projects that use\nthe HTML5 <canvas>. Second, we constructed our taxonomy by manually classifying\na random sample of 332 issue reports. Our manual classification identified five\nbroad categories of testable <canvas> issues, such as Visual and Performance\nissues. We found that Visual issues are the most frequent (35%), while\nPerformance issues are relatively infrequent (5%). We also found that many\ntestable <canvas> issues that present themselves visually on the <canvas> are\nactually caused by other components of the web application. Our taxonomy of\ntestable <canvas> issues can be used to steer future research into <canvas>\nissues and testing.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.00967,review,pre_llm,2022,1,"{'ai_likelihood': 9.602970547146268e-06, 'text': 'The PETSc Community Is the Infrastructure\n\n  The communities who develop and support open source scientific software\npackages are crucial to the utility and success of such packages. Moreover,\nthese communities form an important part of the human infrastructure that\nenables scientific progress. This paper discusses aspects of the PETSc\n(Portable Extensible Toolkit for Scientific Computation) community, its\norganization, and technical approaches that enable community members to help\neach other efficiently.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.01184,review,pre_llm,2022,1,"{'ai_likelihood': 1.4901161193847656e-06, 'text': 'Symptoms of Architecture Erosion in Code Reviews: A Study of Two\n  OpenStack Projects\n\n  The phenomenon of architecture erosion can negatively impact the maintenance\nand evolution of software systems, and manifest in a variety of symptoms during\nsoftware development. While erosion is often considered rather late, its\nsymptoms can act as early warnings to software developers, if detected in time.\nIn addition to static source code analysis, code reviews can be a source of\ndetecting erosion symptoms and subsequently taking action. In this study, we\ninvestigate the erosion symptoms discussed in code reviews, as well as their\ntrends, and the actions taken by developers. Specifically, we conducted an\nempirical study with the two most active Open Source Software (OSS) projects in\nthe OpenStack community (i.e., Nova and Neutron). We manually checked 21,274\ncode review comments retrieved by keyword search and random selection, and\nidentified 502 code review comments (from 472 discussion threads) that discuss\nerosion. Our findings show that (1) the proportion of erosion symptoms is\nrather low, yet notable in code reviews and the most frequently identified\nerosion symptoms are architectural violation, duplicate functionality, and\ncyclic dependency; (2) the declining trend of the identified erosion symptoms\nin the two OSS projects indicates that the architecture tends to stabilize over\ntime; and (3) most code reviews that identify erosion symptoms have a positive\nimpact on removing erosion symptoms, but a few symptoms still remain and are\nignored by developers. The results suggest that (1) code review provides a\npractical way to reduce erosion symptoms; and (2) analyzing the trend of\nerosion symptoms can help get an insight about the erosion status of software\nsystems, and subsequently avoid the potential risk of architecture erosion.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.03061,review,pre_llm,2022,1,"{'ai_likelihood': 3.5762786865234375e-06, 'text': 'A systematic literature review on counterexample explanation\n\n  Context: Safety is of paramount importance for cyber-physical systems in\ndomains such as automotive, robotics, and avionics. Formal methods such as\nmodel checking are one way to ensure the safety of cyber-physical systems.\nHowever, adoption of formal methods in industry is hindered by usability\nissues, particularly the difficulty of understanding model checking results.\nObjective: We want to provide an overview of the state of the art for\ncounterexample explanation by investigating the contexts, techniques, and\nevaluation of research approaches in this field. This overview shall provide an\nunderstanding of current and guide future research. Method: To provide this\noverview, we conducted a systematic literature review. The survey comprises 116\npublications that address counterexample explanations for model checking.\nResults: Most primary studies provide counterexample explanations graphically\nor as traces, minimize counterexamples to reduce complexity, localize errors in\nthe models expressed in the input formats of model checkers, support linear\ntemporal logic or computation tree logic specifications, and use model checkers\nof the Symbolic Model Verifier family. Several studies evaluate their\napproaches in safety-critical domains with industrial applications. Conclusion:\nWe notably see a lack of research on counterexample explanation that targets\nprobabilistic and real-time systems, leverages the explanations to\ndomain-specific models, and evaluates approaches in user studies. We conclude\nby discussing the adequacy of different types of explanations for users with\nvarying domain and formal methods expertise, showing the need to support\nlaypersons in understanding model checking results to increase adoption of\nformal methods in industry.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.10874,regular,pre_llm,2022,1,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'Fuzzing Class Specifications\n\n  Expressing class specifications via executable constraints is important for\nvarious software engineering tasks such as test generation, bug finding and\nautomated debugging, but developers rarely write them. Techniques that infer\nspecifications from code exist to fill this gap, but they are designed to\nsupport specific kinds of assertions and are difficult to adapt to support\ndifferent assertion languages, e.g., to add support for quantification, or\nadditional comparison operators, such as membership or containment.\n  To address the above issue, we present SpecFuzzer, a novel technique that\ncombines grammar-based fuzzing, dynamic invariant detection, and mutation\nanalysis, to automatically produce class specifications. SpecFuzzer uses: (i) a\nfuzzer as a generator of candidate assertions derived from a grammar that is\nautomatically obtained from the class definition; (ii) a dynamic invariant\ndetector -- Daikon -- to filter out assertions invalidated by a test suite; and\n(iii) a mutation-based mechanism to cluster and rank assertions, so that\nsimilar constraints are grouped and then the stronger prioritized.\nGrammar-based fuzzing enables SpecFuzzer to be straightforwardly adapted to\nsupport different specification languages, by manipulating the fuzzing grammar,\ne.g., to include additional operators.\n  We evaluate our technique on a benchmark of 43 Java methods employed in the\nevaluation of the state-of-the-art techniques GAssert and EvoSpex. Our results\nshow that SpecFuzzer can easily support a more expressive assertion language,\nover which is more effective than GAssert and EvoSpex in inferring\nspecifications, according to standard performance metrics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.03606,review,pre_llm,2022,1,"{'ai_likelihood': 4.801485273573134e-06, 'text': 'More Software Analytics Patterns: Broad-Spectrum Diagnostic and Embedded\n  Improvements\n\n  Software analytics is a data-driven approach to decision making, which allows\nsoftware practitioners to leverage valuable insights from data about software\nto achieve higher development process productivity and improve different\naspects of software quality. In previous work, a set of patterns for adopting a\nlean software analytics process was identified through a literature review.\nThis paper presents two patterns to add to the original set, forming a pattern\nlanguage for adopting software analytics practices that aims to inform\ndecision-making activities of software practitioners. The writing of these two\npatterns was informed by the solutions employed in the context of two case\nstudies on software analytics practices, and the patterns were further\nvalidated by searching for their occurrence in the literature. The pattern\nBroad-Spectrum Diagnostic proposes to conduct more broad analysis based on\ncommon metrics when the team does not have the expertise to understand the kind\nof problems that software analytics can help to solve; and the pattern Embedded\nImprovements suggests adding improvement tasks as part of other routine\nactivities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.06865,regular,pre_llm,2022,1,"{'ai_likelihood': 1.1920928955078125e-06, 'text': ""Using Reinforcement Learning for Load Testing of Video Games\n\n  Different from what happens for most types of software systems, testing video\ngames has largely remained a manual activity performed by human testers. This\nis mostly due to the continuous and intelligent user interaction video games\nrequire. Recently, reinforcement learning (RL) has been exploited to partially\nautomate functional testing. RL enables training smart agents that can even\nachieve super-human performance in playing games, thus being suitable to\nexplore them looking for bugs. We investigate the possibility of using RL for\nload testing video games. Indeed, the goal of game testing is not only to\nidentify functional bugs, but also to examine the game's performance, such as\nits ability to avoid lags and keep a minimum number of frames per second (FPS)\nwhen high-demanding 3D scenes are shown on screen. We define a methodology\nemploying RL to train an agent able to play the game as a human while also\ntrying to identify areas of the game resulting in a drop of FPS. We demonstrate\nthe feasibility of our approach on three games. Two of them are used as\nproof-of-concept, by injecting artificial performance bugs. The third one is an\nopen-source 3D game that we load test using the trained agent showing its\npotential to identify areas of the game resulting in lower FPS.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.01549,regular,pre_llm,2022,1,"{'ai_likelihood': 8.940696716308594e-07, 'text': 'SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code\n  Representations\n\n  Recent years have seen the successful application of large pre-trained models\nto code representation learning, resulting in substantial improvements on many\ncode-related downstream tasks. But there are issues surrounding their\napplication to SE tasks. First, the majority of the pre-trained models focus on\npre-training only the encoder of the Transformer. For generation tasks that are\naddressed using models with the encoder-decoder architecture, however, there is\nno reason why the decoder should be left out during pre-training. Second, many\nexisting pre-trained models, including state-of-the-art models such as\nT5-learning, simply reuse the pre-training tasks designed for natural\nlanguages. Moreover, to learn the natural language description of source code\nneeded eventually for code-related tasks such as code summarization, existing\npre-training tasks require a bilingual corpus composed of source code and the\nassociated natural language description, which severely limits the amount of\ndata for pre-training. To this end, we propose SPT-Code, a sequence-to-sequence\npre-trained model for source code. In order to pre-train SPT-Code in a\nsequence-to-sequence manner and address the aforementioned weaknesses\nassociated with existing pre-training tasks, we introduce three pre-training\ntasks that are specifically designed to enable SPT-Code to learn knowledge of\nsource code, the corresponding code structure, as well as a natural language\ndescription of the code without relying on any bilingual corpus, and eventually\nexploit these three sources of information when it is applied to downstream\ntasks. Experimental results demonstrate that SPT-Code achieves state-of-the-art\nperformance on five code-related downstream tasks after fine-tuning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.08698,regular,pre_llm,2022,1,"{'ai_likelihood': 8.046627044677734e-06, 'text': 'Natural Attack for Pre-trained Models of Code\n\n  Pre-trained models of code have achieved success in many important software\nengineering tasks. However, these powerful models are vulnerable to adversarial\nattacks that slightly perturb model inputs to make a victim model produce wrong\noutputs. Current works mainly attack models of code with examples that preserve\noperational program semantics but ignore a fundamental requirement for\nadversarial example generation: perturbations should be natural to human\njudges, which we refer to as naturalness requirement.\n  In this paper, we propose ALERT (nAturaLnEss AwaRe ATtack), a black-box\nattack that adversarially transforms inputs to make victim models produce wrong\noutputs. Different from prior works, this paper considers the natural semantic\nof generated examples at the same time as preserving the operational semantic\nof original inputs. Our user study demonstrates that human developers\nconsistently consider that adversarial examples generated by ALERT are more\nnatural than those generated by the state-of-the-art work by Zhang et al. that\nignores the naturalness requirement. On attacking CodeBERT, our approach can\nachieve attack success rates of 53.62%, 27.79%, and 35.78% across three\ndownstream tasks: vulnerability prediction, clone detection and code authorship\nattribution. On GraphCodeBERT, our approach can achieve average success rates\nof 76.95%, 7.96% and 61.47% on the three tasks. The above outperforms the\nbaseline by 14.07% and 18.56% on the two pre-trained models on average.\nFinally, we investigated the value of the generated adversarial examples to\nharden victim models through an adversarial fine-tuning procedure and\ndemonstrated the accuracy of CodeBERT and GraphCodeBERT against ALERT-generated\nadversarial examples increased by 87.59% and 92.32%, respectively.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.07996,regular,pre_llm,2022,1,"{'ai_likelihood': 1.6225708855523005e-06, 'text': 'Evaluating the Performance of Clone Detection Tools in Detecting Cloned\n  Co-change Candidates\n\n  Co-change candidates are the group of code fragments that require a change if\nany of these fragments experience a modification in a commit operation during\nsoftware evolution. The cloned co-change candidates are a subset of the\nco-change candidates, and the members in this subset are clones of one another.\nThe cloned co-change candidates are usually created by reusing existing code\nfragments in a software system. Detecting cloned co-change candidates is\nessential for clone-tracking, and studies have shown that we can use clone\ndetection tools to find cloned co-change candidates. However, although several\nstudies evaluate clone detection tools for their accuracy in detecting cloned\nfragments, we found no study that evaluates clone detection tools for detecting\ncloned co-change candidates. In this study, we explore the dimension of code\nclone research for detecting cloned co-change candidates. We compare the\nperformance of 12 different configurations of nine promising clone detection\ntools in identifying cloned co-change candidates from eight open-source C and\nJava-based subject systems of various sizes and application domains. A ranked\nlist and analysis of the results provides valuable insights and guidelines into\nselecting and configuring a clone detection tool for identifying co-change\ncandidates and leads to a new dimension of code clone research into change\nimpact analysis.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.10599,review,pre_llm,2022,1,"{'ai_likelihood': 2.5166405571831598e-05, 'text': ""The Unexplored Terrain of Compiler Warnings\n\n  The authors' industry experiences suggest that compiler warnings, a\nlightweight version of program analysis, are valuable early bug detection\ntools. Significant costs are associated with patches and security bulletins for\nissues that could have been avoided if compiler warnings were addressed. Yet,\nthe industry's attitude towards compiler warnings is mixed. Practices range\nfrom silencing all compiler warnings to having a zero-tolerance policy as to\nany warnings. Current published data indicates that addressing compiler\nwarnings early is beneficial. However, support for this value theory stems from\ngrey literature or is anecdotal. Additional focused research is needed to truly\nassess the cost-benefit of addressing warnings.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.02944,regular,pre_llm,2022,1,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Adaptive Performance Anomaly Detection for Online Service Systems via\n  Pattern Sketching\n\n  To ensure the performance of online service systems, their status is closely\nmonitored with various software and system metrics. Performance anomalies\nrepresent the performance degradation issues (e.g., slow response) of the\nservice systems. When performing anomaly detection over the metrics, existing\nmethods often lack the merit of interpretability, which is vital for engineers\nand analysts to take remediation actions. Moreover, they are unable to\neffectively accommodate the ever-changing services in an online fashion. To\naddress these limitations, in this paper, we propose ADSketch, an interpretable\nand adaptive performance anomaly detection approach based on pattern sketching.\nADSketch achieves interpretability by identifying groups of anomalous metric\npatterns, which represent particular types of performance issues. The\nunderlying issues can then be immediately recognized if similar patterns emerge\nagain. In addition, an adaptive learning algorithm is designed to embrace\nunprecedented patterns induced by service updates or user behavior changes. The\nproposed approach is evaluated with public data as well as industrial data\ncollected from a representative online service system in Huawei Cloud. The\nexperimental results show that ADSketch outperforms state-of-the-art approaches\nby a significant margin, and demonstrate the effectiveness of the online\nalgorithm in new pattern discovery. Furthermore, our approach has been\nsuccessfully deployed in industrial practice.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.00736,regular,pre_llm,2022,1,"{'ai_likelihood': 1.9371509552001953e-05, 'text': 'Exception-Driven Fault Localization for Automated Program Repair\n\n  Automated Program Repair (APR) techniques typically exploit spectrum-based\nfault localization (SBFL) to identify the program locations that should be\npatched, making the effectiveness of APR techniques dependent on the\neffectiveness of fault localization. Indeed, results show that SBFL often does\nnot localize faults accurately, hindering the effectiveness of APR. In this\npaper, we propose EXCEPT, a technique that addresses the localization problem\nby focusing on the semantics of failures rather than on the correlation between\nthe executed statements and the failed tests, as SBFL does. We focus on\nfailures due to exceptions and we exploit their type and source to localize and\nguess the faults. Experiments with 43 exception-raising faults from the\nDefects4J benchmark show that EXCEPT can perform better than Ochiai and ssFix.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.00191,review,pre_llm,2022,1,"{'ai_likelihood': 2.1192762586805556e-06, 'text': 'Revisiting Neuron Coverage Metrics and Quality of Deep Neural Networks\n\n  Deep neural networks (DNN) have been widely applied in modern life, including\ncritical domains like autonomous driving, making it essential to ensure the\nreliability and robustness of DNN-powered systems. As an analogy to code\ncoverage metrics for testing conventional software, researchers have proposed\nneuron coverage metrics and coverage-driven methods to generate DNN test cases.\nHowever, Yan et al. doubt the usefulness of existing coverage criteria in DNN\ntesting. They show that a coverage-driven method is less effective than a\ngradient-based method in terms of both uncovering defects and improving model\nrobustness.\n  In this paper, we conduct a replication study of the work by Yan et al. and\nextend the experiments for deeper analysis. A larger model and a dataset of\nhigher resolution images are included to examine the generalizability of the\nresults. We also extend the experiments with more test case generation\ntechniques and adjust the process of improving model robustness to be closer to\nthe practical life cycle of DNN development. Our experiment results confirm the\nconclusion from Yan et al. that coverage-driven methods are less effective than\ngradient-based methods. Yan et al. find that using gradient-based methods to\nretrain cannot repair defects uncovered by coverage-driven methods. They\nattribute this to the fact that the two types of methods use different\nperturbation strategies: gradient-based methods perform differentiable\ntransformations while coverage-driven methods can perform additional\nnon-differentiable transformations. We test several hypotheses and further show\nthat even coverage-driven methods are constrained only to perform\ndifferentiable transformations, the uncovered defects still cannot be repaired\nby adversarial training with gradient-based methods. Thus, defensive strategies\nfor coverage-driven methods should be further studied.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.07278,review,pre_llm,2022,2,"{'ai_likelihood': 4.470348358154297e-06, 'text': ""Worldwide Gender Differences in Public Code Contributions\n\n  Gender imbalance is a well-known phenomenon observed throughout sciences\nwhich is particularly severe in software development and Free/Open Source\nSoftware communities. Little is know yet about the geography of this phenomenon\nin particular when considering large scales for both its time and space\ndimensions. We contribute to fill this gap with a longitudinal study of the\npopulation of contributors to publicly available software source code. We\nanalyze the development history of 160 million software projects for a total of\n2.2 billion commits contributed by 43 million distinct authors over a period of\n50 years. We classify author names by gender using name frequencies and author\ngeographical locations using heuristics based on email addresses and time\nzones. We study the evolution over time of contributions to public code by\ngender and by world region. For the world overall, we confirm previous findings\nabout the low but steadily increasing ratio of contributions by female authors.\nWhen breaking down by world regions we find that the long-term growth of female\nparticipation is a worldwide phenomenon. We also observe a decrease in the\nratio of female participation during the COVID-19 pandemic, suggesting that\nwomen's ability to contribute to public code has been more hindered than that\nof men.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.11861,regular,pre_llm,2022,2,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Deploying Static Analysis\n\n  Static source code analysis is a powerful tool for finding and fixing bugs\nwhen deployed properly; it is, however, all too easy to deploy it in a way that\nlooks good superficially, but which misses important defects, shows many false\npositives, and brings the tool into disrepute. This article is a guide to the\nprocess of deploying a static analysis tool in a large organization while\navoiding the worst organizational and technical pitfalls. My main point is the\nimportance of concentrating on the main goal of getting bugs fixed, against all\nthe competing lesser goals which will arise during the process.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.06562,regular,pre_llm,2022,2,"{'ai_likelihood': 2.0629829830593535e-05, 'text': ""Gamekins: Gamifying Software Testing in Jenkins\n\n  Developers have to write thorough tests for their software in order to find\nbugs and to prevent regressions. Writing tests, however, is not every\ndeveloper's favourite occupation, and if a lack of motivation leads to a lack\nof tests, then this may have dire consequences, such as programs with poor\nquality or even project failures. This paper introduces Gamekins, a tool that\nuses gamification to motivate developers to write more and better tests.\nGamekins is integrated into the Jenkins continuous integration platform where\ngame elements are based on commits to the source code repository: Developers\ncan earn points for completing test challenges and quests posed by Gamekins,\ncompete with other developers or developer teams on a leaderboard, and are\nrewarded for their test-related achievements.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.03943,regular,pre_llm,2022,2,"{'ai_likelihood': 3.5100513034396704e-06, 'text': 'SPIDER: Specification-based Integration Defect Revealer\n\n  Modern software design practice implies widespread use in the development of\nready-made components, usually designed as external libraries. The undoubted\nadvantages of reusing third-party code can be offset by integration errors that\nappear in the developed software. The reason for the appearance of such errors\nis mainly due to misunderstanding or incomplete understanding by the programmer\nof the details of external libraries such as an internal structure and the\nsubtleties of functioning. The documentation provided with the libraries is\noften very sparse and describes only the main intended scenarios for the\ninteraction of the program and the library. In this paper, we propose the\napproach based on the use of formal library specifications, which allows\ndetecting integration errors using static analysis methods. To do this, the\nexternal library is described using the LibSL specification language, the\nresulting description is translated into the internal data structures of the\nKEX analyzer. The execution of the incorrect scenarios of library usage, such\nas the incorrect sequence of method calls or the violation of the API function\ncontract, is marked in the program model with special built-in functions of the\nKEX analyzer. Later, when analyzing the program, KEX becomes able to detect\nintegration errors, since incorrect library usage scenarios are diagnosed as\ncalling marked functions. The proposed approach is implemented as SPIDER\n(SPecification-based Integration Defect Revealer), which is an extension of the\nKex analyzer and has proven its efficiency by detecting integration errors of\ndifferent classes on several special-made projects, as well as on several\nprojects taken from open repositories.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.05329,regular,pre_llm,2022,2,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'Spork: Structured Merge for Java with Formatting Preservation\n\n  The highly parallel workflows of modern software development have made\nmerging of source code a common activity for developers. The state of the\npractice is based on line-based merge, which is ubiquitously used with ""git\nmerge"". Line-based merge is however a generalized technique for any text that\ncannot leverage the structured nature of source code, making merge conflicts a\ncommon occurrence. As a remedy, research has proposed structured merge tools,\nwhich typically operate on abstract syntax trees instead of raw text.\nStructured merging greatly reduces the prevalence of merge conflicts but\nsuffers from important limitations, the main ones being a tendency to alter the\nformatting of the merged code and being prone to excessive running times. In\nthis paper, we present SPORK, a novel structured merge tool for JAVA. SPORK is\nunique as it preserves formatting to a significantly greater degree than\ncomparable state-of-the-art tools. SPORK is also overall faster than the state\nof the art, in particular significantly reducing worst-case running times in\npractice. We demonstrate these properties by replaying 1740 real-world file\nmerges collected from 119 open-source projects, and further demonstrate several\nkey differences between SPORK and the state of the art with in-depth case\nstudies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.08761,regular,pre_llm,2022,2,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'QuerTCI: A Tool Integrating GitHub Issue Querying with Comment\n  Classification\n\n  Empirical Software Engineering (ESE) researchers study (open-source) project\nissues and the comments and threads within to discover -- among others --\nchallenges developers face when incorporating new technologies, platforms, and\nprogramming language constructs. However, such threads accumulate, becoming\nunwieldy and hindering any insight researchers may gain. While existing\napproaches alleviate this burden by classifying issue thread comments, there is\na gap between searching popular open-source software repositories (e.g., those\non GitHub) for issues containing particular keywords and feeding the results\ninto a classification model. This paper demonstrates a research infrastructure\ntool called QuerTCI that bridges this gap by integrating the GitHub issue\ncomment search API with the classification models found in existing approaches.\nUsing queries, ESE researchers can retrieve GitHub issues containing particular\nkeywords, e.g., those related to a specific programming language construct,\nand, subsequently, classify the discussions occurring in those issues. We hope\nESE researchers can use our tool to uncover challenges related to particular\ntechnologies using specific keywords through popular open-source repositories\nmore seamlessly than previously possible. A tool demonstration video may be\nfound at: https://youtu.be/fADKSxn0QUk.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.04068,regular,pre_llm,2022,2,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Test Automation Maturity Improves Product Quality -- Quantitative Study\n  of Open Source Projects Using Continuous Integration\n\n  The popularity of continuous integration (CI) is increasing as a result of\nmarket pressure to release product features or updates frequently. The ability\nof CI to deliver quality at speed depends on reliable test automation. In this\npaper, we present an empirical study to observe the effect of test automation\nmaturity (assessed by standard best practices in the literature) on product\nquality, test automation effort, and release cycle in the CI context of open\nsource projects. We run our test automation maturity survey and got responses\nfrom 37 open source java projects. We also mined software repositories of the\nsame projects. The main results of regression analysis reveal that, higher\nlevels of test automation maturity are positively associated with higher\nproduct quality (p-value=0.000624) and shorter release cycle (p-value=0.01891);\nThere is no statistically significant evidence of increased test automation\neffort due to higher levels of test automation maturity and product quality.\nThus, we conclude that, a potential benefit of improving test automation\nmaturity (using standard best practices) is product quality improvement and\nrelease cycle acceleration in the CI context of open source projects. We\nencourage future research to extend our findings by adding more datasets with\ndifferent programming languages and CI tools, closed source projects, and\nlarge-scale industrial projects. Our recommendation to practitioners (in the\nsimilar CI context) is to utilize standard best practices to improve test\nautomation maturity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.06008,regular,pre_llm,2022,2,"{'ai_likelihood': 2.824597888522678e-05, 'text': ""Applying multi product lines to equity market software ecosystem\n\n  Context: In recent decades, many financial markets and their participants\nhave changed their working method from a completely manual and traditional one\nto an automatic one, benefiting from complex software systems. There are\ndifferent approaches to the development of such software systems. Objective: In\nthis paper, we study the application of the Multi Product Line (MPL) approach\nin the software ecosystem (SECO) of the equity market. By profiting from the\nconcepts and practices of the MPL approach, we want to design a SECO that makes\nthe real-time and automated flow of financial transaction data between market\nparticipants' software pieces possible. Method: We first provide some\nbackground information about the equity market, its participants and their\nrelations, and two primary order life-cycles in which these players cooperate.\nAfter that, we analyze the variability in each market participant's software.\nNext, we describe the employed architecture and the implementation approach.\nFinally, we discuss three scenarios by which the whole proposed SECO is tested\nand validated. Results: To implement the mentioned working method, named\nStraight-through Processing (STP), different technical and non-technical\nelements' contribution is essential. Attaining success in developing the equity\nmarket's SECO addresses the technical aspect and prepares the technical\ninfrastructure for the rest of the work. Conclusion: The successful validation\nof the equity market's SECO indicates that the adoption of the MPL approach is\na viable strategy for the development of equity market SECOs. It also suggests\nthat this approach is worthy of more attention and investment.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.1184,regular,pre_llm,2022,2,"{'ai_likelihood': 2.8477774726019966e-06, 'text': 'Scalpel: The Python Static Analysis Framework\n\n  Despite being the most popular programming language, Python has not yet\nreceived enough attention from the community. To the best of our knowledge,\nthere is no general static analysis framework proposed to facilitate the\nimplementation of dedicated Python static analyzers. To fill this gap, we\ndesign and implement such a framework (named Scalpel) and make it publicly\navailable as an open-source project. The Scalpel framework has already\nintegrated a number of fundamental static analysis functions (e.g., call graph\nconstructions, control-flow graph constructions, alias analysis, etc.) that are\nready to be reused by developers to implement client applications focusing on\nstatically resolving dedicated Python problems such as detecting bugs or fixing\nvulnerabilities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.08701,regular,pre_llm,2022,2,"{'ai_likelihood': 1.2252065870496963e-06, 'text': ""Revisiting reopened bugs in open source software systems\n\n  Reopened bugs can degrade the overall quality of a software system since they\nrequire unnecessary rework by developers. Moreover, reopened bugs also lead to\na loss of trust in the end-users regarding the quality of the software. Thus,\npredicting bugs that might be reopened could be extremely helpful for software\ndevelopers to avoid rework. Prior studies on reopened bug prediction focus only\non three open source projects (i.e., Apache, Eclipse, and OpenOffice) to\ngenerate insights. We observe that one out of the three projects (i.e., Apache)\nhas a data leak issue -- the bug status of reopened was included as training\ndata to predict reopened bugs. In addition, prior studies used an outdated\nprediction model pipeline (i.e., with old techniques for constructing a\nprediction model) to predict reopened bugs. Therefore, we revisit the reopened\nbugs study on a large scale dataset consisting of 47 projects tracked by JIRA\nusing the modern techniques such as SMOTE, permutation importance together with\n7 different machine learning models. We study the reopened bugs using a mixed\nmethods approach (i.e., both quantitative and qualitative study). We find that:\n1) After using an updated reopened bug prediction model pipeline, only 34%\nprojects give an acceptable performance with AUC >= 0.7. 2) There are four\nmajor reasons for a bug getting reopened, that is, technical (i.e.,\npatch/integration issues), documentation, human (i.e., due to incorrect bug\nassessment), and reasons not shown in the bug reports. 3) In projects with an\nacceptable AUC, 94% of the reopened bugs are due to patch issues (i.e., the\nusage of an incorrect patch) identified before bug reopening. Our study\nrevisits reopened bugs and provides new insights into developer's bug reopening\nactivities.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.13114,regular,pre_llm,2022,2,"{'ai_likelihood': 1.6225708855523005e-06, 'text': 'BeDivFuzz: Integrating Behavioral Diversity into Generator-based Fuzzing\n\n  A popular metric to evaluate the performance of fuzzers is branch coverage.\nHowever, we argue that focusing solely on covering many different branches\n(i.e., the richness) is not sufficient since the majority of the covered\nbranches may have been exercised only once, which does not inspire a high\nconfidence in the reliability of the covered code. Instead, the distribution of\nthe executed branches (i.e., the evenness) should also be considered. That is,\nbehavioral diversity is only given if the generated inputs not only trigger\nmany different branches, but also trigger them evenly often with diverse\ninputs. We introduce BeDivFuzz, a feedback-driven fuzzing technique for\ngenerator-based fuzzers. BeDivFuzz distinguishes between structure-preserving\nand structure-changing mutations in the space of syntactically valid inputs,\nand biases its mutation strategy towards validity and behavioral diversity\nbased on the received program feedback. We have evaluated BeDivFuzz on Ant,\nMaven, Rhino, Closure, Nashorn, and Tomcat. The results show that BeDivFuzz\nachieves better behavioral diversity than the state of the art, measured by\nestablished biodiversity metrics, namely the Hill numbers, from the field of\necology.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.09076,review,pre_llm,2022,2,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Improving Test Automation Maturity: a Multivocal Literature Review\n\n  Mature test automation is key for achieving software quality at speed. In\nthis paper, we present a multivocal literature review with the objective to\nsurvey and synthesize the guidelines given in the literature for improving test\nautomation maturity. We selected and reviewed 81 primary studies, consisting of\n26 academic literature and 55 grey literature sources. From primary studies, we\nextracted 26 test automation best practices (e.g., Define an effective test\nautomation strategy, Set up good test environments, Develop high-quality test\nscripts) and collected many pieces of advice (e.g., in forms of\nimplementation/improvement approaches, technical techniques, concepts,\nexperience-based heuristics) on how to conduct these best practices. We made\nmain observations: (1) There are only 6 best practices whose positive effect on\nmaturity improvement have been evaluated by academic studies using formal\nempirical methods; (2) Several technical related best practices in this MLR\nwere not presented in test maturity models; (3) Some best practices can be\nlinked to success factors and maturity impediments proposed by other scholars;\n(4) Most pieces of advice on how to conduct proposed best practices were\nidentified from experience studies and their effectiveness need to be further\nevaluated with cross-site empirical evidence using formal empirical methods;\n(5) In the literature, some advice on how to conduct certain best practices are\nconflicting, and some advice on how to conduct certain best practices still\nneed further qualitative analysis.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.09214,regular,pre_llm,2022,2,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Pinpointing Anomaly Events in Logs from Stability Testing -- N-Grams vs.\n  Deep-Learning\n\n  As stability testing execution logs can be very long, software engineers need\nhelp in locating anomalous events. We develop and evaluate two models for\nscoring individual log-events for anomalousness, namely an N-Gram model and a\nDeep Learning model with LSTM (Long short-term memory). Both are trained on\nnormal log sequences only. We evaluate the models with long log sequences of\nAndroid stability testing in our company case and with short log sequences from\nHDFS (Hadoop Distributed File System) public dataset. We evaluate next event\nprediction accuracy and computational efficiency. The LSTM model is more\naccurate in stability testing logs (0.848 vs 0.865), whereas in HDFS logs the\nN-Gram is slightly more accurate (0.904 vs 0.900). The N-Gram model has far\nsuperior computational efficiency compared to the Deep model (4 to 13 seconds\nvs 16 minutes to nearly 4 hours), making it the preferred choice for our case\ncompany. Scoring individual log events for anomalousness seems like a good aid\nfor root cause analysis of failing test cases, and our case company plans to\nadd it to its online services. Despite the recent surge in using deep learning\nin software system anomaly detection, we found limited benefits in doing so.\nHowever, future work should consider whether our finding holds with different\nLSTM-model hyper-parameters, other datasets, and with other deep-learning\napproaches that promise better accuracy and computational efficiency than LSTM\nbased models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.11397,regular,pre_llm,2022,2,"{'ai_likelihood': 6.5896246168348525e-06, 'text': 'Model-Driven Generation of Microservice Interfaces: From LEMMA Domain\n  Models to Jolie APIs\n\n  We formally define and implement a translation from domain models in the\nLEMMA modelling framework to microservice APIs in the Jolie programming\nlanguage. Our tool enables a software development process whereby microservice\narchitectures can first be designed with the leading method of Domain-Driven\nDesign, and then corresponding data types and service interfaces (APIs) in\nJolie are automatically generated. Developers can extend and use these APIs as\nguides in order to produce compliant implementations. Our tool thus contributes\nto enhancing productivity and improving the design adherence of microservices.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.05641,regular,pre_llm,2022,2,"{'ai_likelihood': 1.2185838487413195e-05, 'text': 'NALABS: Detecting Bad Smells in Natural Language Requirements and Test\n  Specifications\n\n  In large-scale embedded system development, requirement and test\nspecifications are often expressed in natural language. In the context of\ndeveloping such products, requirement review is performed in many cases\nmanually using these specifications as a basis for quality assurance.\nLow-quality specifications can have expensive consequences during the\nrequirement engineering process. Especially, if feedback loops during\nrequirement engineering are long, leading to artifacts that are not easily\nmaintainable, are hard to understand, and are inefficient to port to other\nsystem variants. We use the idea of smells to specifications expressed in\nnatural language, defining a set of specifications for bad smells. We developed\na tool called NALABS (NAtural LAnguage Bad Smells), available on\nhttps://github.com/eduardenoiu/NALABS and used for automatically checking\nspecifications. We discuss some of the decisions made for its implementation,\nand future work.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.00747,regular,pre_llm,2022,2,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Thematic Domain Analysis for Ocean Modeling\n\n  Ocean science is a discipline that employs ocean models as an essential\nresearch asset. Such scientific modeling provides mathematical abstractions of\nreal-world systems, e.g., the oceans. These models are then coded as\nimplementations of the mathematical abstractions. The developed software\nsystems are called models of the real-world system.\n  To advance the state in engineering such ocean models, we intend to better\nunderstand how ocean models are developed and maintained in ocean science. In\nthis paper, we present the results of semi-structured interviews and the\nThematic Analysis~(TA) of the interview results to analyze the domain of ocean\nmodeling. Thereby, we identified developer requirements and impediments to\nmodel development and evolution, and related themes. This analysis can help to\nunderstand where methods from software engineering should be introduced and\nwhich challenges need to be addressed.\n  We suggest that other researchers extend and repeat our TA with model\ndevelopers and research software engineers working in related domains to\nfurther advance our knowledge and skills in scientific modeling.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.04586,regular,pre_llm,2022,2,"{'ai_likelihood': 6.556510925292969e-06, 'text': ""Less is More: Supporting Developers in Vulnerability Detection during\n  Code Review\n\n  Reviewing source code from a security perspective has proven to be a\ndifficult task. Indeed, previous research has shown that developers often miss\neven popular and easy-to-detect vulnerabilities during code review. Initial\nevidence suggests that a significant cause may lie in the reviewers' mental\nattitude and common practices. In this study, we investigate whether and how\nexplicitly asking developers to focus on security during a code review affects\nthe detection of vulnerabilities. Furthermore, we evaluate the effect of\nproviding a security checklist to guide the security review. To this aim, we\nconduct an online experiment with 150 participants, of which 71% report to have\nthree or more years of professional development experience. Our results show\nthat simply asking reviewers to focus on security during the code review\nincreases eight times the probability of vulnerability detection. The presence\nof a security checklist does not significantly improve the outcome further,\neven when the checklist is tailored to the change under review and the existing\nvulnerabilities in the change. These results provide evidence supporting the\nmental attitude hypothesis and call for further work on security checklists'\neffectiveness and design. Data and materials:\nhttps://doi.org/10.5281/zenodo.6026291\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.02565,review,pre_llm,2022,2,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Using SEQUAL for Identifying Requirements to Ecore Editors\n\n  Software engineers who use Model-Driven Development may be using Ecore for\ntheir work. Ecore is traditionally edited in Eclipse IDE, but a recent\ntransition to Web tools allows for development of new Ecore editors. To\ninvestigate the needed functionality of such modeling tools, the model quality\nframework SEQUAL has been applied. The paper presents the current results of\nthis task, producing requirements for tool functionality as quality improving\nmeans for the following quality aspects: physical, empirical, syntactic,\nsemantic, pragmatic, social and deontic. The result is an extensive list of\ntool functionality that could be implemented by the Ecore editor developers.\nAlthough many requirements are identified, the framework should also help in\nmaking trade-offs in case not all requirements can be implemented. In this way\nthe paper both contribute to identifying modeling tool functionality, and to\nhave input to improve SEQUAL as a general model quality framework. Further work\nwill need to be done on the implementation of the tools for properly evaluating\nthis work.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.10985,regular,pre_llm,2022,2,"{'ai_likelihood': 4.735257890489366e-06, 'text': ""A Laboratory Experiment on Using Different Financial-Incentivization\n  Schemes in Software-Engineering Experimentation\n\n  In software-engineering research, many empirical studies are conducted with\nopen-source or industry developers. However, in contrast to other research\ncommunities like economics or psychology, only few experiments use financial\nincentives (i.e., paying money) as a strategy to motivate participants'\nbehavior and reward their performance. The most recent version of the SIGSOFT\nEmpirical Standards mentions payouts only for increasing participation in\nsurveys, but not for mimicking real-world motivations and behavior in\nexperiments. Within this article, we report a controlled experiment in which we\ntackled this gap by studying how different financial incentivization schemes\nimpact developers. For this purpose, we first conducted a survey on financial\nincentives used in the real-world, based on which we designed three\nincentivization schemes: (1) a performance-dependent scheme that employees\nprefer, (2) a scheme that is performance-independent, and (3) a scheme that\nmimics open-source development. Then, using a between-subject experimental\ndesign, we explored how these three schemes impact participants' performance.\nOur findings indicate that the different schemes can impact participants'\nperformance in software-engineering experiments. Due to the small sample sizes,\nour results are not statistically significant, but we can still observe clear\ntendencies. Our contributions help understand the impact of financial\nincentives on participants in experiments as well as real-world scenarios,\nguiding researchers in designing experiments and organizations in compensating\ndevelopers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.1383,regular,pre_llm,2022,2,"{'ai_likelihood': 1.6225708855523005e-06, 'text': ""Curb Your Self-Modifying Code\n\n  Self-modifying code has many intriguing applications in a broad range of\nfields including software security, artificial general intelligence, and\nopen-ended evolution. Having control over self-modifying code, however, is\nstill an open challenge since it is a balancing act between providing as much\nfreedom as possible so as not to limit possible solutions, while at the same\ntime imposing restriction to avoid security issues and invalid code or\nsolutions. In the present study, I provide a prototype implementation of how\none might curb self-modifying code by introducing control mechanisms for code\nmodifications within specific regions and for specific transitions between code\nand data. I show that this is possible to achieve with the so-called allagmatic\nmethod - a framework to formalise, model, implement, and interpret complex\nsystems inspired by Gilbert Simondon's philosophy of individuation and Alfred\nNorth Whitehead's philosophy of organism. Thereby, the allagmatic method serves\nas guidance for self-modification based on concepts defined in a metaphysical\nframework. I conclude that the allagmatic method seems to be a suitable\nframework for control mechanisms in self-modifying code and that there are\nintriguing analogies between the presented control mechanisms and gene\nregulation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.04026,regular,pre_llm,2022,3,"{'ai_likelihood': 2.5828679402669272e-06, 'text': ""Toward Understanding Deep Learning Framework Bugs\n\n  DL frameworks are the basis of constructing all DL programs and models, and\nthus their bugs could lead to the unexpected behaviors of any DL program or\nmodel relying on them. Such a wide effect demonstrates the necessity and\nimportance of guaranteeing DL frameworks' quality. Understanding the\ncharacteristics of DL framework bugs is a fundamental step for this quality\nassurance task, facilitating designing effective bug detection and debugging\napproaches. Hence, in this work we conduct the most large-scale study on 1,000\nbugs from four popular and diverse DL frameworks (i.e., TensorFlow, PyTorch,\nMXNet, and DL4J). By analyzing the root causes and symptoms of DL framework\nbugs associated with 5 components decomposed from DL frameworks, as well as\nmeasuring test coverage achieved by three state-of-the-art testing techniques,\nwe obtain 12 major findings for the comprehensive understanding of DL framework\nbugs and the current status of existing DL framework testing practice, and then\nprovide a series of actionable guidelines for better DL framework bug detection\nand debugging. Finally, based on the guidelines, we design and implement a\nprototype DL-framework testing tool, called TenFuzz, which is evaluated to be\neffective and finds 3 unknown bugs on the latest TensorFlow framework in a\npreliminary study, indicating the significance of our guidelines.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.12212,review,pre_llm,2022,3,"{'ai_likelihood': 1.986821492513021e-06, 'text': 'Supporting Developers in Addressing Human-centric Issues in Mobile Apps\n\n  Failure to consider the characteristics, limitations, and abilities of\ndiverse end-users during mobile apps development may lead to problems for\nend-users such as accessibility and usability issues. We refer to this class of\nproblems as human-centric issues. Despite their importance, there is a limited\nunderstanding of the types of human-centric issues that are encountered by\nend-users and taken into account by the developers of mobile apps. In this\npaper, we examine what human-centric issues end-users report through Google App\nStore reviews, which human-centric issues are a topic of discussion for\ndevelopers on GitHub, and whether end-users and developers discuss the same\nhuman-centric issues. We then investigate whether an automated tool might help\ndetect such human-centric issues and whether developers would find such a tool\nuseful. To do this, we conducted an empirical study by extracting and manually\nanalysing a random sample of 1,200 app reviews and 1,200 issue comments from 12\ndiverse projects that exist on both Google App Store and GitHub. Our analysis\nled to a taxonomy of human-centric issues that categorises human-centric issues\ninto three-high levels: App Usage, Inclusiveness, and User Reaction. We then\ndeveloped machine learning and deep learning models that are promising in\nautomatically identifying and classifying human-centric issues from app reviews\nand developer discussions. A survey of mobile app developers shows that the\nautomated detection of human-centric issues has practical applications. Guided\nby our findings, we highlight some implications and possible future work to\nfurther understand and incorporate human-centric issues in mobile apps\ndevelopment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.05622,regular,pre_llm,2022,3,"{'ai_likelihood': 2.284844716389974e-06, 'text': ""MS2M: A message-based approach for live stateful microservices migration\n\n  In the last few years, the proliferation of edge and cloud computing\ninfrastructures as well as the increasing number of mobile devices has\nfacilitated the emergence of many novel applications. However, that increase of\ncomplexities also creates novel challenges for service providers, for example,\nthe efficient management of interdependent services during runtime. One\nstrategy is to reallocate services dynamically by migrating them to suitable\nservers. However, not every microservice can be deployed as stateless\ninstances, which leads to suboptimal performance of live migration techniques.\nIn this work, we propose a novel live migration scheme focusing on stateful\nmicroservices in edge and cloud environments by utilizing the underlying\nmessaging infrastructure to reconstruct the service's state. Not only can this\napproach be applied in various microservice deployment scenarios, experimental\nevaluation results also show a reduction of 19.92% downtime compared to the\nstop-and-copy migration method.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.10382,regular,pre_llm,2022,3,"{'ai_likelihood': 1.986821492513021e-07, 'text': ""Investigating End-Users' Values in Agriculture Mobile Applications\n  Development: An Empirical Study on Bangladeshi Female Farmers\n\n  The omnipresent nature of mobile applications (apps) in all aspects of daily\nlives raises the necessity of reflecting end-users values (e.g., fairness,\nhonesty, etc.) in apps. However, there are limited considerations of end-users\nvalues in apps development. Value violations by apps have been reported in the\nmedia and are responsible for end-users dissatisfaction and negative\nsocio-economic consequences. Value violations may bring more severe and lasting\nproblems for marginalized and vulnerable end-users of apps, which have been\nexplored less (if at all) in the software engineering community. However,\nunderstanding the values of the end-users of apps is the essential first step\ntowards values-based apps development. This research aims to fill this gap by\ninvestigating the human values of Bangladeshi female farmers as a marginalized\nand vulnerable group of end-users of Bangladeshi agriculture apps. We conducted\nan empirical study that collected and analyzed data from a survey with 193\nBangladeshi female farmers to explore the underlying factor structure of the\nvalues of Bangladeshi female farmers and the significance of demographics on\ntheir values. The results identified three underlying factors of Bangladeshi\nfemale farmers. The first factor comprises of five values: benevolence,\nsecurity, conformity, universalism, and tradition. The second factor consists\nof two values: self-direction and stimulation. The third factor includes three\nvalues: power, achievement, and hedonism. We also identified strong influences\nof demographics on some of the values of Bangladeshi female farmers. For\nexample, area has significant impacts on three values: hedonism, achievement,\nand tradition. Similarly, there are also strong influences of household income\non power and security.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.07473,regular,pre_llm,2022,3,"{'ai_likelihood': 2.0795398288302953e-05, 'text': 'The Unexplored Treasure Trove of Phabricator Code Review\n\n  Phabricator is a modern code collaboration tool used by popular projects like\nFreeBSD and Mozilla. However, unlike the other well-known code review\nenvironments, such as Gerrit or GitHub, there is no readily accessible public\ncode review dataset for Phabricator. This paper describes our experience mining\ncode reviews from five different projects that use Phabricator (Blender,\nFreeBSD, KDE, LLVM, and Mozilla). We discuss the challenges associated with the\ndata retrieval process and our solutions, resulting in a dataset with details\nregarding 317,476 Phabricator code reviews. Our dataset is available in both\nJSON and MySQL database dump formats. The dataset enables analyses of the\nhistory of code reviews at a more granular level than other platforms. In\naddition, given that the projects we mined are publicly accessible via the\nConduit API, our dataset can be used as a foundation to fetch additional\ndetails and insights.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.11115,regular,pre_llm,2022,3,"{'ai_likelihood': 6.9207615322536894e-06, 'text': ""To Type or Not to Type? A Systematic Comparison of the Software Quality\n  of JavaScript and TypeScript Applications on GitHub\n\n  JavaScript (JS) is one of the most popular programming languages, and widely\nused for web apps and even backend development. Due to its dynamic nature,\nhowever, JS applications often have a reputation for poor software quality. As\na type-safe superset of JavaScript, TypeScript (TS) offers features to address\nthis. However, there is currently insufficient empirical evidence to broadly\nsupport the claim that TS apps exhibit better software quality than JS apps.\n  We therefore conducted a repository mining study based on 604 GitHub projects\n(299 for JS, 305 for TS) with over 16M LoC and collected four facets of\nsoftware quality: a) code quality (# of code smells per LoC), b) code\nunderstandability (cognitive complexity per LoC), c) bug proneness (bug fix\ncommit ratio), and d) bug resolution time (mean time a bug issue is open). For\nTS, we also collected how frequently the type-safety ignoring `any` type was\nused.\n  The analysis indicates that TS apps exhibit significantly better code quality\nand understandability than JS apps. Contrary to expectations, however, bug\nproneness and bug resolution time of our TS sample were not significantly lower\nthan for JS: mean bug fix commit ratio was more than 60% larger (0.126 vs.\n0.206), and TS projects needed on average more than an additional day to fix\nbugs (31.86 vs. 33.04 days). Furthermore, reducing the usage of the `any` type\nin TS apps was significantly correlated with all metrics except bug proneness\n(Spearman's rho between 0.17 and 0.26).\n  Our results indicate that the perceived positive influence of TypeScript for\navoiding bugs in comparison to JavaScript may be more complicated than assumed.\nWhile using TS seems to have benefits, it does not automatically lead to less\nand easier to fix bugs. However, more research is needed in this area,\nespecially concerning the potential influence of project complexity and\ndeveloper experience.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.1313,review,pre_llm,2022,3,"{'ai_likelihood': 1.457002427842882e-06, 'text': 'Agile Beyond Teams and Feedback Beyond Software in Automotive Systems\n\n  In order to increase the ability to build complex, software-intensive\nsystems, as well as to decrease time-to-market for new functionality,\nautomotive companies aim to scale agile methods beyond individual teams. This\nis challenging, given the specifics of automotive systems that are often\nsafety-critical and consist of software, hardware, and mechanical components.\nThis paper investigates the concrete reasons for scaling agility beyond teams,\nthe strategies that support such scaling, and foreseeable implications that\nsuch a drastic organizational change will entail. The investigation is based on\na qualitative case study, with data from 20 semi-structured interviews with\nmanagers and technical experts at two automotive companies. At the core of our\nfindings are observations about establishing an agile vehicle-level feedback\nloop beyond individual teams. (I) We find that automotive OEMs aim to decrease\nlead-time of development. (II) We also identify 7 strategies that aim to enable\nscaled-agile beyond teams. (III) Finally, we extract 6 foreseeable implications\nand side-effects of scaling agile beyond teams in automotive. By charting the\nlandscape of expected benefits, strategies, and implications of scaling agile\nbeyond teams in automotive, we enable further research and process\nimprovements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.13737,regular,pre_llm,2022,3,"{'ai_likelihood': 5.629327562120226e-06, 'text': ""Flexible and Optimal Dependency Management via Max-SMT\n\n  Package managers such as NPM have become essential for software development.\nThe NPM repository hosts over 2 million packages and serves over 43 billion\ndownloads every week. Unfortunately, the NPM dependency solver has several\nshortcomings. 1) NPM is greedy and often fails to install the newest versions\nof dependencies; 2) NPM's algorithm leads to duplicated dependencies and\nbloated code, which is particularly bad for web applications that need to\nminimize code size; 3) NPM's vulnerability fixing algorithm is also greedy, and\ncan even introduce new vulnerabilities; and 4) NPM's ability to duplicate\ndependencies can break stateful frameworks and requires a lot of care to\nworkaround. Although existing tools try to address these problems they are\neither brittle, rely on post hoc changes to the dependency tree, do not\nguarantee optimality, or are not composable.\n  We present PacSolve, a unifying framework and implementation for dependency\nsolving which allows for customizable constraints and optimization goals. We\nuse PacSolve to build MaxNPM, a complete, drop-in replacement for NPM, which\nempowers developers to combine multiple objectives when installing\ndependencies. We evaluate MaxNPM with a large sample of packages from the NPM\necosystem and show that it can: 1) reduce more vulnerabilities in dependencies\nthan NPM's auditing tool in 33% of cases; 2) chooses newer dependencies than\nNPM in 14% of cases; and 3) chooses fewer dependencies than NPM in 21% of\ncases. All our code and data is open and available.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.00107,review,pre_llm,2022,3,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Comments on Comments: Where Code Review and Documentation Meet\n\n  A central function of code review is to increase understanding; helping\nreviewers understand a code change aids in knowledge transfer and finding bugs.\nComments in code largely serve a similar purpose, helping future readers\nunderstand the program. It is thus natural to study what happens when these two\nforms of understanding collide. We ask: what documentation-related comments do\nreviewers make and how do they affect understanding of the contribution? We\nanalyze ca.700K review comments on 2,000 (Java and Python) GitHub projects, and\npropose several filters to identify which comments are likely to be either in\nresponse to a change in documentation and/or call for such a change. We\nidentify 65K such cases. We next develop a taxonomy of the reviewer intents\nbehind such ""comments on comments"". We find that achieving a shared\nunderstanding of the code is key: reviewer comments most often focused on\nclarification, followed by pointing out issues to fix, such as typos and\noutdated comments. Curiously, clarifying comments were frequently suggested\n(often verbatim) by the reviewer, indicating a desire to persist their\nunderstanding acquired during code review. We conclude with a discussion of\nimplications of our comments-on-comments dataset for research on improving code\nreview, including the potential benefits for automating code review.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.04472,regular,pre_llm,2022,3,"{'ai_likelihood': 2.317958407931858e-06, 'text': ""BinMLM: Binary Authorship Verification with Flow-aware Mixture-of-Shared\n  Language Model\n\n  Binary authorship analysis is a significant problem in many software\nengineering applications. In this paper, we formulate a binary authorship\nverification task to accurately reflect the real-world working process of\nsoftware forensic experts. It aims to determine whether an anonymous binary is\ndeveloped by a specific programmer with a small set of support samples, and the\nactual developer may not belong to the known candidate set but from the wild.\nWe propose an effective binary authorship verification framework, BinMLM.\nBinMLM trains the RNN language model on consecutive opcode traces extracted\nfrom the control-flow-graph (CFG) to characterize the candidate developers'\nprogramming styles. We build a mixture-of-shared architecture with multiple\nshared encoders and author-specific gate layers, which can learn the\ndevelopers' combination preferences of universal programming patterns and\nalleviate the problem of low training resources. Through an optimization\npipeline of external pre-training, joint training, and fine-tuning, our\nframework can eliminate additional noise and accurately distill developers'\nunique styles. Extensive experiments show that BinMLM achieves promising\nresults on Google Code Jam (GCJ) and Codeforces datasets with different numbers\nof programmers and supporting samples. It significantly outperforms the\nbaselines built on the state-of-the-art feature set (4.73% to 19.46%\nimprovement) and remains robust in multi-author collaboration scenarios.\nFurthermore, BinMLM can perform organization-level verification on a real-world\nAPT malware dataset, which can provide valuable auxiliary information for\nexploring the group behind the APT attack.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.05045,regular,pre_llm,2022,3,"{'ai_likelihood': 5.298190646701389e-07, 'text': 'Do Small Code Changes Merge Faster? A Multi-Language Empirical\n  Investigation\n\n  Code velocity, or the speed with which code changes are integrated into a\nproduction environment, plays a crucial role in Continuous Integration and\nContinuous Deployment. Many studies report factors influencing code velocity.\nHowever, solutions to increase code velocity are unclear. Meanwhile, the\nindustry continues to issue guidelines on ""ideal"" code change size, believing\nit increases code velocity despite lacking evidence validating the practice.\nSurprisingly, this fundamental question has not been studied to date. This\nstudy investigates the practicality of improving code velocity by optimizing\npull request size and composition (ratio of insertions, deletions, and\nmodifications). We start with a hypothesis that a moderate correlation exists\nbetween pull request size and time-to-merge. We selected 100 most popular,\nactively developed projects from 10 programming languages on GitHub. We\nanalyzed our dataset of 845,316 pull requests by size, composition, and context\nto explore its relationship to time-to-merge - a proxy to measure code\nvelocity. Our study shows that pull request size and composition do not relate\nto time-to-merge. Regardless of the contextual factors that can influence pull\nrequest size or composition (e.g., programming language), the observation\nholds. Pull request data from two other platforms: Gerrit and Phabricator\n(401,790 code reviews) confirms the lack of relationship. This negative result\nas in ""... eliminate useless hypotheses ..."" challenges a widespread belief by\nshowing that small code changes do not merge faster to increase code velocity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.01634,regular,pre_llm,2022,3,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'License Incompatibilities in Software Ecosystems\n\n  Contemporary software is characterized by reuse of components that are\ndeclared as dependencies and that are received from package\nmanagers/registries, such as, NPM, PyPI, RubyGems, Maven Central, etc. Direct\nand indirect dependency relations often form opaque dependency networks, that\nsometimes lead to conflicting software licenses within these. In this paper, we\nstudy license use and license incompatibilities between all components from\nseven package registries (Cargo, Maven, NPM, NuGet, Packagist, PyPI, RubyGems)\nwith a closer investigation of license incompatibilities caused by the GNU\nAffero General Public License (AGPL). We find that the relative amount of used\nlicenses vary between ecosystems (permissive licenses such as MIT and Apache\nare most frequent), that the number of direct license incompatibilities ranges\nfrom low 2.3% in Cargo to a large 20.8% in PyPI, that only a low amount of\ndirect license incompatibilities are caused by AGPL licenses (max. 0.04% in\nPyPI), but that a whopping 6.62% of Maven packages are violating the AGPL\nlicense of an indirect dependency. Our results suggest that it is not too\nunlikely that applications that are reusing packages from PyPI or Maven are\nconfronted with license incompatibilities that could mean that applications\nwould have to be open-sourced on distribution (PyPI) or as soon as they are\npublicly available as web-applications (Maven).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.06502,regular,pre_llm,2022,3,"{'ai_likelihood': 1.0927518208821615e-05, 'text': 'Characterizing and Understanding Software Security Vulnerabilities in\n  Machine Learning Libraries\n\n  The application of machine learning (ML) libraries has been tremendously\nincreased in many domains, including autonomous driving systems, medical, and\ncritical industries. Vulnerabilities of such libraries result in irreparable\nconsequences. However, the characteristics of software security vulnerabilities\nhave not been well studied. In this paper, to bridge this gap, we take the\nfirst step towards characterizing and understanding the security\nvulnerabilities of five well-known ML libraries, including Tensorflow, PyTorch,\nSickit-learn, Pandas, and Numpy. To do so, in total, we collected 596\nsecurity-related commits to exploring five major factors: 1) vulnerability\ntypes, 2) root causes, 3) symptoms, 4) fixing patterns, and 5) fixing efforts\nof security vulnerabilities in ML libraries. The findings of this study can\nassist developers in having a better understanding of software security\nvulnerabilities across different ML libraries and gain a better insight into\ntheir weaknesses of them. To make our finding actionable, we further developed\nDeepMut, an automated mutation testing tool, as a proof-of-concept application\nof our findings. DeepMut is designed to assess the adequacy of existing test\nsuites of ML libraries against security-aware mutation operators extracted from\nthe vulnerabilities studied in this work. We applied DeepMut on the Tensorflow\nkernel module and found more than 1k alive mutants not considered by the\nexisting test suits. The results demonstrate the usefulness of our findings.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.15369,regular,pre_llm,2022,3,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Geographic Diversity in Public Code Contributions\n\n  We conduct an exploratory, large-scale, longitudinal study of 50 years of\ncommits to publicly available version control system repositories, in order to\ncharacterize the geographic diversity of contributors to public code and its\nevolution over time. We analyze in total 2.2 billion commits collected by\nSoftware Heritage from 160 million projects and authored by 43 million authors\nduring the 1971-2021 time period. We geolocate developers to 12 world regions\nderived from the United Nation geoscheme, using as signals email top-level\ndomains, author names compared with names distributions around the world, and\nUTC offsets mined from commit metadata.We find evidence of the early dominance\nof North America in open source software, later joined by Europe. After that\nperiod, the geographic diversity in public code has been constantly increasing.\nWe also identify relevant historical shifts related to the UNIX wars, the\nincrease of coding literacy in Central and South Asia, and broader phenomena\nlike colonialism and people movement across countries (immigration/emigration).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.15577,regular,pre_llm,2022,3,"{'ai_likelihood': 3.344482845730252e-06, 'text': 'Understanding Code Snippets in Code Reviews: A Preliminary Study of the\n  OpenStack Community\n\n  Code review is a mature practice for software quality assurance in software\ndevelopment with which reviewers check the code that has been committed by\ndevelopers, and verify the quality of code. During the code review discussions,\nreviewers and developers might use code snippets to provide necessary\ninformation (e.g., suggestions or explanations). However, little is known about\nthe intentions and impacts of code snippets in code reviews. To this end, we\nconducted a preliminary study to investigate the nature of code snippets and\ntheir purposes in code reviews. We manually collected and checked 10,790 review\ncomments from the Nova and Neutron projects of the OpenStack community, and\nfinally obtained 626 review comments that contain code snippets for further\nanalysis. The results show that: (1) code snippets are not prevalently used in\ncode reviews, and most of the code snippets are provided by reviewers. (2) We\nidentified two high-level purposes of code snippets provided by reviewers\n(i.e., Suggestion and Citation) with six detailed purposes, among which,\nImproving Code Implementation is the most common purpose. (3) For the code\nsnippets in code reviews with the aim of suggestion, around 68.1% was accepted\nby developers. The results highlight promising research directions on using\ncode snippets in code reviews.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.05283,regular,pre_llm,2022,3,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Beyond the Badge: Reproducibility Engineering as a Lifetime Skill\n\n  Ascertaining reproducibility of scientific experiments is receiving increased\nattention across disciplines. We argue that the necessary skills are important\nbeyond pure scientific utility, and that they should be taught as part of\nsoftware engineering (SWE) education. They serve a dual purpose: Apart from\nacquiring the coveted badges assigned to reproducible research, reproducibility\nengineering is a lifetime skill for a professional industrial career in\ncomputer science. SWE curricula seem an ideal fit for conveying such\ncapabilities, yet they require some extensions, especially given that even at\nflagship conferences like ICSE, only slightly more than one-third of the\ntechnical papers (at the 2021 edition) receive recognition for artefact\nreusability. Knowledge and capabilities in setting up engineering environments\nthat allow for reproducing artefacts and results over decades (a standard\nrequirement in many traditional engineering disciplines), writing semi-literate\ncommit messages that document crucial steps of a decision-making process and\nthat are tightly coupled with code, or sustainably taming dynamic, quickly\nchanging software dependencies, to name a few: They all contribute to solving\nthe scientific reproducibility crisis, and enable software engineers to build\nsustainable, long-term maintainable, software-intensive, industrial systems. We\npropose to teach these skills at the undergraduate level, on par with\ntraditional SWE topics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.11343,regular,pre_llm,2022,3,"{'ai_likelihood': 5.960464477539062e-07, 'text': ""Using Evolutionary Coupling to Establish Relevance Links Between Tests\n  and Code Units. A case study on fault localization\n\n  Many software engineering techniques, such as fault localization, operate\nbased on relevance relationships between tests and code. These relationships\nare often inferred through the use of dynamic test execution information (test\nexecution traces) that approximate the link between relevant code units and\nasserted, by the tests, program behaviour. Unfortunately, in practice dynamic\ninformation is not always available due to the overheads introduced by the\ninstrumentation or the nature of the production environments. To deal with this\nissue, we propose CEMENT, a static technique that automatically infers such\ntest and code relationships given the projects' evolution. The key idea is that\ndevelopers make relevant changes on test and code units at the same period of\ntime, i.e., co-evolution of tests and code units reflects a probable link\nbetween them. We evaluate CEMENT on 15 open source projects and show that it\nindeed captures relevant links. Additionally, we perform a fault localization\ncase study where we compare CEMENT with an existing Information Retrieval-based\nFault Localization (IRFL) technique and show that it achieves comparable\nperformance. A further analysis of our results reveals a small overlap between\nthe faults successfully localized by the two approaches suggesting\ncomplementarity. In particular, out of the 39 successfully localized faults,\ntwo are common while CEMENT and IRFL localize 16 and 21. These results\ndemonstrate that test and code evolutionary coupling can effectively support\ntest and debugging activities.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.16653,review,pre_llm,2022,3,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Error Identification Strategies for Python Jupyter Notebooks\n\n  Computational notebooks -- such as Jupyter or Colab -- combine text and data\nanalysis code. They have become ubiquitous in the world of data science and\nexploratory data analysis. Since these notebooks present a different\nprogramming paradigm than conventional IDE-driven programming, it is plausible\nthat debugging in computational notebooks might also be different. More\nspecifically, since creating notebooks blends domain knowledge, statistical\nanalysis, and programming, the ways in which notebook users find and fix errors\nin these different forms might be different. In this paper, we present an\nexploratory, observational study on how Python Jupyter notebook users find and\nunderstand potential errors in notebooks. Through a conceptual replication of\nstudy design investigating the error identification strategies of R notebook\nusers, we presented users with Python Jupyter notebooks pre-populated with\ncommon notebook errors -- errors rooted in either the statistical data\nanalysis, the knowledge of domain concepts, or in the programming. We then\nanalyzed the strategies our study participants used to find these errors and\ndetermined how successful each strategy was at identifying errors. Our findings\nindicate that while the notebook programming environment is different from the\nenvironments used for traditional programming, debugging strategies remain\nquite similar. It is our hope that the insights presented in this paper will\nhelp both notebook tool designers and educators make changes to improve how\ndata scientists discover errors more easily in the notebooks they write.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.03289,regular,pre_llm,2022,3,"{'ai_likelihood': 1.321236292521159e-05, 'text': ""$\\mu$BERT: Mutation Testing using Pre-Trained Language Models\n\n  We introduce $\\mu$BERT, a mutation testing tool that uses a pre-trained\nlanguage model (CodeBERT) to generate mutants. This is done by masking a token\nfrom the expression given as input and using CodeBERT to predict it. Thus, the\nmutants are generated by replacing the masked tokens with the predicted ones.\nWe evaluate $\\mu$BERT on 40 real faults from Defects4J and show that it can\ndetect 27 out of the 40 faults, while the baseline (PiTest) detects 26 of them.\nWe also show that $\\mu$BERT can be 2 times more cost-effective than PiTest,\nwhen the same number of mutants are analysed. Additionally, we evaluate the\nimpact of $\\mu$BERT's mutants when used by program assertion inference\ntechniques, and show that they can help in producing better specifications.\nFinally, we discuss about the quality and naturalness of some interesting\nmutants produced by $\\mu$BERT during our experimental evaluation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.00771,regular,pre_llm,2022,3,"{'ai_likelihood': 5.662441253662109e-06, 'text': 'Mining Domain Models in Ethereum DApps using Code Cloning\n\n  This research study explores the use of near-miss clone detection to support\nthe characterization of domain models of smart contracts for each of the\npopular domains in which smart contracts are being rapidly adopted. In this\npaper, we leverage the code clone detection techniques to detect similarities\nin functions of the smart contracts deployed onto the Ethereum blockchain\nnetwork. We analyze the clusters of code clones and the semantics of the code\nfragments in the clusters in an attempt to categorize them and discover the\nstructural models of the patterns in code clones.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.04562,regular,pre_llm,2022,4,"{'ai_likelihood': 7.947285970052084e-07, 'text': ""What are the characteristics of highly-selected packages? A case study\n  on the npm ecosystem\n\n  With the popularity of software ecosystems, the number of open source\ncomponents (known as packages) has grown rapidly. Identifying high-quality and\nwell-maintained packages from a large pool of packages to depend on is a basic\nand important problem, as it is beneficial for various applications, such as\npackage recommendation and package search. However, no systematic and\ncomprehensive work focuses on addressing this problem except in online\ndiscussions or informal literature and interviews. To fill this gap, in this\npaper, we conducted a mixed qualitative and quantitative analysis to understand\nhow developers identify and select relevant open source packages. In\nparticular, we started by surveying 118 JavaScript developers from the npm\necosystem to qualitatively understand the factors that make a package to be\nhighly-selected within the npm ecosystem. The survey results showed that\nJavaScript developers believe that highly-selected packages are\nwell-documented, receive a high number of stars on GitHub, have a large number\nof downloads, and do not suffer from vulnerabilities. Then, we conducted an\nexperiment to quantitatively validate the developers' perception of the factors\nthat make a highly-selected package. In this analysis, we collected and mined\nhistorical data from 2,527 packages divided into highly-selected and not\nhighly-selected packages. For each package in the dataset, we collected\nquantitative data to present the factors studied in the developers' survey.\nNext, we used regression analysis to quantitatively investigate which of the\nstudied factors are the most important. Our regression analysis complements our\nsurvey results about highly-selected packages. In particular, the results\nshowed that highly-selected packages tend to be correlated by the number of\ndownloads, stars, and how large the package's readme file is.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.05209,regular,pre_llm,2022,4,"{'ai_likelihood': 8.940696716308594e-07, 'text': 'Can instability variations warn developers when open-source projects\n  boost?\n\n  Although architecture instability has been studied and measured using a\nvariety of metrics, a deeper analysis of which project parts are less stable\nand how such instability varies over time is still needed. While having more\ninformation on architecture instability is, in general, useful for any software\ndevelopment project, it is especially important in Open Source Software (OSS)\nprojects where the supervision of the development process is more difficult to\nachieve. In particular, we are interested when OSS projects grow from a small\ncontrolled environment (i.e., the cathedral phase) to a community-driven\nproject (i.e., the bazaar phase). In such a transition, the project often\nexplodes in terms of software size and number of contributing developers.\nHence, the complexity of the newly added features, and the frequency of the\ncommits and files modified may cause significant variations of the instability\nof the structure of the classes and packages. Consequently, in this registered\nreport we suggest ways to analyze the instability in OSS projects, especially\nduring that sensitive phase where they become community-driven. We intend to\nsuggest ways to predict the evolution of the instability in several OSS\nprojects. Our preliminary results show that it seems possible to provide\nmeaningful estimations that can be useful for OSS teams before a project grows\nin excess.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.04423,review,pre_llm,2022,4,"{'ai_likelihood': 7.516807980007596e-06, 'text': 'Revisiting the Effect of Branch Handling Strategies on Change\n  Recommendation\n\n  Although literature has noted the effects of branch handling strategies on\nchange recommendation based on evolutionary coupling, they have been tested in\na limited experimental setting. Additionally, the branches characteristics that\nlead to these effects have not been investigated. In this study, we revisited\nthe investigation conducted by Kovalenko et al. on the effect to change\nrecommendation using two different branch handling strategies: including\nchangesets from commits on a branch and excluding them. In addition to the\nsetting by Kovalenko et al., we introduced another setting to compare:\nextracting a changeset for a branch from a merge commit at once. We compared\nthe change recommendation results and the similarity of the extracted\nco-changes to those in the future obtained using two strategies through 30\nopen-source software systems. The results show that handling commits on a\nbranch separately is often more appropriate in change recommendation, although\nthe comparison in an additional setting resulted in a balanced performance\namong the branch handling strategies. Additionally, we found that the merge\ncommit size and the branch length positively influence the change\nrecommendation results.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.00197,regular,pre_llm,2022,4,"{'ai_likelihood': 4.437234666612413e-06, 'text': 'Measuring Cognitive Load of Software Developers Based on Nasal Skin\n  Temperature\n\n  It has recently become increasingly important to measure the cognitive load\nof developers. This is because continuing with a development under a high\ncognitive load may cause human errors. Therefore, to automatically measure the\ncognitive load, existing studies have used biometric measures such as brain\nwaves and the heart rate. However, developers are often required to equip\ncertain devices when measuring them, and can therefore be physically burdened.\nIn this study, we evaluated the feasibility of non-invasive biometric measures\nbased on the nasal skin temperature. The nasal skin temperature has been widely\nused in other fields to measure mental status. In the present experiment, the\nsubjects created small Java programs, and we estimated their cognitive load\nusing the proposed biometric measures based on the nasal skin temperature. As a\nresult, the proposed biometric measures were shown to be more effective than\nnon-biometric measures. Hence, biometric measures based on nasal skin\ntemperature are promising for estimating the cognitive load of developers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.10107,regular,pre_llm,2022,4,"{'ai_likelihood': 1.6225708855523005e-06, 'text': ""An Industrial Experience Report on Retro-inspection\n\n  To reinforce the quality of code delivery, especially to improve future\ncoding quality, one global Information and Communication Technology (ICT)\nenterprise has institutionalized a retrospective style inspection (namely\nretro-inspection), which is similar to Fagan inspection but differs in terms of\nstage, participants, etc. This paper reports an industrial case study that aims\nto investigate the experiences and lessons from this software practice. To this\nend, we collected and analyzed various empirical evidence for data\ntriangulation. The results reflect that retro-inspection distinguishes itself\nfrom peer code review by identifying more complicated and underlying defects,\nproviding more indicative and suggestive comments. Many experienced inspectors\nindicate defects together with their rationale behind and offer suggestions for\ncorrection and prevention. As a result, retro-inspection can benefit not only\nquality assurance (like Fagan inspection), but also internal audit,\ninter-division communication, and competence promotion. On the other side, we\nidentify several lessons of retro-inspection at this stage, e.g., developers'\nacceptance and organizers' predicament, for next-step improvement of this\npractice. To be specific, some recommendations are discussed for\nretro-inspection, e.g., more adequate preparation and more careful publicity.\nThis study concludes that most of the expected benefits of retro-inspection can\nbe empirically confirmed in this enterprise and its value on the progress to\ncontinuous maturity can be recognized organization-wide. The experiences on\nexecuting this altered practice in a large enterprise provide reference value\non code quality assurance to other software organizations.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.06816,review,pre_llm,2022,4,"{'ai_likelihood': 1.7219119601779514e-06, 'text': 'Preliminary Results of a Survey on the Use of Self-Adaptation in\n  Industry\n\n  Self-adaptation equips a software system with a feedback loop that automates\ntasks that otherwise need to be performed by operators. Such feedback loops\nhave found their way to a variety of practical applications, one typical\nexample is an elastic cloud. Yet, the state of the practice in self-adaptation\nis currently not clear. To get insights into the use of self-adaptation in\npractice, we are running a large-scale survey with industry. This paper reports\npreliminary results based on survey data that we obtained from 113\npractitioners spread over 16 countries, 62 of them work with concrete\nself-adaptive systems. We highlight the main insights obtained so far:\nmotivations for self-adaptation, concrete use cases, and difficulties\nencountered when applying self-adaptation in practice. We conclude the paper\nwith outlining our plans for the remainder of the study.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.08743,regular,pre_llm,2022,4,"{'ai_likelihood': 1.357661353217231e-06, 'text': 'On the Use of Causal Graphical Models for Designing Experiments in the\n  Automotive Domain\n\n  Randomized field experiments are the gold standard for evaluating the impact\nof software changes on customers. In the online domain, randomization has been\nthe main tool to ensure exchangeability. However, due to the different\ndeployment conditions and the high dependence on the surrounding environment,\ndesigning experiments for automotive software needs to consider a higher number\nof restricted variables to ensure conditional exchangeability. In this paper,\nwe show how at Volvo Cars we utilize causal graphical models to design\nexperiments and explicitly communicate the assumptions of experiments. These\ngraphical models are used to further assess the experiment validity, compute\ndirect and indirect causal effects, and reason on the transportability of the\ncausal conclusions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.06325,regular,pre_llm,2022,4,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Lessons learned from replicating a study on information-retrieval based\n  test case prioritization\n\n  Objective: In this study, we aim to replicate an artefact-based study on\nsoftware testing to address the gap. We focus on (a) providing a step by step\nguide of the replication, reflecting on challenges when replicating\nartefact-based testing research, (b) Evaluating the replicated study concerning\nits validity and robustness of the findings.\n  Method: We replicate a test case prioritization technique by Kwon et al. We\nreplicated the original study using four programs, two from the original study\nand two new programs. The replication study was implemented using Python to\nsupport future replications. Results: Various general factors facilitating\nreplications are identified, such as: (1) the importance of documentation; (2)\nthe need of assistance from the original authors; (3) issues in the maintenance\nof open source repositories (e.g., concerning needed software dependencies);\n(4) availability of scripts. We also raised several observations specific to\nthe study and its context, such as insights from using different mutation tools\nand strategies for mutant generation. Conclusion: We conclude that the study by\nKwon et al. is replicable for small and medium programs and could be automated\nto facilitate software practitioners, given the availability of required\ninformation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.09533,review,pre_llm,2022,4,"{'ai_likelihood': 2.185503641764323e-06, 'text': 'Evaluating Commit Message Generation: To BLEU Or Not To BLEU?\n\n  Commit messages play an important role in several software engineering tasks\nsuch as program comprehension and understanding program evolution. However,\nprogrammers neglect to write good commit messages. Hence, several Commit\nMessage Generation (CMG) tools have been proposed. We observe that the recent\nstate of the art CMG tools use simple and easy to compute automated evaluation\nmetrics such as BLEU4 or its variants. The advances in the field of Machine\nTranslation (MT) indicate several weaknesses of BLEU4 and its variants. They\nalso propose several other metrics for evaluating Natural Language Generation\n(NLG) tools. In this work, we discuss the suitability of various MT metrics for\nthe CMG task. Based on the insights from our experiments, we propose a new\nvariant specifically for evaluating the CMG task. We re-evaluate the state of\nthe art CMG tools on our new metric. We believe that our work fixes an\nimportant gap that exists in the understanding of evaluation metrics for CMG\nresearch.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.08108,review,pre_llm,2022,4,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'How are Software Repositories Mined? A Systematic Literature Review of\n  Workflows, Methodologies, Reproducibility, and Tools\n\n  With the advent of open source software, a veritable treasure trove of\npreviously proprietary software development data was made available. This\nopened the field of empirical software engineering research to anyone in\nacademia. Data that is mined from software projects, however, requires\nextensive processing and needs to be handled with utmost care to ensure valid\nconclusions. Since the software development practices and tools have changed\nover two decades, we aim to understand the state-of-the-art research workflows\nand to highlight potential challenges. We employ a systematic literature review\nby sampling over one thousand papers from leading conferences and by analyzing\nthe 286 most relevant papers from the perspective of data workflows,\nmethodologies, reproducibility, and tools. We found that an important part of\nthe research workflow involving dataset selection was particularly problematic,\nwhich raises questions about the generality of the results in existing\nliterature. Furthermore, we found a considerable number of papers provide\nlittle or no reproducibility instructions -- a substantial deficiency for a\ndata-intensive field. In fact, 33% of papers provide no information on how\ntheir data was retrieved. Based on these findings, we propose ways to address\nthese shortcomings via existing tools and also provide recommendations to\nimprove research workflows and the reproducibility of research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.03401,regular,pre_llm,2022,4,"{'ai_likelihood': 2.185503641764323e-06, 'text': 'Energy Consumption and Performance of Heapsort in Hardware and Software\n\n  In this poster abstract we will report on a case study on implementing the\nHeapsort algorithm in hardware and software and comparing their time and energy\nconsumption. Our experiment shows that the Hardware implementation is more\nenergy efficient, but slower than the Software implementation due to a low\nclock frequency. It also indicate that the optimal degree of parallelization\ndiffers when optimizing for time compared to optimizing for time.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.02787,regular,pre_llm,2022,4,"{'ai_likelihood': 2.6490953233506944e-06, 'text': 'DiffSearch: A Scalable and Precise Search Engine for Code Changes\n\n  The source code of successful projects is evolving all the time, resulting in\nhundreds of thousands of code changes stored in source code repositories. This\nwealth of data can be useful, e.g., to find changes similar to a planned code\nchange or examples of recurring code improvements. This paper presents\nDiffSearch, a search engine that, given a query that describes a code change,\nreturns a set of changes that match the query. The approach is enabled by three\nkey contributions. First, we present a query language that extends the\nunderlying programming language with wildcards and placeholders, providing an\nintuitive way of formulating queries that is easy to adapt to different\nprogramming languages. Second, to ensure scalability, the approach indexes code\nchanges in a one-time preprocessing step, mapping them into a feature space,\nand then performs an efficient search in the feature space for each query.\nThird, to guarantee precision, i.e., that any returned code change indeed\nmatches the given query, we present a tree-based matching algorithm that checks\nwhether a query can be expanded to a concrete code change. We present\nimplementations for Java, JavaScript, and Python, and show that the approach\nresponds within seconds to queries across one million code changes, has a\nrecall of 80.7% for Java, 89.6% for Python, and 90.4% for JavaScript, enables\nusers to find relevant code changes more effectively than a regular\nexpression-based search, and is helpful for gathering a large-scale dataset of\nreal-world bug fixes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.01438,review,pre_llm,2022,4,"{'ai_likelihood': 4.3047799004448785e-07, 'text': ""How Can We Develop Explainable Systems? Insights from a Literature\n  Review and an Interview Study\n\n  Quality aspects such as ethics, fairness, and transparency have been proven\nto be essential for trustworthy software systems. Explainability has been\nidentified not only as a means to achieve all these three aspects in systems,\nbut also as a way to foster users' sentiments of trust. Despite this, research\nhas only marginally focused on the activities and practices to develop\nexplainable systems. To close this gap, we recommend six core activities and\nassociated practices for the development of explainable systems based on the\nresults of a literature review and an interview study. First, we identified and\nsummarized activities and corresponding practices in the literature. To\ncomplement these findings, we conducted interviews with 19 industry\nprofessionals who provided recommendations for the development process of\nexplainable systems and reviewed the activities and practices based on their\nexpertise and knowledge. We compared and combined the findings of the\ninterviews and the literature review to recommend the activities and assess\ntheir applicability in industry. Our findings demonstrate that the activities\nand practices are not only feasible, but can also be integrated in different\ndevelopment processes.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.12148,regular,pre_llm,2022,4,"{'ai_likelihood': 2.2517310248480903e-06, 'text': 'Morest: Model-based RESTful API Testing with Execution Feedback\n\n  RESTful APIs are arguably the most popular endpoints for accessing Web\nservices. Blackbox testing is one of the emerging techniques for ensuring the\nreliability of RESTful APIs. The major challenge in testing RESTful APIs is the\nneed for correct sequences of API operation calls for in-depth testing. To\nbuild meaningful operation call sequences, researchers have proposed techniques\nto learn and utilize the API dependencies based on OpenAPI specifications.\nHowever, these techniques either lack the overall awareness of how all the APIs\nare connected or the flexibility of adaptively fixing the learned knowledge. In\nthis paper, we propose Morest, a model-based RESTful API testing technique that\nbuilds and maintains a dynamically updating RESTful-service Property Graph\n(RPG) to model the behaviors of RESTful-services and guide the call sequence\ngeneration. We empirically evaluated Morest and the results demonstrate that\nMorest can successfully request an average of 152.66%-232.45% more API\noperations, cover 26.16%-103.24% more lines of code, and detect 40.64%-215.94%\nmore bugs than state-of-the-art techniques. In total, we applied Morest to 6\nreal-world projects and found 44 bugs (13 of them cannot be detected by\nexisting approaches). Specifically, 2 of the confirmed bugs are from Bitbucket,\na famous code management service with more than 6 million users.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.00256,regular,pre_llm,2022,4,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'A Large-scale Dataset of (Open Source) License Text Variants\n\n  We introduce a large-scale dataset of the complete texts of free/open source\nsoftware (FOSS) license variants. To assemble it we have collected from the\nSoftware Heritage archive-the largest publicly available archive of FOSS source\ncode with accompanying development history-all versions of files whose names\nare commonly used to convey licensing terms to software users and\ndevelopers.The dataset consists of 6.5 million unique license files that can be\nused to conduct empirical studies on open source licensing, training of\nautomated license classifiers, natural language processing (NLP) analyses of\nlegal texts, as well as historical and phylogenetic studies on FOSS licensing.\nAdditional metadata about shipped license files are also provided, making the\ndataset ready to use in various contexts; they include: file length measures,\ndetected MIME type, detected SPDX license (using ScanCode), example origin\n(e.g., GitHub repository), oldest public commit in which the license\nappeared.The dataset is released as open data as an archive file containing all\ndeduplicated license files, plus several portable CSV files for metadata,\nreferencing files via cryptographic checksums.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.03366,review,pre_llm,2022,4,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'Impact of Software Engineering Research in Practice: A Patent and Author\n  Survey Analysis\n\n  Existing work on the practical impact of software engineering (SE) research\nexamines industrial relevance rather than adoption of study results, hence the\nquestion of how results have been practically applied remains open. To answer\nthis and investigate the outcomes of impactful research, we performed a\nquantitative and qualitative analysis of 4,354 SE patents citing 1,690 SE\npapers published in four leading SE venues between 1975-2017. Moreover, we\nconducted a survey on 475 authors of 593 top-cited and awarded publications,\nachieving 26% response rate. Overall, researchers have equipped practitioners\nwith various tools, processes, and methods, and improved many existing\nproducts. SE practice values knowledge-seeking research and is impacted by\ndiverse cross-disciplinary SE areas. Practitioner-oriented publication venues\nappear more impactful than researcher-oriented ones, while industry-related\ntracks in conferences could enhance their impact. Some research works did not\nreach a wide footprint due to limited funding resources or unfavorable\ncost-benefit trade-off of the proposed solutions. The need for higher SE\nresearch funding could be corroborated through a dedicated empirical study. In\ngeneral, the assessment of impact is subject to its definition. Therefore,\nacademia and industry could jointly agree on a formal description to set a\ncommon ground for subsequent research on the topic.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.10911,regular,pre_llm,2022,4,"{'ai_likelihood': 6.02669186062283e-06, 'text': 'Die Einfl\\""usse von Arbeitsbelastung auf die Arbeitsqualit\\""at agiler\n  Software-Entwicklungsteams\n\n  Due to the Covid 19 pandemic and the associated effects on the world of work,\nthe burden on employees has been brought into focus. This fact also applies to\nagile software development teams in many companies due to the extensive switch\nto remote work. Too high a workload can lead to various negative effects, such\nas increased sick leave, the well-being of employees, or reduced productivity.\nIt is also known that the workload in knowledge work impacts the quality of the\nwork results. This research article identifies potential factors of the\nworkload of the agile software development team members at Otto GmbH & Co KG.\nBased on the factors, we present measures to reduce workload and explain our\nfindings, which we have validated in an experiment. Our results show that even\nsmall-scale actions, such as the introduction of rest work phases during the\nworking day, lead to positive effects, for example, increased ability to\nconcentrate and how these affect the quality of the work results.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.01343,regular,pre_llm,2022,4,"{'ai_likelihood': 9.602970547146267e-07, 'text': 'SEAByTE: A Self-adaptive Micro-service System Artifact for Automating\n  A/B Testing\n\n  Micro-services are a common architectural approach to software development\ntoday. An indispensable tool for evolving micro-service systems is A/B testing.\nIn A/B testing, two variants, A and B, are applied in an experimental setting.\nBy measuring the outcome of an evaluation criterion, developers can make\nevidence-based decisions to guide the evolution of their software. Recent\nstudies highlight the need for enhancing the automation when such experiments\nare conducted in iterations. To that end, we contribute a novel artifact that\naims at enhancing the automation of an experimentation pipeline of a\nmicro-service system relying on the principles of self-adaptation. Concretely,\nwe propose SEAByTE, an experimental framework for testing novel self-adaptation\nsolutions to enhance the automation of continuous A/B testing of a\nmicro-service based system. We illustrate the use of the SEAByTE artifact with\na concrete example.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.08348,regular,pre_llm,2022,4,"{'ai_likelihood': 4.006756676567926e-06, 'text': 'Automated Test Generation for REST APIs: No Time to Rest Yet\n\n  Modern web services routinely provide REST APIs for clients to access their\nfunctionality. These APIs present unique challenges and opportunities for\nautomated testing, driving the recent development of many techniques and tools\nthat generate test cases for API endpoints using various strategies.\nUnderstanding how these techniques compare to one another is difficult, as they\nhave been evaluated on different benchmarks and using different metrics. To\nfill this gap, we performed an empirical study aimed to understand the\nlandscape in automated testing of REST APIs and guide future research in this\narea. We first identified, through a systematic selection process, a set of 10\nstate-of-the-art REST API testing tools that included tools developed by both\nresearchers and practitioners. We then applied these tools to a benchmark of 20\nreal-world open-source RESTful services and analyzed their performance in terms\nof code coverage achieved and unique failures triggered. This analysis allowed\nus to identify strengths, weaknesses, and limitations of the tools considered\nand of their underlying strategies, as well as implications of our findings for\nfuture research in this area.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.02342,regular,pre_llm,2022,4,"{'ai_likelihood': 5.629327562120226e-07, 'text': ""From Monolith to Microservices: Software Architecture for Autonomous UAV\n  Infrastructure Inspection\n\n  Linear-infrastructure Mission Control (LiMiC) is an application for\nautonomous Unmanned Aerial Vehicle (UAV) infrastructure inspection mission\nplanning developed in monolithic software architecture. The application\ncalculates routes along the infrastructure based on the users' inputs, the\nnumber of UAVs participating in the mission, and UAVs' locations. LiMiC1.0 is\nthe latest application version migrated from monolith to microservices,\ncontinuously integrated, and deployed using DevOps tools to facilitate future\nfeatures development, enable better traffic management, and improve the route\ncalculation processing time. Processing time was improved by refactoring the\nroute calculation algorithm into services, scaling them in the Kubernetes\ncluster, and enabling asynchronous communication in between. In this paper, we\ndiscuss the differences between the monolith and microservice architecture to\njustify our decision for migration. We describe the methodology for the\napplication's migration and implementation processes, technologies we use for\ncontinuous integration and deployment, and we present microservices improved\nperformance results compared with the monolithic application.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.09591,regular,pre_llm,2022,5,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'Breathing Life into Models: The Next Generation of Enterprise Modeling\n\n  Edsger W. Dijkstra has frequently suggested building a ""firewall"" between the\ntechnology- and application-side of computer science. His justification: The\nmethods to attack the computer scientists\' formal, mathematical ""correctness\nproblem"" differ fundamentally from the methods to attack the applicants\'\ninformal ""pleasantness problem"". In this setting, a model is always confined to\none side or the other of this wall. This keynote shows that a seamless\ntransition between both sides can be achieved by a framework with architecture,\nstatics, and dynamics as the three pillars of modeling computer-integrated\nsystems. Selected examples justify this framework. It allows to ""breath life""\ninto (static) models, and it implies a new understanding of the ""pleasantness""\nof computer-integrated systems, which is well-needed in the age of ""digital\nfirst"".\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.15767,regular,pre_llm,2022,5,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Quality Characteristics of a Software Platform for Human-AI Teaming in\n  Smart Manufacturing\n\n  As AI-enabled software systems become more prevalent in smart manufacturing,\ntheir role shifts from a reactive to a proactive one that provides\ncontext-specific support to machine operators. In the context of an\ninternational research project, we develop an AI-based software platform that\nshall facilitate the collaboration between human operators and manufacturing\nmachines. We conducted 14 structured interviews with stakeholders of the\nprospective software platform in order to determine the individual relevance of\nselected quality characteristics for human-AI teaming in smart manufacturing.\nThese characteristics include the ISO 25010:2011 standard for software quality\nand AI-specific quality characteristics such as trustworthiness, explicability,\nand auditability. The interviewees rated trustworthiness, functional\nsuitability, reliability, and security as the most important quality\ncharacteristics for this context, and portability, compatibility, and\nmaintainability as the least important. Also, we observed agreement regarding\nthe relevance of the quality characteristics among interviewees having the same\nrole. On the other hand, the relevance of each quality characteristics varied\ndepending on the concrete use case of the prospective software platform. The\ninterviewees also were asked about the key success factors related to human-AI\nteaming in smart manufacturing. They identified improving the production cycle,\nincreasing operator efficiency, reducing scrap, and reducing ergonomic risks as\nkey success criteria. In this paper, we also discuss metrics for measuring the\nfulfillment of these quality characteristics, which we intend to operationalize\nand monitor during operation of the prospective software platform.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.13375,regular,pre_llm,2022,5,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Embedded System Evolution in IoT System Development Based on MAPE-K Loop\n  Mechanism\n\n  Embedded systems including IoT devices are designed for specialized\nfunctions; thus, changes in functions are not considered following their\nrelease. For this reason, changing functions to satisfy the requirements of IoT\nsystems is difficult. In this study, we focus on updating existing embedded\nsystems without modifying them. We investigate the design of new functions and\ntheir implementation with limited resources. This paper describes an evolution\nmechanism for updating the functionalities of existing embedded systems. The\nevolution mechanism uses a control unit that is deployed outside the embedded\nsystem. To guide the steady implementation of the evolution mechanism, we\ndefine an evolution process that effectively uses the state machine diagram at\nthe design time and runtime to update the embedded systems. The programming\nframework implemented in this study supports the evolution process. We evaluate\nthe evolution mechanism based on the results from two experiments. The first\nexperiment involved applying the evolution mechanism to a cleaning robot, this\ndemonstrated that the evolution mechanism systematically enables the injection\nof new functions into an embedded system in the real world. The second\nexperiment, on the probabilistic model checking, demonstrated that the\nmechanism provides almost the same performance as the ordinary embedded system\nwith an improved robustness.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.10133,review,pre_llm,2022,5,"{'ai_likelihood': 8.940696716308594e-06, 'text': 'Survey on Tools and Techniques Detecting Microservice API Patterns\n\n  It is well recognized that design patterns improve system development and\nmaintenance in many aspects. While we commonly recognize these patterns in\nmonolithic systems, many patterns emerged for cloud computing, specifically\nmicroservices. Unfortunately, while various patterns have been proposed,\navailable quality assessment tools often do not recognize many. This article\nperforms a grey literature review to find and catalog available tools to detect\nmicroservice API patterns (MAP). It reasons about mechanisms that can be used\nto detect these patterns. Furthermore, the results indicate gaps and\nopportunities for improvements for quality assessment tools. Finally, the\nreader is provided with a route map to detection techniques that can be used to\nmine MAPs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.08809,review,pre_llm,2022,5,"{'ai_likelihood': 2.2186173333062066e-06, 'text': 'Software Fairness: An Analysis and Survey\n\n  In the last decade, researchers have studied fairness as a software property.\nIn particular, how to engineer fair software systems? This includes specifying,\ndesigning, and validating fairness properties. However, the landscape of works\naddressing bias as a software engineering concern is unclear, i.e., techniques\nand studies that analyze the fairness properties of learning-based software. In\nthis work, we provide a clear view of the state-of-the-art in software fairness\nanalysis. To this end, we collect, categorize and conduct an in-depth analysis\nof 164 publications investigating the fairness of learning-based software\nsystems. Specifically, we study the evaluated fairness measure, the studied\ntasks, the type of fairness analysis, the main idea of the proposed approaches,\nand the access level (e.g., black, white, or grey box). Our findings include\nthe following: (1) Fairness concerns (such as fairness specification and\nrequirements engineering) are under-studied; (2) Fairness measures such as\nconditional, sequential, and intersectional fairness are under-explored; (3)\nUnstructured datasets (e.g., audio, image, and text) are barely studied for\nfairness analysis; and (4) Software fairness analysis techniques hardly employ\nwhite-box, in-processing machine learning (ML) analysis methods. In summary, we\nobserved several open challenges including the need to study\nintersectional/sequential bias, policy-based bias handling, and\nhuman-in-the-loop, socio-technical bias mitigation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.04986,regular,pre_llm,2022,5,"{'ai_likelihood': 0.0, 'text': 'On the Value of Project Productivity for Early Effort Estimation\n\n  In general, estimating software effort using a Use Case Point (UCP) size\nrequires the use of productivity as a second prediction factor. However, there\nare three drawbacks to this approach: (1) there is no clear procedure for\npredicting productivity in the early stages, (2) the use of fixed or limited\nproductivity ratios does not allow research to reflect the realities of the\nsoftware industry, and (3) productivity from historical data is often\nchallenging. The new UCP datasets now available allow us to perform further\nempirical investigations of the productivity variable in order to estimate the\nUCP effort. Accordingly, four different prediction models based on productivity\nwere used. The results showed that learning productivity from historical data\nis more efficient than using classical approaches that rely on default or\nlimited productivity values. In addition, predicting productivity from\nhistorical environmental factors is not often accurate. From here we conclude\nthat productivity is an effective factor for estimating the software effort\nbased on the UCP in the presence and absence of previous historical data.\nMoreover, productivity measurement should be flexible and adjustable when\nhistorical data is available\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.15535,regular,pre_llm,2022,5,"{'ai_likelihood': 1.457002427842882e-06, 'text': 'Do Customized Android Frameworks Keep Pace with Android?\n\n  To satisfy varying customer needs, device vendors and OS providers often rely\non the open-source nature of the Android OS and offer customized versions of\nthe Android OS. When a new version of the Android OS is released, device\nvendors and OS providers need to merge the changes from the Android OS into\ntheir customizations to account for its bug fixes, security patches, and new\nfeatures. Because developers of customized OSs might have made changes to code\nlocations that were also modified by the developers of the Android OS, the\nmerge task can be characterized by conflicts, which can be time-consuming and\nerror-prone to resolve.\n  To provide more insight into this critical aspect of the Android ecosystem,\nwe present an empirical study that investigates how eight open-source\ncustomizations of the Android OS merge the changes from the Android OS into\ntheir projects. The study analyzes how often the developers from the customized\nOSs merge changes from the Android OS, how often the developers experience\ntextual merge conflicts, and the characteristics of these conflicts.\nFurthermore, to analyze the effect of the conflicts, the study also analyzes\nhow the conflicts can affect a randomly selected sample of 1,000 apps. After\nanalyzing 1,148 merge operations, we identified that developers perform these\noperations for 9.7\\% of the released versions of the Android OS, developers\nwill encounter at least one conflict in 41.3\\% of the merge operations, 58.1\\%\nof the conflicts required developers to change the customized OSs, and 64.4\\%\nof the apps considered use at least one method affected by a conflict. In\naddition to detailing our results, the paper also discusses the implications of\nour findings and provides insights for researchers and practitioners working\nwith Android and its customizations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.1208,regular,pre_llm,2022,5,"{'ai_likelihood': 5.695554945203993e-06, 'text': 'Application of Orthogonal Defect Classification for Software Reliability\n  Analysis\n\n  The modernization of existing and new nuclear power plants with digital\ninstrumentation and control systems (DI&C) is a recent and highly trending\ntopic. However, there lacks strong consensus on best-estimate reliability\nmethodologies by both the United States (U.S.) Nuclear Regulatory Commission\n(NRC) and the industry. In this work, we develop an approach called\nOrthogonal-defect Classification for Assessing Software Reliability (ORCAS) to\nquantify probabilities of various software failure modes in a DI&C system. The\nmethod utilizes accepted industry methodologies for quality assurance that are\nverified by experimental evidence. In essence, the approach combines a semantic\nfailure classification model with a reliability growth model to predict the\nprobability of failure modes of a software system. A case study was conducted\non a representative I&C platform (ChibiOS) running a smart sensor acquisition\nsoftware developed by Virginia Commonwealth University (VCU). The testing and\nevidence collection guidance in ORCAS was applied, and defects were uncovered\nin the software. Qualitative evidence, such as modified condition decision\ncoverage, was used to gauge the completeness and trustworthiness of the\nassessment while quantitative evidence was used to determine the software\nfailure probabilities. The reliability of the software was then estimated and\ncompared to existing operational data of the sensor device. It is demonstrated\nthat by using ORCAS, a semantic reasoning framework can be developed to justify\nif the software is reliable (or unreliable) while still leveraging the strength\nof the existing methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.11399,regular,pre_llm,2022,5,"{'ai_likelihood': 3.3775965372721357e-06, 'text': 'Energy Efficiency of Web Browsers in the Android Ecosystem\n\n  This paper presents an empirical study regarding the energy consumption of\nthe most used web browsers on the Android ecosystem. In order to properly\ncompare the web browsers in terms of energy consumption, we defined a set of\ntypical usage scenarios to be replicated in the different browsers, executed in\nthe same testing environment and conditions. The results of our study show that\nthere are significant differences in terms of energy consumption among the\nconsidered browsers. Furthermore, we conclude that some browsers are energy\nefficient in several user actions, but energy greedy in other ones, allowing us\nto conclude that no browser is universally more efficient for all usage\nscenarios.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.13231,regular,pre_llm,2022,5,"{'ai_likelihood': 5.265076955159506e-06, 'text': ""Giving Back: Contributions Congruent to Library Dependency Changes in a\n  Software Ecosystem\n\n  Popular adoption of third-party libraries for contemporary software\ndevelopment has led to the creation of large inter-dependency networks, where\nsustainability issues of a single library can have widespread network effects.\nMaintainers of these libraries are often overworked, relying on the\ncontributions of volunteers to sustain these libraries. In this work, we\nmeasure contributions that are aligned with dependency changes, to understand\nwhere they come from (i.e., non-maintainer, client maintainer, library\nmaintainer, and library and client maintainer), analyze whether they contribute\nto library dormancy (i.e., a lack of activity), and investigate the\nsimilarities between these contributions and developers' typical contributions.\nHence, we leverage socio-technical techniques to measure the\ndependency-contribution congruence (DC congruence), i.e., the degree to which\ncontributions align with dependencies. We conduct a large-scale empirical study\nto measure the DC congruence for the NPM ecosystem using 1.7 million issues,\n970 thousand pull requests (PR), and over 5.3 million commits belonging to\n107,242 NPM packages. At the ecosystem level, we pinpoint in time peaks of\ncongruence with dependency changes (i.e., 16% DC congruence score).\nSurprisingly, these contributions came from the ecosystem itself (i.e.,\nnon-maintainers of either client and library). At the project level, we find\nthat DC congruence shares a statistically significant relationship with the\nlikelihood of a package becoming dormant. Finally, by comparing source code of\ncontributions, we find that congruent contributions are statistically different\nto typical contributions. Our work has implications to encourage and sustain\ncontributions, especially to support library maintainers that require\ndependency changes.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.11303,regular,pre_llm,2022,5,"{'ai_likelihood': 4.1855706108940976e-05, 'text': 'Real-time Collaborative Multi-Level Modeling by Conflict-Free Replicated\n  Data Types\n\n  The need for real-time collaborative solutions in model-driven engineering\nhas been increasing over the past years. Conflict-free replicated data types\n(CRDT) provide scalable and robust replication mechanisms that align well with\nthe requirements of real-time collaborative environments. In this paper, we\npropose a real-time collaborative multi-level modeling framework to support\nadvanced modeling scenarios, built on a collection of custom CRDT, specifically\ntailored for the needs of modeling environments. We demonstrate the benefits of\nthe framework through an illustrative modeling case and compare it with other\nstate-of-the-art modeling frameworks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.09239,regular,pre_llm,2022,5,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Readle: A Formal Framework for Designing AI-based Edge Systems\n\n  With the wide spread use of AI-driven systems in the edge (a.k.a edge\nintelligence systems), such as autonomous driving vehicles, wearable biotech\ndevices, intelligent manufacturing, etc., such systems are becoming very\ncritical for our day-to-day lives. A challenge in designing edge intelligence\nsystems is that we have to deal with a large number of constraints in two\ndesign spaces that form the basis of such systems: the edge design space and\nthe deep learning design space. Thus in this work, a new systematic,\nextendable, manual approach, READLE, is proposed for creating representations\nof specifications in edge intelligent systems, capturing constraints in the\nedge system design space (e.g. timing constraints and other performance\nconstraints) and constraints in the deep learning space (e.g. model training\nduration, required level of accuracy) in a coherent fashion. In particular,\nREADLE leverages benefits of real-time logic and binary decision diagrams to\ngenerate unified specifications. Several insights learned in building READLE\nare also discussed, which should help in future research in the domain of\nformal specifications for edge intelligent systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.006,regular,pre_llm,2022,5,"{'ai_likelihood': 1.5563435024685332e-05, 'text': 'HatCUP: Hybrid Analysis and Attention based Just-In-Time Comment\n  Updating\n\n  When changing code, developers sometimes neglect updating the related\ncomments, bringing inconsistent or outdated comments. These comments increase\nthe cost of program understanding and greatly reduce software maintainability.\nResearchers have put forward some solutions, such as CUP and HEBCUP, which\nupdate comments efficiently for simple code changes (i.e. modifying of a single\ntoken), but not good enough for complex ones. In this paper, we propose an\napproach, named HatCUP (Hybrid Analysis and Attention based Comment UPdater),\nto provide a new mechanism for comment updating task. HatCUP pays attention to\nhybrid analysis and information. First, HatCUP considers the code structure\nchange information and introduces a structure-guided attention mechanism\ncombined with code change graph analysis and optimistic data flow dependency\nanalysis. With a generally popular RNN-based encoder-decoder architecture,\nHatCUP takes the action of the code edits, the syntax, semantics and structure\ncode changes, and old comments as inputs and generates a structural\nrepresentation of the changes in the current code snippet. Furthermore, instead\nof directly generating new comments, HatCUP proposes a new edit or non-edit\nmechanism to mimic human editing behavior, by generating a sequence of edit\nactions and constructing a modified RNN model to integrate newly developed\ncomponents. Evaluation on a popular dataset demonstrates that HatCUP\noutperforms the state-of-the-art deep learning-based approaches (CUP) by 53.8%\nfor accuracy, 31.3% for recall and 14.3% for METEOR of the original metrics.\nCompared with the heuristic-based approach (HEBCUP), HatCUP also shows better\noverall performance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.08314,regular,pre_llm,2022,5,"{'ai_likelihood': 2.6490953233506944e-06, 'text': 'Self-Sovereign Identity as a Service: Architecture in Practice\n\n  Self-sovereign identity (SSI) has gained a large amount of interest. It\nenables physical entities to retain ownership and control of their digital\nidentities, which naturally forms a conceptual decentralized architecture. With\nthe support of the distributed ledger technology (DLT), it is possible to\nimplement this conceptual decentralized architecture in practice and further\nbring technical advantages such as privacy protection, security enhancement,\nhigh availability. However, developing such a relatively new identity model has\nhigh costs and risks with uncertainty. To facilitate the use of the DLT-based\nSSI in practice, we formulate Self-Sovereign Identity as a Service (SSIaaS), a\nconcept that enables a system, especially a system cluster, to readily adopt\nSSI as its identity model for identification, authentication, and\nauthorization. We propose a practical architecture by elaborating the service\nconcept, SSI, and DLT to implement SSIaaS platforms and SSI services. Besides,\nwe present an architecture for constructing and customizing SSI services with a\nset of architectural patterns and provide corresponding evaluations.\nFurthermore, we demonstrate the feasibility of our proposed architecture in\npractice with Selfid, an SSIaaS platform based on our proposed architecture.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.01351,regular,pre_llm,2022,5,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'Tooling for Time- and Space-efficient git Repository Mining\n\n  Software projects under version control grow with each commit, accumulating\nup to hundreds of thousands of commits per repository. Especially for such\nlarge projects, the traversal of a repository and data extraction for static\nsource code analysis poses a trade-off between granularity and speed. We\nshowcase the command-line tool pyrepositoryminer that combines a set of\noptimization approaches for efficient traversal and data extraction from git\nrepositories while being adaptable to third-party and custom software metrics\nand data extractions. The tool is written in Python and combines bare\nrepository access, in-memory storage, parallelization, caching, change-based\nanalysis, and optimized communication between the traversal and custom data\nextraction components. The tool allows for both metrics written in Python and\nexternal programs for data extraction. A single-thread performance evaluation\nbased on a basic mining use case shows a mean speedup of 15.6x to other freely\navailable tools across four mid-sized open source projects. A multi-threaded\nexecution allows for load distribution among cores and, thus, a mean speedup up\nto 86.9x using 12 threads.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.02727,regular,pre_llm,2022,5,"{'ai_likelihood': 4.900826348198785e-05, 'text': ""Replicating Data Pipelines with GrimoireLab\n\n  In this paper, we present our MSR Hackathon 2022 project that replicates an\nexisting Gitter study using GrimoireLab. We compare the previous study's\npipeline with our GrimoireLab implementation in terms of speed, data\nconsistency, organization, and the learning curve to get started. We believe\nour experience with GrimoireLab can help future researchers in making the right\nchoice while implementing their data pipelines over Gitter and Github data.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.01372,regular,pre_llm,2022,5,"{'ai_likelihood': 1.8212530348036025e-06, 'text': 'A Process To Support Cloud Release Preparation\n\nThis paper presents concepts and methods to support preparing software and system releases to production.\n  Keywords: Operational Readiness Review, ORR, IT Services, IT Operations, ITIL, Process Engineering, Reliability, Availability, Software Architecture, Cloud Computing, Networking, Site Reliability Engineering, DevOps, Agile Methods, Quality, Defect Prevention, Release Management, Risk Management, Data Visualization, Organizational Change Management.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.10992,regular,pre_llm,2022,5,"{'ai_likelihood': 3.1789143880208335e-06, 'text': 'Exploring Apache Incubator Project Trajectories with APEX\n\n  Open Source Software (OSS) is a major component of our digital\ninfrastructure, yet more than 80% of such projects fail. Seeking less\nuncertainty, many OSS projects join established software communities, e.g., the\nApache Software Foundation (ASF), with established rules and community support\nto guide projects toward sustainability. In their nascent stage, ASF projects\nare incubated in the ASF incubator (ASFI), which provides systematic mentorship\ntoward long-term sustainability. Projects in ASFI eventually conclude their\nincubation by either graduating, if successful, or retiring, if not.\n  Time-stamped traces of developer activities are publicly available from ASF,\nand can be used for monitoring project trajectories toward sustainability. Here\nwe present a web app dashboard tool, APEX, that allows internal and external\nstakeholders to monitor and explore ASFI project sustainability trajectories,\nincluding social and technical networks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.01254,regular,pre_llm,2022,5,"{'ai_likelihood': 2.5828679402669272e-06, 'text': 'Deep API Learning Revisited\n\n  Understanding the correct API usage sequences is one of the most important\ntasks for programmers when they work with unfamiliar libraries. However,\nprogrammers often encounter obstacles to finding the appropriate information\ndue to either poor quality of API documentation or ineffective query-based\nsearching strategy. To help solve this issue, researchers have proposed various\nmethods to suggest the sequence of APIs given natural language queries\nrepresenting the information needs from programmers. Among such efforts, Gu et\nal. adopted a deep learning method, in particular an RNN Encoder-Decoder\narchitecture, to perform this task and obtained promising results on common\nAPIs in Java. In this work, we aim to reproduce their results and apply the\nsame methods for APIs in Python. Additionally, we compare the performance with\na more recent Transformer-based method, i.e., CodeBERT, for the same task. Our\nexperiment reveals a clear drop in performance measures when careful data\ncleaning is performed. Owing to the pretraining from a large number of source\ncode files and effective encoding technique, CodeBERT outperforms the method by\nGu et al., to a large extent.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.13522,regular,pre_llm,2022,5,"{'ai_likelihood': 2.8477774726019966e-06, 'text': 'Dynamically Relative Position Encoding-Based Transformer for Automatic\n  Code Edit\n\n  Adapting Deep Learning (DL) techniques to automate non-trivial coding\nactivities, such as code documentation and defect detection, has been\nintensively studied recently. Learning to predict code changes is one of the\npopular and essential investigations. Prior studies have shown that DL\ntechniques such as Neural Machine Translation (NMT) can benefit meaningful code\nchanges, including bug fixing and code refactoring. However, NMT models may\nencounter bottleneck when modeling long sequences, thus are limited in\naccurately predicting code changes. In this work, we design a Transformer-based\napproach, considering that Transformer has proven effective in capturing\nlong-term dependencies. Specifically, we propose a novel model named DTrans.\nFor better incorporating the local structure of code, i.e., statement-level\ninformation in this paper, DTrans is designed with dynamically relative\nposition encoding in the multi-head attention of Transformer. Experiments on\nbenchmark datasets demonstrate that DTrans can more accurately generate patches\nthan the state-of-the-art methods, increasing the performance by at least\n5.45\\%-46.57\\% in terms of the exact match metric on different datasets.\nMoreover, DTrans can locate the lines to change with 1.75\\%-24.21\\% higher\naccuracy than the existing methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.08589,regular,pre_llm,2022,6,"{'ai_likelihood': 2.715322706434462e-06, 'text': ""Business Process Model for Interoperability Improvement in the\n  Agricultural Domain Using Digital Twins\n\n  A farm generates a lot of data from various systems, which is then stored in\na distributed manner, usually in non-standardized formats, which bears the risk\nof data inconsistencies. This work addresses this issue by using business\nprocess management (BPM) to demonstrate that the use of digital twins (DTs) can\nimprove interoperability between services in the agriculture domain. Steps from\nthe BPM lifecycle were applied to a farming use case in Germany. First, the\nas-is business process model was discovered and modeled without DTs, analyzed\nand then redesigned into the to-be model according to the DT integration. The\nto-be model showed a reduction in the number of tasks needed to be performed by\nthe farmer as well as an improvement of process data quality, interoperability,\nand efficiency. Finally, a comparison of the' average processing times of both\nmodels with the help of process simulation revealed improvements in the to-be\nprocess.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.04397,regular,pre_llm,2022,6,"{'ai_likelihood': 9.602970547146267e-05, 'text': 'ESBMC-Jimple: Verifying Kotlin Programs via Jimple Intermediate\n  Representation\n\n  In this work, we describe and evaluate the first model checker for verifying\nKotlin programs through the Jimple intermediate representation. The verifier,\nnamed ESBMC-Jimple, is built on top of the Efficient SMT-based Context-Bounded\nModel Checker (ESBMC). It uses the Soot framework to obtain the Jimple IR,\nrepresenting a simplified version of the Kotlin source code, containing a\nmaximum of three operands per instruction. ESBMC-Jimple processes Kotlin source\ncode together with a model of the standard Kotlin libraries and checks a set of\nsafety properties. Experimental results show that ESBMC-Jimple can correctly\nverify a set of Kotlin benchmarks from the literature and that it is\ncompetitive with state-of-the-art Java bytecode verifiers. A demonstration is\navailable at https://youtu.be/J6WhNfXvJNc.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.0626,regular,pre_llm,2022,6,"{'ai_likelihood': 1.0927518208821616e-06, 'text': ""OpenCBS: An Open-Source COBOL Defects Benchmark Suite\n\n  As the current COBOL workforce retires, entry-level developers are left to\nkeep complex legacy systems maintained and operational. This creates a massive\ngap in knowledge and ability as companies are having their veteran developers\nreplaced with a new, inexperienced workforce. Additionally, the lack of COBOL\nand mainframe technology in the current academic curriculum further increases\nthe learning curve for this new generation of developers. These issues are\nbecoming even more pressing due to the business-critical nature of these\nsystems, which makes migrating or replacing the mainframe and COBOL anytime\nsoon very unlikely. As a result, there is now a huge need for tools and\nresources to increase new developers' code comprehension and ability to perform\nroutine tasks such as debugging and defect location. Extensive work has been\ndone in the software engineering field on the creation of such resources.\nHowever, the proprietary nature of COBOL and mainframe systems has restricted\nthe amount of work and the number of open-source tools available for this\ndomain. To address this issue, our work leverages the publicly available\ntechnical forum data to build an open-source collection of COBOL programs\nembodying issues/defects faced by COBOL developers. These programs were\nreconstructed and organized in a benchmark suite to facilitate the testing of\ndeveloper tools. Our goal is to provide an open-source COBOL benchmark and\ntesting suite that encourage community contribution and serve as a resource for\nresearchers and tool-smiths in this domain.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.10358,review,pre_llm,2022,6,"{'ai_likelihood': 1.2252065870496963e-06, 'text': 'Open Source Software: An Approach to Controlling Usage and Risk in\n  Application Ecosystems\n\n  The Open Source Software movement has been growing exponentially for a number\nof years with no signs of slowing. Driving this growth is the widespread\navailability of libraries and frameworks that provide many functionalities.\nDevelopers are saving time and money incorporating this functionality into\ntheir applications resulting in faster more feature-rich releases. Despite the\ngrowing success and the advantages that open source software provides, there is\na dark side. Due to its community construction and largely unregulated\ndistribution, the majority of open source software contains bugs,\nvulnerabilities and other issues making it highly susceptible to exploits. The\nlack of oversight, in general, hinders the quality of this software resulting\nin a trickle-down effect in the applications that use it. Additionally,\ndevelopers who use open source tend to arbitrarily download the software into\ntheir build systems but rarely keep track of what they have downloaded\nresulting in an excessive amount of open source software in their applications\nand in their ecosystem. This paper discusses processes and practices that users\nof open source software can implement into their environments that can safely\ntrack and control the introduction and usage of open source software into their\napplications, and report on some preliminary results obtained in an industrial\ncontext. We conclude by discussing governance issues related to the disciplined\nuse and reuse of open source and areas for further improvements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.09894,regular,pre_llm,2022,6,"{'ai_likelihood': 1.2252065870496963e-06, 'text': 'NoteG: A Computational Notebook to Facilitate Rapid Game Prototyping\n\n  Game development-based approaches are increasingly used to design curricula\nthat can engage students, as these can help them apply and practice learnt\ncomputer science concepts. However, it can become complex to develop a minimum\nworking game or a prototype with the help of high-end game engines. Game\nprototyping is one of the most essential parts of the game design and\ndevelopment cycle as it allows developers to continuously test and improve\ntheir ideas. In recent years, computational notebooks have gained widespread\npopularity among developers. They can help run individual code snippets,\nvisualize the output, consolidate the source code, and share live code easily.\nHowever, its use has not been explored in the field of game development and\nprototyping. In this paper, we propose NoteG, a computational notebook towards\nrapid game prototyping. We evaluated the tool with 18 novice game developers\nthrough a questionnaire-based user survey. A majority of the volunteers (66%)\nfound it easy to use and were of the opinion that it saves time. A few of the\nparticipants successfully extended the existing framework to implement new game\nmechanics within their prototypes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.08688,regular,pre_llm,2022,6,"{'ai_likelihood': 2.947118547227648e-06, 'text': ""Detecting Connectivity Issues in Android Apps\n\n  Android is the most popular mobile operating system in the world, running on\nmore than 70% of mobile devices. This implies a gigantic and very competitive\nmarket for Android apps. Being successful in such a market is far from trivial\nand requires, besides the tackling of a problem or need felt by a vast\naudience, the development of high-quality apps. As recently showed in the\nliterature, connectivity issues (e.g., mishandling of zero/unreliable Internet\nconnection) can result in bugs and/or crashes, negatively affecting the app's\nuser experience. While these issues have been studied in the literature, there\nare no techniques able to automatically detect and report them to developers.\nWe present CONAN, a tool able to detect statically 16 types of connectivity\nissues affecting Android apps. We assessed the ability of CONAN to precisely\nidentify these issues in a set of 44 open source apps, observing an average\nprecision of 80%. Then, we studied the relevance of these issues for developers\nby (i) conducting interviews with six practitioners working with commercial\nAndroid apps, and (ii) submitting 84 issue reports for 27 open source apps. Our\nresults show that several of the identified connectivity issues are considered\nas relevant by practitioners in specific contexts, in which connectivity is\nconsidered a first-class feature.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.04177,review,pre_llm,2022,6,"{'ai_likelihood': 1.2020270029703777e-05, 'text': 'Towards Continuous Systematic Literature Review in Software Engineering\n\n  Context: New scientific evidence continuously arises with advances in\nSoftware Engineering (SE) research. Conventionally, Systematic Literature\nReviews (SLRs) are not updated or updated intermittently, leaving gaps between\nupdates, during which time the SLR may be missing crucial new evidence. Goal:\nWe propose and evaluate a concept and process called Continuous Systematic\nLiterature Review (CSLR) in SE. Method: To elaborate on the CSLR concept and\nprocess, we performed a synthesis of evidence by conducting a meta-ethnography,\naddressing knowledge from varied research areas. Furthermore, we conducted a\ncase study to evaluate the CSLR process. Results: We describe the resulting\nCSLR process in BPMN format. The case study results provide indications on the\nimportance and feasibility of applying CSLR in practice to continuously update\nSLR evidence in SE. Conclusion: The CSLR concept and process provide a feasible\nand systematic way to continuously incorporate new evidence into SLRs,\nsupporting trustworthy and up-to-date evidence for SLRs in SE.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.13562,regular,pre_llm,2022,6,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Incorporating Failure Knowledge into Design Decisions for IoT Systems: A\n  Controlled Experiment on Novices\n\n  Internet of Things (IoT) systems allow software to directly interact with the\nphysical world. Recent IoT failures can be attributed to recurring software\ndesign flaws, suggesting IoT software engineers may not be learning from past\nfailures. We examine the use of failure stories to improve IoT system designs.\nWe conducted an experiment to evaluate the influence of failure-related\nlearning treatments on design decisions. Our experiment used a between-subjects\ncomparison of novices (computer engineering students) completing a design\nquestionnaire. There were three treatments: a control group (N=7); a group\nconsidering a set of design guidelines (N=8); and a group considering failure\nstories (proposed treatment, N=6). We measured their design decisions and their\ndesign rationales. All subjects made comparable decisions. Their rationales\nvaried by treatment: subjects treated with guidelines and failure stories made\ngreater use of criticality as a rationale, while subjects exposed to failure\nstories more frequently used safety as a rationale. Building on these findings,\nwe suggest several research directions toward a failure-aware IoT engineering\nprocess.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.08574,regular,pre_llm,2022,6,"{'ai_likelihood': 7.947285970052084e-07, 'text': ""Using Transfer Learning for Code-Related Tasks\n\n  Deep learning (DL) techniques have been used to support several code-related\ntasks such as code summarization and bug-fixing. In particular, pre-trained\ntransformer models are on the rise, also thanks to the excellent results they\nachieved in Natural Language Processing (NLP) tasks. The basic idea behind\nthese models is to first pre-train them on a generic dataset using a\nself-supervised task (e.g, filling masked words in sentences). Then, these\nmodels are fine-tuned to support specific tasks of interest (e.g, language\ntranslation). A single model can be fine-tuned to support multiple tasks,\npossibly exploiting the benefits of transfer learning. This means that\nknowledge acquired to solve a specific task (e.g, language translation) can be\nuseful to boost performance on another task (e.g, sentiment classification).\nWhile the benefits of transfer learning have been widely studied in NLP,\nlimited empirical evidence is available when it comes to code-related tasks. In\nthis paper, we assess the performance of the Text-To-Text Transfer Transformer\n(T5) model in supporting four different code-related tasks: (i) automatic\nbug-fixing, (ii) injection of code mutants, (iii) generation of assert\nstatements, and (iv) code summarization. We pay particular attention in\nstudying the role played by pre-training and multi-task fine-tuning on the\nmodel's performance. We show that (i) the T5 can achieve better performance as\ncompared to state-of-the-art baselines; and (ii) while pre-training helps the\nmodel, not all tasks benefit from a multi-task fine-tuning.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.10446,review,pre_llm,2022,6,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Deep dive into Interledger: Understanding the Interledger ecosystem\n\n  At the technical level, the goal of Interledger is to provide an architecture\nand a minimal set of protocols to enable interoperability between any value\ntransfer systems. The Interledger protocol is a protocol for inter-blockchain\npayments which can also accommodate FIAT currencies. To understand how it is\npossible to achieve this goal, several aspects of the technology require a\ndeeper analysis. For this reason, in our journey to become knowledgeable and\nactive contributors we decided to create our own test-bed on our premises. By\ndoing so, we noticed that some aspects are well documented but we found that\nothers might need more attention and clarification. Despite a large community\neffort, the task to keep information on a fast evolving software ecosystem\nup-to-date is tedious and not always the main priority for such a project. The\npurpose of this tutorial is to guide, through several examples and hands-on\nactivities, community members who want to engage at different levels. The\ntutorial consolidates all the relevant information from generating a simple\npayment to ultimately creating a test-bed with the Interledger protocol suite\nbetween Ripple and other distributed ledger technologies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.11728,review,pre_llm,2022,6,"{'ai_likelihood': 7.450580596923828e-06, 'text': ""There Ain't No Such Thing as a Free Custom Memory Allocator\n\n  Using custom memory allocators is an efficient performance optimization\ntechnique. However, dependency on a custom allocator can introduce several\nmaintenance-related issues. We present lessons learned from the industry and\nprovide critical guidance for using custom memory allocators and enumerate\nvarious challenges associated with integrating them. These recommendations are\nbased on years of experience incorporating custom allocators into different\nindustrial software projects.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.13204,regular,pre_llm,2022,6,"{'ai_likelihood': 2.053048875596788e-06, 'text': 'A Model-Based Approach for Specifying Changes in Replications of\n  Empirical Studies in Computer Science\n\n  Context: The need of replicating empirical studies in Computer Science (CS)\nis widely recognized among the research community to consolidate acquired\nknowledge generalizing results. It is essential to report the changes of each\nreplication to understand the evolution of the experimental validity across a\nfamily of studies. Unfortunately, the lack of proposals undermines these\nobjectives.\n  Objective. The main goal of our work is to provide researchers in CS, and in\nother areas of research, with a systematic, tool-supported approach for the\nreporting of changes in the replications of their empirical studies.\n  Method: Applying DSR, we have developed and validated a composite artifact\nconsisting of (i) a metamodel of the relevant concepts of replications and\ntheir changes; (ii) templates and linguistic patterns for reporting those\nconcepts; and (iii) a proof-of-concept model-based software tool that supports\nthe proposed approach. For its validation, we have carried out a multiple case\nstudy including 9 families of empirical studies from CS and Agrobiology. The 9\nfamilies encompass 23 replication studies and 92 replication changes, for which\nwe have analyzed the suitability of our proposal.\n  Results: The multiple case study revealed some initial limitations of our\napproach related to threats to experimental validity or context variables.\nAfter several improvement iterations, all the 92 replication changes could be\nproperly specified, including also their qualitatively estimated effects on\nexperimental validity across the family of experiments and its corresponding\nvisualization.\n  Conclusions: Our proposal for the specification of replication changes seems\nto fit the needs not only of replications in CS, but also in other research\nareas. Nevertheless, further research is needed to improve it and to\ndisseminate its use among the research community.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.10344,review,pre_llm,2022,6,"{'ai_likelihood': 2.682209014892578e-06, 'text': 'Static Analysis of Infrastructure as Code: a Survey\n\n  The increasing use of Infrastructure as Code (IaC) in DevOps leads to\nbenefits in speed and reliability of deployment operation, but extends to\ninfrastructure challenges typical of software systems. IaC scripts can contain\ndefects that result in security and reliability issues in the deployed\ninfrastructure: techniques for detecting and preventing them are needed. We\nanalyze and survey the current state of research in this respect by conducting\na literature review on static analysis techniques for IaC. We describe analysis\ntechniques, defect categories and platforms targeted by tools in the\nliterature.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.13373,regular,pre_llm,2022,6,"{'ai_likelihood': 5.861123402913412e-06, 'text': 'Conceptual Modeling of Actions\n\n  Modeling in software engineering includes constructing static, dynamic, and\nbehavioral representations. In describing system behavior, actions and states\nare two of the most commonly used concepts. In this paper, we focus on the\nnotion of action. It generally held that the meaning of the concept of action\nis not that easy to grasp. According to some researchers, many existing systems\ndo involve the notion of action, but in an obscure way. In Unified Modeling\nLanguage (UML), an action is a single atomic step within an activity, i.e., it\nis not further decomposed within the activity. Activity represents a behavior\nthat is composed of actions. This paper contributes to the establishment of a\nbroader interdisciplinary understanding of the notion of action in conceptual\nmodeling based on a model called the thinging machine (TM). The TM uses only\nfive primitive actions: create, process, release, transfer, and receive. The\ngoal of such a venture is to improve the process of developing conceptual\nmodels by refining basic concepts such as action and event. To demonstrate how\nTM modeling represents actions, UML activity and Business Process Model and\nNotation (BPMN) diagrams are re-modeled in terms of the five TM actions. The\nresults reveal the viability of the TM s five actions in modeling and relate\nthem to other important notions such as activity, event, and behavior.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.01111,regular,pre_llm,2022,6,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'MorphQ: Metamorphic Testing of the Qiskit Quantum Computing Platform\n\n  As quantum computing is becoming increasingly popular, the underlying quantum\ncomputing platforms are growing both in ability and complexity. Unfortunately,\ntesting these platforms is challenging due to the relatively small number of\nexisting quantum programs and because of the oracle problem, i.e., a lack of\nspecifications of the expected behavior of programs. This paper presents\nMorphQ, the first metamorphic testing approach for quantum computing platforms.\nOur two key contributions are (i) a program generator that creates a large and\ndiverse set of valid (i.e., non-crashing) quantum programs, and (ii) a set of\nprogram transformations that exploit quantum-specific metamorphic relationships\nto alleviate the oracle problem. Evaluating the approach by testing the popular\nQiskit platform shows that the approach creates over 8k program pairs within\ntwo days, many of which expose crashes. Inspecting the crashes, we find 13\nbugs, nine of which have already been confirmed. MorphQ widens the slim\nportfolio of testing techniques of quantum computing platforms, helping to\ncreate a reliable software stack for this increasingly important field.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.14118,regular,pre_llm,2022,6,"{'ai_likelihood': 2.7815500895182294e-06, 'text': 'GitHub Actions: The Impact on the Pull Request Process\n\n  Software projects frequently use automation tools to perform repetitive\nactivities in the distributed software development process. Recently, GitHub\nintroduced GitHub Actions, a feature providing automated workflows for software\nprojects. Understanding and anticipating the effects of adopting such\ntechnology is important for planning and management. Our research investigates\nhow projects use GitHub Actions, what the developers discuss about them, and\nhow project activity indicators change after their adoption. Our results\nindicate that 1,489 out of 5,000 most popular repositories (almost 30% of our\nsample) adopt GitHub Actions and that developers frequently ask for help\nimplementing them. Our findings also suggest that the adoption of GitHub\nActions leads to more rejections of pull requests (PRs), more communication in\naccepted PRs and less communication in rejected PRs, fewer commits in accepted\nPRs and more commits in rejected PRs, and more time to accept a PR. We found\nsimilar results when segmenting our results by categories of GitHub Actions. We\nsuggest practitioners consider these effects when adopting GitHub Actions on\ntheir projects.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.04462,regular,pre_llm,2022,6,"{'ai_likelihood': 1.3079908159044055e-05, 'text': ""When Traceability Goes Awry: an Industrial Experience Report\n\n  The concept of traceability between artifacts is considered an enabler for\nsoftware project success. This concept has received plenty of attention from\nthe research community and is by many perceived to always be available in an\nindustrial setting. In this industry-academia collaborative project, a team of\nresearchers, supported by testing practitioners from a large telecommunication\ncompany, sought to investigate the partner company's issues related to software\nquality. However, it was soon identified that the fundamental traceability\nlinks between requirements and test cases were missing. This lack of\ntraceability impeded the implementation of a solution to help the company deal\nwith its quality issues. In this experience report, we discuss lessons learned\nabout the practical value of creating and maintaining traceability links in\ncomplex industrial settings and provide a cautionary tale for researchers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.12927,regular,pre_llm,2022,6,"{'ai_likelihood': 5.033281114366319e-06, 'text': 'An Empirical Study on Bug Severity Estimation using Source Code Metrics\n  and Static Analysis\n\n  In the past couple of decades, significant research efforts have been devoted\nto the prediction of software bugs (i.e., defects). In general, these works\nleverage a diverse set of metrics, tools, and techniques to predict which\nclasses, methods, lines, or commits are buggy. However, most existing work in\nthis domain treats all bugs the same, which is not the case in practice. The\nmore severe the bugs the higher their consequences. Therefore, it is important\nfor a defect prediction method to estimate the severity of the identified bugs,\nso that the higher severity ones get immediate attention. In this paper, we\nprovide a quantitative and qualitative study on two popular datasets (Defects4J\nand Bugs.jar), using 10 common source code metrics, and two popular static\nanalysis tools (SpotBugs and Infer) for analyzing their capability to predict\ndefects and their severity. We studied 3,358 buggy methods with different\nseverity labels from 19 Java open-source projects. Results show that although\ncode metrics are useful in predicting buggy code (Lines of the Code,\nMaintainable Index, FanOut, and Effort metrics are the best), they cannot\nestimate the severity level of the bugs. In addition, we observed that static\nanalysis tools have weak performance in both predicting bugs (F1 score range of\n3.1%-7.1%) and their severity label (F1 score under 2%). We also manually\nstudied the characteristics of the severe bugs to identify possible reasons\nbehind the weak performance of code metrics and static analysis tools in\nestimating their severity. Also, our categorization shows that Security bugs\nhave high severity in most cases while Edge/Boundary faults have low severity.\nFinally, we discuss the practical implications of the results and propose new\ndirections for future research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.11936,review,pre_llm,2022,6,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Towards a Maturity Model for Systematic Literature Review Process\n\n  Systematic literature reviews (SLR) have been increasingly conducted in\nsoftware engineering and they provide significant benefits in terms of\nsummarizing the state of the research. The process of conducting SLR is\ncomplex, involving several activities and consuming considerable effort and\ntime from researchers. Researchers often skip or poorly conduct essential\nactivities, which introduce threats to validity, resulting in lower-quality\nSLR. But researchers are often unaware of what they could do to mature their\nSLR process, thus improving the SLR quality. The main goal of this paper is to\nintroduce a maturity model for the SLR process named MM4SLR. To this end, we\nwere inspired by well-known models like CMMI (Capability Maturity Model\nIntegration). We first identified 39 key practices for SLR from the literature\nand grouped them into nine goals that were further grouped into five process\nareas. We then organized the process areas into five maturity levels which\ncompose our model. Our proof of concept, applying the MM4SLR to four published\nSLR showed that the MM4SLR is suitable for appraising SLR and can identify\nimportant flaws in SLR quality. MM4SLR can therefore support researchers in\ncreating their SLR processes and selecting practices that could be adopted to\nmature their processes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.00054,review,pre_llm,2022,6,"{'ai_likelihood': 1.7219119601779514e-06, 'text': '""Communication Is a Scarce Resource!\'\': A Summary of CHASE\'22 Conference\n  Discussions\n\n  Background: Software Engineering regularly views communication between\nproject participants as a tool for solving various problems in software\ndevelopment. Objective: Formulate research questions in areas related to CHASE.\nMethod: A day-long discussion of five participants at the in-person day of the\n15th International Conference on Cooperative and Human Aspects of Software\nEngineering (CHASE 2022) on May 23rd 2022. Results: It is not rare in\nindustrial SE projects that communication is not just a tool or technique to be\napplied but also represents a resource, which, when lacking, threatens project\nsuccess. This situation might arise when a person required to make decisions\n(especially regarding requirements, budgets, or priorities) is often\nunavailable. It may be helpful to frame communication as a scarce resource to\nunderstand the key difficulty of such situations. Conclusion: We call for\nstudies that focus on the allocation and management of scarce communication\nresources of stakeholders as a lens to analyze software engineering projects.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.05613,regular,pre_llm,2022,7,"{'ai_likelihood': 8.90758302476671e-06, 'text': 'Making Python Code Idiomatic by Automatic Refactoring Non-Idiomatic\n  Python Code with Pythonic Idioms\n\n  Compared to other programming languages (e.g., Java), Python has more idioms\nto make Python code concise and efficient. Although pythonic idioms are well\naccepted in the Python community, Python programmers are often faced with many\nchallenges in using them, for example, being unaware of certain pythonic idioms\nor do not know how to use them properly. Based on an analysis of 7,638 Python\nrepositories on GitHub, we find that non-idiomatic Python code that can be\nimplemented with pythonic idioms occurs frequently and widely. Unfortunately,\nthere is no tool for automatically refactoring such non-idiomatic code into\nidiomatic code. In this paper, we design and implement an automatic refactoring\ntool to make Python code idiomatic. We identify nine pythonic idioms by\nsystematically contrasting the abstract syntax grammar of Python and Java. Then\nwe define the syntactic patterns for detecting non-idiomatic code for each\npythonic idiom. Finally, we devise atomic AST-rewriting operations and\nrefactoring steps to refactor non-idiomatic code into idiomatic code. We test\nand review over 4,115 refactorings applied to 1,065 Python projects from\nGitHub, and submit 90 pull requests for the 90 randomly sampled refactorings to\n84 projects. These evaluations confirm the high-accuracy, practicality and\nusefulness of our refactoring tool on real-world Python code. Our refactoring\ntool can be accessed at 47.242.131.128:5000.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.04823,regular,pre_llm,2022,7,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'Adaptive Behavioral Model Learning for Software Product Lines\n\n  Behavioral models enable the analysis of the functionality of software\nproduct lines (SPL), e.g., model checking and model-based testing. Model\nlearning aims at constructing behavioral models for software systems in some\nform of a finite state machine. Due to the commonalities among the products of\nan SPL, it is possible to reuse the previously learned models during the model\nlearning process. In this paper, an adaptive approach (the $\\text{PL}^*$\nmethod) for learning the product models of an SPL is presented based on the\nwell-known $L^*$ algorithm. In this method, after model learning of each\nproduct, the sequences in the final observation table are stored in a\nrepository which will be used to initialize the observation table of the\nremaining products to be learned. The proposed algorithm is evaluated on two\nopen-source SPLs and the total learning cost is measured in terms of the number\nof rounds, the total number of resets and input symbols. The results show that\nfor complex SPLs, the total learning cost for the $\\text{PL}^*$ method is\nsignificantly lower than that of the non-adaptive learning method in terms of\nall three metrics. Furthermore, it is observed that the order in which the\nproducts are learned affects the efficiency of the $\\text{PL}^*$ method. Based\non this observation, we introduced a heuristic to determine an ordering which\nreduces the total cost of adaptive learning in both case studies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.11018,regular,pre_llm,2022,7,"{'ai_likelihood': 3.2782554626464844e-06, 'text': 'Learning from what we know: How to perform vulnerability prediction\n  using noisy historical data\n\n  Vulnerability prediction refers to the problem of identifying system\ncomponents that are most likely to be vulnerable. Typically, this problem is\ntackled by training binary classifiers on historical data. Unfortunately,\nrecent research has shown that such approaches underperform due to the\nfollowing two reasons: a) the imbalanced nature of the problem, and b) the\ninherently noisy historical data, i.e., most vulnerabilities are discovered\nmuch later than they are introduced. This misleads classifiers as they learn to\nrecognize actual vulnerable components as non-vulnerable. To tackle these\nissues, we propose TROVON, a technique that learns from known vulnerable\ncomponents rather than from vulnerable and non-vulnerable components, as\ntypically performed. We perform this by contrasting the known vulnerable, and\ntheir respective fixed components. This way, TROVON manages to learn from the\nthings we know, i.e., vulnerabilities, hence reducing the effects of noisy and\nunbalanced data. We evaluate TROVON by comparing it with existing techniques on\nthree security-critical open source systems, i.e., Linux Kernel, OpenSSL, and\nWireshark, with historical vulnerabilities that have been reported in the\nNational Vulnerability Database (NVD). Our evaluation demonstrates that the\nprediction capability of TROVON significantly outperforms existing\nvulnerability prediction techniques such as Software Metrics, Imports, Function\nCalls, Text Mining, Devign, LSTM, and LSTM-RF with an improvement of 40.84% in\nMatthews Correlation Coefficient (MCC) score under Clean Training Data\nSettings, and an improvement of 35.52% under Realistic Training Data Settings.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.00308,regular,pre_llm,2022,7,"{'ai_likelihood': 1.8212530348036025e-06, 'text': 'Quality increases as the error rate decreases\n\n  In this paper we propose an approach to the design of processes and software\nthat aims at decreasing human and software errors, that so frequently happen,\nmaking affected people using and wasting a lot of time for the need of fixing\nthe errors. We base our statements on the natural relationship between quality\nand error rate, increasing the latter as the error rate decreases. We try to\nclassify errors into several types and address techniques to reduce the\nlikelihood of making mistakes, depending on the type of error.\n  We focus on this approach related to organization, management and software\ndesign that will allow to be more effective and efficient in this period where\nmankind has been affected by a severe pandemic and where we need to be more\nefficient and effective in all processes, aiming at an industrial renaissance\nwhich we know to be not too far and easily reachable once the path to follow\nhas been characterized, also in the light of the experience.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.11104,regular,pre_llm,2022,7,"{'ai_likelihood': 7.020102606879341e-06, 'text': 'Two Sides of the Same Coin: Exploiting the Impact of Identifiers in\n  Neural Code Comprehension\n\n  Previous studies have demonstrated that neural code comprehension models are\nvulnerable to identifier naming. By renaming as few as one identifier in the\nsource code, the models would output completely irrelevant results, indicating\nthat identifiers can be misleading for model prediction. However, identifiers\nare not completely detrimental to code comprehension, since the semantics of\nidentifier names can be related to the program semantics. Well exploiting the\ntwo opposite impacts of identifiers is essential for enhancing the robustness\nand accuracy of neural code comprehension, and still remains under-explored. In\nthis work, we propose to model the impact of identifiers from a novel causal\nperspective, and propose a counterfactual reasoning-based framework named\nCREAM. CREAM explicitly captures the misleading information of identifiers\nthrough multi-task learning in the training stage, and reduces the misleading\nimpact by counterfactual inference in the inference stage. We evaluate CREAM on\nthree popular neural code comprehension tasks, including function naming,\ndefect detection and code classification. Experiment results show that CREAM\nnot only significantly outperforms baselines in terms of robustness (e.g.,\n+37.9% on the function naming task at F1 score), but also achieve improved\nresults on the original datasets (e.g., +0.5% on the function naming task at F1\nscore).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.13263,review,pre_llm,2022,7,"{'ai_likelihood': 1.9205941094292534e-06, 'text': 'Software Engineering for Serverless Computing\n\n  Serverless computing is an emerging cloud computing paradigm that has been\napplied to various domains, including machine learning, scientific computing,\nvideo processing, etc. To develop serverless computing-based software\napplications (a.k.a., serverless applications), developers follow the new\ncloud-based software architecture, where they develop event-driven applications\nwithout the need for complex and error-prone server management. The great\ndemand for developing serverless applications poses unique challenges to\nsoftware developers. However, Software Engineering (SE) has not yet\nwholeheartedly tackled these challenges. In this paper, we outline a vision for\nhow SE can facilitate the development of serverless applications and call for\nactions by the SE research community to reify this vision. Specifically, we\ndiscuss possible directions in which researchers and cloud providers can\nfacilitate serverless computing from the SE perspective, including\nconfiguration management, data security, application migration, performance,\ntesting and debugging, etc.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.00037,regular,pre_llm,2022,7,"{'ai_likelihood': 2.7484363979763457e-06, 'text': 'On infrastructure for facilitation of inner source in small development\n  teams\n\n  The phenomenon of adopting open source software development practices in a\ncorporate environment is known by many names, one being inner source. The\nobjective of this study is to investigate how an organization consisting of\nsmall development teams can benefit from adopting inner source and assess the\nlevel of applicability. The research has been conducted as a case study at a\nsoftware development company. Data collection was carried out through\ninterviews and a series of focus group meetings, and then analyzed by mapping\nit to an available framework. The analysis shows that the organization\npossesses potential, and also identified a number of challenges and benefits of\nspecial importance to the case company. To address these challenges, the case\nstudy synthesized the organizational and infrastructural needs of the\norganization in a requirements specification describing a technical\ninfrastructure, also known as a software forge, with an adapted organizational\ncontext and work process.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.00222,regular,pre_llm,2022,7,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'Bayesian causal inference in automotive software engineering and online\n  evaluation\n\n  Randomised field experiments, such as A/B testing, have long been the gold\nstandard for evaluating software changes. In the automotive domain, running\nrandomised field experiments is not always desired, possible, or even ethical.\nIn the face of such limitations, we develop a framework BOAT (Bayesian causal\nmodelling for ObvservAtional Testing), utilising observational studies in\ncombination with Bayesian causal inference, in order to understand real-world\nimpacts from complex automotive software updates and help software development\norganisations arrive at causal conclusions. In this study, we present three\ncausal inference models in the Bayesian framework and their corresponding cases\nto address three commonly experienced challenges of software evaluation in the\nautomotive domain. We develop the BOAT framework with our industry\ncollaborator, and demonstrate the potential of causal inference by conducting\nempirical studies on a large fleet of vehicles. Moreover, we relate the causal\nassumption theories to their implications in practise, aiming to provide a\ncomprehensive guide on how to apply the causal models in automotive software\nengineering. We apply Bayesian propensity score matching for producing balanced\ncontrol and treatment groups when we do not have access to the entire user\nbase, Bayesian regression discontinuity design for identifying covariate\ndependent treatment assignments and the local treatment effect, and Bayesian\ndifference-in-differences for causal inference of treatment effect overtime and\nimplicitly control unobserved confounding factors. Each one of the\ndemonstrative case has its grounds in practise, and is a scenario experienced\nwhen randomisation is not feasible. With the BOAT framework, we enable online\nsoftware evaluation in the automotive domain without the need of a fully\nrandomised experiment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.00488,review,pre_llm,2022,7,"{'ai_likelihood': 1.7186005910237632e-05, 'text': ""Editorial: Special Issue on Collaborative Aspects of Open Data in\n  Software EngineeringJohan\n\n  High-quality data has become increasingly important to software engineers in\ndesigning and implementing today's software, for example, as an input to\nmachine-learning algorithms and visualisation- and analytics-based features.\nOpen data - i.e., data shared under a licence that gives users the right to\nstudy, process, and distribute the data to anyone and for any purpose - offers\na mechanism to address this need. Data may originate from multiple sources,\nwhether crowdsourced, shared by government agencies, or shared between\ncommercial entities, and is undoubtedly inherent to all business and revenue\nmodels across the public sector, business and industry today. In this guest\neditorial for the Special Issue on Collaborative Aspects of Open Data in\nSoftware Engineering, we explore the collaborative aspects of open data in\nsoftware engineering. We highlight how these aspects can benefit organisations,\nwhat challenges may exist and how these may be addressed based on current\npractice, and introduce the four papers included in this special issue.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.11784,regular,pre_llm,2022,7,"{'ai_likelihood': 1.986821492513021e-07, 'text': ""CARGO: AI-Guided Dependency Analysis for Migrating Monolithic\n  Applications to Microservices Architecture\n\n  Microservices Architecture (MSA) has become a de-facto standard for designing\ncloud-native enterprise applications due to its efficient infrastructure setup,\nservice availability, elastic scalability, dependability, and better security.\nExisting (monolithic) systems must be decomposed into microservices to harness\nthese characteristics. Since manual decomposition of large scale applications\ncan be laborious and error-prone, AI-based systems to detect decomposition\nstrategies are gaining popularity. However, the usefulness of these approaches\nis limited by the expressiveness of the program representation and their\ninability to model the application's dependency on critical external resources\nsuch as databases. Consequently, partitioning recommendations offered by\ncurrent tools result in architectures that result in (a) distributed monoliths,\nand/or (b) force the use of (often criticized) distributed transactions. This\nwork attempts to overcome these challenges by introducing CARGO({short for\n[C]ontext-sensitive l[A]bel p[R]opa[G]ati[O]n})-a novel un-/semi-supervised\npartition refinement technique that uses a context- and flow-sensitive system\ndependency graph of the monolithic application to refine and thereby enrich the\npartitioning quality of the current state-of-the-art algorithms. CARGO was used\nto augment four state-of-the-art microservice partitioning techniques that were\napplied on five Java EE applications (including one industrial scale\nproprietary project). Experiments demostrate that CARGO can improve the\npartition quality of all modern microservice partitioning techniques. Further,\nCARGO substantially reduces distributed transactions and a real-world\nperformance evaluation of a benchmark application (deployed under varying\nloads) shows that CARGO also lowers the overall the latency of the deployed\nmicroservice application by 11% and increases throughput by 120% on average.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.01047,regular,pre_llm,2022,7,"{'ai_likelihood': 1.986821492513021e-06, 'text': 'An Empirical Study of Flaky Tests in JavaScript\n\n  Flaky tests (tests with non-deterministic outcomes) can be problematic for\ntesting efficiency and software reliability. Flaky tests in test suites can\nalso significantly delay software releases. There have been several studies\nthat attempt to quantify the impact of test flakiness in different programming\nlanguages (e.g., Java and Python) and application domains (e.g., mobile and\nGUI-based). In this paper, we conduct an empirical study of the state of flaky\ntests in JavaScript. We investigate two aspects of flaky tests in JavaScript\nprojects: the main causes of flaky tests in these projects and common fixing\nstrategies. By analysing 452 commits from large, top-scoring JavaScript\nprojects from GitHub, we found that flakiness caused by concurrency-related\nissues (e.g., async wait, race conditions or deadlocks) is the most dominant\nreason for test flakiness. The other top causes of flaky tests are operating\nsystem-specific (e.g., features that work on specific OS or OS versions) and\nnetwork stability (e.g., internet availability or bad socket management). In\nterms of how flaky tests are dealt with, the majority of those flaky tests\n(>80%) are fixed to eliminate flaky behaviour and developers sometimes skip,\nquarantine or remove flaky tests.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.02776,regular,pre_llm,2022,7,"{'ai_likelihood': 3.543164994981554e-06, 'text': 'Using Microservice Telemetry Data for System Dynamic Analysis\n\n  Microservices bring various benefits to software systems. They also bring\ndecentralization and lose coupling across self-contained system parts. Since\nthese systems likely evolve in a decentralized manner, they need to be\nmonitored to identify when possibly poorly designed extensions deteriorate the\noverall system quality. For monolith systems, such tasks have been commonly\naddressed through static analysis. However, given the decentralization and\npossible language diversity across microservices, static analysis tools are\nlacking. On the other hand, there are available tools commonly used by\npractitioners that offer centralized logging, tracing, and metric collection\nfor microservices. In this paper, we assess the opportunity to combine current\ndynamic analysis tools with anomaly detection in the form of quality metrics\nand anti-patterns. We develop a tool prototype that we use to assess a large\nmicroservice system benchmark demonstrating the feasibility and potential of\nsuch an approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.00332,review,pre_llm,2022,7,"{'ai_likelihood': 5.629327562120226e-07, 'text': ""GitHub Marketplace for Practitioners and Researchers to Date: A\n  Systematic Analysis of the Knowledge Mobilization Gap in Open Source Software\n  Automation\n\n  Marketplaces for distributing software products and services have been\ngetting increasing popularity. GitHub, which is most known for its version\ncontrol functionality through Git, launched its own marketplace in 2017. GitHub\nMarketplace hosts third party apps and actions to automate workflows in\nsoftware teams. Currently, this marketplace hosts 440 Apps and 7,878 Actions\nacross 32 different categories. Overall, 419 Third party developers released\ntheir apps on this platform which 111 distinct customers adopted. The\npopularity and accessibility of GitHub projects have made this platform and the\nprojects hosted on it one of the most frequent subjects for experimentation in\nthe software engineering research. A simple Google Scholar search shows that\n24,100 Research papers have discussed GitHub within the Software Engineering\nfield since 2017, but none have looked into the marketplace. The GitHub\nMarketplace provides a unique source of information on the tools used by the\npractitioners in the Open Source Software (OSS) ecosystem for automating their\nproject's workflow. In this study, we (i) mine and provide a descriptive\noverview of the GitHub Marketplace, (ii) perform a systematic mapping of\nresearch studies in automation for open source software, and (iii) compare the\nstate of the art with the state of the practice on the automation tools. We\nconclude the paper by discussing the potential of GitHub Marketplace for\nknowledge mobilization and collaboration within the field. This is the first\nstudy on the GitHub Marketplace in the field.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.00062,regular,pre_llm,2022,7,"{'ai_likelihood': 1.556343502468533e-06, 'text': ""A method for analyzing stakeholders' influence on an open source\n  software ecosystem's requirements engineering process\n\n  For a firm in an open source software (OSS) ecosystem, the requirements\nengineering (RE) process is rather multifaceted. Apart from its typical RE\nprocess, there is a competing process, external to the firm and inherent to the\nfirm's ecosystem. When trying to impose an agenda in competition with other\nfirms, and aiming to align internal product planning with the ecosystem's RE\nprocess, firms need to consider who and how influential the other stakeholders\nare, and what their agendas are. The aim of the presented research is to help\nfirms identify and analyze stakeholders in OSS ecosystems, in terms of their\ninfluence and interactions, to create awareness of their agendas, their\ncollaborators, and how they invest their resources. To arrive at a solution\nartifact, we applied a design science research approach where we base artifact\ndesign on the literature and earlier work. A stakeholder influence analysis\n(SIA) method is proposed and demonstrated in terms of applicability and utility\nthrough a case study on the Apache Hadoop OSS ecosystem. SIA uses social\nnetwork constructs to measure the stakeholders' influence and interactions and\nconsiders the special characteristics of OSS RE to help firms structure their\nstakeholder analysis processes in relation to an OSS ecosystem. SIA adds a\nstrategic aspect to the stakeholder analysis process by addressing the concepts\nof influence and interactions, which are important to consider while acting in\ncollaborative and meritocratic RE cultures of OSS ecosystems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.00462,regular,pre_llm,2022,7,"{'ai_likelihood': 2.317958407931858e-07, 'text': ""The Unnecessity of Assuming Statistically Independent Tests in Bayesian\n  Software Reliability Assessments\n\n  When assessing a software-based system, the results of Bayesian statistical\ninference on operational testing data can provide strong support for software\nreliability claims. For inference, this data (i.e. software successes and\nfailures) is often assumed to arise in an independent, identically distributed\n(i.i.d.) manner. In this paper we show how conservative Bayesian approaches\nmake this assumption unnecessary, by incorporating one's doubts about the\nassumption into the assessment. We derive conservative confidence bounds on a\nsystem's probability of failure on demand (pfd), when operational testing\nreveals no failures. The generality and utility of the confidence bounds are\nillustrated in the assessment of a nuclear power-plant safety-protection\nsystem, under varying levels of skepticism about the i.i.d. assumption. The\nanalysis suggests that the i.i.d. assumption can make Bayesian reliability\nassessments extremely optimistic - such assessments do not explicitly account\nfor how software can be very likely to exhibit no failures during extensive\noperational testing despite the software's pfd being undesirably large.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.02995,review,pre_llm,2022,7,"{'ai_likelihood': 1.1258655124240452e-06, 'text': ""An Overview on Designs and Applications of Context-Aware Automation\n  Systems\n\n  Automation systems are increasingly being used in dynamic and various\noperating conditions. With higher flexibility demands, they need to promptly\nrespond to surrounding dynamic changes by adapting their operation. Context\ninformation collected during runtime can be useful to enhance the system's\nadaptability. Context-aware systems represent a design paradigm for modeling\nand applying context in various applications such as decision-making. In order\nto address context for automation systems, a state of the art assessment of\nexisting approaches is necessary. Thus, the objective of this work is to\nprovide an overview on the design and applications of context and context\nmodels for automation systems. A systematic literature review has been\nconducted, the results of which are represented as a knowledge graph.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.11057,regular,pre_llm,2022,7,"{'ai_likelihood': 1.1920928955078125e-06, 'text': 'Efficient Prior Publication Identification for Open Source Code\n\n  Free/Open Source Software (FOSS) enables large-scale reuse of preexisting\nsoftware components. The main drawback is increased complexity in software\nsupply chain management. A common approach to tame such complexity is automated\nopen source compliance, which consists in automating the verication of\nadherence to various open source management best practices about license\nobligation fulllment, vulnerability tracking, software composition analysis,\nand nearby concerns.We consider the problem of auditing a source code base to\ndetermine which of its parts have been published before, which is an important\nbuilding block of automated open source compliance toolchains. Indeed, if\nsource code allegedly developed in house is recognized as having been\npreviously published elsewhere, alerts should be raised to investigate where it\ncomes from and whether this entails that additional obligations shall be\nfullled before product shipment.We propose an ecient approach for prior\npublication identication that relies on a knowledge base of known source code\nartifacts linked together in a global Merkle direct acyclic graph and a\ndedicated discovery protocol. We introduce swh-scanner, a source code scanner\nthat realizes the proposed approach in practice using as knowledge base\nSoftware Heritage, the largest public archive of source code artifacts. We\nvalidate experimentally the proposed approach, showing its eciency in both\nabstract (number of queries) and concrete terms (wall-clock time), performing\nbenchmarks on 16 845 real-world public code bases of various sizes, from small\nto very large.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.01124,regular,pre_llm,2022,7,"{'ai_likelihood': 5.728668636745877e-06, 'text': 'Characterizing Python Library Migrations\n\n  Developers heavily rely on Application Programming Interfaces (APIs) from\nlibraries to build their software. As software evolves, developers may need to\nreplace the used libraries with alternate libraries, a process known as library\nmigration. Doing this manually can be tedious, time-consuming, and prone to\nerrors. Automated migration techniques can help alleviate some of this burden.\nHowever, designing effective automated migration techniques requires\nunderstanding the types of code changes required to transform client code that\nused the old library to the new library. This paper contributes an empirical\nstudy that provides a holistic view of Python library migrations, both in terms\nof the code changes required in a migration and the typical development effort\ninvolved. We manually label 3,096 migration-related code changes in 335 Python\nlibrary migrations from 311 client repositories spanning 141 library pairs from\n35 domains. Based on our labeled data, we derive a taxonomy for describing\nmigration-related code changes, PyMigTax. Leveraging PyMigTax and our labeled\ndata, we investigate various characteristics of Python library migrations, such\nas the types of program elements and properties of API mappings, the\ncombinations of types of migration-related code changes in a migration, and the\ntypical development effort required for a migration. Our findings highlight\nvarious potential shortcomings of current library migration tools. For example,\nwe find that 40% of library pairs have API mappings that involve non-function\nprogram elements, while most library migration techniques typically assume that\nfunction calls from the source library will map into (one or more) function\ncalls from the target library. As an approximation for the development effort\ninvolved, we find that, on average, a developer needs to learn about 4 APIs and\n2 API mappings to perform a migration, and ... (truncated)\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.09167,regular,pre_llm,2022,7,"{'ai_likelihood': 4.006756676567926e-06, 'text': 'Visual Notations in Container Orchestrations: An Empirical Study with\n  Docker Compose\n\n  Context: Container orchestration tools supporting infrastructure-as-code\nallow new forms of collaboration between developers and operatives. Still,\ntheir text-based nature permits naive mistakes and is more difficult to read as\ncomplexity increases. We can find few examples of low-code approaches for\ndefining the orchestration of containers, and there seems to be a lack of\nempirical studies showing the benefits and limitations of such approaches. Goal\n& method: We hypothesize that a complete visual notation for Docker-based\norchestrations could reduce the effort, the error rate, and the development\ntime. Therefore, we developed a tool featuring such a visual notation for\nDocker Compose configurations, and we empirically evaluated it in a controlled\nexperiment with novice developers. Results: The results show a significant\nreduction in development time and error-proneness when defining Docker Compose\nfiles, supporting our hypothesis. The participants also thought the prototype\neasier to use and useful, and wanted to use it in the future.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.09092,review,pre_llm,2022,7,"{'ai_likelihood': 4.006756676567926e-06, 'text': 'IoT Anomaly Detection Methods and Applications: A Survey\n\n  Ongoing research on anomaly detection for the Internet of Things (IoT) is a\nrapidly expanding field. This growth necessitates an examination of application\ntrends and current gaps. The vast majority of those publications are in areas\nsuch as network and infrastructure security, sensor monitoring, smart home, and\nsmart city applications and are extending into even more sectors. Recent\nadvancements in the field have increased the necessity to study the many IoT\nanomaly detection applications. This paper begins with a summary of the\ndetection methods and applications, accompanied by a discussion of the\ncategorization of IoT anomaly detection algorithms. We then discuss the current\npublications to identify distinct application domains, examining papers chosen\nbased on our search criteria. The survey considers 64 papers among recent\npublications published between January 2019 and July 2021. In recent\npublications, we observed a shortage of IoT anomaly detection methodologies,\nfor example, when dealing with the integration of systems with various sensors,\ndata and concept drifts, and data augmentation where there is a shortage of\nGround Truth data. Finally, we discuss the present such challenges and offer\nnew perspectives where further research is required.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.01971,regular,pre_llm,2022,8,"{'ai_likelihood': 4.371007283528646e-06, 'text': 'API Usage Recommendation via Multi-View Heterogeneous Graph\n  Representation Learning\n\n  Developers often need to decide which APIs to use for the functions being\nimplemented. With the ever-growing number of APIs and libraries, it becomes\nincreasingly difficult for developers to find appropriate APIs, indicating the\nnecessity of automatic API usage recommendation. Previous studies adopt\nstatistical models or collaborative filtering methods to mine the implicit API\nusage patterns for recommendation. However, they rely on the occurrence\nfrequencies of APIs for mining usage patterns, thus prone to fail for the\nlow-frequency APIs. Besides, prior studies generally regard the API call\ninteraction graph as homogeneous graph, ignoring the rich information (e.g.,\nedge types) in the structure graph. In this work, we propose a novel method\nnamed MEGA for improving the recommendation accuracy especially for the\nlow-frequency APIs. Specifically, besides call interaction graph, MEGA\nconsiders another two new heterogeneous graphs: global API co-occurrence graph\nenriched with the API frequency information and hierarchical structure graph\nenriched with the project component information. With the three multi-view\nheterogeneous graphs, MEGA can capture the API usage patterns more accurately.\nExperiments on three Java benchmark datasets demonstrate that MEGA\nsignificantly outperforms the baseline models by at least 19% with respect to\nthe Success Rate@1 metric. Especially, for the low-frequency APIs, MEGA also\nincreases the baselines by at least 55% regarding the Success Rate@1.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.06234,regular,pre_llm,2022,8,"{'ai_likelihood': 2.5828679402669272e-06, 'text': 'Ad Hoc HLA Simulation Model Derived From a Model-Based Traffic Scenario\n\n  Modern highly automated and autonomous traffic systems and subsystems require\nnew approaches to test their functional safety in the context of validation and\nverification. One approach that has taken a leading role in current research is\nscenario-based testing. For various reasons, simulation is considered to be the\nmost practicable solution for a wide range of test scenarios. However, this is\nwhere many existing simulation systems in research reach their limits. In order\nto be able to integrate the widest possible range of systems to be tested into\nthe simulation, the use of co-simulation has proven to be particularly useful.\nIn this work, the High Level Architecture defined in the IEEE 1516-2010\nstandard is specifically addressed and a concept is developed that establishes\nthe foundation for the feasible use of scenario-based distributed co-simulation\non its basis. The main challenge identified and addressed is the resolution of\nthe double-sided dependency between scenario and simulation models. The\nsolution was to fully automate the generation and instantiation of the\nsimulation environment on the basis of a scenario instance. Finally, the\ndeveloped concept was implemented as a prototype and the resulting process for\nits use is presented here using an example scenario. Based on the experience\ngained during the creation of the concept and the prototype, the next steps for\nfuture work are outlined in conclusion.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.10492,regular,pre_llm,2022,8,"{'ai_likelihood': 4.735257890489366e-06, 'text': 'Improving Counterexample Quality from Failed Program Verification\n\n  In software verification, a successful automated program proof is the\nultimate triumph. The road to such success is, however, paved with many failed\nproof attempts. The message produced by the prover when a proof fails is often\nobscure, making it very hard to know how to proceed further. The work reported\nhere attempts to help in such cases by providing immediately understandable\ncounterexamples.\n  To this end, it introduces an approach called Counterexample Extraction and\nMinimization (CEAM). When a proof fails, CEAM turns the counterexample model\ngenerated by the prover into a a clearly understandable version; it can in\naddition simplify the counterexamples further by minimizing the integer values\nthey contain. We have implemented the CEAM approach as an extension to the\nAutoProof verifier and demonstrate its application to a collection of examples.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.11352,regular,pre_llm,2022,8,"{'ai_likelihood': 3.082884682549371e-05, 'text': 'Ai4EComponentLib.jl: A Component-base Model Library in Julia\n\n  Ai4EComponentLib.jl(Ai4EComponentLib) is a component-base model library based\non Julia language, which relies on the differential equation solver\nDifferentialEquations.jl and the symbolic modeling tool Modelingtoolkit.jl. To\nhandle problems in different physical domains, Ai4EComponentLib tries to build\nthem with component-base model. Supported by a new generation of symbolic\nmodeling tools, models built with Ai4EComponentLib are more flexible and\nscalable than models built with traditional tools like Modelica. This paper\nwill introduce the instance and general modeling methods of Ai4EComponentLib\nmodel library.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.14679,regular,pre_llm,2022,8,"{'ai_likelihood': 3.311369154188368e-08, 'text': ""Mapping aids using source location tracking increase novices'\n  performance in programming cyber-physical systems\n\n  Novices need to overcome initial barriers while programming cyber-physical\nsystems behavior, like coding quadcopter missions, and should thus be supported\nby an adequately designed programming environment. Using multiple\nrepresentations by including graphical previews is a common approach to ease\ncoding and program understanding. However, novices struggle to map information\nof the code and graphical previews. Previous studies imply that mapping aids in\na live programming environment might support novices while programming and\nfoster a deeper understanding of the content. To implement these mapping aids\nin a domain independent way Source Location Tracking based on run-time\ninformation can be used. In our study, we tested N=82 participants while\ninteracting and learning in an online programming environment. Using our 2x2\nbetween-subject design study, we investigated the effects of two mapping aids:\nhighlighting and dynamic linking on coding correctness including typical\nerrors, and learning outcomes. Based on process data, successful strategies\nwere analyzed. Combining both mapping aids compared to one aid resulted in\nhigher performance. While highlights were more helpful for implementing the\nquadcopter missions, dynamic linking improved learning outcomes on the\ncomprehension and application level . Traces of learning strategies were\nrelated to higher coding correctness and higher learning outcomes. Based on\nprocess data, users in the group with both aids had a higher chance of avoiding\ncertain typical implementation mistakes. Implementing dynamic linking and\nhighlighting through source location tracking is a promising approach to\nsupport novices to develop a better semantic understanding of the domain\nspecific language. Depending on the coding tasks different mapping aids might\nbe effective.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.03136,regular,pre_llm,2022,8,"{'ai_likelihood': 4.569689432779948e-06, 'text': 'A Method for Deriving Technical Requirements of Digital Twins as\n  Industrial Product-Service System Enablers\n\n  Industrial Product-Service Systems (IPSS) are increasingly dominant in\nseveral sectors. Predominant value-adding services provided for industrial\nassets such as production systems, electric power plants, and car fleets are\nremote asset maintenance, monitoring, control, and reconfiguration. IPSS\ndesigners lack methods and tools supporting them in systematically deriving\ntechnical design requirements for the underlying Cyber-Physical System (CPS)\nIPSS services. At the same time, the use of Digital Twins (DTs) as digital\nrepresentations of CPS as-sets is becoming increasingly feasible thanks to\npowerful, networked information technology (IT) and operation technology (OT)\ninfrastructures and the ubiquity of sensors and data. This paper proposes a\nmethod for guiding IPSS designers in the specification and implementation of DT\ninstances to serve as the key enablers of IPSS services. The systematic mapping\nof the continuous IT design-build-deployment cycle concept to the OT domain of\nCPS is at the heart of the applied methodology, which is complemented by a\nstakeholder-driven requirements elicitation. The key contribution is a\nstructured method for deriving technical design requirements for DT instances\nas IPSS. This method is validated on real-world use cases in an evaluation\nenvironment for distributed CPS IPSS.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.13632,regular,pre_llm,2022,8,"{'ai_likelihood': 2.980232238769531e-07, 'text': ""Neuroevolution-Based Generation of Tests and Oracles for Games\n\n  Game-like programs have become increasingly popular in many software\nengineering domains such as mobile apps, web applications, or programming\neducation. However, creating tests for programs that have the purpose of\nchallenging human players is a daunting task for automatic test generators.\nEven if test generation succeeds in finding a relevant sequence of events to\nexercise a program, the randomized nature of games means that it may neither be\npossible to reproduce the exact program behavior underlying this sequence, nor\nto create test assertions checking if observed randomized game behavior is\ncorrect. To overcome these problems, we propose Neatest, a novel test generator\nbased on the NeuroEvolution of Augmenting Topologies (NEAT) algorithm. Neatest\nsystematically explores a program's statements, and creates neural networks\nthat operate the program in order to reliably reach each statement -- that is,\nNeatest learns to play the game in a way to reliably cover different parts of\nthe code. As the networks learn the actual game behavior, they can also serve\nas test oracles by evaluating how surprising the observed behavior of a program\nunder test is compared to a supposedly correct version of the program. We\nevaluate this approach in the context of Scratch, an educational programming\nenvironment. Our empirical study on 25 non-trivial Scratch games demonstrates\nthat our approach can successfully train neural networks that are not only far\nmore resilient to random influences than traditional test suites consisting of\nstatic input sequences, but are also highly effective with an average mutation\nscore of more than 65%.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.07624,regular,pre_llm,2022,8,"{'ai_likelihood': 1.655684577094184e-07, 'text': ""Don't Reinvent the Wheel: Towards Automatic Replacement of Custom\n  Implementations with APIs\n\n  Reusing code is a common practice in software development: It helps\ndevelopers speedup the implementation task while also reducing the chances of\nintroducing bugs, given the assumption that the reused code has been tested,\npossibly in production. Despite these benefits, opportunities for reuse are not\nalways in plain sight and, thus, developers may miss them. We present our\npreliminary steps in building RETIWA, a recommender able to automatically\nidentify custom implementations in a given project that are good candidates to\nbe replaced by open source APIs. RETIWA relies on a ``knowledge base''\nconsisting of real examples of custom implementation-to-API replacements. In\nthis work, we present the mining strategy we tailored to automatically and\nreliably extract replacements of custom implementations with APIs from open\nsource projects. This is the first step towards building the envisioned\nrecommender.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.00677,regular,pre_llm,2022,8,"{'ai_likelihood': 3.4769376118977866e-06, 'text': 'Similarity-based web element localization for robust test automation\n\n  Non-robust (fragile) test execution is a commonly reported challenge in\nGUI-based test automation, despite much research and several proposed\nsolutions. A test script needs to be resilient to (minor) changes in the tested\napplication but, at the same time, fail when detecting potential issues that\nrequire investigation. Test script fragility is a multi-faceted problem, but\none crucial challenge is reliably identifying and locating the correct target\nweb elements when the website evolves between releases or otherwise fails and\nreports an issue. This paper proposes and evaluates a novel approach called\nsimilarity-based web element localization (Similo), which leverages information\nfrom multiple web element locator parameters to identify a target element using\na weighted similarity score. The experimental study compares Similo to a\nbaseline approach for web element localization. To get an extensive empirical\nbasis, we target 40 of the most popular websites on the Internet in our\nevaluation. Robustness is considered by counting the number of web elements\nfound in a recent website version compared to how many of these existed in an\nolder version. Results of the experiment show that Similo outperforms the\nbaseline representing the current state-of-the-art; it failed to locate the\ncorrect target web element in 72 out of 598 considered cases compared to 146\nfailed cases for the baseline approach. This study presents evidence that\nquantifying the similarity between multiple attributes of web elements when\ntrying to locate them, as in our proposed Similo approach, is beneficial. With\nacceptable efficiency, Similo gives significantly higher effectiveness (i.e.,\nrobustness) than the baseline web element localization approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.12164,review,pre_llm,2022,8,"{'ai_likelihood': 2.7484363979763457e-06, 'text': 'Sistematic mapping protocol - estimation accuracy on software\n  development using agile technologies\n\n  Protocol for a Systematic Mapping of the Literature, which aims to identify\nand classify the estimations techniques used in software development agile\nmethodologies based on the results found, and to compare their estimation\naccuracies against those obtained in traditional software development\nmethodologies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.12797,review,pre_llm,2022,8,"{'ai_likelihood': 2.3510720994737412e-06, 'text': 'Half-Empty Offices in Flexible Work Arrangements: Why are Employees Not\n  Returning?\n\n  Although the pandemic times of the world-wide forced working from home seem\nto be in the past, many knowledge workers choose to continue working\npredominantly from home as a partial or permanent practice. Related studies\nshow that employees of companies from various industries, diverse in size and\nlocation, prefer to alter working in the office with working at home, coined as\nhybrid or flexible working arrangements. As a result, the post-pandemic times\nare associated with empty offices, confused managers and organizational leaders\nnot knowing what to do with the often-expensive rental contracts. In this\npaper, we investigate the employee presence in the offices in two software\ncompanies and dive deeper into the reasons behind the preferences to work\nremotely, practices that help to attract employees back into the offices and,\nin cases when this is not possible, the ways companies can repurpose the office\nspace for the future needs of their employees. The latter are based on the\nqualitative analysis of interviews and survey responses. Our findings suggest\nthat since the fall 2021 the offices were half-empty and that, on average, the\ndaily office presence varies between 15-30%. The reasons for working remotely\ninclude behavioural and practical motivations, as well as factors related to\noffice equipment and facilities, and the nature of the work tasks. Finally, we\ndiscuss the practical implications of our findings on the future work\narrangements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.14656,regular,pre_llm,2022,8,"{'ai_likelihood': 1.0497040218777127e-05, 'text': 'LawBreaker: An Approach for Specifying Traffic Laws and Fuzzing\n  Autonomous Vehicles\n\n  Autonomous driving systems (ADSs) must be tested thoroughly before they can\nbe deployed in autonomous vehicles. High-fidelity simulators allow them to be\ntested against diverse scenarios, including those that are difficult to\nrecreate in real-world testing grounds. While previous approaches have shown\nthat test cases can be generated automatically, they tend to focus on weak\noracles (e.g. reaching the destination without collisions) without assessing\nwhether the journey itself was undertaken safely and satisfied the law. In this\nwork, we propose LawBreaker, an automated framework for testing ADSs against\nreal-world traffic laws, which is designed to be compatible with different\nscenario description languages. LawBreaker provides a rich driver-oriented\nspecification language for describing traffic laws, and a fuzzing engine that\nsearches for different ways of violating them by maximising specification\ncoverage. To evaluate our approach, we implemented it for Apollo+LGSVL and\nspecified the traffic laws of China. LawBreaker was able to find 14 violations\nof these laws, including 173 test cases that caused accidents.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.01105,review,pre_llm,2022,8,"{'ai_likelihood': 4.0398703681098095e-06, 'text': ""How to characterize the health of an Open Source Software project? A\n  snowball literature review of an emerging practice\n\n  Motivation: Society's dependence on Open Source Software (OSS) and the\ncommunities that maintain the OSS is ever-growing. So are the potential risks\nof, e.g., vulnerabilities being introduced in projects not actively maintained.\nBy assessing an OSS project's capability to stay viable and maintained over\ntime without interruption or weakening, i.e., the OSS health, users can\nconsider the risk implied by using the OSS as is, and if necessary, decide\nwhether to help improve the health or choose another option. However, such\nassessment is complex as OSS health covers a wide range of sub-topics, and\nexisting support is limited. Aim: We aim to create an overview of\ncharacteristics that affect the health of an OSS project and enable the\nassessment thereof. Method: We conduct a snowball literature review based on a\nstart set of 9 papers, and identify 146 relevant papers over two iterations of\nforward and backward snowballing. Health characteristics are elicited and coded\nusing structured and axial coding into a framework structure. Results: The\nfinal framework consists of 104 health characteristics divided among 15 themes.\nCharacteristics address the socio-technical spectrum of the community of actors\nmaintaining the OSS project, the software and other deliverables being\nmaintained, and the orchestration facilitating the maintenance. Characteristics\nare further divided based on the level of abstraction they address, i.e., the\nOSS project-level specifically, or the project's overarching ecosystem of\nrelated OSS projects. Conclusion: The framework provides an overview of the\nwide span of health characteristics that may need to be considered when\nevaluating OSS health and can serve as a foundation both for research and\npractice.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.02815,regular,pre_llm,2022,8,"{'ai_likelihood': 6.622738308376736e-07, 'text': ""On-the-Fly Syntax Highlighting using Neural Networks\n\n  With the presence of online collaborative tools for software developers,\nsource code is shared and consulted frequently, from code viewers to merge\nrequests and code snippets. Typically, code highlighting quality in such\nscenarios is sacrificed in favor of system responsiveness. In these on-the-fly\nsettings, performing a formal grammatical analysis of the source code is not\nonly expensive, but also intractable for the many times the input is an invalid\nderivation of the language. Indeed, current popular highlighters heavily rely\non a system of regular expressions, typically far from the specification of the\nlanguage's lexer. Due to their complexity, regular expressions need to be\nperiodically updated as more feedback is collected from the users and their\ndesign unwelcome the detection of more complex language formations. This paper\ndelivers a deep learning-based approach suitable for on-the-fly grammatical\ncode highlighting of correct and incorrect language derivations, such as code\nviewers and snippets. It focuses on alleviating the burden on the developers,\nwho can reuse the language's parsing strategy to produce the desired\nhighlighting specification. Moreover, this approach is compared to nowadays\nonline syntax highlighting tools and formal methods in terms of accuracy and\nexecution time, across different levels of grammatical coverage, for three\nmainstream programming languages. The results obtained show how the proposed\napproach can consistently achieve near-perfect accuracy in its predictions,\nthereby outperforming regular expression-based strategies.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.00398,regular,pre_llm,2022,8,"{'ai_likelihood': 3.1888484954833984e-05, 'text': 'End-to-End Rationale Reconstruction\n\n  The logic behind design decisions, called design rationale, is very valuable.\nIn the past, researchers have tried to automatically extract and exploit this\ninformation, but prior techniques are only applicable to specific contexts and\nthere is insufficient progress on an end-to-end rationale information\nextraction pipeline. Here we outline a path towards such a pipeline that\nleverages several Machine Learning (ML) and Natural Language Processing (NLP)\ntechniques. Our proposed context-independent approach, called Kantara, produces\na knowledge graph representation of decisions and of their rationales, which\nconsiders their historical evolution and traceability. We also propose\nvalidation mechanisms to ensure the correctness of the extracted information\nand the coherence of the development process. We conducted a preliminary\nevaluation of our proposed approach on a small example sourced from the Linux\nKernel, which shows promising results.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.13424,regular,pre_llm,2022,8,"{'ai_likelihood': 3.1458006964789497e-06, 'text': 'BFL: a Logic to Reason about Fault Trees\n\n  Safety-critical infrastructures must operate safely and reliably. Fault tree\nanalysis is a widespread method used to assess risks in these systems: fault\ntrees (FTs) are required - among others - by the Federal Aviation Authority,\nthe Nuclear Regulatory Commission, in the ISO26262 standard for autonomous\ndriving and for software development in aerospace systems. Although popular\nboth in industry and academia, FTs lack a systematic way to formulate powerful\nand understandable analysis queries. In this paper, we aim to fill this gap and\nintroduce Boolean Fault tree Logic (BFL), a logic to reason about FTs. BFL is a\nsimple, yet expressive logic that supports easier formulation of complex\nscenarios and specification of FT properties. Alongside BFL, we present model\nchecking algorithms based on binary decision diagrams (BDDs) to analyse\nspecified properties in BFL, patterns and an algorithm to construct\ncounterexamples. Finally, we propose a case-study application of BFL by\nanalysing a COVID19-related FT.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.01317,review,pre_llm,2022,8,"{'ai_likelihood': 1.0927518208821616e-06, 'text': ""An Exploratory Study of Documentation Strategies for Product Features in\n  Popular GitHub Projects\n\n  [Background] In large open-source software projects, development knowledge is\noften fragmented across multiple artefacts and contributors such that\nindividual stakeholders are generally unaware of the full breadth of the\nproduct features. However, users want to know what the software is capable of,\nwhile contributors need to know where to fix, update, and add features.\n[Objective] This work aims at understanding how feature knowledge is documented\nin GitHub projects and how it is linked (if at all) to the source code.\n[Method] We conducted an in-depth qualitative exploratory content analysis of\n25 popular GitHub repositories that provided the documentation artefacts\nrecommended by GitHub's Community Standards indicator. We first extracted\nstrategies used to document software features in textual artefacts and then\nstrategies used to link the feature documentation with source code. [Results]\nWe observed feature documentation in all studied projects in artefacts such as\nREADMEs, wikis, and website resource files. However, the features were often\ndescribed in an unstructured way. Additionally, tracing techniques to connect\nfeature documentation and source code were rarely used. [Conclusions] Our\nresults suggest a lacking (or a low-prioritised) feature documentation in\nopen-source projects, little use of normalised structures, and a rare explicit\nreferencing to source code. As a result, product feature traceability is likely\nto be very limited, and maintainability to suffer over time.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.12308,regular,pre_llm,2022,8,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'Continuous Deep Learning: A Workflow to Bring Models into Production\n\n  Researchers have been highly active to investigate the classical machine\nlearning workflow and integrate best practices from the software engineering\nlifecycle. However, deep learning exhibits deviations that are not yet covered\nin this conceptual development process. This includes the requirement of\ndedicated hardware, dispensable feature engineering, extensive hyperparameter\noptimization, large-scale data management, and model compression to reduce size\nand inference latency. Individual problems of deep learning are under thorough\nexamination, and numerous concepts and implementations have gained traction.\nUnfortunately, the complete end-to-end development process still remains\nunspecified. In this paper, we define a detailed deep learning workflow that\nincorporates the aforementioned characteristics on the baseline of the\nclassical machine learning workflow. We further transferred the conceptual idea\ninto practice by building a prototypic deep learning system using some of the\nlatest technologies on the market. To examine the feasibility of the workflow,\ntwo use cases are applied to the prototype.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.07037,regular,pre_llm,2022,8,"{'ai_likelihood': 3.5100513034396704e-06, 'text': 'A Drift Handling Approach for Self-Adaptive ML Software in Scalable\n  Industrial Processes\n\n  Most industrial processes in real-world manufacturing applications are\ncharacterized by the scalability property, which requires an automated strategy\nto self-adapt machine learning (ML) software systems to the new conditions. In\nthis paper, we investigate an Electroslag Remelting (ESR) use case process from\nthe Uddeholms AB steel company. The use case involves predicting the minimum\npressure value for a vacuum pumping event. Taking into account the long time\nrequired to collect new records and efficiently integrate the new machines with\nthe built ML software system. Additionally, to accommodate the changes and\nsatisfy the non-functional requirement of the software system, namely\nadaptability, we propose an automated and adaptive approach based on a drift\nhandling technique called importance weighting. The aim is to address the\nproblem of adding a new furnace to production and enable the adaptability\nattribute of the ML software. The overall results demonstrate the improvements\nin ML software performance achieved by implementing the proposed approach over\nthe classical non-adaptive approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.0712,regular,pre_llm,2022,8,"{'ai_likelihood': 8.609559800889757e-07, 'text': ""Compressing Pre-trained Models of Code into 3 MB\n\n  Although large pre-trained models of code have delivered significant\nadvancements in various code processing tasks, there is an impediment to the\nwide and fluent adoption of these powerful models in software developers' daily\nworkflow: these large models consume hundreds of megabytes of memory and run\nslowly on personal devices, which causes problems in model deployment and\ngreatly degrades the user experience.\n  It motivates us to propose Compressor, a novel approach that can compress the\npre-trained models of code into extremely small models with negligible\nperformance sacrifice. Our proposed method formulates the design of tiny models\nas simplifying the pre-trained model architecture: searching for a\nsignificantly smaller model that follows an architectural design similar to the\noriginal pre-trained model. Compressor proposes a genetic algorithm (GA)-based\nstrategy to guide the simplification process. Prior studies found that a model\nwith higher computational cost tends to be more powerful. Inspired by this\ninsight, the GA algorithm is designed to maximize a model's Giga floating-point\noperations (GFLOPs), an indicator of the model computational cost, to satisfy\nthe constraint of the target model size. Then, we use the knowledge\ndistillation technique to train the small model: unlabelled data is fed into\nthe large model and the outputs are used as labels to train the small model. We\nevaluate Compressor with two state-of-the-art pre-trained models, i.e.,\nCodeBERT and GraphCodeBERT, on two important tasks, i.e., vulnerability\nprediction and clone detection. We use our method to compress pre-trained\nmodels to a size (3 MB), which is 160$\\times$ smaller than the original size.\nThe results show that compressed CodeBERT and GraphCodeBERT are 4.31$\\times$\nand 4.15$\\times$ faster than the original model at inference, respectively.\nMore importantly, ...\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.08348,review,pre_llm,2022,9,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Some Long-Standing Quality Practices in Software Development\n\n  The desire to build quality software systems has been the focus of most\nsoftware developers and researchers for decades. This has culminated in the\ndesign of practices that promote quality in the designed software. Originating\nfrom the inception of the traditional software development life cycle (SDLC),\nthrough to the object-oriented methods, Iterative development, and now the\nagile methods, these practices have persisted through different periods. Such\npractices play the same quality role regardless of the perspective of the\nsoftware development process they are part of. In this paper we review three\nsoftware development methods representative of the software development\nhistory, with the aim of i) identifying key quality practices, ii) identifying\nthe quality role played by the practice in the method, and iii) noting those\nquality practices that have persisted through the software development history.\nThe identified quality practices that have persisted throughout the history of\nthe software development processes include prototyping, iterative development,\nincremental development, risk-driven development, phase planning, and phase\nretrospection. These results would be useful to method engineers who seek to\ndesign high-quality software development methods as these practices serve as\ncandidates for inclusion in their development processes. Software development\npractitioners seeking to design quality software would also benefit from\nadopting these practices in developing their software.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.14422,regular,pre_llm,2022,9,"{'ai_likelihood': 6.622738308376736e-07, 'text': ""StacerBot: A Stacktrace Search Engine for Stack Overflow\n\n  We as software developers or researchers very often get stacktrace error\nmessages while we are trying to write some code or install some packages. Many\ntimes these error messages are very obscure and verbose; do not make much sense\nto us. There is a good chance that someone else has also faced similar issues\nprobably shared similar stacktrace in various online developers' forums.\nHowever traditional google searches or other search engines are not very\nhelpful to find web pages with similar stacktraces. In order to address this\nproblem, we have developed a web interface; a better search engine: as an\noutcome of this research project where users can find appropriate stack\noverflow posts by submitting the whole stacktrace error message. The current\ndeveloped solution can serve real-time parallel user queries with top-matched\nstack overflow posts within 50 seconds using a server with 300GB RAM. This\nstudy provides a comprehensive overview of the NLP techniques used in this\nstudy and an extensive overview of the research pipeline. This comprehensive\nresult, limitations, and computational overhead mentioned in this study can be\nused by future researchers and software developers to build a better solution\nfor this same problem or similar large-scale text matching-related tasks.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.08719,regular,pre_llm,2022,9,"{'ai_likelihood': 2.1689467959933812e-05, 'text': 'Detecting and Fixing Data Loss Issues in Android Apps\n\n  Android apps are event-driven, and their execution is often interrupted by\nexternal events. This interruption can cause data loss issues that annoy users.\nFor instance, when the screen is rotated, the current app page will be\ndestroyed and recreated. If the app state is improperly preserved, user data\nwill be lost. In this work, we present an approach and tool iFixDataloss that\nautomatically detects and fixes data loss issues in Android apps. To achieve\nthis, we identify scenarios in which data loss issues may occur, develop\nstrategies to reveal data loss issues, and design patch templates to fix them.\nOur experiments on 66 Android apps show iFixDataloss detected 374 data loss\nissues (284 of them were previously unknown) and successfully generated patches\nfor 188 of the 374 issues. Out of 20 submitted patches, 16 have been accepted\nby developers. In comparison with state-of-the-art techniques, iFixDataloss\nperformed significantly better in terms of the number of detected data loss\nissues and the quality of generated patches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.07211,regular,pre_llm,2022,9,"{'ai_likelihood': 3.973642985026042e-07, 'text': ""On the acceptance by code reviewers of candidate security patches\n  suggested by Automated Program Repair tools\n\n  Background: Testing and validation of the semantic correctness of patches\nprovided by tools for Automated Program Repairs (APR) has received a lot of\nattention. Yet, the eventual acceptance or rejection of suggested patches for\nreal world projects by humans patch reviewers has received a limited attention.\nObjective: To address this issue, we plan to investigate whether (possibly\nincorrect) security patches suggested by APR tools are recognized by human\nreviewers. We also want to investigate whether knowing that a patch was\nproduced by an allegedly specialized tool does change the decision of human\nreviewers. Method: In the first phase, using a balanced design, we propose to\nhuman reviewers a combination of patches proposed by APR tools for different\nvulnerabilities and ask reviewers to adopt or reject the proposed patches. In\nthe second phase, we tell participants that some of the proposed patches were\ngenerated by security specialized tools (even if the tool was actually a\n`normal' APR tool) and measure whether the human reviewers would change their\ndecision to adopt or reject a patch. Limitations: The experiment will be\nconducted in an academic setting, and to maintain power, it will focus on a\nlimited sample of popular APR tools and popular vulnerability types.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.05105,review,pre_llm,2022,9,"{'ai_likelihood': 1.357661353217231e-06, 'text': 'Classical to Quantum Software Migration Journey Begins: A Conceptual\n  Readiness Model\n\n  With recent advances in the development of more powerful quantum computers,\nthe re-search area of quantum software engineering is emerging. Quantum\nsoftware plays a critical role in exploiting the full potential of quantum\ncomputing systems. As a result, it has been drawing increasing attention\nrecently to provide concepts, principles, and guidelines to address the ongoing\nchallenges of quantum software development. The importance of the topic\nmotivated us to voice out a call for action to develop a readiness model that\nwill help an organization assess its capability of migration from classic\nsoftware engineering to quan-tum software engineering. The proposed model will\nbe based on the existing multivocal literature, industrial empirical study,\nunderstanding of the process areas, challenging factors and enablers that could\nimpact the quantum software engineering process. We believe that the proposed\nmodel will provide a roadmap for software development organizations to measure\ntheir readiness concerning to transformation from classic to quantum software\nengineering by suggesting best practices and highlighting important process\nareas, challeng-es, and enablers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.03311,regular,pre_llm,2022,9,"{'ai_likelihood': 4.967053731282552e-07, 'text': ""SZZ in the time of Pull Requests\n\n  In the multi-commit development model, programmers complete tasks (e.g.,\nimplementing a feature) by organizing their work in several commits and\npackaging them into a commit-set. Analyzing data from developers using this\nmodel can be useful to tackle challenging developers' needs, such as knowing\nwhich features introduce a bug as well as assessing the risk of integrating\ncertain features in a release. However, to do so one first needs to identify\nfix-inducing commit-sets. For such an identification, the SZZ algorithm is the\nmost natural candidate, but its performance has not been evaluated in the\nmulti-commit context yet. In this study, we conduct an in-depth investigation\non the reliability and performance of SZZ in the multi-commit model. To obtain\na reliable ground truth, we consider an already existing SZZ dataset and adapt\nit to the multi-commit context. Moreover, we devise a second dataset that is\nmore extensive and directly created by developers as well as Quality Assurance\n(QA) engineers of Mozilla. Based on these datasets, we (1) test the performance\nof B-SZZ and its non-language-specific SZZ variations in the context of the\nmulti-commit model, (2) investigate the reasons behind their specific behavior,\nand (3) analyze the impact of non-relevant commits in a commit-set and\nautomatically detect them before using SZZ.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.08165,review,pre_llm,2022,9,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'A Decade of Code Comment Quality Assessment: A Systematic Literature\n  Review\n\n  Code comments are important artifacts in software systems and play a\nparamount role in many software engineering (SE) tasks related to maintenance\nand program comprehension. However, while it is widely accepted that high\nquality matters in code comments just as it matters in source code, assessing\ncomment quality in practice is still an open problem. First and foremost, there\nis no unique definition of quality when it comes to evaluating code comments.\nThe few existing studies on this topic rather focus on specific attributes of\nquality that can be easily quantified and measured. Existing techniques and\ncorresponding tools may also focus on comments bound to a specific programming\nlanguage, and may only deal with comments with specific scopes and clear goals\n(e.g., Javadoc comments at the method level, or in-body comments describing\nTODOs to be addressed). In this paper, we present a Systematic Literature\nReview (SLR) of the last decade of research in SE to answer the following\nresearch questions: (i) What types of comments do researchers focus on when\nassessing comment quality? (ii) What quality attributes (QAs) do they consider?\n(iii) Which tools and techniques do they use to assess comment quality?, and\n(iv) How do they evaluate their studies on comment quality assessment in\ngeneral? Our evaluation, based on the analysis of 2353 papers and the actual\nreview of 47 relevant ones, shows that (i) most studies and techniques focus on\ncomments in Java code, thus may not be generalizable to other languages, and\n(ii) the analyzed studies focus on four main QAs of a total of 21 QAs\nidentified in the literature, with a clear predominance of checking consistency\nbetween comments and the code. We observe that researchers rely on manual\nassessment and specific heuristics rather than the automated assessment of the\ncomment quality attributes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.00162,regular,pre_llm,2022,9,"{'ai_likelihood': 2.947118547227648e-06, 'text': 'Prioritization of Metamorphic Relations to reduce the cost of testing\n\n  An oracle is a mechanism to decide whether the outputs of the program for the\nexecuted test cases are correct. For machine learning programs, such oracle is\nnot available or too difficult to apply. Metamorphic testing is a testing\napproach that uses metamorphic relations, which are necessary properties of the\nsoftware under test to help verify the correctness of a program. Prioritization\nof metamorphic relations helps to reduce the cost of metamorphic testing [1].\nHowever, prioritizing metamorphic relations based on code coverage is often not\neffective for prioritizing MRs for machine learning programs, since the\ndecision logic of a machine learning model is learned from training data, and\n100% code coverage can be easily achieved with a single test input. To this\nend, in this work, we propose a cost-effective approach based on diversity in\nthe source and follow-up data set to prioritize metamorphic relations for\nmachine learning programs. We show that the proposed data diversity-based\nprioritization approach increase the fault detection effectiveness by up to 40%\nwhen compared to the code coverage-based approach and reduce the time taken to\ndetect a fault by 29% when compared to random execution of MRs. Overall, our\napproach leads to saving time and cost during testing.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.12004,review,pre_llm,2022,9,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Systematic Mapping Protocol -- Agile strategies for software development\n  according to technical and environmental complexity factors\n\n  Software development projects management is a complex endeavor because it\nrequires dealing with numerous unforeseen events that constantly arise along\nthe way and that go against the expectations that had been established at the\nbeginning. A good project leader is not so much who carries out what is\nplanned, but rather who is able to deal with all the inconveniences that arise\nand, in the end, achieve a result that is as close as possible to what was\nexpected [1]. In other words, what is most valued is the ability to adapt to\nchanges, to face unforeseen events, to make the best decisions regarding a\nreality that is imposed as the project progresses. On the other hand, those of\nus who dedicate ourselves to IT projects have the tendency to cling to the\ntools that gave us the best results, although many times they are not the most\nappropriate for the case or, if they are, we continue to maintain them even\nwhen the context of the project has changed. We are interested in portraying\nthe state of the art of software development project management with respect to\nthe selection of management strategies based on the complexity of the problem\nto be faced. Our goal is to collect all the available evidence, analyze it, and\nstudy the possibility of guiding in the selection of most appropriate\napproaches, techniques, and tools, depending on the complexity of the problem,\nto better manage projects. In this document we detail the planning phase of a\nSystematic Mapping Study, used to structure the findings on a research area,\nbased on the guidelines from Petersen et al. Our goal is twofold: to identify\nthe evidence present in the scientific literature about criteria to select\nagile or plan-based project management approaches, techniques and tools, and\nthe frameworks used to characterize the projects, in case they are used. We\nwill perform a systematic mapping study of the literature for this purpose.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.02189,review,pre_llm,2022,9,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Object-Oriented Requirements: a Unified Framework for Specifications,\n  Scenarios and Tests\n\n  A paradox of requirements specifications as dominantly practiced in the\nindustry is that they often claim to be object-oriented (OO) but largely rely\non procedural (non-OO) techniques. Use cases and user stories describe\nfunctional flows, not object types. To gain the benefits provided by object\ntechnology (such as extendibility, reusability, reliability), requirements\nshould instead take advantage of the same data abstraction concepts - classes,\ninheritance, information hiding - as OO design and OO programs.\n  Many people find use cases and user stories appealing because of the\nsimplicity and practicality of the concepts. Can we reconcile requirements with\nobject-oriented principles and get the best of both worlds?\n  This article proposes a unified framework. It shows that the concept of class\nis general enough to describe not only ""objects"" in a narrow sense but also\nscenarios such as use cases and user stories and other important artifacts such\nas test cases and oracles.\n  Having a single framework opens the way to requirements that enjoy the\nbenefits of both approaches: like use cases and user stories, they reflect the\npractical views of stakeholders; like object-oriented requirements, they lend\nthemselves to evolution and reuse.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.08978,regular,pre_llm,2022,9,"{'ai_likelihood': 4.76837158203125e-06, 'text': ""MMF3: Neural Code Summarization Based on Multi-Modal Fine-Grained\n  Feature Fusion\n\n  Background: Code summarization automatically generates the corresponding\nnatural language descriptions according to the input code. Comprehensiveness of\ncode representation is critical to code summarization task. However, most\nexisting approaches typically use coarse-grained fusion methods to integrate\nmulti-modal features. They generally represent different modalities of a piece\nof code, such as an Abstract Syntax Tree (AST) and a token sequence, as two\nembeddings and then fuse the two ones at the AST/code levels. Such a coarse\nintegration makes it difficult to learn the correlations between fine-grained\ncode elements across modalities effectively. Aims: This study intends to\nimprove the model's prediction performance for high-quality code summarization\nby accurately aligning and fully fusing semantic and syntactic structure\ninformation of source code at node/token levels. Method: This paper proposes a\nMulti-Modal Fine-grained Feature Fusion approach (MMF3) for neural code\nsummarization. We introduce a novel fine-grained fusion method, which allows\nfine-grained fusion of multiple code modalities at the token and node levels.\nSpecifically, we use this method to fuse information from both token and AST\nmodalities and apply the fused features to code summarization. Results: We\nconduct experiments on one Java and one Python datasets, and evaluate generated\nsummaries using four metrics. The results show that: 1) the performance of our\nmodel outperforms the current state-of-the-art models, and 2) the ablation\nexperiments show that our proposed fine-grained fusion method can effectively\nimprove the accuracy of generated summaries. Conclusion: MMF3 can mine the\nrelationships between crossmodal elements and perform accurate fine-grained\nelement-level alignment fusion accordingly. As a result, more clues can be\nprovided to improve the accuracy of the generated code summaries.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.03815,regular,pre_llm,2022,9,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Presentation: SymDefFix -- Sound Automatic Repair Using Symbolic\n  Execution\n\n  In this presentation, we introduce our constraint-based repair approach,\ncalled SymDefFix. SymDefFix is based on ExtractFix [3] and replaces the dynamic\nanalysis steps of ExtractFix to detect the error and find the potential fix\nlocations in an input program with symbolic execution. We first briefly\nmotivate and introduce our modifications of ExtractFix, and then demonstrate it\nwith an example.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.08955,regular,pre_llm,2022,9,"{'ai_likelihood': 8.90758302476671e-06, 'text': ""Adopting Automated Bug Assignment in Practice: A Longitudinal Case Study\n  at Ericsson\n\n  The continuous inflow of bug reports is a considerable challenge in large\ndevelopment projects. Inspired by contemporary work on mining software\nrepositories, we designed a prototype bug assignment solution based on machine\nlearning in 2011-2016. The prototype evolved into an internal Ericsson product,\nTRR, in 2017-2018. TRR's first bug assignment without human intervention\nhappened in April 2019. Our study evaluates the adoption of TRR within its\nindustrial context at Ericsson. Moreover, we investigate 1) how TRR performs in\nthe field, 2) what value TRR provides to Ericsson, and 3) how TRR has\ninfluenced the ways of working. We conduct an industrial case study combining\ninterviews with TRR stakeholders, minutes from sprint planning meetings, and\nbug tracking data. The data analysis includes thematic analysis, descriptive\nstatistics, and Bayesian causal analysis. TRR is now an incorporated part of\nthe bug assignment process. Considering the abstraction levels of the\ntelecommunications stack, high-level modules are more positive while low-level\nmodules experienced some drawbacks. On average, TRR automatically assigns 30%\nof the incoming bug reports with an accuracy of 75%. Auto-routed TRs are\nresolved around 21% faster within Ericsson, and TRR has saved highly seasoned\nengineers many hours of work. Indirect effects of adopting TRR include process\nimprovements, process awareness, increased communication, and higher job\nsatisfaction. TRR has saved time at Ericsson, but the adoption of automated bug\nassignment was more intricate compared to similar endeavors reported from other\ncompanies. We primarily attribute the difference to the very large size of the\norganization and the complex products. Key facilitators in the successful\nadoption include a gradual introduction, product champions, and careful\nstakeholder analysis.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.10258,regular,pre_llm,2022,9,"{'ai_likelihood': 4.735257890489366e-06, 'text': 'A graph-based knowledge representation and pattern mining supporting the\n  Digital Twin creation of existing manufacturing systems\n\n  The creation of a Digital Twin for existing manufacturing systems, so-called\nbrownfield systems, is a challenging task due to the needed expert knowledge\nabout the structure of brownfield systems and the effort to realize the digital\nmodels. Several approaches and methods have already been proposed that at least\npartially digitalize the information about a brownfield manufacturing system. A\nDigital Twin requires linked information from multiple sources. This paper\npresents a graph-based approach to merge information from heterogeneous\nsources. Furthermore, the approach provides a way to automatically identify\ntemplates using graph structure analysis to facilitate further work with the\nresulting Digital Twin and its further enhancement.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.14055,regular,pre_llm,2022,9,"{'ai_likelihood': 3.4769376118977866e-06, 'text': 'Dealing with Data Challenges when Delivering Data-Intensive Software\n  Solutions\n\n  The predicted increase in demand for data-intensive solution development is\ndriving the need for software, data, and domain experts to effectively\ncollaborate in multi-disciplinary data-intensive software teams (MDSTs). We\nconducted a socio-technical grounded theory study through interviews with 24\npractitioners in MDSTs to better understand the challenges these teams face\nwhen delivering data-intensive software solutions. The interviews provided\nperspectives across different types of roles including domain, data and\nsoftware experts, and covered different organisational levels from team\nmembers, team managers to executive leaders. We found that the key concern for\nthese teams is dealing with data-related challenges. In this paper, we present\nthe theory of dealing with data challenges that explains the challenges faced\nby MDSTs including gaining access to data, aligning data, understanding data,\nand resolving data quality issues; the context in and condition under which\nthese challenges occur, the causes that lead to the challenges, and the related\nconsequences such as having to conduct remediation activities, inability to\nachieve expected outcomes and lack of trust in the delivered solutions. We also\nidentified contingencies or strategies applied to address the challenges\nincluding high-level strategic approaches such as implementing data governance,\nimplementing new tools and techniques such as data quality visualisation and\nmonitoring tools, as well as building stronger teams by focusing on people\ndynamics, communication skill development and cross-skilling. Our findings have\ndirect implications for practitioners and researchers to better understand the\nlandscape of data challenges and how to deal with them.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.03558,regular,pre_llm,2022,9,"{'ai_likelihood': 1.7219119601779514e-06, 'text': 'Automated Validation of Insurance Applications against Calculation\n  Specifications\n\n  Insurance companies rely on their Legacy Insurance System (LIS) to govern\nday-to-day operations. These LIS operate as per the companys business rules\nthat are formally specified in Calculation Specification (CS) sheets. To meet\never-changing business demands, insurance companies are increasingly\ntransforming their outdated LIS to modern Policy Administration Systems (PAS).\nQuality Assurance (QA) of such PAS involves manual validation of calculations\nimplementation against the corresponding CS sheets from the LIS. This manual QA\napproach is effort-intensive and error-prone, which may fail to detect\ninconsistencies in PAS implementations and ultimately result in monetary loss.\nTo address this challenge, we propose a novel low-code no-code technique to\nautomatically validate PAS implementation against CS sheets. Our technique has\nbeen evaluated on a digital transformation project of a large insurance company\non 12 real-world calculations through 254 policies. The evaluation resulted in\neffort savings of approximately 92 percent against the conventional manual\nvalidation approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.07172,review,pre_llm,2022,9,"{'ai_likelihood': 4.6690305074055995e-06, 'text': 'Two case studies on implementing best practices for Software Process\n  Improvement\n\n  Software Process Improvement requires significant effort related not only to\nthe identification of relevant issues and providing an adequate response to\nthem but also to the implementation and adoption of the changes. Best practices\nprovide recommendations to software teams on how to address the identified\nobjectives in practice, based on aggregated experience and knowledge. In the\npaper, we present the GEANT experience and observations from the process of\nadopting the best practices and present the setting we have been using.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.03958,review,pre_llm,2022,9,"{'ai_likelihood': 6.622738308376736e-07, 'text': ""The Science Gateway Community Institute's Consulting Services Program:\n  Lessons for Research Software Engineering Organizations\n\n  The Science Gateways Community Institute (SGCI) is an NSF Software\nInfrastructure for Sustained Innovation (S2I2) funded project that leads and\nsupports the science gateway community. Major activities for SGCI include a)\nsustainability training, including the Focus Week week-long course designed to\nhelp science gateway operators develop sustainability plans, and the Jumpstart\nvirtual short-course; b) usability and user experience consulting; c) a\ncommunity catalog of science gateways and science gateway software; d)\nworkforce development activities, including a coding institute for students,\ninternship opportunities, and hackathons; e) an annual conference; and f)\nin-depth technical support for client gateway projects. The goals of SGCI's\nEmbedded Technical Support component are to help the institute's clients to\ncreate new science gateways or to significantly enhance existing science\ngateways. Examples of the latter include helping to implement major new\ncapabilities and to implement significant usability improvements suggested by\nSGCI's usability consultants. The Embedded Technical Support component was\nmanaged by Indiana University and involved research software engineers at San\nDiego Supercomputer Center, Texas Advanced Computing Center, Indiana\nUniversity, and Purdue University (through 2019). Since 2016, the component has\ninvolved 20 research software engineers as consultants and has conducted 59\nclient consultations. This short paper provides a summary of lessons learned\nfrom the Embedded Technical Support program that may be useful for the research\nsoftware engineering community.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.00844,regular,pre_llm,2022,9,"{'ai_likelihood': 4.967053731282552e-07, 'text': ""Developer Discussion Topics on the Adoption and Barriers of Low Code\n  Software Development Platforms\n\n  Low-code software development (LCSD) is an emerging approach to democratize\napplication development for software practitioners from diverse backgrounds.\nLCSD platforms promote rapid application development with a drag-and-drop\ninterface and minimal programming by hand. As it is a relatively new paradigm,\nit is vital to study developers' difficulties when adopting LCSD platforms.\nSoftware engineers frequently use the online developer forum Stack Overflow\n(SO) to seek assistance with technical issues. We observe a growing body of\nLCSD-related posts in SO. This paper presents an empirical study of around 33K\nSO posts containing discussions of 38 popular LCSD platforms. We use Topic\nModeling to determine the topics discussed in those posts. Additionally, we\nexamine how these topics are spread across the various phases of the agile\nsoftware development life cycle (SDLC) and which part of LCSD is the most\npopular and challenging. Our study offers several interesting findings. First,\nwe find 40 LCSD topics that we group into five categories: Application\nCustomization, Database, and File Management, Platform Adoption, Platform\nMaintenance, and Third-party API Integration. Second, while the Application\nCustomization (30\\%) and Data Storage (25\\%) \\rev{topic} categories are the\nmost common, inquiries relating to several other categories (e.g., the Platform\nAdoption \\rev{topic} category) have gained considerable attention in recent\nyears. Third, all topic categories are evolving rapidly, especially during the\nCovid-19 pandemic. The findings of this study have implications for all three\nLCSD stakeholders: LCSD platform vendors, LCSD developers/practitioners,\nResearchers, and Educators. Researchers and LCSD platform vendors can\ncollaborate to improve different aspects of LCSD, such as better tutorial-based\ndocumentation, testing, and DevOps support.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.01546,regular,pre_llm,2022,9,"{'ai_likelihood': 4.0398703681098095e-06, 'text': 'Scenario-Based Test Reduction and Prioritization for Multi-Module\n  Autonomous Driving Systems\n\n  When developing autonomous driving systems (ADS), developers often need to\nreplay previously collected driving recordings to check the correctness of\nnewly introduced changes to the system. However, simply replaying the entire\nrecording is not necessary given the high redundancy of driving scenes in a\nrecording (e.g., keeping the same lane for 10 minutes on a highway). In this\npaper, we propose a novel test reduction and prioritization approach for\nmulti-module ADS. First, our approach automatically encodes frames in a driving\nrecording to feature vectors based on a driving scene schema. Then, the given\nrecording is sliced into segments based on the similarity of consecutive\nvectors. Lengthy segments are truncated to reduce the length of a recording and\nredundant segments with the same vector are removed. The remaining segments are\nprioritized based on both the coverage and the rarity of driving scenes. We\nimplemented this approach on an industry level, multi-module ADS called Apollo\nand evaluated it on three road maps in various regression settings. The results\nshow that our approach significantly reduced the original recordings by over\n34% while keeping comparable test effectiveness, identifying almost all\ninjected faults. Furthermore, our test prioritization method achieves about 22%\nto 39% and 41% to 53% improvements over three baselines in terms of both the\naverage percentage of faults detected (APFD) and TOP-K.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.15256,regular,pre_llm,2022,10,"{'ai_likelihood': 0.0, 'text': 'PolyGloT: A Personalized and Gamified eTutoring System\n\n  The digital age is changing the role of educators and pushing for a paradigm\nshift in the education system as a whole. Growing demand for general and\nspecialized education inside and outside classrooms is at the heart of this\nrising trend. In modern, heterogeneous learning environments, the\none-size-fits-all approach is proven to be fundamentally flawed.\nIndividualization through adaptivity is, therefore, crucial to nurture\nindividual potential and address accessibility needs and neurodiversity. By\nformalizing a learning framework that takes into account all these different\naspects, we aim to define and implement an open, content-agnostic, and\nextensible eTutoring platform to design and consume adaptive and gamified\nlearning experiences. Adaptive technology supplementing teaching can extend the\nreach of every teacher, making it possible to scale 1-1 learning experiences.\nThere are many successful existing technologies available but they come with\nfixed environments that are not always suitable for the targeted audiences of\nthe course material. This paper presents PolyGloT, a system able to help\nteachers to design and implement a gamified and adaptive learning paths.\nThrough it we address some important issues including the engagement, fairness,\nand effectiveness of learning environments. We do not only propose an\ninnovative platform that could foster the learning process of different\ndisciplines, but it could also help teachers and instructors in organizing\nlearning material in an easy-access repository\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.11124,regular,pre_llm,2022,10,"{'ai_likelihood': 8.609559800889757e-07, 'text': ""Forest: Structural Code Editing with Multiple Cursors\n\n  Software developers frequently refactor code. Often, a single logical\nrefactoring change involves changing multiple related components in a source\nbase such as renaming each occurrence of a variable or function. While many\ncode editors can perform such common and generic refactorings, they do not\nsupport more complex refactorings or those that are specific to a given code\nbase. For those, as a flexible - albeit less interactive - alternative,\ndevelopers can write refactoring scripts that can implement arbitrarily complex\nlogic by manipulating the program's tree representation. In this work, we\npresent Forest, a structural code editor that aims to bridge the gap between\nthe interactiveness of code editors and the expressiveness of refactoring\nscripts. While structural editors have occupied a niche as general code\neditors, the key insight of this work is that they enable a novel structural\nmulti-cursor design that allows Forest to reach a similar expressiveness as\nrefactoring scripts; Forest allows to perform a single action simultaneously in\nmultiple program locations and thus support complex refactorings. To support\ninteractivity, Forest provides features typical for text code editors such as\nwriting and displaying the program through its textual representation. Our\nevaluation demonstrates that Forest allows performing edits similar to those\nfrom refactoring scripts, while still being interactive. We attempted to\nperform edits from 48 real-world refactoring scripts using Forest and found\nthat 11 were possible, while another 17 would be possible with added features.\nWe believe that a multi-cursor setting plays to the strengths of structural\nediting, since it benefits from reliable and expressive commands. Our results\nsuggest that multi-cursor structural editors could be practical for performing\nsmall-scale specialized refactorings.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.07486,regular,pre_llm,2022,10,"{'ai_likelihood': 6.953875223795574e-07, 'text': 'AFETM: Adaptive function execution trace monitoring for fault diagnosis\n\n  The high tracking overhead, the amount of up-front effort required to\nselecting the trace points, and the lack of effective data analysis model are\nthe significant barriers to the adoption of intra-component tracking for fault\ndiagnosis today. This paper introduces a novel method for fault diagnosis by\ncombining adaptive function level dynamic tracking, target fault injection, and\ngraph convolutional network. In order to implement this method, we introduce\ntechniques for (i) selecting function level trace points, (ii) constructing\napproximate function call tree of program when using adaptive tracking, and\n(iii) constructing graph convolutional network with fault injection campaign.\nWe evaluate our method using a web service benchmark composed of Redis, Nginx,\nHttpd, and SQlite. The experimental results show that this method outperforms\nlog based method, full tracking method, and Gaussian influence method in the\naccuracy of fault diagnosis, overhead, and performance impact on the diagnosis\ntarget.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.15159,regular,pre_llm,2022,10,"{'ai_likelihood': 9.503629472520617e-06, 'text': 'Comparing One with Many -- Solving Binary2source Function Matching Under\n  Function Inlining\n\n  Binary2source function matching is a fundamental task for many security\napplications, including Software Component Analysis (SCA). The ""1-to-1""\nmechanism has been applied in existing binary2source matching works, in which\none binary function is matched against one source function. However, we\ndiscovered that such mapping could be ""1-to-n"" (one query binary function maps\nmultiple source functions), due to the existence of function inlining.\n  To help conduct binary2source function matching under function inlining, we\npropose a method named O2NMatcher to generate Source Function Sets (SFSs) as\nthe matching target for binary functions with inlining. We first propose a\nmodel named ECOCCJ48 for inlined call site prediction. To train this model, we\nleverage the compilable OSS to generate a dataset with labeled call sites\n(inlined or not), extract several features from the call sites, and design a\ncompiler-opt-based multi-label classifier by inspecting the inlining\ncorrelations between different compilations. Then, we use this model to predict\nthe labels of call sites in the uncompilable OSS projects without compilation\nand obtain the labeled function call graphs of these projects. Next, we regard\nthe construction of SFSs as a sub-tree generation problem and design root node\nselection and edge extension rules to construct SFSs automatically. Finally,\nthese SFSs will be added to the corpus of source functions and compared with\nbinary functions with inlining. We conduct several experiments to evaluate the\neffectiveness of O2NMatcher and results show our method increases the\nperformance of existing works by 6% and exceeds all the state-of-the-art works.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.06893,regular,pre_llm,2022,10,"{'ai_likelihood': 2.274910608927409e-05, 'text': ""Bug Analysis in Jupyter Notebook Projects: An Empirical Study\n\n  Computational notebooks, such as Jupyter, have been widely adopted by data\nscientists to write code for analyzing and visualizing data. Despite their\ngrowing adoption and popularity, there has been no thorough study to understand\nJupyter development challenges from the practitioners' point of view. This\npaper presents a systematic study of bugs and challenges that Jupyter\npractitioners face through a large-scale empirical investigation. We mined\n14,740 commits from 105 GitHub open-source projects with Jupyter notebook code.\nNext, we analyzed 30,416 Stack Overflow posts which gave us insights into bugs\nthat practitioners face when developing Jupyter notebook projects. Finally, we\nconducted nineteen interviews with data scientists to uncover more details\nabout Jupyter bugs and to gain insights into Jupyter developers' challenges. We\npropose a bug taxonomy for Jupyter projects based on our results. We also\nhighlight bug categories, their root causes, and the challenges that Jupyter\npractitioners face.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.11228,regular,pre_llm,2022,10,"{'ai_likelihood': 9.602970547146267e-07, 'text': ""Intramorphic Testing: A New Approach to the Test Oracle Problem\n\n  A test oracle determines whether a system behaves correctly for a given\ninput. Automatic testing techniques rely on an automated test oracle to test\nthe system without user interaction. Important families of automated test\noracles include Differential Testing and Metamorphic Testing, which are both\nblack-box approaches; that is, they provide a test oracle that is oblivious to\nthe system's internals. In this work, we propose Intramorphic Testing as a\nwhite-box methodology to tackle the test oracle problem. To realize an\nIntramorphic Testing approach, a modified version of the system is created, for\nwhich, given a single input, a test oracle can be provided that relates the\noutput of the original and modified systems. As a concrete example, by\nreplacing a greater-equals operator in the implementation of a sorting\nalgorithm with smaller-equals, it would be expected that the output of the\nmodified implementation is the reverse output of the original implementation.\nIn this paper, we introduce the methodology and illustrate it via a set of use\ncases.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.08404,regular,pre_llm,2022,10,"{'ai_likelihood': 3.046459621853299e-06, 'text': 'Using Answer Set Programming for HPC Dependency Solving\n\n  Modern scientific software stacks have become extremely complex, using many\nprogramming models and libraries to exploit a growing variety of GPUs and\naccelerators. Package managers can mitigate this complexity using dependency\nsolvers, but they are reaching their limits. Finding compatible dependency\nversions is NP-complete, and modeling the semantics of package compatibility\nmodulo build-time options, GPU runtimes, flags, and other parameters is\nextremely difficult. Within this enormous configuration space, defining a\n""good"" configuration is daunting.\n  We tackle this problem using Answer Set Programming (ASP), a declarative\nmodel for combinatorial search problems. We show, using the Spack package\nmanager, that ASP programs can concisely express the compatibility rules of HPC\nsoftware stacks and provide strong quality-of-solution guarantees. Using ASP,\nwe can mix new builds with preinstalled binaries, and solver performance is\nacceptable even when considering tens of thousands of packages.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.11843,regular,pre_llm,2022,10,"{'ai_likelihood': 2.3874971601698136e-05, 'text': ""Low-Resources Project-Specific Code Summarization\n\n  Code summarization generates brief natural language descriptions of source\ncode pieces, which can assist developers in understanding code and reduce\ndocumentation workload. Recent neural models on code summarization are trained\nand evaluated on large-scale multi-project datasets consisting of independent\ncode-summary pairs. Despite the technical advances, their effectiveness on a\nspecific project is rarely explored. In practical scenarios, however,\ndevelopers are more concerned with generating high-quality summaries for their\nworking projects. And these projects may not maintain sufficient documentation,\nhence having few historical code-summary pairs. To this end, we investigate\nlow-resource project-specific code summarization, a novel task more consistent\nwith the developers' requirements. To better characterize project-specific\nknowledge with limited training samples, we propose a meta transfer learning\nmethod by incorporating a lightweight fine-tuning mechanism into a\nmeta-learning framework. Experimental results on nine real-world projects\nverify the superiority of our method over alternative ones and reveal how the\nproject-specific knowledge is learned.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.0946,regular,pre_llm,2022,10,"{'ai_likelihood': 9.43740208943685e-06, 'text': 'System-Specific Interpreters Make Megasystems Friendlier\n\n  Modern operating systems, browsers, and office suites have become megasystems\nbuilt on millions of lines of code. Their sheer size can intimidate even\nexperienced users and programmers away from attempting to understand and modify\nthe software running on their machines. This paper introduces system-specific\ninterpreters (SSIs) as a tool to help users regain knowledge of and control\nover megasystems. SSIs directly execute individual modules of a megasystem in a\ngdb-like environment without forcing the user to build, run, and trace the\nentire system. A prototype framework to help write SSIs is described in this\npaper and available for download at https://github.com/matthewsot/ssi-live22.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.09358,regular,pre_llm,2022,10,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'UMLsec4Edge: Extending UMLsec to model data-protection-compliant edge\n  computing systems\n\n  Edge computing enables the processing of data - frequently personal data - at\nthe edge of the network. For personal data, legislation such as the European\nGeneral Data Protection Regulation requires data protection by design. Hence,\ndata protection has to be accounted for in the design of edge computing systems\nwhenever personal data is involved. This leads to specific requirements for\nmodeling the architecture of edge computing systems, e.g., representation of\ndata and network properties.\n  To the best of our knowledge, no existing modeling language fulfils all these\nrequirements. In our previous work we showed that the commonly used UML profile\nUMLsec fulfils some of these requirements, and can thus serve as a starting\npoint.\n  The aim of this paper is to create a modeling language which meets all\nrequirements concerning the design of the architecture of edge computing\nsystems accounting for data protection. Thus, we extend UMLsec to satisfy all\nrequirements. We call the resulting UML profile UMLsec4Edge. We follow a\nsystematic approach to develop UMLsec4Edge. We apply UMLsec4Edge to real-world\nuse cases from different domains, and create appropriate deployment diagrams\nand class diagrams. These diagrams show UMLsec4Edge is capable of meeting the\nrequirements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.16269,regular,pre_llm,2022,10,"{'ai_likelihood': 4.337893591986763e-06, 'text': 'ATM: Black-box Test Case Minimization based on Test Code Similarity and\n  Evolutionary Search\n\n  Executing large test suites is time and resource consuming, sometimes\nimpossible, and such test suites typically contain many redundant test cases.\nHence, test case minimization is used to remove redundant test cases that are\nunlikely to detect new faults. However, most test case (suite) minimization\ntechniques rely on code coverage (white-box), model-based features, or\nrequirements specifications, which are not always accessible by test engineers.\nRecently, a set of novel techniques was proposed, called FAST-R, relying solely\non test case code for test case minimization, which appeared to be much more\nefficient than white-box techniques. However, it achieved a comparable low\nfault detection capability for Java projects, making its application\nchallenging in practice. This paper proposes ATM (AST-based Test case\nMinimizer), a similarity-based, search-based test case minimization technique,\ntaking a specific budget as input, that also relies exclusively on the source\ncode of test cases but attempts to achieve higher fault detection through\nfiner-grained similarity analysis and a dedicated search algorithm. ATM\ntransforms test case code into Abstract Syntax Trees (AST) and relies on four\ntree-based similarity measures to apply evolutionary search, specifically\ngenetic algorithms, to minimize test cases. We evaluated the effectiveness and\nefficiency of ATM on a large dataset of 16 Java projects with 661 faulty\nversions using three budgets ranging from 25% to 75% of test suites. ATM\nachieved significantly higher fault detection rates (0.82 on average), compared\nto FAST-R (0.61 on average) and random minimization (0.52 on average), when\nrunning only 50% of the test cases, within practically acceptable time (1.1-4.3\nhours, on average), given that minimization is only occasionally applied when\nmany new test cases are created (major releases). Results achieved for other\nbudgets were consistent.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.01661,regular,pre_llm,2022,10,"{'ai_likelihood': 1.986821492513021e-06, 'text': 'Putting Them under Microscope: A Fine-Grained Approach for Detecting\n  Redundant Test Cases in Natural Language\n\n  Natural language (NL) documentation is the bridge between software managers\nand testers, and NL test cases are prevalent in system-level testing and other\nquality assurance activities. Due to reasons such as requirements redundancy,\nparallel testing, and tester turnover within long evolving history, there are\ninevitably lots of redundant test cases, which significantly increase the cost.\nPrevious redundancy detection approaches typically treat the textual\ndescriptions as a whole to compare their similarity and suffer from low\nprecision. Our observation reveals that a test case can have explicit\ntest-oriented entities, such as tested function Components, Constraints, etc;\nand there are also specific relations between these entities. This inspires us\nwith a potential opportunity for accurate redundancy detection. In this paper,\nwe first define five test-oriented entity categories and four associated\nrelation categories and re-formulate the NL test case redundancy detection\nproblem as the comparison of detailed testing content guided by the\ntest-oriented entities and relations. Following that, we propose Tscope, a\nfine-grained approach for redundant NL test case detection by dissecting test\ncases into atomic test tuple(s) with the entities restricted by associated\nrelations. To serve as the test case dissection, Tscope designs a context-aware\nmodel for the automatic entity and relation extraction. Evaluation on 3,467\ntest cases from ten projects shows Tscope could achieve 91.8% precision, 74.8%\nrecall, and 82.4% F1, significantly outperforming state-of-the-art approaches\nand commonly-used classifiers. This new formulation of the NL test case\nredundant detection problem can motivate the follow-up studies to further\nimprove this task and other related tasks involving NL descriptions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.02435,regular,pre_llm,2022,10,"{'ai_likelihood': 2.4172994825575088e-06, 'text': ""IRJIT: A Simple, Online, Information Retrieval Approach for Just-In-Time\n  Software Defect Prediction\n\n  Just-in-Time software defect prediction (JIT-SDP) prevents the introduction\nof defects into the software by identifying them at commit check-in time.\nCurrent software defect prediction approaches rely on manually crafted features\nsuch as change metrics and involve expensive to train machine learning or deep\nlearning models. These models typically involve extensive training processes\nthat may require significant computational resources and time. These\ncharacteristics can pose challenges when attempting to update the models in\nreal-time as new examples become available, potentially impacting their\nsuitability for fast online defect prediction. Furthermore, the reliance on a\ncomplex underlying model makes these approaches often less explainable, which\nmeans the developers cannot understand the reasons behind models' predictions.\nAn approach that is not explainable might not be adopted in real-life\ndevelopment environments because of developers' lack of trust in its results.\nTo address these limitations, we propose an approach called IRJIT that employs\ninformation retrieval on source code and labels new commits as buggy or clean\nbased on their similarity to past buggy or clean commits. IRJIT approach is\nonline and explainable as it can learn from new data without expensive\nretraining, and developers can see the documents that support a prediction,\nproviding additional context. By evaluating 10 open-source datasets in a within\nproject setting, we show that our approach is up to 112 times faster than the\nstate-of-the-art ML and DL approaches, offers explainability at the commit and\nline level, and has comparable performance to the state-of-the-art.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.06235,regular,pre_llm,2022,10,"{'ai_likelihood': 3.642506069607205e-06, 'text': ""Listening to Users' Voice: Automatic Summarization of Helpful App\n  Reviews\n\n  App reviews are crowdsourcing knowledge of user experience with the apps,\nproviding valuable information for app release planning, such as major bugs to\nfix and important features to add. There exist prior explorations on app review\nmining for release planning, however, most of the studies strongly rely on\npre-defined classes or manually-annotated reviews. Also, the new review\ncharacteristic, i.e., the number of users who rated the review as helpful,\nwhich can help capture important reviews, has not been considered previously.\n  In the paper, we propose a novel framework, named SOLAR, aiming at accurately\nsummarizing helpful user reviews to developers. The framework mainly contains\nthree modules: The review helpfulness prediction module, topic-sentiment\nmodeling module, and multi-factor ranking module. The review helpfulness\nprediction module assesses the helpfulness of reviews, i.e., whether the review\nis useful for developers. The topic-sentiment modeling module groups the topics\nof the helpful reviews and also predicts the associated sentiment, and the\nmulti-factor ranking module aims at prioritizing semantically representative\nreviews for each topic as the review summary. Experiments on five popular apps\nindicate that SOLAR is effective for review summarization and promising for\nfacilitating app release planning.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.02543,regular,pre_llm,2022,10,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Diverse End User Requirements\n\n  As part of our larger research effort to improve support for diverse end user\nhuman-centric aspects during software development, we wanted to better\nunderstand how developers currently go about addressing these challenging\nhuman-centric aspects of their end users in contemporary software development\nprojects. We wanted to find out which are the key end user human-centric\naspects that software developers currently find challenging to address, and how\nthey currently go about trying to address diverse end user human-centric\naspects. We wanted to find out what sorts of end user human-centric aspects\nthey tend to encounter, which ones they view as more important and which more\nchallenging to address, what techniques (if any) they currently use to address\n(some of) them, and where they perceive further research in this area could be\ndone to provide them practical support. To this end we carried out a detailed\nonline survey of developers and development team managers, receiving 60 usable\nresponses. We interviewed 12 developers and managers from a range of different\npractice domains, role specialisations and experience levels to explore further\ndetails about issues.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.02236,review,pre_llm,2022,10,"{'ai_likelihood': 1.6887982686360678e-06, 'text': 'On the Use of Deep Learning in Software Defect Prediction\n\n  Context: Automated software defect prediction (SDP) methods are increasingly\napplied, often with the use of machine learning (ML) techniques. Yet, the\nexisting ML-based approaches require manually extracted features, which are\ncumbersome, time consuming and hardly capture the semantic information reported\nin bug reporting tools. Deep learning (DL) techniques provide practitioners\nwith the opportunities to automatically extract and learn from more complex and\nhigh-dimensional data. Objective: The purpose of this study is to\nsystematically identify, analyze, summarize, and synthesize the current state\nof the utilization of DL algorithms for SDP in the literature. Method: We\nsystematically selected a pool of 102 peer-reviewed studies and then conducted\na quantitative and qualitative analysis using the data extracted from these\nstudies. Results: Main highlights include: (1) most studies applied supervised\nDL; (2) two third of the studies used metrics as an input to DL algorithms; (3)\nConvolutional Neural Network is the most frequently used DL algorithm.\nConclusion: Based on our findings, we propose to (1) develop more comprehensive\nDL approaches that automatically capture the needed features; (2) use diverse\nsoftware artifacts other than source code; (3) adopt data augmentation\ntechniques to tackle the class imbalance problem; (4) publish replication\npackages.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.16261,review,pre_llm,2022,10,"{'ai_likelihood': 2.947118547227648e-06, 'text': ""An RSE Group Model: Operational and Organizational Approaches From\n  Princeton University's Central Research Software Engineering Group\n\n  The Princeton Research Software Engineering Group has grown rapidly since its\ninception in late 2016. The group, housed in the central Research Computing\nDepartment, comprised of professional Research Software Engineers (RSEs), works\ndirectly with researchers to create high quality research software to enable\nnew scientific advances. As the group has matured so has the need for\nformalizing operational details and procedures. The RSE group uses an RSE\npartnership model, where Research Software Engineers work long-term with a\ndesignated academic department, institute, center, consortium, or individual\nprincipal investigator (PI). This article describes the operation of the\ncentral Princeton RSE group including funding, partner & project selection, and\nbest practices for defining expectations for a successful partnership with\nresearchers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.02651,review,pre_llm,2022,10,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Tracking the Evolution of Static Code Warnings: the State-of-the-Art and\n  a Better Approach\n\n  Static bug detection tools help developers detect problems in the code,\nincluding bad programming practices and potential defects. Recent efforts to\nintegrate static bug detectors in modern software development workflows, such\nas in code review and continuous integration, are shown to better motivate\ndevelopers to fix the reported warnings on the fly. A proper mechanism to track\nthe evolution of the reported warnings can better support such integration.\nMoreover, tracking the static code warnings will benefit many downstream\nsoftware engineering tasks, such as learning the fix patterns for automated\nprogram repair, and learning which warnings are of more interest, so they can\nbe prioritized automatically. In addition, the utilization of tracking tools\nenables developers to concentrate on the most recent and actionable static\nwarnings rather than being overwhelmed by the thousands of warnings from the\nentire project. This, in turn, enhances the utilization of static analysis\ntools. Hence, precisely tracking the warnings by static bug detectors is\ncritical to improving the utilization of static bug detectors further.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.09136,regular,pre_llm,2022,10,"{'ai_likelihood': 1.8543667263454863e-06, 'text': ""SA4U: Practical Static Analysis for Unit Type Error Detection\n\n  Unit type errors, where values with physical unit types (e.g., meters, hours)\nare used incorrectly in a computation, are common in today's unmanned aerial\nsystem (UAS) firmware. Recent studies show that unit type errors represent over\n10% of bugs in UAS firmware. Moreover, the consequences of unit type errors are\nsevere. Over 30% of unit type errors cause UAS crashes. This paper proposes\nSA4U: a practical system for detecting unit type errors in real-world UAS\nfirmware. SA4U requires no modifications to firmware or developer annotations.\nIt deduces the unit types of program variables by analyzing simulation traces\nand protocol definitions. SA4U uses the deduced unit types to identify when\nunit type errors occur. SA4U is effective: it identified 14 previously\nundetected bugs in two popular open-source firmware (ArduPilot & PX4.)\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.15845,regular,pre_llm,2022,10,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'I Know What You Are Searching For: Code Snippet Recommendation from\n  Stack Overflow Posts\n\n  Stack Overflow has been heavily used by software developers to seek\nprogramming-related information. More and more developers use Community\nQuestion and Answer forums, such as Stack Overflow, to search for code examples\nof how to accomplish a certain coding task. This is often considered to be more\nefficient than working from source documentation, tutorials or full worked\nexamples. However, due to the complexity of these online Question and Answer\nforums and the very large volume of information they contain, developers can be\noverwhelmed by the sheer volume of available information. This makes it hard to\nfind and/or even be aware of the most relevant code examples to meet their\nneeds. To alleviate this issue, in this work we present a query-driven code\nrecommendation tool, named Que2Code, that identifies the best code snippets for\na user query from Stack Overflow posts. Our approach has two main stages: (i)\nsemantically-equivalent question retrieval and (ii) best code snippet\nrecommendation. To evaluate the performance of our proposed model, we conduct a\nlarge scale experiment to evaluate the effectiveness of the\nsemantically-equivalent question retrieval task and best code snippet\nrecommendation task separately on Python and Java datasets in Stack Overflow.\nWe also perform a human study to measure how real-world developers perceive the\nresults generated by our model. Both the automatic and human evaluation results\ndemonstrate the promising performance of our model, and we have released our\ncode and data to assist other researchers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.02341,review,pre_llm,2022,11,"{'ai_likelihood': 1.3245476616753473e-07, 'text': ""Better Call Saltzer \\& Schroeder: A Retrospective Security Analysis of\n  SolarWinds \\& Log4j\n\n  Saltzer \\& Schroeder's principles aim to bring security to the design of\ncomputer systems. We investigate SolarWinds Orion update and Log4j to unpack\nthe intersections where observance of these principles could have mitigated the\nembedded vulnerabilities. The common principles that were not observed include\n\\emph{fail safe defaults}, \\emph{economy of mechanism}, \\emph{complete\nmediation} and \\emph{least privilege}. Then we explore the literature on secure\nsoftware development interventions for developers to identify usable analysis\ntools and frameworks that can contribute towards improved observance of these\nprinciples. We focus on a system wide view of access of codes, checking access\npaths and aiding application developers with safe libraries along with an\nappropriate security task list for functionalities.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.0939,review,pre_llm,2022,11,"{'ai_likelihood': 5.298190646701389e-07, 'text': ""A Study of Adoption and Effects of DevOps Practices\n\n  Many organizations adopt DevOps practices and tools in order to break down\nsilos within the organization, improve software quality and delivery, and\nincrease customer satisfaction. However, the impact of the individual practices\non the performance of the organization is not well known. In this paper, we\ncollect evidence on the effects of DevOps practices and tools on organizational\nperformance. In an extensive literature search we identified 14 DevOps\npractices, consisting of 47 subpractices. Based on these practices, we\nconducted a global survey to study their effects in practice, and measure\nDevOps maturity. Across 123 respondents, working in 11 different industries, we\nfound that 13 of the 14 DevOps practices are adopted, determined by 50\\% of the\nparticipants indicating that practices are `always', `most of the time', and\n'about half of the time' applied. There is a positive correlation between the\nadoption of all practices and independently measured maturity. In particular,\npractices concerning sandboxes for minimum deployment, test-driven development,\nand trunk based development show the lowest correlations in our data. Effects\nof software delivery and organizational performance are mainly perceived\npositive. Yet, DevOps is also considered by some to have a negative impact such\nas respondents mentioning the predictability of product delivery has decreased\nand work is less fun. Concluding, our detailed overview of DevOps practices\nallows more targeted application of DevOps practices to obtain its positive\neffects while minimizing any negative effects.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.14585,regular,pre_llm,2022,11,"{'ai_likelihood': 7.649262746175131e-05, 'text': 'Safety Verification of Declarative Smart Contracts\n\n  Smart contracts manage a large number of digital assets nowadays. Bugs in\nthese contracts have led to significant financial loss. Verifying the\ncorrectness of smart contracts is, therefore, an important task. This paper\npresents an automated safety verification tool, DCV, that targets declarative\nsmart contracts written in DeCon, a logic-based domain-specific language for\nsmart contract implementation and specification. DCV proves safety properties\nby mathematical induction and can automatically infer inductive invariants\nusing heuristic patterns, without annotations from the developer. Our\nevaluation on 20 benchmark contracts shows that DCV is effective in verifying\nsmart contracts adapted from public repositories, and can verify contracts not\nsupported by other tools. Furthermore, DCV significantly outperforms baseline\ntools in verification time.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.0889,regular,pre_llm,2022,11,"{'ai_likelihood': 7.020102606879341e-06, 'text': 'Challenges related to system-of-systems for greening and climate\n  adaptation in smart cities\n\n  This paper presents the results of interviews conducted as part of the\nDYNASOS project. The objective was to collect challenges related to the design,\nimplementation and management of system-of-systems (SoS) in the context of\nclimate adaptation and greening of smart cities. 23 individuals from cities,\nacademia, and industry were interviewed between March and May 2022 and 57\ndistinct challenges were collected and analyzed. Our results show that while\ntechnical issues (such as interoperability or data acquisition) persist,\nnon-technical issues are the main obstacles. Difficulties in information\nsharing, effective communication, and synchronization between different actors\nare the most important challenges.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.04712,regular,pre_llm,2022,11,"{'ai_likelihood': 2.185503641764323e-06, 'text': 'Improve Model Testing by Integrating Bounded Model Checking and Coverage\n  Guided Fuzzing\n\n  The control logic models built by Simulink or Ptolemy have been widely used\nin industry scenes. It is an urgent need to ensure the safety and security of\nthe control logic models. Test case generation technologies are widely used to\nensure the safety and security. State-of-the-art model testing tools employ\nmodel checking techniques or search-based methods to generate test cases.\nTraditional search based techniques based on Simulink simulation are plagued by\nproblems such as low speed and high overhead. Traditional model checking\ntechniques such as symbolic execution have limited performance when dealing\nwith nonlinear elements and complex loops. Recently, coverage guided fuzzing\ntechnologies are known to be effective for test case generation, due to their\nhigh efficiency and impressive effects over complex branches of loops.\n  In this paper, we apply fuzzing methods to improve model testing and\ndemonstrate the effectiveness. The fuzzing methods aim to cover more program\nbranches by mutating valuable seeds. Inspired by this feature, we propose a\nnovel integration technology SPsCGF, which leverages bounded model checking for\nsymbolic execution to generate test cases as initial seeds and then conduct\nfuzzing based upon these worthy seeds. In this manner, our work combines the\nadvantages of the model checking methods and fuzzing techniques in a novel way.\nSince the control logic models always receive signal inputs, we specifically\ndesign novel mutation operators for signals to improve the existing fuzzing\nmethod in model testing. Over the evaluated benchmarks which consist of\nindustrial cases, SPsCGF could achieve 8% to 38% higher model coverage and\n3x-10x time efficiency compared with the state-of-the-art works.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.06122,review,pre_llm,2022,11,"{'ai_likelihood': 1.6887982686360678e-06, 'text': ""Requirements Quality vs Process and Stakeholders' Well-being: A Case of\n  a Nordic Bank\n\n  Requirements are key artefacts to describe the intended purpose of a software\nsystem. The quality of requirements is crucial for deciding what to do next,\nimpacting the development process's effectiveness and efficiency. However, we\nknow very little about the connection between practitioners' perceptions\nregarding requirements quality and its impact on the process or the feelings of\nthe professionals involved in the development process.\n  Objectives: This study investigates: i) How software development\npractitioners define requirements quality, ii) how the perceived quality of\nrequirements impact process and stakeholders' well-being, and iii) what are the\ncauses and potential solutions for poor-quality requirements. Method: This\nstudy was performed as a descriptive interview study at a sub-organization of a\nNordic bank that develops its own web and mobile apps. The data collection\ncomprises interviews with 20 practitioners, including requirements engineers,\ndevelopers, testers, and newly employed developers, with five interviewees from\neach group.\n  Results: The results show that different roles have different views on what\nmakes a requirement good quality. Participants highlighted that, in general,\nthey experience negative emotions, more work, and overhead communication when\nthey work with requirements they perceive to be of poor quality. The\npractitioners also describe positive effects on their performance and positive\nfeelings when they work with requirements that they perceive to be good.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.00105,regular,pre_llm,2022,11,"{'ai_likelihood': 4.735257890489366e-06, 'text': 'An Empirical Study on the Bugs Found while Reusing Pre-trained Natural\n  Language Processing Models\n\n  In NLP, reusing pre-trained models instead of training from scratch has\ngained popularity; however, NLP models are mostly black boxes, very large, and\noften require significant resources. To ease, models trained with large corpora\nare made available, and developers reuse them for different problems. In\ncontrast, developers mostly build their models from scratch for traditional\nDL-related problems. By doing so, they have control over the choice of\nalgorithms, data processing, model structure, tuning hyperparameters, etc.\nWhereas in NLP, due to the reuse of the pre-trained models, NLP developers are\nlimited to little to no control over such design decisions. They either apply\ntuning or transfer learning on pre-trained models to meet their requirements.\nAlso, NLP models and their corresponding datasets are significantly larger than\nthe traditional DL models and require heavy computation. Such reasons often\nlead to bugs in the system while reusing the pre-trained models. While bugs in\ntraditional DL software have been intensively studied, the nature of extensive\nreuse and black-box structure motivates us to understand the different types of\nbugs that occur while reusing NLP models? What are the root causes of those\nbugs? How do these bugs affect the system? To answer these questions, We\nstudied the bugs reported while reusing the 11 popular NLP models. We mined\n9,214 issues from GitHub repositories and identified 984 bugs. We created a\ntaxonomy with bug types, root causes, and impacts. Our observations led to\nseveral findings, including limited access to model internals resulting in a\nlack of robustness, lack of input validation leading to the propagation of\nalgorithmic and data bias, and high-resource consumption causing more crashes,\netc. Our observations suggest several bug patterns, which would greatly\nfacilitate further efforts in reducing bugs in pre-trained models and code\nreuse.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.06386,regular,pre_llm,2022,11,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'An Agent-based Approach to Automated Game Testing: an Experience Report\n\n  Computer games are very challenging to handle for traditional automated\ntesting algorithms. In this paper we will look at intelligent agents as a\nsolution. Agents are suitable for testing games, since they are reactive and\nable to reason about their environment to decide the action they want to take.\nThis paper presents the experience of using an agent-based automated testing\nframework called \\ivxr\\ to test computer games. Three games will be discussed,\nincluding a sophisticated 3D game called Space Engineers. We will show how the\nframework can be used in different ways, either directly to drive a test agent,\nor as an intelligent functionality that can be driven by a traditional\nautomated testing algorithm such as a random algorithm or a model based testing\nalgorithm.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.12821,regular,pre_llm,2022,11,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'Explainable AI for Pre-Trained Code Models: What Do They Learn? When\n  They Do Not Work?\n\n  In recent years, there has been a wide interest in designing deep neural\nnetwork-based models that automate downstream software engineering tasks on\nsource code, such as code document generation, code search, and program repair.\nAlthough the main objective of these studies is to improve the effectiveness of\nthe downstream task, many studies only attempt to employ the next best neural\nnetwork model, without a proper in-depth analysis of why a particular solution\nworks or does not, on particular tasks or scenarios. In this paper, using an\nexample eXplainable AI (XAI) method (attention mechanism), we study two recent\nlarge language models (LLMs) for code (CodeBERT and GraphCodeBERT) on a set of\nsoftware engineering downstream tasks: code document generation (CDG), code\nrefinement (CR), and code translation (CT). Through quantitative and\nqualitative studies, we identify what CodeBERT and GraphCodeBERT learn (put the\nhighest attention on, in terms of source code token types), on these tasks. We\nalso show some of the common patterns when the model does not work as expected\n(performs poorly even on easy problems) and suggest recommendations that may\nalleviate the observed challenges.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.13827,review,pre_llm,2022,11,"{'ai_likelihood': 6.755193074544271e-06, 'text': ""Lessons Learned to Improve the UX Practices in Agile Projects Involving\n  Data Science and Process Automation\n\n  Context: User-Centered Design and Agile methodologies focus on human issues.\nNevertheless, agile methodologies focus on contact with contracting customers\nand generating value for them. Usually, the communication between end users and\nthe agile team is mediated by customers. However, they do not know the problems\nend users face in their routines. Hence, UX issues are typically identified\nonly after the implementation, during user testing and validation. Objective:\nAiming to improve the understanding and definition of the problem in agile\nprojects, this research investigates the practices and difficulties experienced\nby agile teams during the development of data science and process automation\nprojects. Also, we analyze the benefits and the teams' perceptions regarding\nuser participation in these projects. Method: We collected data from four agile\nteams in an academia-industry collaboration focusing on delivering data science\nand process automation solutions. Therefore, we applied a carefully designed\nquestionnaire answered by developers, scrum masters, and UX designers. In\ntotal, 18 subjects answered the questionnaire. Results: From the results, we\nidentify practices used by the teams to define and understand the problem and\nto represent the solution. The practices most often used are prototypes and\nmeetings with stakeholders. Another practice that helped the team to understand\nthe problem was using Lean Inceptions. Also, our results present some specific\nissues regarding data science projects. Conclusion: We observed that end-user\nparticipation can be critical to understanding and defining the problem. They\nhelp to define elements of the domain and barriers in the implementation. We\nidentified a need for approaches that facilitate user-team communication in\ndata science projects and the need for more detailed requirements\nrepresentations to support data science solutions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.15007,regular,pre_llm,2022,11,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'An Exploration of Cross-Patch Collaborations via Patch Linkage in\n  OpenStack\n\n  Contemporary development projects benefit from code review as it improves the\nquality of a project. Large ecosystems of inter-dependent projects like\nOpenStack generate a large number of reviews, which poses new challenges for\ncollaboration (improving patches, fixing defects). Review tools allow\ndevelopers to link between patches, to indicate patch dependency, competing\nsolutions, or provide broader context. We hypothesize that such patch linkage\nmay also simulate cross-collaboration.\n  With a case study of OpenStack, we take a first step to explore\ncollaborations that occur after a patch linkage was posted between two patches\n(i.e., cross-patch collaboration). Our empirical results show that although\npatch linkage that requests collaboration is relatively less prevalent, the\nprobability of collaboration is relatively higher. Interestingly, the results\nalso show that collaborative contributions via patch linkage are non-trivial,\ni.e, contributions can affect the review outcome (such as voting) or even\nimprove the patch (i.e., revising). This work opens up future directions to\nunderstand barriers and opportunities related to this new kind of\ncollaboration, that assists with code review and development tasks in large\necosystems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.00273,regular,pre_llm,2022,11,"{'ai_likelihood': 6.821420457628038e-06, 'text': ""ActGraph: Prioritization of Test Cases Based on Deep Neural Network\n  Activation Graph\n\n  Widespread applications of deep neural networks (DNNs) benefit from DNN\ntesting to guarantee their quality. In the DNN testing, numerous test cases are\nfed into the model to explore potential vulnerabilities, but they require\nexpensive manual cost to check the label. Therefore, test case prioritization\nis proposed to solve the problem of labeling cost, e.g., activation-based and\nmutation-based prioritization methods. However, most of them suffer from\nlimited scenarios (i.e. high confidence adversarial or false positive cases)\nand high time complexity. To address these challenges, we propose the concept\nof the activation graph from the perspective of the spatial relationship of\nneurons. We observe that the activation graph of cases that triggers the\nmodels' misbehavior significantly differs from that of normal cases. Motivated\nby it, we design a test case prioritization method based on the activation\ngraph, ActGraph, by extracting the high-order node features of the activation\ngraph for prioritization. ActGraph explains the difference between the test\ncases to solve the problem of scenario limitation. Without mutation operations,\nActGraph is easy to implement, leading to lower time complexity. Extensive\nexperiments on three datasets and four models demonstrate that ActGraph has the\nfollowing key characteristics. (i) Effectiveness and generalizability: ActGraph\nshows competitive performance in all of the natural, adversarial and mixed\nscenarios, especially in RAUC-100 improvement (~1.40). (ii) Efficiency:\nActGraph does not use complex mutation operations and runs in less time (~1/50)\nthan the state-of-the-art method.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.13063,regular,pre_llm,2022,11,"{'ai_likelihood': 0.00013682577345106337, 'text': ""Guidelines for Developing Bots for GitHub\n\n  Projects on GitHub rely on the automation provided by software development\nbots to uphold quality and alleviate developers' workload. Nevertheless, the\npresence of bots can be annoying and disruptive to the community. Backed by\nmultiple studies with practitioners, this paper provides guidelines for\ndeveloping and maintaining software bots. These guidelines aim to support the\ndevelopment of future and current bots and social coding platforms.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.16587,regular,pre_llm,2022,11,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Rigorous Assessment of Model Inference Accuracy using Language\n  Cardinality\n\n  Models such as finite state automata are widely used to abstract the behavior\nof software systems by capturing the sequences of events observable during\ntheir execution. Nevertheless, models rarely exist in practice and, when they\ndo, get easily outdated; moreover, manually building and maintaining models is\ncostly and error-prone. As a result, a variety of model inference methods that\nautomatically construct models from execution traces have been proposed to\naddress these issues.\n  However, performing a systematic and reliable accuracy assessment of inferred\nmodels remains an open problem. Even when a reference model is given, most\nexisting model accuracy assessment methods may return misleading and biased\nresults. This is mainly due to their reliance on statistical estimators over a\nfinite number of randomly generated traces, introducing avoidable uncertainty\nabout the estimation and being sensitive to the parameters of the random trace\ngenerative process.\n  This paper addresses this problem by developing a systematic approach based\non analytic combinatorics that minimizes bias and uncertainty in model accuracy\nassessment by replacing statistical estimation with deterministic accuracy\nmeasures. We experimentally demonstrate the consistency and applicability of\nour approach by assessing the accuracy of models inferred by state-of-the-art\ninference tools against reference models from established specification mining\nbenchmarks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.02013,regular,pre_llm,2022,11,"{'ai_likelihood': 2.6490953233506944e-06, 'text': ""Analyzing Performance Issues of Virtual Reality Applications\n\n  Extended Reality (XR) includes Virtual Reality (VR), Augmented Reality (AR)\nand Mixed Reality (MR). XR is an emerging technology that simulates a realistic\nenvironment for users. XR techniques have provided revolutionary user\nexperiences in various application scenarios (e.g., training, education,\nproduct/architecture design, gaming, remote conference/tour, etc.). Due to the\nhigh computational cost of rendering real-time animation in limited-resource\ndevices and constant interaction with user activity, XR applications often face\nperformance bottlenecks, and these bottlenecks create a negative impact on the\nuser experience of XR software. Thus, performance optimization plays an\nessential role in many industry-standard XR applications. Even though\nidentifying performance bottlenecks in traditional software (e.g., desktop\napplications) is a widely explored topic, those approaches cannot be directly\napplied within XR software due to the different nature of XR applications.\nMoreover, XR applications developed in different frameworks such as Unity and\nUnreal Engine show different performance bottleneck patterns and thus,\nbottleneck patterns of Unity projects can't be applied for Unreal Engine\n(UE)-based XR projects. To fill the knowledge gap for XR performance\noptimizations of Unreal Engine-based XR projects, we present the first\nempirical study on performance optimizations from seven UE XR projects, 78 UE\nXR discussion issues and three sources of UE documentation. Our analysis\nidentified 14 types of performance bugs, including 12 types of bugs related to\nUE settings issues and two types of CPP source code-related issues. To further\nassist developers in detecting performance bugs based on the identified bug\npatterns, we also developed a static analyzer, UEPerfAnalyzer, that can detect\nperformance bugs in both configuration files and source code.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.16716,regular,pre_llm,2022,11,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'Automated Generating Natural Language Requirements based on Domain\n  Ontology\n\n  Software requirements specification is undoubtedly critical for the whole\nsoftware life-cycle. Nowadays, writing software requirements specifications\nprimarily depends on human work. Although massive studies have been proposed to\nfasten the process via proposing advanced elicitation and analysis techniques,\nit is still a time-consuming and error-prone task that needs to take domain\nknowledge and business information into consideration. In this paper, we\npropose an approach, named ReqGen, which can provide recommendations by\nautomatically generating natural language requirements specifications based on\ncertain given keywords. Specifically, ReqGen consists of three critical steps.\nFirst, keywords-oriented knowledge is selected from domain ontology and is\ninjected to the basic Unified pre-trained Language Model (UniLM) for domain\nfine-tuning. Second, a copy mechanism is integrated to ensure the occurrence of\nkeywords in the generated statements. Finally, a requirement syntax constrained\ndecoding is designed to close the semantic and syntax distance between the\ncandidate and reference specifications. Experiments on two public datasets from\ndifferent groups and domains show that ReqGen outperforms six popular natural\nlanguage generation approaches with respect to the hard constraint of\nkeywords(phrases) inclusion, BLEU, ROUGE and syntax compliance. We believe that\nReqGen can promote the efficiency and intelligence of specifying software\nrequirements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.08695,regular,pre_llm,2022,11,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Distributed and Adversarial Resistant Workflow Execution on the Algorand\n  Blockchain\n\n  We provide a practical translation from the Dynamic Condition Response (DCR)\nprocess modelling language to the Transaction Execution Approval Language\n(TEAL) used by the Algorand blockchain. Compared to earlier implementations of\nbusiness process notations on blockchains, particularly Ethereum, the present\nimplementation is four orders of magnitude cheaper. This translation has the\nfollowing immediate ramifications: (1) It allows decentralised execution of\nDCR-specified business processes in the absence of expensive intermediaries\n(lawyers, brokers) or counterparty risk. (2) It provides a possibly helpful\nhigh-level language for implementing business processes on Algorand. (3) It\ndemonstrates that despite the strict limitations on Algorand smart contracts,\nthey are powerful enough to encode models of a modern process notation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.05286,regular,pre_llm,2022,11,"{'ai_likelihood': 2.2517310248480903e-06, 'text': 'Deep Learning Methods for Software Requirement Classification: A\n  Performance Study on the PURE dataset\n\n  Requirement engineering (RE) is the first and the most important step in\nsoftware production and development. The RE is aimed to specify software\nrequirements. One of the tasks in RE is the categorization of software\nrequirements as functional and non-functional requirements. The functional\nrequirements (FR) show the responsibilities of the system while non-functional\nrequirements represent the quality factors of software. Discrimination between\nFR and NFR is a challenging task. Nowadays Deep Learning (DL) has entered all\nfields of engineering and has increased accuracy and reduced time in their\nimplementation process. In this paper, we use deep learning for the\nclassification of software requirements. Five prominent DL algorithms are\ntrained for classifying requirements. Also, two voting classification\nalgorithms are utilized for creating ensemble classifiers based on five DL\nmethods. The PURE, a repository of Software Requirement Specification (SRS)\ndocuments, is selected for our experiments. We created a dataset from PURE\nwhich contains 4661 requirements where 2617 requirements are functional and the\nremaining are non-functional. Our methods are applied to the dataset and their\nperformance analysis is reported. The results show that the performance of deep\nlearning models is satisfactory and the voting mechanisms provide better\nresults.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.03911,regular,pre_llm,2022,11,"{'ai_likelihood': 3.543164994981554e-06, 'text': 'Towards Extending the Range of Bugs That Automated Program Repair Can\n  Handle\n\n  Modern automated program repair (APR) is well-tuned to finding and repairing\nbugs that introduce observable erroneous behavior to a program. However, a\nsignificant class of bugs does not lead to such observable behavior (e.g.,\nliveness/termination bugs, non-functional bugs, and information flow bugs).\nSuch bugs can generally not be handled with current APR approaches, so, as a\ncommunity, we need to develop complementary techniques.\n  To stimulate the systematic study of alternative APR approaches and hybrid\nAPR combinations, we devise a novel bug classification system that enables\nmethodical analysis of their bug detection power and bug repair capabilities.\nTo demonstrate the benefits, we analyze the repair of termination bugs in\nsequential and concurrent programs. The study shows that integrating dynamic\nAPR with formal analysis techniques, such as termination provers and software\nmodel checkers, reduces complexity and improves the overall reliability of\nthese repairs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.11197,regular,pre_llm,2022,11,"{'ai_likelihood': 9.139378865559896e-06, 'text': 'An Empirical Study of Package Management Issues via Stack Overflow\n\n  The package manager (PM) is crucial to most technology stacks, acting as a\nbroker to ensure that a verified dependency package is correctly installed,\nconfigured, or removed from an application. Diversity in technology stacks has\nled to dozens of PMs with various features. While our recent study indicates\nthat package management features of PM are related to end-user experiences, it\nis unclear what those issues are and what information is required to resolve\nthem. In this paper, we have investigated PM issues faced by end-users through\nan empirical study of content on Stack Overflow (SO). We carried out a\nqualitative analysis of 1,131 questions and their accepted answer posts for\nthree popular PMs (i.e., Maven, npm, and NuGet) to identify issue types,\nunderlying causes, and their resolutions. Our results confirm that end-users\nstruggle with PM tool usage (approximately 64-72%). We observe that most issues\nare raised by end-users due to lack of instructions and errors messages from PM\ntools. In terms of issue resolution, we find that external link sharing is the\nmost common practice to resolve PM issues. Additionally, we observe that links\npointing to useful resources (i.e., official documentation websites, tutorials,\netc.) are most frequently shared, indicating the potential for tool support and\nthe ability to provide relevant information for PM end-users.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.01479,regular,pre_llm,2022,12,"{'ai_likelihood': 1.8543667263454863e-06, 'text': 'Detecting Outdated Code Element References in Software Repository\n  Documentation\n\n  Outdated documentation is a pervasive problem in software development,\npreventing effective use of software, and misleading users and developers\nalike. We posit that one possible reason why documentation becomes out of sync\nso easily is that developers are unaware of when their source code\nmodifications render the documentation obsolete. Ensuring that the\ndocumentation is always in sync with the source code takes considerable effort,\nespecially for large codebases. To address this situation, we propose an\napproach that can automatically detect code element references that survive in\nthe documentation after all source code instances have been deleted. In this\nwork, we analysed over 3,000 GitHub projects and found that most projects\ncontain at least one outdated code element reference at some point in their\nhistory. We submitted GitHub issues to real-world projects containing outdated\nreferences detected by our approach, some of which have already led to\ndocumentation fixes. As an initiative toward keeping documentation in software\nrepositories up-to-date, we have made our implementation available for\ndevelopers to scan their GitHub projects for outdated code element references.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.0561,regular,pre_llm,2022,12,"{'ai_likelihood': 4.5365757412380644e-06, 'text': 'Authorship Identification of Source Code Segments Written by Multiple\n  Authors Using Stacking Ensemble Method\n\n  Source code segment authorship identification is the task of identifying the\nauthor of a source code segment through supervised learning. It has vast\nimportance in plagiarism detection, digital forensics, and several other law\nenforcement issues. However, when a source code segment is written by multiple\nauthors, typical author identification methods no longer work. Here, an author\nidentification technique, capable of predicting the authorship of source code\nsegments, even in the case of multiple authors, has been proposed which uses a\nstacking ensemble classifier. This proposed technique is built upon several\ndeep neural networks, random forests and support vector machine classifiers. It\nhas been shown that for identifying the author group, a single classification\ntechnique is no longer sufficient and using a deep neural network-based\nstacking ensemble method can enhance the accuracy significantly. The\nperformance of the proposed technique has been compared with some existing\nmethods which only deal with the source code segments written precisely by a\nsingle author. Despite the harder task of authorship identification for source\ncode segments written by multiple authors, our proposed technique has achieved\npromising results evidenced by the identification accuracy, compared to the\nrelated works which only deal with code segments written by a single author.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.04038,regular,pre_llm,2022,12,"{'ai_likelihood': 9.636084238688152e-06, 'text': ""SkipFuzz: Active Learning-based Input Selection for Fuzzing Deep\n  Learning Libraries\n\n  Many modern software systems are enabled by deep learning libraries such as\nTensorFlow and PyTorch. As deep learning is now prevalent, the security of deep\nlearning libraries is a key concern. Fuzzing deep learning libraries presents\ntwo challenges. Firstly, to reach the functionality of the libraries, fuzzers\nhave to use inputs from the valid input domain of each API function, which may\nbe unknown. Secondly, many inputs are redundant. Randomly sampled invalid\ninputs are likely not to trigger new behaviors. While existing approaches\npartially address the first challenge, they overlook the second challenge.\n  We propose SkipFuzz, an approach for fuzzing deep learning libraries. To\ngenerate valid inputs, SkipFuzz learns the input constraints of each API\nfunction using active learning. By using information gained during fuzzing,\nSkipFuzz infers a model of the input constraints, and, thus, generate valid\ninputs. SkipFuzz comprises an active learner which queries a test executor to\nobtain feedback for inference. After constructing hypotheses, the active\nlearner poses queries and refines the hypotheses using the feedback from the\ntest executor, which indicates if the library accepts or rejects an input,\ni.e., if it satisfies the input constraints or not. Inputs from different\ncategories are used to invoke the library to check if a set of inputs satisfies\na function's input constraints. Inputs in one category are distinguished from\nother categories by possible input constraints they would satisfy, e.g. they\nare tensors of a certain shape. As such, SkipFuzz is able to refine its\nhypothesis by eliminating possible candidates of the input constraints. This\nactive learning-based approach addresses the challenge of redundant inputs.\nUsing SkipFuzz, we have found and reported 43 crashes. 28 of them have been\nconfirmed, with 13 unique CVEs assigned.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.10808,review,pre_llm,2022,12,"{'ai_likelihood': 2.2186173333062066e-06, 'text': 'Agile Assessment Methods: Current State of the Art\n\n  Agility Assessment (AA) comprises tools, assessment techniques, and\nframeworks that focus on indicating how a company or a team is applying agile\ntechniques and eventually pointing out problems in adopting agile practices at\na project-level, organization-level or individual-level. There are many\napproaches for AA such as agility assessment models, agility checklists,\nagility surveys, and agility assessment tools. This report presents the state\nof the art approaches that support agility assessment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.07475,review,pre_llm,2022,12,"{'ai_likelihood': 1.304679446750217e-05, 'text': 'Comparative review of selected Internet communication protocols\n\n  With a large variety of communication methods and protocols, many software\narchitects face the problem of choosing the best way for services to share\ninformation. For communication technology to be functional and practical, it\nshould enable developers to define a complete set of CRUD methods for the\nprocessed data. The research team compared the most commonly used data transfer\nprotocols and concepts in this paper: REST, WebSocket, gRPC GraphQL and SOAP.\nTo do that, a set of web servers was implemented in Python, each using one of\nthe examined technologies. Then, the team performed an automated benchmark\nmeasuring time and data transfer overhead for a set of defined operations:\ncreating an entity, retrieving a list of 100 entities and fetching details of\none entity. Tests were designed to avoid the results being interfered by\ndatabase connection or docker-compose environment characteristics. The research\nteam has concluded that gRPC was the most efficient and reliable data transfer\nmethod. On the other hand, GraphQL turned out to be the slowest communication\nmethod of all. Moreover, its server and client libraries caused the most\nproblems with proper usage in a web server. SOAP did not participate in\nbenchmarking due to limited compatibility with Python and a lack of popularity\nin modern web solutions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.053,review,pre_llm,2022,12,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Conceptual Modeling Founded on the Stoic Ontology: Reality with Dynamic\n  Existence and Static Subsistence\n\n  According to the software engineering community, the acknowledgement is\ngrowing that a theory of software development is needed to integrate the\ncurrently myriad popular methodologies, some of which are based on opposing\nperspectives. Conceptual modeling (CM) can contribute to such a theory. CM\ndefines fundamental concepts to create representations of reality to achieve\nontologically sound software behavior that is characterized by truthfulness to\nreality and conceptual clarity. In this context, CM is founded on theories\nabout the world that serve to represent a given domain. Ontologies have made\ntheir way into CM as tools in requirements analysis, implementation\nspecification, and software architecture. This paper involves building a direct\nconnection between reality and CM by establishing mapping between reality and\nmodeling thinging machines (TMs). Specifically, Stoic ontology serves to define\nthe existence of TM things and actions in reality. Such a development would\nbenefit CM in addition to demonstrating that classical concepts in philosophy\ncan be applied to modern fields of study. The TM model includes static and\ndynamic specifications. The dynamic level involves time-based events that can\nbe mapped to reality. The problem concerns the nature of a time-less static\ndescription, which provides regions where the actions in events take place;\nwithout them, the dynamic description collapses. The Stoics came up with a\nbrilliant move: the assumed reality to be a broader category than being.\nReality is made of things that exist and things that subsist. In this case, the\ndynamic TM description is in existence, whereas the static, mapped portion of\nthe dynamic description is in subsistence. We apply such ontology to a contract\nworkflow example. The result seems to open a new avenue of CM that may enhance\nthe theoretical foundation for software and system development.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.09289,regular,pre_llm,2022,12,"{'ai_likelihood': 2.6490953233506944e-06, 'text': 'Mining User Privacy Concern Topics from App Reviews\n\n  Context: As mobile applications (Apps) widely spread over our society and\nlife, various personal information is constantly demanded by Apps in exchange\nfor more intelligent and customized functionality. An increasing number of\nusers are voicing their privacy concerns through app reviews on App stores.\n  Objective: The main challenge of effectively mining privacy concerns from\nuser reviews lies in the fact that reviews expressing privacy concerns are\noverridden by a large number of reviews expressing more generic themes and\nnoisy content. In this work, we propose a novel automated approach to overcome\nthat challenge.\n  Method: Our approach first employs information retrieval and document\nembeddings to unsupervisedly extract candidate privacy reviews that are further\nlabeled to prepare the annotation dataset. Then, supervised classifiers are\ntrained to automatically identify privacy reviews. Finally, we design an\ninterpretable topic mining algorithm to detect privacy concern topics contained\nin the privacy reviews.\n  Results: Experimental results show that the best performed document embedding\nachieves an average precision of 96.80% in the top 100 retrieved candidate\nprivacy reviews. All of the trained privacy review classifiers can achieve an\nF1 value of more than 91%, outperforming the recent keywords matching baseline\nwith the maximum F1 margin being 7.5%. For detecting privacy concern topics\nfrom privacy reviews, our proposed algorithm achieves both better topic\ncoherence and diversity than three strong topic modeling baselines including\nLDA.\n  Conclusion: Empirical evaluation results demonstrate the effectiveness of our\napproach in identifying privacy reviews and detecting user privacy concerns\nexpressed in App reviews.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.13245,regular,pre_llm,2022,12,"{'ai_likelihood': 2.384185791015625e-06, 'text': ""Studying the Characteristics of AIOps Projects on GitHub\n\n  Artificial Intelligence for IT Operations (AIOps) leverages AI approaches to\nhandle the massive amount of data generated during the operations of software\nsystems. Prior works have proposed various AIOps solutions to support different\ntasks in system operations and maintenance, such as anomaly detection. In this\nstudy, we conduct an in-depth analysis of open-source AIOps projects to\nunderstand the characteristics of AIOps in practice. We first carefully\nidentify a set of AIOps projects from GitHub and analyze their repository\nmetrics (e.g., the used programming languages). Then, we qualitatively examine\nthe projects to understand their input data, analysis techniques, and goals.\nFinally, we assess the quality of these projects using different quality\nmetrics, such as the number of bugs. To provide context, we also sample two\nsets of baseline projects from GitHub: a random sample of machine learning\nprojects and a random sample of general-purposed projects. By comparing\ndifferent metrics between our identified AIOps projects and these baselines, we\nderive meaningful insights. Our results reveal a recent and growing interest in\nAIOps solutions. However, the quality metrics indicate that AIOps projects\nsuffer from more issues than our baseline projects. We also pinpoint the most\ncommon issues in AIOps approaches and discuss potential solutions to address\nthese challenges. Our findings offer valuable guidance to researchers and\npractitioners, enabling them to comprehend the current state of AIOps practices\nand shed light on different ways of improving AIOps' weaker aspects. To the\nbest of our knowledge, this work marks the first attempt to characterize\nopen-source AIOps projects.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.13179,review,pre_llm,2022,12,"{'ai_likelihood': 5.9604644775390625e-06, 'text': 'Mining Architectural Information: A Systematic Mapping Study\n\n  Mining Software Repositories (MSR) has become an essential activity in\nsoftware development. Mining architectural information to support architecting\nactivities, such as architecture understanding, has received significant\nattention in recent years. However, there is a lack of clarity on what\nliterature on mining architectural information is available. Consequently, this\nmay create difficulty for practitioners to understand and adopt the\nstate-of-the-art research results, such as what approaches should be adopted to\nmine what architectural information in order to support architecting\nactivities. It also hinders researchers from being aware of the challenges and\nremedies for the identified research gaps. We aim to identify, analyze, and\nsynthesize the literature on mining architectural information in terms of\narchitectural information and sources mined, architecting activities supported,\napproaches and tools used, and challenges faced. An SMS has been conducted on\nthe literature published between January 2006 and December 2022. Of the 104\nprimary studies selected, 7 categories of architectural information have been\nmined, among which architectural description is the most mined architectural\ninformation; 11 categories of sources have been leveraged for mining\narchitectural information, among which version control system is the most\npopular source; 11 architecting activities can be supported by the mined\narchitectural information, among which architecture understanding is the most\nsupported activity; 95 approaches and 56 tools were proposed and employed in\nmining architectural information; and 4 types of challenges in mining\narchitectural information were identified. This SMS provides researchers with\nfuture directions and help practitioners be aware of what approaches and tools\ncan be used to mine what architectural information from what sources to support\nvarious architecting activities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.01011,regular,pre_llm,2022,12,"{'ai_likelihood': 1.8775463104248047e-05, 'text': 'CLeBPI: Contrastive Learning for Bug Priority Inference\n\n  Automated bug priority inference can reduce the time overhead of bug triagers\nfor priority assignments, improving the efficiency of software maintenance.\nCurrently, there are two orthogonal lines for this task, i.e., traditional\nmachine learning based (TML-based) and neural network based (NN-based)\napproaches. Although these approaches achieve competitive performance, our\nobservation finds that existing approaches face the following two issues: 1)\nTML-based approaches require much manual feature engineering and cannot learn\nthe semantic information of bug reports; 2) Both TML-based and NN-based\napproaches cannot effectively address the label imbalance problem because they\nare difficult to distinguish the semantic difference between bug reports with\ndifferent priorities. In this paper, we propose CLeBPI (Contrastive Learning\nfor Bug Priority Inference), which leverages pre-trained language model and\ncontrastive learning to tackle the above-mentioned two issues. Specifically,\nCLeBPI is first pre-trained on a large-scale bug report corpus in a\nself-supervised way, thus it can automatically learn contextual representations\nof bug reports without manual feature engineering. Afterward, it is further\npre-trained by a contrastive learning objective, which enables it to\ndistinguish semantic differences between bug reports, learning more precise\ncontextual representations for each bug report. When finishing pre-training, we\ncan connect a classification layer to CLeBPI and fine-tune it for bug priority\ninference in a supervised way. To verify the effectiveness of CLeBPI, we choose\nfour baseline approaches and conduct comparison experiments on a public\ndataset. The experimental results show that CLeBPI outperforms all baseline\napproaches by 23.86%-77.80% in terms of weighted average F1-score, showing its\neffectiveness.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.05537,regular,pre_llm,2022,12,"{'ai_likelihood': 3.132555219862196e-05, 'text': ""Technical Debt Management in OSS Projects: An Empirical Study on GitHub\n\n  Technical debt (TD) refers to delayed tasks and immature artifacts that may\nbring short-term benefits but incur extra costs of change during maintenance\nand evolution in the long term. TD has been extensively studied in the past\ndecade, and numerous open source software (OSS) projects were used to explore\nspecific aspects of TD and validate various approaches for TD management (TDM).\nHowever, there still lacks a comprehensive understanding on the practice of TDM\nin OSS development, which penetrates the OSS community's perception of the TD\nconcept and how TD is managed in OSS development. To this end, we conducted an\nempirical study on the whole GitHub to explore the adoption and execution of\nTDM based on issues in OSS projects. We collected 35,278 issues labeled as TD\n(TD issues) distributed over 3,598 repositories in total from the issue\ntracking system of GitHub between 2009 and 2020. The findings are that: (1) the\nOSS community is embracing the TD concept; (2) the analysis of TD instances\nshows that TD may affect both internal and external quality of software\nsystems; (3) only one TD issue was identified in 31.1% of the repositories and\nall TD issues were identified by only one developer in 69.0% of the\nrepositories; (4) TDM was ignored in 27.3% of the repositories after TD issues\nwere identified; and (5) among the repositories with TD labels, 32.9% have\nabandoned TDM while only 8.2% adopt TDM as a consistent practice. These\nfindings provide valuable insights for practitioners in TDM and promising\nresearch directions for further investigation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.14274,regular,pre_llm,2022,12,"{'ai_likelihood': 0.006162855360243056, 'text': 'Meta-Path Based Attentional Graph Learning Model for Vulnerability\n  Detection\n\n  In recent years, deep learning (DL)-based methods have been widely used in\ncode vulnerability detection. The DL-based methods typically extract structural\ninformation from source code, e.g., code structure graph, and adopt neural\nnetworks such as Graph Neural Networks (GNNs) to learn the graph\nrepresentations. However, these methods fail to consider the heterogeneous\nrelations in the code structure graph, i.e., the heterogeneous relations mean\nthat the different types of edges connect different types of nodes in the\ngraph, which may obstruct the graph representation learning. Besides, these\nmethods are limited in capturing long-range dependencies due to the deep levels\nin the code structure graph. In this paper, we propose a Meta-path based\nAttentional Graph learning model for code vulNErability deTection, called\nMAGNET. MAGNET constructs a multi-granularity meta-path graph for each code\nsnippet, in which the heterogeneous relations are denoted as meta-paths to\nrepresent the structural information. A meta-path based hierarchical\nattentional graph neural network is also proposed to capture the relations\nbetween distant nodes in the graph. We evaluate MAGNET on three public datasets\nand the results show that MAGNET outperforms the best baseline method in terms\nof F1 score by 6.32%, 21.50%, and 25.40%, respectively. MAGNET also achieves\nthe best performance among all the baseline methods in detecting Top-25 most\ndangerous Common Weakness Enumerations (CWEs), further demonstrating its\neffectiveness in vulnerability detection.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.11629,regular,pre_llm,2022,12,"{'ai_likelihood': 3.6093923780653213e-06, 'text': 'Graph-Based Specification and Automated Construction of ILP Problems\n\n  In the Model-Driven Software Engineering (MDSE) community, the combination of\ntechniques operating on graph-based models (e.g., Pattern Matching (PM) and\nGraph Transformation (GT)) and Integer Linear Programming (ILP) is a common\noccurrence, since ILP solvers offer a powerful approach to solve linear\noptimization problems and help to enforce global constraints while delivering\noptimal solutions. However, designing and specifying complex optimization\nproblems from more abstract problem descriptions can be a challenging task. A\ndesigner must be an expert in the specific problem domain as well as the ILP\noptimization domain to translate the given problem into a valid ILP problem.\nTypically, domain-specific ILP problem generators are hand-crafted by experts,\nto avoid specifying a new ILP problem by hand for each new instance of a\nproblem domain. Unfortunately, the task of writing ILP problem generators is an\nexercise, which has to be repeated for each new scenario, tool, and approach.\nFor this purpose, we introduce the GIPS (Graph-Based ILP Problem Specification\nTool) framework that simplifies the development of ILP problem generators for\ngraph-based optimization problems and a new Domain-Specific Language (DSL)\ncalled GIPSL (Graph-Based ILP Problem Specification Language) that integrates\nGT and ILP problems on an abstract level. Our approach uses GIPSL\nspecifications as a starting point to derive ILP problem generators for a\nspecific application domain automatically. First experiments show that the\nderived ILP problem generators can compete with hand-crafted programs developed\nby ILP experts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.03399,regular,pre_llm,2022,12,"{'ai_likelihood': 1.294745339287652e-05, 'text': ""Utilizing Source Code Syntax Patterns to Detect Bug Inducing Commits\n  using Machine Learning Models\n\n  Detecting Bug Inducing Commit (BIC) or Just in Time (JIT) defect prediction\nusing Machine Learning (ML) based models requires tabulated feature values\nextracted from the source code or historical maintenance data of a software\nsystem. Existing studies have utilized meta-data from source code repositories\n(we named them GitHub Statistics or GS), n-gram-based source code text\nprocessing, and developer's information (e.g., the experience of a developer)\nas the feature values in ML-based bug detection models. However, these feature\nvalues do not represent the source code syntax styles or patterns that a\ndeveloper might prefer over available valid alternatives provided by\nprogramming languages. This investigation proposed a method to extract features\nfrom its source code syntax patterns to represent software commits and\ninvestigate whether they are helpful in detecting bug proneness in software\nsystems. We utilize six manually and two automatically labeled datasets from\neight open-source software projects written in Java, C++, and Python\nprogramming languages. Our datasets contain 642 manually labeled and 4,014\nautomatically labeled buggy and non-buggy commits from six and two subject\nsystems, respectively. The subject systems contain a diverse number of\nrevisions, and they are from various application domains. Our investigation\nshows the inclusion of the proposed features increases the performance of\ndetecting buggy and non-buggy software commits using five different machine\nlearning classification models. Our proposed features also perform better in\ndetecting buggy commits using the Deep Belief Network generated features and\nclassification model. This investigation also implemented a state-of-the-art\ntool to compare the explainability of predicted buggy commits using our\nproposed and traditional features and found that our proposed features provide\nbetter reasoning about buggy.....\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.00548,regular,pre_llm,2022,12,"{'ai_likelihood': 9.238719940185547e-06, 'text': 'Duplicate Bug Report Detection: How Far Are We?\n\n  Many Duplicate Bug Report Detection (DBRD) techniques have been proposed in\nthe research literature. The industry uses some other techniques.\nUnfortunately, there is insufficient comparison among them, and it is unclear\nhow far we have been. This work fills this gap by comparing the aforementioned\ntechniques. To compare them, we first need a benchmark that can estimate how a\ntool would perform if applied in a realistic setting today. Thus, we first\ninvestigated potential biases that affect the fair comparison of the accuracy\nof DBRD techniques. Our experiments suggest that data age and issue tracking\nsystem choice cause a significant difference. Based on these findings, we\nprepared a new benchmark. We then used it to evaluate DBRD techniques to\nestimate better how far we have been. Surprisingly, a simpler technique\noutperforms recently proposed sophisticated techniques on most projects in our\nbenchmark. In addition, we compared the DBRD techniques proposed in research\nwith those used in Mozilla and VSCode. Surprisingly, we observe that a simple\ntechnique already adopted in practice can achieve comparable results as a\nrecently proposed research tool. Our study gives reflections on the current\nstate of DBRD, and we share our insights to benefit future DBRD research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.0596,regular,pre_llm,2022,12,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Industrially Applicable System Regression Test Prioritization in\n  Production Automation\n\n  When changes are performed on an automated production system (aPS), new\nfaults can be accidentally introduced in the system, which are called\nregressions. A common method for finding these faults is regression testing. In\nmost cases, this regression testing process is performed under high time\npressure and on-site in a very uncomfortable environment. Until now, there is\nno automated support for finding and prioritizing system test cases regarding\nthe fully integrated aPS that are suitable for finding regressions. Thus, the\ntesting technician has to rely on personal intuition and experience, possibly\nchoosing an inappropriate order of test cases, finding regressions at a very\nlate stage of the test run. Using a suitable prioritization, this iterative\nprocess of finding and fixing regressions can be streamlined and a lot of time\ncan be saved by executing test cases likely to identify new regressions\nearlier. Thus, an approach is presented in this paper that uses previously\nacquired runtime data from past test executions and performs a change\nidentification and impact analysis to prioritize test cases that have a high\nprobability to unveil regressions caused by side effects of a system change.\nThe approach was developed in cooperation with reputable industrial partners\nactive in the field of aPS engineering, ensuring a development in line with\nindustrial requirements. An industrial case study and an expert evaluation were\nperformed, showing promising results.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.13773,regular,pre_llm,2022,12,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'A Bayesian Framework for Automated Debugging\n\n  Debugging takes up a significant portion of developer time. As a result,\nautomated debugging techniques including Fault Localization (FL) and Automated\nProgram Repair (APR) have garnered significant attention due to their potential\nto aid developers in debugging tasks. Despite intensive research on these\nsubjects, we are unaware of a theoretic framework that highlights the\nprinciples behind automated debugging and allows abstract analysis of\ntechniques. Such a framework would heighten our understanding of the endeavor\nand provide a way to formally analyze techniques and approaches. To this end,\nwe first propose a Bayesian framework of understanding automated repair and\nfind that in conjunction with a concrete statement of the objective of\nautomated debugging, we can recover maximal fault localization formulae from\nprior work, as well as analyze existing APR techniques and their underlying\nassumptions.\n  As a means of empirically demonstrating our framework, we further propose\nBAPP, a Bayesian Patch Prioritization technique that incorporates intermediate\nprogram values to analyze likely patch locations and repair actions, with its\ncore equations being derived by our Bayesian framework. We find that\nincorporating program values allows BAPP to identify correct patches more\nprecisely: when applied to the patches generated by kPAR, the rankings produced\nby BAPP reduce the number of required patch validation by 68% and consequently\nreduce the repair time by 34 minutes on average. Further, BAPP improves the\nprecision of FL, increasing acc@5 on the studied bugs from 8 to 11. These\nresults highlight the potential of value-cognizant automated debugging\ntechniques, and further validates our theoretical framework. Finally, future\ndirections that the framework suggests are provided.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.11077,regular,pre_llm,2022,12,"{'ai_likelihood': 2.4504131740993925e-06, 'text': 'Augmenting Diffs With Runtime Information\n\n  Source code diffs are used on a daily basis as part of code review,\ninspection, and auditing. To facilitate understanding, they are typically\naccompanied by explanations that describe the essence of what is changed in the\nprogram. As manually crafting high-quality explanations is a cumbersome task,\nresearchers have proposed automatic techniques to generate code diff\nexplanations. Existing explanation generation methods solely focus on static\nanalysis, i.e., they do not take advantage of runtime information to explain\ncode changes. In this paper, we propose Collector-Sahab, a novel tool that\naugments code diffs with runtime difference information. Collector-Sahab\ncompares the program states of the original (old) and patched (new) versions of\na program to find unique variable values. Then, Collector-Sahab adds this novel\nruntime information to the source code diff as shown, for instance, in code\nreviewing systems. As an evaluation, we run Collector-Sahab on 584 code diffs\nfor Defects4J bugs and find it successfully augments the code diff for 95%\n(555/584) of them. We also perform a user study and ask eight participants to\nscore the augmented code diffs generated by Collector-Sahab. Per this user\nstudy, we conclude that developers find the idea of adding runtime data to code\ndiffs promising and useful. Overall, our experiments show the effectiveness and\nusefulness of Collector-Sahab in augmenting code diffs with runtime difference\ninformation. Publicly-available repository:\nhttps://github.com/ASSERT-KTH/collector-sahab.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.04769,regular,pre_llm,2022,12,"{'ai_likelihood': 1.7219119601779514e-06, 'text': 'Machine Learning-based Test Selection for Simulation-based Testing of\n  Self-driving Cars Software\n\n  Simulation platforms facilitate the development of emerging Cyber-Physical\nSystems (CPS) like self-driving cars (SDC) because they are more efficient and\nless dangerous than field operational test cases. Despite this, thoroughly\ntesting SDCs in simulated environments remains challenging because SDCs must be\ntested in a sheer amount of long-running test cases. Past results on software\ntesting optimization have shown that not all the test cases contribute equally\nto establishing confidence in test subjects\' quality and reliability, and the\nexecution of ""safe and uninformative"" test cases can be skipped to reduce\ntesting effort. However, this problem is only partially addressed in the\ncontext of SDC simulation platforms. In this paper, we investigate test\nselection strategies to increase the cost-effectiveness of simulation-based\ntesting in the context of SDCs. We propose an approach called SDC-Scissor (SDC\ncoSt-effeCtIve teSt SelectOR) that leverages Machine Learning (ML) strategies\nto identify and skip test cases that are unlikely to detect faults in SDCs\nbefore executing them.\n  Our evaluation shows that SDC-Scissor outperforms the baselines. With the\nLogistic model, we achieve an accuracy of 70%, a precision of 65%, and a recall\nof 80% in selecting tests leading to a fault and improved testing\ncost-effectiveness. Specifically, SDC-Scissor avoided the execution of 50% of\nunnecessary tests as well as outperformed two baseline strategies.\nComplementary to existing work, we also integrated SDC-Scissor into the context\nof an industrial organization in the automotive domain to demonstrate how it\ncan be used in industrial settings.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.04732,regular,pre_llm,2022,12,"{'ai_likelihood': 5.132622188991971e-06, 'text': 'Fill in the Blank: Context-aware Automated Text Input Generation for\n  Mobile GUI Testing\n\n  Automated GUI testing is widely used to help ensure the quality of mobile\napps. However, many GUIs require appropriate text inputs to proceed to the next\npage which remains a prominent obstacle for testing coverage. Considering the\ndiversity and semantic requirement of valid inputs (e.g., flight departure,\nmovie name), it is challenging to automate the text input generation. Inspired\nby the fact that the pre-trained Large Language Model (LLM) has made\noutstanding progress in text generation, we propose an approach named QTypist\nbased on LLM for intelligently generating semantic input text according to the\nGUI context. To boost the performance of LLM in the mobile testing scenario, we\ndevelop a prompt-based data construction and tuning method which automatically\nextracts the prompts and answers for model tuning. We evaluate QTypist on 106\napps from Google Play and the result shows that the passing rate of QTypist is\n87%, which is 93% higher than the best baseline. We also integrate QTypist with\nthe automated GUI testing tools and it can cover 42% more app activities, 52%\nmore pages, and subsequently help reveal 122% more bugs compared with the raw\ntool.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
